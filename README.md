# ChatGPT Discriminator

Developed by Farran Regan and Anand Advani  
Brown University Department of Computer Science  
CSCI 1470 Spring 2022; taught by Dr. Ritambhara Singh

## Abstract
As natural language models improve in their ability to replicate human language, it becomes increasing difficult to distinguish between human-written and artificially generated writing. In particular, the release of ChatGPT, a highly advanced, publically accessible language model delivered in the form of a chatbot, has raised significant conerns about the inability of humans to identify AI-generated writing. As such, it becomes necessary to develop new deep learning-based models that can classify text as human written or model generated. We consider the task of discriminating between research paper abstracts that have been written by the original paper authors and abstracts that have been generated by ChatGPT, and trained a model to predict on this task. By using a pretrained large language model (LLM), distilBert, and building a classifier on top of the LLM's outputted embeddings, we were able to achieve high accuracy when discriminating between human-written abstracts and those that have been either rephrased by ChatGPT or generated from scratch by ChatGPT. Although this project is limited in scope, we demonstrate that it is possible to identify artifically generated text by using an adversarially trained deep learning model.

## Introduction
AI has become a topic of increasing public awareness in the past several months, particularly with the release of advanced chatbots such as ChatGPT. One significant concern that arose when ChatGPT was released was plagiarism. Individuals can use ChatGPT to generate writing that they then misrepresent that as their own. This is especially a concern in academic spaces, where authenticity of one’s work is vital. Natural language models are already advanced enough that humans cannot accurately differentiate between human-written and AI-generated text and will only continue to improve, so it is vital to simultaneously develop tools that can discriminate between the two.  
More generally, the fact that “detecting human- or machine-generated text” is a problem worth solving represents the growing uncertainty about what makes humans special, because one of the greatest hallmarks of our uniqueness is our command of language for effective communication, and the fantastic variety in its style to convey different ideas and emotions. Irrespective of conceptual content, even the possibility that a DNN can capture that stylistic variation calls into question a long-celebrated component of human ingenuity.  
In this project, we attempt to build a supervised classifier to distinguish between text authored by humans and text authored by ChatGPT, based on the 2022 version of the paper preprint [“ChatGPT or Human? Detect and Explain”](https://arxiv.org/abs/2301.13852) by Sandra Mitrović, Davide Andreoletti, and Omran Ayoub. The paper consists of three experiments: 1) fine-tuning a modification of BERT (a large pre-trained transformer model) for the classification task, 2) qualitatively interpreting the kinds of words that model focuses on, and thus what the main differences between human and ChatGPT writing are, and 3) evaluating a simple classifier that assumes GPT-2 will produce closer output to ChatGPT than to human writing. The paper uses data from restaurant reviews, and asks ChatGPT to write its own restaurant reviews, but we believe that academic writing is more relevant to issues of plagiarism and poses a more difficult challenge. For example, many restaurant reviews contain uncapitalized sentences, nonstandard punctuation, and an abundance of emojis and misspellings, which are proxy variables that a model might hone in on instead of capturing the more fundamental difference in vocabulary and style between human and ChatGPT writing. Thus, rather than restaurant reviews, we will use research paper abstracts. We will focus on the first and second experiments, and build our own model for classifying research paper abstracts. 

## Usage
See [USAGE.md](USAGE.md)

## Methodology

### Data
For human-generated abstracts, we utilized [this kaggle dataset](https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts), which contains roughly 39,000 abstracts pulled from arxiv.org. The abstracts are from the most recently uploaded papers that contain a selected list of query terms related to deep learning, computer vision, and other technical topics (see [here](https://github.com/soumik12345/multi-label-text-classification/blob/master/beam_arxiv_scrape.ipynb) for the complete data collection method). 

For ChatGPT-generated abstracts, we constructed two datasets. First, we prompted ChatGPT to rephrase each of the human-written abstracts. Specifically, our prompt was “Rephrase the following paragraph: \<human-written abstract\>”. We ran out of free API credits while gathering this dataset, so our second dataset was constructed using [hugchat](https://github.com/Soulter/hugging-chat-api) rather than the official ChatGPT. Hugchat is a free, open source alternative to ChatGPT released by HuggingFace. For the remainder of this paper, we will refer to text generated by hugchat as written by ChatGPT, although this is only an unofficial version of ChatGPT. For the second dataset, we prompted hugchat to generate an abstract based only on the title of each paper, rather than rephrasing the human abstract. Specifically, our prompt was “Write an abstract around 150 to 250 words long for a paper titled \<paper title\>. Do not include the paper title in the abstract. Do not start with the word this.”

During data generation, if any ChatGPT- or human-generated abstract was shorter than 200 characters or longer than 3500 characters, it was discarded. Similarly, if we asked ChatGPT to rephrase an abstract and the rephrased version was identical to the original, both the original and rephrased version were discarded. As a result, our first dataset consisted of 78954 abstracts (39477 human, 39477 rephrased by ChatGPT) and our second dataset consisted of 78355 abstracts (39334 human, 39021 generated from titles by ChatGPT). 

### Architecture

For our model, we built a classifier on top of a pre-trained large language model that encoded each abstract. We first used a tokenizer to tokenize each abstract, breaking it down into understandable and easy-to-parse word chunks. We then used an attention mask to mask unimportant tokens, ie padding tokens, so that these were not taken into account when making predictions. Then, we passed the tokenized and masked abstracts into a [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) model, a pre-trained transformer that produced encodings for each abstract. Then, we passed those encodings into our classifier model, which consisted of three dense layers, two leaky relu activation layers in between the dense layers, and a sigmoid activation output layer. The output was a tensor with the shape `(2,)` that represented the probability of the abstract being written by a human and the probability of it being written by ChatGPT.


### Evaluation Metric
To evaluate our model quantitatively, we will use accuracy, representing the proportion of test abstracts that our model correctly classified as either human-written or ChatGPT-generated. Our original base, target, and stretch goals for accuracy were >50%, 60%, and 69%, respectively. 

### Training
Initially, we planned to train our model by locking distilBert's weights and only training the classifier for the first several epochs, then unlocking distilBert and fine tuning the model as we continued training the classifier. However, due to limited memory resources, we were unable to fine tune distilBert, so our model used the deafult distilBert weights and only trained the classifier. Each model was trained for 10 epochs using the Adam optimizer with a learning rate of 0.001 and binary cross entropy loss.

## Results

| Dataset | Accuracy |
| ----- | -------- |
| Rephrased human abstracts | 72% |
| Generated from titles | 98% |


## Limitations

This model was trained on a very specific set of deep learning research paper abstracts, and a very specific set of ChatGPT-generated texts based on a single type of prompt. It has not been evaluated at distinguishing between human-written and AI-generated text in any other contexts, and should not be generalized beyond the task it was trained for. Further, no deep learning model can achieve 100% accuracy, and there will always be false positives and false negatives in its predictions. Although this model is useful for exploratory and investigative purposes, it should not be treated as a source of absolute truth and its predictions should be taken as predictions, not facts.




