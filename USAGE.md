# ChatGPT Discriminator

## Available models
There are two trained models available: `rephrased` and `from-titles`. The `rephrased` model was trained for 10 epochs as outlined in the Methods section of the [README](README.md) on human-written abstracts and abstracts rephrased by ChatGPT. The `from-titles` model was similarly trained for 10 epochs on human written abstracts and abstracts generated by ChatGPT from the title of the associated paper. Unfortunately, due to GitHub file size limits these models are not avilable on this repository. However, you can download the files from google drive here:  
[`rephrased`](https://drive.google.com/drive/folders/1f-xROHmFpxZ_R1sSG_vd5c-CAJ4klwGK?usp=sharing)  
[`from-titles`](https://drive.google.com/drive/folders/1hTN8euCAzencZATCMZkPRscFglFTsXcR?usp=sharing)  
If you have any difficulties accessing or running the models, please reach out to me at farran_regan@brown.edu.


## Running the model
To run the model, install the required dependencies fonud in `dependencies.yml`, then run `python model.py`  
<br>
With no arguments set, the default behavior when running `model.py` is to start training the model for 10 epochs on human written absrtacts and those rephrased by ChatGPT. To train with different options or to test the model instead, see the following optional arguments:  

`--save_weights`: takes a string representing the name you want to give the model you train, which will be the name of the folder it is saved in in the `model_checkpts` folder  
`--load_weights`: takes a string representing the name of the model (in `model_checkpts`) that you want to load and use for training/testing  
`--test`: will test the model instead of training; takes a string representing the name of the model (in `model_checkpts`) that you want to test (this uses our testing data, so remember to set the `--from_titles` flag if you want to test on that data rather than the rephrased data)  
`--predict_gui`: will allow the user to provide an input, runs the model on that input and prints the outputted prediction (indicate which model you wish to use with the `--load_weights` argument)  
`--bert`: indicates the model should use BERT instead of distilBERT (note this will use a significant amount of time and memory)  
`--from_titles`: indicates you want to use the data generated from paper titles rather than rephrased from human-written abstracts  
`--batch_size`: takes an int of the batch size you wish to use; defaults to 256  
`--num_epochs`: takes an int of the number of epochs you want to train the model for; defaults to 10  
`--percent_data`: takes a float of the proportion of data you wish to use for training and testing; defaults to 1  
`--max_num_tokens`: takes an int of the max number of tokens to use when tokenizing inputs; defaults to 512  
<br>
<details>
<summary>Help! I'm getting a zero division error</summary>
<br>
Most likely, you are using a small percentage of data and a large batch size, such that your batch size is larger than the available data. If you don't have enough data for one full batch (in either the training or testing set), you will get a zero division error. To get around this, when you use a small percentage of data, lower the batch size correspondingly.
</details>




