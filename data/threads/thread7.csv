"Many animals and humans process the visual field with a varying spatial resolution (foveated vision) and use peripheral processing to make eye movements and point the fovea to acquire high-resolution information about objects of interest. This architecture results in computationally efficient rapid scene exploration. Recent progress in vision Transformers has brought about new alternatives to the traditionally convolution-reliant computer vision systems. However, these models do not explicitly model the foveated properties of the visual system nor the interaction between eye movements and the classification task. We propose foveated Transformer (FoveaTer) model, which uses pooling regions and saccadic movements to perform object classification tasks using a vision Transformer architecture. Our proposed model pools the image features using squared pooling regions, an approximation to the biologically-inspired foveated architecture, and uses the pooled features as an input to a Transformer Network. It decides on the following fixation location based on the attention assigned by the Transformer to various locations from previous and present fixations. The model uses a confidence threshold to stop scene exploration, allowing to dynamically allocate more fixation/computational resources to more challenging images. We construct an ensemble model using our proposed model and unfoveated model, achieving an accuracy 1.36% below the unfoveated model with 22% computational savings. Finally, we demonstrate our model's robustness against adversarial attacks, where it outperforms the unfoveated model.",0
"Foveation transformers have been shown to improve performance over non-foveated models at equivalent computational cost on some image classification tasks. This work demonstrates that a simple foveation transformation applied post hoc can enhance performance across most architectures with negligible added costs, which could enable more computationally efficient deployment in cloud computing. Our approach uses a pretrained convolutional neural network model from the PyTorch Image Classification Challenge as a fixed feature extractor. It then applies the ""Foveatized"" operator - a simple elementwise multiplication by a learned foveating map - after passing through the last convolutional layer but before feeding into a linear classifier. We test our method using ResNet and DenseNet base models with five different backbones ranging in depth (96 layers) to width (48). On average we see up to +2% top-1 accuracy increase compared against strong baseline models without changing any parameters except adding an elementwise multiply and learnable weights. Our approach works better than the previously published state of the art methods such as SEAMless, MSDiffused, and SEMFIB across all tested models by large margins (up to +4%) while reducing computations. In summary, we present FoveaTer: a novel approach for improving image classification accuracies via Foveated Transformers, making use of existing models and data without retraining them. As there seems little scope for improvement based solely on architecture alone nowadays, our method would be beneficial for those looking to deploy large scale image classification systems cheaply and efficiently.",1
"In this paper, we study the task of source-free domain adaptation (SFDA), where the source data are not available during target adaptation. Previous works on SFDA mainly focus on aligning the cross-domain distributions. However, they ignore the generalization ability of the pretrained source model, which largely influences the initial target outputs that are vital to the target adaptation stage. To address this, we make the interesting observation that the model accuracy is highly correlated with whether or not attention is focused on the objects in an image. To this end, we propose a generic and effective framework based on Transformer, named TransDA, for learning a generalized model for SFDA. Specifically, we apply the Transformer as the attention module and inject it into a convolutional network. By doing so, the model is encouraged to turn attention towards the object regions, which can effectively improve the model's generalization ability on the target domains. Moreover, a novel self-supervised knowledge distillation approach is proposed to adapt the Transformer with target pseudo-labels, thus further encouraging the network to focus on the object regions. Experiments on three domain adaptation tasks, including closed-set, partial-set, and open-set adaption, demonstrate that TransDA can greatly improve the adaptation accuracy and produce state-of-the-art results. The source code and trained models are available at https://github.com/ygjwd12345/TransDA.",0
"Abstract: In recent years, domain adaptation has been an active area of research due to increasing applications across different fields such as computer vision, natural language processing (NLP), and speech recognition. One popular approach to tackle domain adaptation problems is through using deep neural networks, specifically architectures like convolutional neural networks (CNNs) or transformers. However, most existing methods still rely on labeled source data during training, which can limit their generalization ability in real-world scenarios where annotated labels may not always be available. To address this challenge, we propose a novel framework that leverages transformer architecture without any explicit access to the source domain data during adaptation. Our method adapts the features learned by the target model to match those from the source domain by minimizing the discrepancy between them, while ensuring robustness against outliers present in the target data. We evaluate our proposed algorithm on several benchmark datasets including digit classification, object detection, and sentiment analysis tasks, demonstrating state-of-the-art performance compared to other approaches relying solely on unlabeled target data. This work advances the field of source-free domain adaptation, opening up new possibilities for enabling more flexible and reliable machine learning models in a wide range of domains.",1
"This article presents a model for traffic incident prediction. Specifically, we address the fundamental problem of data scarcity in road traffic accident prediction by training our model on emergency braking events instead of accidents. Based on relevant risk factors for traffic accidents and corresponding data categories, we evaluate different options for preprocessing sparse data and different Machine Learning models. Furthermore, we present a prototype implementing a traffic incident prediction model for Germany based on emergency braking data from Mercedes-Benz vehicles as well as weather, traffic and road data, respectively. After model evaluation and optimisation, we found that a Random Forest model trained on artificially balanced (under-sampled) data provided the highest classification accuracy of 85% on the original imbalanced data. Finally, we present our conclusions and discuss further work; from gathering more data over a longer period of time to build stronger classification systems, to addition of internal factors such as the driver's visual and cognitive attention.",0
"Traffic incidents like accidents or breakdowns often cause congestion on roads by creating bottlenecks that slow down or even stop the flow of vehicles. Predicting these events could therefore greatly benefit road operators in terms of managing their networkâ€™s capacity. To achieve such predictions, we propose hereafter a novel methodology based on emergency brake logs recorded directly from vehicles equipped with sensors and GPS geo-localization. This database was provided to us through a partnership agreement with a major car manufacturer, which allows access to millions of trips. We have applied our methodology firstly on a reduced dataset covering only one city over several months in order to fine tune the parameters of our algorithm before testing it on the larger available set including multiple cities at once. After comparing numerous regression models for their performance, XGBoost showed to provide the most accurate results both qualitatively on ROC curves as well as quantitatively on the MAPE (Mean Absolute Percentage Error) and RMSE (Root Mean Square Error). Our approach leads to predictions with good accuracy for upcoming hour periods. In addition, we explore other variables such as weather conditions but they seem not to bring significant improvement in predictive power. Although emergencies represent only a small part of all incidents, further studies need to be conducted on additional sources of information or machine learning techniques to capture better correlations within more general events. Nevertheless, emergency breaking datasets offer promising perspectives for nowcasting traffic incidents. Finally, the integration of live feeds into our system would allow real time forecast updates as new records become available, potentially providing valuable feedback for road management stakeholde",1
"A number of problems in the processing of sound and natural language, as well as in other areas, can be reduced to simultaneously reading an input sequence and writing an output sequence of generally different length. There are well developed methods that produce the output sequence based on the entirely known input. However, efficient methods that enable such transformations on-line do not exist. In this paper we introduce an architecture that learns with reinforcement to make decisions about whether to read a token or write another token. This architecture is able to transform potentially infinite sequences on-line. In an experimental study we compare it with state-of-the-art methods for neural machine translation. While it produces slightly worse translations than Transformer, it outperforms the autoencoder with attention, even though our architecture translates texts on-line thereby solving a more difficult problem than both reference methods.",0
"In this paper, we propose a novel reinforcement learning approach for online sequence transformation tasks. We frame the problem as a Markov decision process (MDP) and train our agent using deep Q-learning. Our method learns directly from raw input data, rather than relying on any hand-engineered features or domain knowledge. Experimental results show that our algorithm outperforms several state-of-the-art baselines across a range of benchmark datasets, demonstrating its effectiveness at transforming sequences under uncertainty. Furthermore, our ablation studies provide insights into the key components of our model and their impact on performance. Overall, these findings highlight the potential of RL for solving real-world sequence transformation problems.",1
"Ride-hailing demand prediction is an essential task in spatial-temporal data mining. Accurate Ride-hailing demand prediction can help to pre-allocate resources, improve vehicle utilization and user experiences. Graph Convolutional Networks (GCN) is commonly used to model the complicated irregular non-Euclidean spatial correlations. However, existing GCN-based ride-hailing demand prediction methods only assign the same importance to different neighbor regions, and maintain a fixed graph structure with static spatial relationships throughout the timeline when extracting the irregular non-Euclidean spatial correlations. In this paper, we propose the Spatial-Temporal Dynamic Graph Attention Network (STDGAT), a novel ride-hailing demand prediction method. Based on the attention mechanism of GAT, STDGAT extracts different pair-wise correlations to achieve the adaptive importance allocation for different neighbor regions. Moreover, in STDGAT, we design a novel time-specific commuting-based graph attention mode to construct a dynamic graph structure for capturing the dynamic time-specific spatial relationships throughout the timeline. Extensive experiments are conducted on a real-world ride-hailing demand dataset, and the experimental results demonstrate the significant improvement of our method on three evaluation metrics RMSE, MAPE and MAE over state-of-the-art baselines.",0
"In recent years, ride-hailing services have become increasingly popular due to their convenience and efficiency. However, predicting demand for these services remains challenging due to fluctuations in factors such as traffic congestion, weather conditions, holidays, and special events. Accurate prediction of ride-hailing demand can enable operators to better allocate resources and improve customer satisfaction. This study proposes Spatial-Temporal Dynamic Graph Attention Network (SDGAN), which leverages graph convolutional networks (GCN) to model spatial relationships among different regions and time dependencies within each region. Our approach utilizes historical data from GPS records to learn spatial representation, temporal representation, and feature interaction between nodes in both spatial and temporal domains. Experiments on real datasets demonstrate that SDGAN outperforms state-of-the-art models by significant margins in terms of mean absolute error (MAE) and mean squared error (MSE). Furthermore, our analysis shows that SDGAN effectively captures global patterns and regional characteristics of ride-hailing demand. These results suggest that SDGAN has promising potential applications in other areas requiring spatio-temporal predictions.",1
"Borrowing from the transformer models that revolutionized the field of natural language processing, self-supervised feature learning for visual tasks has also seen state-of-the-art success using these extremely deep, isotropic networks. However, the typical AI researcher does not have the resources to evaluate, let alone train, a model with several billion parameters and quadratic self-attention activations. To facilitate further research, it is necessary to understand the features of these huge transformer models that can be adequately studied by the typical researcher. One interesting characteristic of these transformer models is that they remove most of the inductive biases present in classical convolutional networks. In this work, we analyze the effect of these and more inductive biases on small to moderately-sized isotropic networks used for unsupervised visual feature learning and show that their removal is not always ideal.",0
"In recent years, there has been significant progress in developing artificial intelligence (AI) systems that can perform complex tasks such as image recognition, natural language processing, and game playing. However, one aspect of these systems that remains poorly understood is their reliance on ""inductive biases."" An inductive bias is a prior belief or assumption that guides learning and inference in machine learning algorithms, helping them generalize well beyond the training data and make accurate predictions. Despite their importance, relatively little attention has been paid to the role of inductive biases in AI. This paper seeks to address this gap by examining how inductive biases shape our understanding of AI models, and why they should be treated as central rather than peripheral components of these systems. By highlighting the crucial role of inductive biases in shaping AI's capabilities and limitations, we hope to encourage further research into how to design and use these priors effectively, leading to more reliable and trustworthy AI applications.",1
"Using deep latent variable models in causal inference has attracted considerable interest recently, but an essential open question is their ability to yield consistent causal estimates. While they have demonstrated promising results and theory exists on some simple model formulations, we also know that causal effects are not even identifiable in general with latent variables. We investigate this gap between theory and empirical results with analytical considerations and extensive experiments under multiple synthetic and real-world data sets, using the causal effect variational autoencoder (CEVAE) as a case study. While CEVAE seems to work reliably under some simple scenarios, it does not estimate the causal effect correctly with a misspecified latent variable or a complex data distribution, as opposed to its original motivation. Hence, our results show that more attention should be paid to ensuring the correctness of causal estimates with deep latent variable models.",0
"This abstract presents a critical examination of deep latent variable models used for causal estimation, identifying key challenges faced by such models and discussing potential solutions. In particular, we focus on three major issues: inconsistent causal effect estimates due to nonlinear confounding bias; unstable estimated causal effects due to instability in model selection and optimization processes; and violation of causal assumptions caused by error propagation throughout the network architecture. To address these problems, we propose methods grounded in statistical theory that can improve the consistency and stability of causal estimations produced by deep latent variable models. We illustrate our approach using case studies from real-world applications and demonstrate how our proposals yield more accurate and reliable results. Ultimately, our work contributes to improved understanding of the limitations and capabilities of existing deep learning techniques for causality inference, opening doors for future development in this rapidly evolving field. Keywords: Deep Learning, Causal Effects, Confounding Bias, Optimization Instability, Error Propagation This paper provides an evaluation of the consistency of causal estimation with deep latent variable models (DLVM). These types of models face several challenges which lead to issues like biased causal effect estimates or instability when making predictions. The authors address these concerns through the introduction of new methodologies drawn from statistical theory. They apply their techniques to real world examples to verify their suitability and usefulness. Overall, this research offers valuable insight into the strengths and weaknesses of DLVM approaches for assessing cause-and-effect relationships. By doing so, it helps pave the path towards advancements in this area of study while simultaneously improving accuracy in data analysis.",1
"Camera pose estimation or camera relocalization is the centerpiece in numerous computer vision tasks such as visual odometry, structure from motion (SfM) and SLAM. In this paper we propose a neural network approach with a graph transformer backbone, namely TransCamP, to address the camera relocalization problem. In contrast with prior work where the pose regression is mainly guided by photometric consistency, TransCamP effectively fuses the image features, camera pose information and inter-frame relative camera motions into encoded graph attributes and is trained towards the graph consistency and accuracy instead, yielding significantly higher computational efficiency. By leveraging graph transformer layers with edge features and enabling tensorized adjacency matrix, TransCamP dynamically captures the global attention and thus endows the pose graph with evolving structures to achieve improved robustness and accuracy. In addition, optional temporal transformer layers actively enhance the spatiotemporal inter-frame relation for sequential inputs. Evaluation of the proposed network on various public benchmarks demonstrates that TransCamP outperforms state-of-the-art approaches.",0
"In order to write an effective abstract, I would need more information about your paper, such as the key contributions and findings. Please provide me with that information so I can craft an informative summary. The purpose of an abstract is to give readers a brief overview of your research without requiring them to read through every detail. It should capture their interest, highlight the main points of your work, and make clear how it relates to current literature on the subject. An abstract should never contain any new information compared to the full text. Rather, it provides succinct exposition for those who have already decided they may wish to read further. If you provide the necessary details, I will compose an appropriate abstract using language suitable to your field of study.",1
"Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Transformer Memory (HTM), which helps agents to remember the past in detail. HTM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HTM can therefore ""mentally time-travel"" -- remember past events in detail without attending to all intervening events. We show that agents with HTM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HTM can extrapolate to task sequences an order of magnitude longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HTM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.",0
"In recent years, there has been growing interest in developing artificial intelligence (AI) systems that can learn from past experiences and make decisions based on those experiences. One approach to achieving this goal is through the use of reinforcement learning (RL), which allows agents to learn by trial and error and receive rewards or penalties based on their actions. However, current RL algorithms have limitations when it comes to handling delayed rewards, uncertain outcomes, and long-term planning. To address these challenges, we propose a new approach called ""mental time travel"" (MTT), which involves developing a hierarchical memory system for RL agents. Our proposed MTT framework enables agents to access different levels of memories, including episodic memory, semantic memory, and procedural memory, and use them to guide decision making in complex environments. We evaluate our approach through simulations and real-world experiments, demonstrating its effectiveness in improving agent performance across several domains. Overall, our work represents an important step towards creating more intelligent and adaptive AI systems capable of navigating diverse and dynamic environments.",1
"Exploiting the relationships between attributes is a key challenge for improving multiple facial attribute recognition. In this work, we are concerned with two types of correlations that are spatial and non-spatial relationships. For the spatial correlation, we aggregate attributes with spatial similarity into a part-based group and then introduce a Group Attention Learning to generate the group attention and the part-based group feature. On the other hand, to discover the non-spatial relationship, we model a group-based Graph Correlation Learning to explore affinities of predefined part-based groups. We utilize such affinity information to control the communication between all groups and then refine the learned group features. Overall, we propose a unified network called Multi-scale Group and Graph Network. It incorporates these two newly proposed learning strategies and produces coarse-to-fine graph-based group features for improving facial attribute recognition. Comprehensive experiments demonstrate that our approach outperforms the state-of-the-art methods.",0
"This paper presents a novel approach to improving facial attribute recognition using group and graph learning techniques. The authors propose a two-stage system that first uses convolutional neural networks (CNNs) to extract features from face images, and then applies group and graph learning algorithms to identify attributes such as gender, age, and ethnicity. In the first stage, multiple CNN models are trained on different feature maps generated at each layer of the network, allowing for more diverse representations. In the second stage, a novel method called group sparse representation guided dictionary learning is used to construct dictionaries specific to certain attributes groups. Finally, a label propagation algorithm is applied to predict missing labels in the data. Experiments on three benchmark datasets demonstrate that the proposed method significantly outperforms state-of-the-art approaches in terms of accuracy and robustness. Overall, this work represents an important contribution to the field of computer vision and highlights the potential of group and graph learning techniques for enhancing facial attribute recognition systems.",1
"Semantic segmentation is an essential part of deep learning. In recent years, with the development of remote sensing big data, semantic segmentation has been increasingly used in remote sensing. Deep convolutional neural networks (DCNNs) face the challenge of feature fusion: very-high-resolution remote sensing image multisource data fusion can increase the network's learnable information, which is conducive to correctly classifying target objects by DCNNs; simultaneously, the fusion of high-level abstract features and low-level spatial features can improve the classification accuracy at the border between target objects. In this paper, we propose a multipath encoder structure to extract features of multipath inputs, a multipath attention-fused block module to fuse multipath features, and a refinement attention-fused block module to fuse high-level abstract features and low-level spatial features. Furthermore, we propose a novel convolutional neural network architecture, named attention-fused network (AFNet). Based on our AFNet, we achieve state-of-the-art performance with an overall accuracy of 91.7% and a mean F1 score of 90.96% on the ISPRS Vaihingen 2D dataset and an overall accuracy of 92.1% and a mean F1 score of 93.44% on the ISPRS Potsdam 2D dataset.",0
"This paper presents an attention-fused network (AFN) for semantic segmentation of very high resolution remote sensing imagery. AFN adaptively allocates computational resources by selectively attending to informative regions in input images. We evaluate AFN against state-of-the-art methods on two popular benchmarks: Potsdam dataset and RSO dataset, demonstrating consistent improvement in accuracy and efficiency. Our findings show that AFN significantly reduces the computation cost without compromising quality. Overall, we believe AFN provides a promising new direction towards real-time processing of very-high-resolution remote sensing imagery.",1
"Street scene change detection continues to capture researchers' interests in the computer vision community. It aims to identify the changed regions of the paired street-view images captured at different times. The state-of-the-art network based on the encoder-decoder architecture leverages the feature maps at the corresponding level between two channels to gain sufficient information of changes. Still, the efficiency of feature extraction, feature correlation calculation, even the whole network requires further improvement. This paper proposes the temporal attention and explores the impact of the dependency-scope size of temporal attention on the performance of change detection. In addition, based on the Temporal Attention Module (TAM), we introduce a more efficient and light-weight version - Dynamic Receptive Temporal Attention Module (DRTAM) and propose the Concurrent Horizontal and Vertical Attention (CHVA) to improve the accuracy of the network on specific challenging entities. On street scene datasets `GSV', `TSUNAMI' and `VL-CMU-CD', our approach gains excellent performance, establishing new state-of-the-art scores without bells and whistles, while maintaining high efficiency applicable in autonomous vehicles.",0
"Abstract: This paper presents DR-TANet (Dynamic Receptive Temporal Attention Network), a novel approach for street scene change detection that achieves state-of-the-art performance on challenging datasets. Our method leverages temporal attention mechanisms to dynamically weight features from both the current frame and past frames, enabling efficient capture of spatial context. We introduce a dynamic receptive field module to adaptively control the range of spatiotemporal interactions based on local content, facilitating better representation learning and inference. Comprehensive experiments showcase the effectiveness of our design choices, outperforming several recent competitors by significant margins. Overall, DR-TANet represents a powerful solution for real-world applications such as urban monitoring, autonomous driving, and surveillance systems.",1
"Convolutional Neural Networks (CNNs) have dominated computer vision for years, due to its ability in capturing locality and translation invariance. Recently, many vision transformer architectures have been proposed and they show promising performance. A key component in vision transformers is the fully-connected self-attention which is more powerful than CNNs in modelling long range dependencies. However, since the current dense self-attention uses all image patches (tokens) to compute attention matrix, it may neglect locality of images patches and involve noisy tokens (e.g., clutter background and occlusion), leading to a slow training process and potentially degradation of performance. To address these problems, we propose a sparse attention scheme, dubbed k-NN attention, for boosting vision transformers. Specifically, instead of involving all the tokens for attention matrix calculation, we only select the top-k similar tokens from the keys for each query to compute the attention map. The proposed k-NN attention naturally inherits the local bias of CNNs without introducing convolutional operations, as nearby tokens tend to be more similar than others. In addition, the k-NN attention allows for the exploration of long range correlation and at the same time filter out irrelevant tokens by choosing the most similar tokens from the entire image. Despite its simplicity, we verify, both theoretically and empirically, that $k$-NN attention is powerful in distilling noise from input tokens and in speeding up training. Extensive experiments are conducted by using ten different vision transformer architectures to verify that the proposed k-NN attention can work with any existing transformer architectures to improve its prediction performance.",0
"The rapid advancement of computer vision technology has led to significant improvements in tasks such as image classification, object detection, and segmentation. Among these techniques, transformer architectures have emerged as one of the most powerful tools due to their ability to process input sequentially while maintaining global dependencies. However, traditional dense attention mechanisms used by transformers suffer from scalability issues and high computational cost as they require pairwise comparisons between all elements. In response, we propose the use of k-nearest neighbors (k-NN) attention as an alternative approach that alleviates these drawbacks. Our proposed method, dubbed KVT, significantly boosts the performance of vanilla transformer models across various benchmark datasets without requiring additional complex modules or large scale models. We empirically demonstrate that our KVT method outperforms previous state-of-the-art results on several challenging benchmark tasks. Furthermore, we provide ablation studies and visualizations that showcase the effectiveness of each component in our proposed model. By introducing k-NN attention into vision transformers, we hope to pave the way towards more efficient and accurate solutions in the field of computer vision.",1
"Well-annotated medical images are costly and sometimes even impossible to acquire, hindering landmark detection accuracy to some extent. Semi-supervised learning alleviates the reliance on large-scale annotated data by exploiting the unlabeled data to understand the population structure of anatomical landmarks. The global shape constraint is the inherent property of anatomical landmarks that provides valuable guidance for more consistent pseudo labelling of the unlabeled data, which is ignored in the previously semi-supervised methods. In this paper, we propose a model-agnostic shape-regulated self-training framework for semi-supervised landmark detection by fully considering the global shape constraint. Specifically, to ensure pseudo labels are reliable and consistent, a PCA-based shape model adjusts pseudo labels and eliminate abnormal ones. A novel Region Attention loss to make the network automatically focus on the structure consistent regions around pseudo labels. Extensive experiments show that our approach outperforms other semi-supervised methods and achieves notable improvement on three medical image datasets. Moreover, our framework is flexible and can be used as a plug-and-play module integrated into most supervised methods to improve performance further.",0
"This work presents a novel approach to semi-supervised anatomical landmark detection using shape-regulated self-training. By leveraging unlabeled data and incorporating shape prior knowledge into the training process, we can significantly improve the accuracy and robustness of landmark detection algorithms. We propose a framework that combines supervised learning on labeled data and self-training on both labeled and unlabeled data. Our method employs a shape regression module to regularize the self-training procedure by enforcing consistency between predicted shapes and true shapes, which effectively reduces the impact of noisy predictions and ensures high quality pseudo labels for model improvement. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance while requiring substantially less labeled data compared to existing methods. Additionally, we perform comprehensive ablation studies and provide insights into the importance of each component in our framework. Overall, our research advances the field of computer vision by addressing the challenges associated with limited annotated data and enables more efficient development of accurate and reliable landmark detection models.",1
"Neural architecture search (NAS) is gaining more and more attention in recent years due to its flexibility and remarkable capability to reduce the burden of neural network design. To achieve better performance, however, the searching process usually costs massive computations that might not be affordable for researchers and practitioners. While recent attempts have employed ensemble learning methods to mitigate the enormous computational cost, however, they neglect a key property of ensemble methods, namely diversity, which leads to collecting more similar sub-architectures with potential redundancy in the final design. To tackle this problem, we propose a pruning method for NAS ensembles called ""Sub-Architecture Ensemble Pruning in Neural Architecture Search (SAEP)."" It targets to leverage diversity and to achieve sub-ensemble architectures at a smaller size with comparable performance to ensemble architectures that are not pruned. Three possible solutions are proposed to decide which sub-architectures to prune during the searching process. Experimental results exhibit the effectiveness of the proposed method by largely reducing the number of sub-architectures without degrading the performance.",0
"This paper presents a new algorithm for neural architecture search (NAS) that utilizes sub-architecture ensemble pruning to improve efficiency and accuracy compared to traditional NAS methods. By training multiple small sub-architectures within larger models and using their predictions as proxy tasks, the algorithm can iteratively determine which operations and connections are most important while minimizing computational cost. Experimental results show that our method achieves state-of-the art performance on benchmark datasets across different model sizes, outperforming previous NAS algorithms. Our approach provides a promising direction towards automating architecture design for deep learning practitioners.",1
"Functional connectivity (FC) between regions of the brain can be assessed by the degree of temporal correlation measured with functional neuroimaging modalities. Based on the fact that these connectivities build a network, graph-based approaches for analyzing the brain connectome have provided insights into the functions of the human brain. The development of graph neural networks (GNNs) capable of learning representation from graph structured data has led to increased interest in learning the graph representation of the brain connectome. Although recent attempts to apply GNN to the FC network have shown promising results, there is still a common limitation that they usually do not incorporate the dynamic characteristics of the FC network which fluctuates over time. In addition, a few studies that have attempted to use dynamic FC as an input for the GNN reported a reduction in performance compared to static FC methods, and did not provide temporal explainability. Here, we propose STAGIN, a method for learning dynamic graph representation of the brain connectome with spatio-temporal attention. Specifically, a temporal sequence of brain graphs is input to the STAGIN to obtain the dynamic graph representation, while novel READOUT functions and the Transformer encoder provide spatial and temporal explainability with attention, respectively. Experiments on the HCP-Rest and the HCP-Task datasets demonstrate exceptional performance of our proposed method. Analysis of the spatio-temporal attention also provide concurrent interpretation with the neuroscientific knowledge, which further validates our method. Code is available at https://github.com/egyptdj/stagin",0
"This paper proposes a new method for learning dynamic graph representations of brain connectomes using spatio-temporal attention mechanisms. We aim to capture both spatial and temporal patterns in functional magnetic resonance imaging (fMRI) data by constructing time-varying graphs from voxelwise correlations. Our approach involves applying self-attention mechanisms on these graphs to weight different connections based on their importance at each time point. By doing so, we can learn more meaningful graph representations that better capture the underlying dynamics of the brain network. To validate our method, we apply it to simulated fMRI datasets and demonstrate improved performance over baseline methods. Additionally, we showcase promising results on real resting state fMRI scans from the Human Connectome Project dataset. Overall, our proposed framework has great potential for advancing the field of connectomics and understanding neural activity across diverse cognitive states.",1
"Traditional machine learning especially supervised learning follows the assumptions of closed-world learning i.e., for each testing class a training class is available. However, such machine learning models fail to identify the classes which were not available during training time. These classes can be referred to as unseen classes. Whereas, open-world machine learning deals with arbitrary inputs (data with unseen classes) to machine learning systems. Moreover, traditional machine learning is static learning which is not appropriate for an active environment where the perspective and sources, and/or volume of data are changing rapidly. In this paper, first, we present an overview of open-world learning with importance to the real-world context. Next, different dimensions of open-world learning are explored and discussed. The area of open-world learning gained the attention of the research community in the last decade only. We have searched through different online digital libraries and scrutinized the work done in the last decade. This paper presents a systematic review of various techniques for open-world machine learning. It also presents the research gaps, challenges, and future directions in open-world learning. This paper will help researchers to understand the comprehensive developments of open-world learning and the likelihoods to extend the research in suitable areas. It will also help to select applicable methodologies and datasets to explore this further.",0
"This article presents an overview of open-world machine learning (OWML), which refers to situations where algorithms have access to unlimited data and computational resources without constraints on accuracy, scalability, interpretability, or ethical considerations. We describe several applications of OWML in different domains such as computer vision, natural language processing, robotics, and recommender systems. Additionally, we discuss challenges faced by researchers working in these areas due to limitations in current technologies, lack of large datasets, and privacy concerns related to user data. Finally, we identify opportunities for future research that aim to address these challenges and make OWML more accessible to practitioners. Overall, our goal is to provide readers with a comprehensive understanding of the promises and pitfalls associated with OWML.",1
"For over a decade, model-based reinforcement learning has been seen as a way to leverage control-based domain knowledge to improve the sample-efficiency of reinforcement learning agents. While model-based agents are conceptually appealing, their policies tend to lag behind those of model-free agents in terms of final reward, especially in non-trivial environments. In response, researchers have proposed model-based agents with increasingly complex components, from ensembles of probabilistic dynamics models, to heuristics for mitigating model error. In a reversal of this trend, we show that simple model-based agents can be derived from existing ideas that not only match, but outperform state-of-the-art model-free agents in terms of both sample-efficiency and final reward. We find that a model-free soft value estimate for policy evaluation and a model-based stochastic value gradient for policy improvement is an effective combination, achieving state-of-the-art results on a high-dimensional humanoid control task, which most model-based agents are unable to solve. Our findings suggest that model-based policy evaluation deserves closer attention.",0
"Recent years have seen significant progress in deep reinforcement learning (DRL) that enables agents trained via trial and error using high-dimensional observation spaces to solve complex realworld problems such as game playing and robot control tasks. Despite these advances, most successful DRL algorithms rely on strong assumptions such as access to global rewards, dense replay buffers, or explicit state representation. These limitations become increasingly burdensome in large-scale environments with sparse rewards where storing and replaying experiences is challenging, and either full observability is absent or delayed reward feedback hinders sample efficiency. We present a novel framework based on probabilistic graphical models called Model-Based Stochastic Value Gradient (MBSVG), which combines recent developments from model-free and model-based RL into one algorithm capable of handling both discrete actions and continuous state spaces while addressing issues related to sparsity, generalization across states, and action smoothness. Our method learns both a deterministic model of the environment dynamics and a flexible generative model capturing uncertainty during training. At test time, we employ model predictive control (MPC) leveraging both the learned model and optimality equations derived from dynamic programming principles. Through extensive empirical evaluations, our results show that MBSVG outperforms several benchmark methods in domains involving both discrete and continuous controls: games, locomotion, and robot manipulation tasks; all under settings with sparse rewards or delay conditions. Importantly, our approach enjoys greater stability during training due to an efficient use of experience buffer size without compromising performance. Overall, we believe that this work takes a step toward more generalizable solutions at t...",1
"With the advances of data-driven machine learning research, a wide variety of prediction problems have been tackled. It has become critical to explore how machine learning and specifically deep learning methods can be exploited to analyse healthcare data. A major limitation of existing methods has been the focus on grid-like data; however, the structure of physiological recordings are often irregular and unordered which makes it difficult to conceptualise them as a matrix. As such, graph neural networks have attracted significant attention by exploiting implicit information that resides in a biological system, with interactive nodes connected by edges whose weights can be either temporal associations or anatomical junctions. In this survey, we thoroughly review the different types of graph architectures and their applications in healthcare. We provide an overview of these methods in a systematic manner, organized by their domain of application including functional connectivity, anatomical structure and electrical-based analysis. We also outline the limitations of existing techniques and discuss potential directions for future research.",0
"Title: Graph-Based Deep Learning for Medical Diagnosis and Analysis: Past, Present and Future  Abstract: With the rapid advancement of technology, medical diagnosis has become increasingly reliant on computational models that can analyze large amounts of data effectively. In recent years, deep learning techniques have been used to develop these models, leading to significant improvements in accuracy and efficiency. Among them, graph-based deep learning (GDL) methods have proven particularly effective due to their ability to capture both structural and semantic relationships in data. This paper provides an overview of GDL approaches in medical diagnosis and analysis, including their history, current applications, and future prospects. We discuss the challenges faced by traditional methods and how GDL addresses these limitations, as well as the impact of data quality and availability on model performance. Finally, we highlight promising areas of research and potential opportunities for further development in this field.",1
"Recent advances in deep learning have enabled the development of automated frameworks for analysing medical images and signals, including analysis of cervical cancer. Many previous works focus on the analysis of isolated cervical cells, or do not offer sufficient methods to explain and understand how the proposed models reach their classification decisions on multi-cell images. Here, we evaluate various state-of-the-art deep learning models and attention-based frameworks for the classification of images of multiple cervical cells. As we aim to provide interpretable deep learning models to address this task, we also compare their explainability through the visualization of their gradients. We demonstrate the importance of using images that contain multiple cells over using isolated single-cell images. We show the effectiveness of the residual channel attention model for extracting important features from a group of cells, and demonstrate this model's efficiency for this classification task. This work highlights the benefits of channel attention mechanisms in analyzing multiple-cell images for potential relations and distributions within a group of cells. It also provides interpretable models to address the classification of cervical cells.",0
"Automatic analysis methods based on machine learning have shown great potential to improve the accuracy and consistency of diagnostic decisions made by human experts. Recent works have demonstrated the effectiveness of deep learning algorithms such as convolutional neural networks (CNN) and recurrent neural networks (RNN). However, these models often suffer from the problem of interpretability: their internal workings are difficult to comprehend, which makes them less trustworthy and harder to deploy in medical settings where clear explanations and transparency may matter more than raw performance. In our study, we propose a novel attention mechanism that addresses this issue while improving model efficiency over previous techniques. Our method leverages a variant of RNN called Long Short Term Memory (LSTM) network augmented with self-attention mechanisms inspired by recent advances in natural language processing tasks. We evaluate our approach on real datasets extracted from digitized images of cervix slides used during routine screening exams. By comparing against several baselines and analyzing different metrics including area under ROC curve, precision/recall, F1 score, and sensitivity / specificity tradeoffs, we show the superiority of our interpretable attention networks in identifying abnormal cells and delineating lesions accurately. Future work includes extending this approach towards other medical domains like histopathology image analysis and drug discovery informatics.",1
"Leading methods in the domain of action recognition try to distill information from both the spatial and temporal dimensions of an input video. Methods that reach State of the Art (SotA) accuracy, usually make use of 3D convolution layers as a way to abstract the temporal information from video frames. The use of such convolutions requires sampling short clips from the input video, where each clip is a collection of closely sampled frames. Since each short clip covers a small fraction of an input video, multiple clips are sampled at inference in order to cover the whole temporal length of the video. This leads to increased computational load and is impractical for real-world applications. We address the computational bottleneck by significantly reducing the number of frames required for inference. Our approach relies on a temporal transformer that applies global attention over video frames, and thus better exploits the salient information in each frame. Therefore our approach is very input efficient, and can achieve SotA results (on Kinetics dataset) with a fraction of the data (frames per video), computation and latency. Specifically on Kinetics-400, we reach $80.5$ top-1 accuracy with $\times 30$ less frames per video, and $\times 40$ faster inference than the current leading method. Code is available at: https://github.com/Alibaba-MIIL/STAM",0
"This research work presents an empirical study on video compression. It investigates how much data can be saved by compressing a video while preserving its quality and how that compares to the amount of data required to store images of similar size. We focus specifically on popular codecs used in practice such as H.264, HEVC, AV1, VP9, among others. Our findings suggest that modern codecs achieve impressive gains in terms of compression efficiency and therefore one would expect videos to require significantly less storage space than a set of corresponding still frames. Additionally we analyze factors affecting the tradeoff between image quality and compression rate and provide recommendations for efficient usage of storage resources considering available bandwidth for streaming or downloading media content. Finally we present experimental evaluation results showing the improvement over state of the art algorithms providing guidelines for developers working in multimedia filed designing new applications based on the most recent developments. For example, our experiments show an average gain in terms of bits per pixel (bpp) ranging from 7% up to 30%.",1
"Inpatient falls are a serious safety issue in hospitals and healthcare facilities. Recent advances in video analytics for patient monitoring provide a non-intrusive avenue to reduce this risk through continuous activity monitoring. However, in-bed fall risk assessment systems have received less attention in the literature. The majority of prior studies have focused on fall event detection, and do not consider the circumstances that may indicate an imminent inpatient fall. Here, we propose a video-based system that can monitor the risk of a patient falling, and alert staff of unsafe behaviour to help prevent falls before they occur. We propose an approach that leverages recent advances in human localisation and skeleton pose estimation to extract spatial features from video frames recorded in a simulated environment. We demonstrate that body positions can be effectively recognised and provide useful evidence for fall risk assessment. This work highlights the benefits of video-based models for analysing behaviours of interest, and demonstrates how such a system could enable sufficient lead time for healthcare professionals to respond and address patient needs, which is necessary for the development of fall intervention programs.",0
"This abstract presents a case study on video-based fall risk assessments (FRAs) performed at an acute care hospital. FRAs have been recommended as part of standardized patient falls prevention protocols; however, conducting these assessments can be resource intensive, requiring trained nurses to physically observe patients while they perform tasks. We implemented a novel video-based FRA system designed to automate data collection, allowing staff members to focus their time on interventions that mitigate fall risks identified by the system rather than solely observing patients during testing. Our results show that video-based fall risk assessments were effective in identifying high-risk patients while saving nursing resources through automation. Ultimately, the implementation of our system allowed us to better allocate healthcare worker resources to directly impact patient outcomes related to fall reduction.",1
"Self-attention has been successfully applied to video representation learning due to the effectiveness of modeling long range dependencies. Existing approaches build the dependencies merely by computing the pairwise correlations along spatial and temporal dimensions simultaneously. However, spatial correlations and temporal correlations represent different contextual information of scenes and temporal reasoning. Intuitively, learning spatial contextual information first will benefit temporal modeling. In this paper, we propose a separable self-attention (SSA) module, which models spatial and temporal correlations sequentially, so that spatial contexts can be efficiently used in temporal modeling. By adding SSA module into 2D CNN, we build a SSA network (SSAN) for video representation learning. On the task of video action recognition, our approach outperforms state-of-the-art methods on Something-Something and Kinetics-400 datasets. Our models often outperform counterparts with shallower network and fewer modalities. We further verify the semantic learning ability of our method in visual-language task of video retrieval, which showcases the homogeneity of video representations and text embeddings. On MSR-VTT and Youcook2 datasets, video representations learnt by SSA significantly improve the state-of-the-art performance.",0
"Abstract: In recent years, self-attention mechanisms have emerged as a powerful tool for processing sequential data such as text, speech, images, videos, etc., achieving state-of-the-art results on many tasks across different domains. This paper introduces a new approach called Separable Self-Attention Network (SSAN) that effectively learns video representations by operating directly on pixel values rather than on extracted features. Our approach decomposes the conventional multi-head self-attention operation into multiple separable attention modules, allowing efficient parallelization through dynamic swapping of attention weights. We evaluate our method on several benchmark datasets for action recognition and scene understanding, demonstrating significant improvements over current models. Overall, SSAN provides a faster yet comparably effective alternative for learning meaningful video representations using separable self-attention operations.",1
"This paper presents a new Vision Transformer (ViT) architecture Multi-Scale Vision Longformer, which significantly enhances the ViT of \cite{dosovitskiy2020image} for encoding high-resolution images using two techniques. The first is the multi-scale model structure, which provides image encodings at multiple scales with manageable computational cost. The second is the attention mechanism of vision Longformer, which is a variant of Longformer \cite{beltagy2020longformer}, originally developed for natural language processing, and achieves a linear complexity w.r.t. the number of input tokens. A comprehensive empirical study shows that the new ViT significantly outperforms several strong baselines, including the existing ViT models and their ResNet counterparts, and the Pyramid Vision Transformer from a concurrent work \cite{wang2021pyramid}, on a range of vision tasks, including image classification, object detection, and segmentation. The models and source code are released at \url{https://github.com/microsoft/vision-longformer}.",0
"In recent years there has been rapid development in natural language processing (NLP) using transformer models such as BERT, GPT2, Huggingface transformers etc. These NLP transformers can now match human accuaracy on some text classification tasks. However, these transformer architectures have yet to see wide spread use in Computer Vision (CV). One possible reason for this may be due to CV data typically having higher dimensionality than textual data found in NLP i.e images vs sequences of tokens. Thus we need CV specific tranformers that can effectively encode high resolution image features efficiently. We propose such a model called ""Multi-scale Vison Longformer"". Our contributions are two fold : Firstly ,We show how vision based Longformer models have better efficiency over previous works like ViTs. Secondly, unlike traditional CNN methods where filters/weights share same weights across scale levels, our new architecture uses multiple encodings at different scales which allows sharing and reusing of learnnt parameters allowing more efficient learning through backpropogation and reduced computational requirements compared to other state of art systems. This paper shows results demonstrating superior performance on several benchmark datasets both quantitatively and qualitatively and even matches real time object detection capabilities of current RNN free state of arts. Therefore MVL could open up opportunities towards next generation computer vision models by enabling faster training times while achieving par accuracy with less GPU memories usage than previous approaches. With improvements expected from future hardware advances we anticipate MVL to potentially revolutionize large scale automated feature extraction in many application domains.",1
"Building machine learning models using EEG recorded outside of the laboratory setting requires methods robust to noisy data and randomly missing channels. This need is particularly great when working with sparse EEG montages (1-6 channels), often encountered in consumer-grade or mobile EEG devices. Neither classical machine learning models nor deep neural networks trained end-to-end on EEG are typically designed or tested for robustness to corruption, and especially to randomly missing channels. While some studies have proposed strategies for using data with missing channels, these approaches are not practical when sparse montages are used and computing power is limited (e.g., wearables, cell phones). To tackle this problem, we propose dynamic spatial filtering (DSF), a multi-head attention module that can be plugged in before the first layer of a neural network to handle missing EEG channels by learning to focus on good channels and to ignore bad ones. We tested DSF on public EEG data encompassing ~4,000 recordings with simulated channel corruption and on a private dataset of ~100 at-home recordings of mobile EEG with natural corruption. Our proposed approach achieves the same performance as baseline models when no noise is applied, but outperforms baselines by as much as 29.4% accuracy when significant channel corruption is present. Moreover, DSF outputs are interpretable, making it possible to monitor channel importance in real-time. This approach has the potential to enable the analysis of EEG in challenging settings where channel corruption hampers the reading of brain signals.",0
"Title: ""Learning from Corrupted EEG Data using Dynamic Spatial Filtering""  This research aims to address the problem of accurately identifying neural activity patterns in electroencephalography (EEG) data that has been contaminated by noise and artifacts. To overcome these issues, we propose a novel approach that combines advanced signal processing techniques with machine learning algorithms.  The proposed method involves two key components: dynamic spatial filtering and robust classification. In the first stage, we apply dynamic spatial filters to clean up the raw EEG signals by minimizing noise and artifacts. Our filter design exploits the spatial structure of EEG signals, adaptively selecting a set of coefficients that maximize the signal-to-noise ratio at each time point. This leads to improved detection of brain rhythms and more accurate feature extraction.  Next, we use the filtered EEG signals as input features for training machine learning models to classify different mental states. By combining multiple spatial filters, our approach can capture complementary information about brain dynamics and lead to better generalization performance across subjects. We evaluate the effectiveness of our approach on several benchmark datasets, demonstrating significant improvements over state-of-the-art methods.  Overall, our work shows promise in advancing the field of Brain Computer Interface (BCI), where real-time and accurate detection of user intentions based on EEG signals is crucial. With further refinement and validation through experiments, our method may open new doors for assistive technologies and non-invasive neurorehabilitation applications.",1
"Time-series forecasting is one of the most active research topics in artificial intelligence. Applications in real-world time series should consider two factors for achieving reliable predictions: modeling dynamic dependencies among multiple variables and adjusting the model's intrinsic hyperparameters. A still open gap in that literature is that statistical and ensemble learning approaches systematically present lower predictive performance than deep learning methods. They generally disregard the data sequence aspect entangled with multivariate data represented in more than one time series. Conversely, this work presents a novel neural network architecture for time-series forecasting that combines the power of graph evolution with deep recurrent learning on distinct data distributions; we named our method Recurrent Graph Evolution Neural Network (ReGENN). The idea is to infer multiple multivariate relationships between co-occurring time-series by assuming that the temporal data depends not only on inner variables and intra-temporal relationships (i.e., observations from itself) but also on outer variables and inter-temporal relationships (i.e., observations from other-selves). An extensive set of experiments was conducted comparing ReGENN with dozens of ensemble methods and classical statistical ones, showing sound improvement of up to 64.87% over the competing algorithms. Furthermore, we present an analysis of the intermediate weights arising from ReGENN, showing that by looking at inter and intra-temporal relationships simultaneously, time-series forecasting is majorly improved if paying attention to how multiple multivariate data synchronously evolve.",0
"This should describe the general idea behind the methodology and how it differs from existing methods. Please provide references at the end of the abstract if applicable. ---------------------------  The process of accurately predicting future events using time series data remains one of the most important tasks in many fields including finance, healthcare, engineering, and transportation. In order to tackle this problem, researchers have developed a variety of approaches based on deep learning (DL) models such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), and attention mechanisms. However, these models often suffer from overfitting due to their complex architectures that require large amounts of training data and computational resources. Additionally, they fail to capture the temporal dependencies of the underlying processes generating the data. To address these issues, we propose a new method called Deep Graph-Evolution Learning (DGRL). DGRL combines graph representations with evolutionary computation techniques by incorporating evolution strategies into RNN models. This hybrid approach has two advantages. Firstly, graphs enable a more compact representation of complex structures while providing interpretable outputs. Secondly, evolution strategies optimize the model parameters without relying on gradient descent algorithms. We evaluate our proposed method on several real world datasets and compare its performance against other state-of-the-art DL baselines. Experimental results show that DGRL achieves superior accuracy in time series forecasting while outperforming competitive benchmarks across different metrics. Our work provides insights into combining graph structured representation learning and evolutionary optimization techniques as alternatives to traditional deep learning methods. References: [1] L. Yin, et al., ""Time series forecasting with deep learning,"" Nature Scientific Reports, vol. 9, p. 8624, 2019.[2] G. Huang, et al., ""Recurrent neural network architectures for time series prediction,"" in Proceedings o",1
"Pedestrian Detection is the most critical module of an Autonomous Driving system. Although a camera is commonly used for this purpose, its quality degrades severely in low-light night time driving scenarios. On the other hand, the quality of a thermal camera image remains unaffected in similar conditions. This paper proposes an end-to-end multimodal fusion model for pedestrian detection using RGB and thermal images. Its novel spatio-contextual deep network architecture is capable of exploiting the multimodal input efficiently. It consists of two distinct deformable ResNeXt-50 encoders for feature extraction from the two modalities. Fusion of these two encoded features takes place inside a multimodal feature embedding module (MuFEm) consisting of several groups of a pair of Graph Attention Network and a feature fusion unit. The output of the last feature fusion unit of MuFEm is subsequently passed to two CRFs for their spatial refinement. Further enhancement of the features is achieved by applying channel-wise attention and extraction of contextual information with the help of four RNNs traversing in four different directions. Finally, these feature maps are used by a single-stage decoder to generate the bounding box of each pedestrian and the score map. We have performed extensive experiments of the proposed framework on three publicly available multimodal pedestrian detection benchmark datasets, namely KAIST, CVC-14, and UTokyo. The results on each of them improved the respective state-of-the-art performance. A short video giving an overview of this work along with its qualitative results can be seen at https://youtu.be/FDJdSifuuCs.",0
"""Pedestrian detection plays a crucial role in autonomous driving systems as they provide necessary input regarding potential obstacles on the road. This work focuses on developing a deep learning based approach for multimodal pedestrian detection using spatio-contextual deep networks. Specifically, we propose a novel network architecture that can fuse both spatial features from RGB images and temporal features from depth maps extracted using LiDAR sensors. We introduce two modules: Spatial Pyramid Module (SPM) and Temporal Pyramid Module (TPM), which can learn different levels of feature representation across space and time dimensions respectively. Moreover, we design a new fusion mechanism using densely connected layers at multiple stages of our proposed framework that effectively combines multi-modal features while preserving spatial context.""  The abstract should capture the main ideas and research contributions of your paper in <150 words without repeating the title. Use active voice when possible. Make sure you clearly state how your approach differs from existing approaches. ---- In this work, we present a novel method for multimodal pedestrian detection using spatio-contextual deep networks for use in autonomous vehicles. Our approach utilizes RGB images and depth maps derived from LiDAR sensors, merging them into one model through a combination of spatial pyramid module (SPM) and temporal pyramid module (TPM). These modules allow the model to learn distinctive features across varying scales and timelines while maintaining a clear understanding of the scene's geometry via SPM. By incorporating dense connections throughout the network, our system accurately fuses the complementary data streams from these diverse modalities, resulting in more accurate object localization and fewer false positives than previous methods relying solely upon single modality inputs. Ultimately, this framework offers significant advancements towards enabling self-driving cars to safely operate alongside pedestrians within complex environments.",1
"Weakly-Supervised Object Detection (WSOD) and Localization (WSOL), i.e., detecting multiple and single instances with bounding boxes in an image using image-level labels, are long-standing and challenging tasks in the CV community. With the success of deep neural networks in object detection, both WSOD and WSOL have received unprecedented attention. Hundreds of WSOD and WSOL methods and numerous techniques have been proposed in the deep learning era. To this end, in this paper, we consider WSOL is a sub-task of WSOD and provide a comprehensive survey of the recent achievements of WSOD. Specifically, we firstly describe the formulation and setting of the WSOD, including the background, challenges, basic framework. Meanwhile, we summarize and analyze all advanced techniques and training tricks for improving detection performance. Then, we introduce the widely-used datasets and evaluation metrics of WSOD. Lastly, we discuss the future directions of WSOD. We believe that these summaries can help pave a way for future research on WSOD and WSOL.",0
"This survey provides an overview of deep learning methods applied on weak supervision tasks such as object detection and localization. Various deep architectures like convolutional neural networks (CNNs), recurrent neural networks (RNNs) and generative adversarial networks (GANs) have been leveraged by researchers to achieve state-of-the-art results on datasets ranging from COCO to Pascal VOC using diverse types of annotations such as bounding boxes and image tags. In addition to discussing popular models, we examine recent work that has explored new sources of annotations like natural language prompts and web search data to mitigate label scarcity issues. Lastly, future directions towards more generalizable few shot or zero shot object detection/localization are discussed with emphasis placed on semi-supervised learning techniques. Overall, our goal is to provide practitioners and interested readers alike a comprehensive understanding of current progress and challenges in the field of weakly-supervised computer vision.",1
"Camouflaged object detection (COD) is a challenging task due to the low boundary contrast between the object and its surroundings. In addition, the appearance of camouflaged objects varies significantly, e.g., object size and shape, aggravating the difficulties of accurate COD. In this paper, we propose a novel Context-aware Cross-level Fusion Network (C2F-Net) to address the challenging COD task. Specifically, we propose an Attention-induced Cross-level Fusion Module (ACFM) to integrate the multi-level features with informative attention coefficients. The fused features are then fed to the proposed Dual-branch Global Context Module (DGCM), which yields multi-scale feature representations for exploiting rich global context information. In C2F-Net, the two modules are conducted on high-level features using a cascaded manner. Extensive experiments on three widely used benchmark datasets demonstrate that our C2F-Net is an effective COD model and outperforms state-of-the-art models remarkably. Our code is publicly available at: https://github.com/thograce/C2FNet.",0
"This abstract presents a new approach to camouflaged object detection that utilizes context-aware cross-level fusion networks (CAFNs). Traditional approaches to camouflaged object detection often rely on hand-crafted features, which may miss important details due to the complex nature of real-world environments. To address this issue, CAFNs fuse image level features extracted from deep convolutional neural networks (DCNN) with higher-level semantic features obtained using spatial attention mechanisms to produce more accurate results. In addition, the proposed method can learn the optimal weighting factors of these features by considering their relative importance according to different regions and objects within images. Our experiments demonstrate significant improvements over state-of-the-art methods across multiple benchmark datasets, highlighting the effectiveness of our method in tackling difficult challenges such as partial occlusions and background clutter.",1
"Within Transformer, self-attention is the key module to learn powerful context-aware representations. However, self-attention suffers from quadratic memory requirements with respect to the sequence length, which limits us to process longer sequence on GPU. In this work, we propose sequence parallelism, a memory efficient parallelism method to help us break input sequence length limitation and train with longer sequence on GPUs. Compared with existing parallelism, our approach no longer requires a single device to hold the whole sequence. Specifically, we split the input sequence into multiple chunks and feed each chunk into its corresponding device (i.e. GPU). To compute the attention output, we communicate attention embeddings among GPUs. Inspired by ring all-reduce, we integrated ring-style communication with self-attention calculation and proposed Ring Self-Attention (RSA). Our implementation is fully based on PyTorch. Without extra compiler or library changes, our approach is compatible with data parallelism and pipeline parallelism. Experiments show that sequence parallelism performs well when scaling with batch size and sequence length. Compared with tensor parallelism, our approach achieved $13.7\times$ and $3.0\times$ maximum batch size and sequence length respectively when scaling up to 64 NVIDIA P100 GPUs. We plan to integrate our sequence parallelism with data, pipeline and tensor parallelism to further train large-scale models with 4D parallelism in our future work.",0
"In recent years, there has been increasing interest in parallel processing techniques that allow multiple tasks to be executed simultaneously on different hardware components. One such technique is 4D parallelism, which involves dividing a task into subtasks that can be processed concurrently by several agents working together in time as well as space. However, implementing 4D parallelism effectively remains a challenging problem due to the difficulties involved in coordinating and synchronizing the activities of multiple agents over time.  This paper proposes a novel approach to address these issues called sequence parallelism. Instead of relying on explicit coordination mechanisms like locks or semaphores, our method uses heuristics based on the properties of the data dependencies among tasks to make informed decisions about which operations should be performed next. By doing so, we aim to minimize both communication overhead and potential race conditions while maximizing performance benefits from parallel execution. Our experiments demonstrate that our proposed method leads to significant improvements over traditional parallelization methods across a variety of benchmarks, showing the promise of this new approach towards making 4D parallelism possible in practice.",1
"Pedestrian trajectory prediction in urban scenarios is essential for automated driving. This task is challenging because the behavior of pedestrians is influenced by both their own history paths and the interactions with others. Previous research modeled these interactions with pooling mechanisms or aggregating with hand-crafted attention weights. In this paper, we present the Social Interaction-Weighted Spatio-Temporal Convolutional Neural Network (Social-IWSTCNN), which includes both the spatial and the temporal features. We propose a novel design, namely the Social Interaction Extractor, to learn the spatial and social interaction features of pedestrians. Most previous works used ETH and UCY datasets which include five scenes but do not cover urban traffic scenarios extensively for training and evaluation. In this paper, we use the recently released large-scale Waymo Open Dataset in urban traffic scenarios, which includes 374 urban training scenes and 76 urban testing scenes to analyze the performance of our proposed algorithm in comparison to the state-of-the-art (SOTA) models. The results show that our algorithm outperforms SOTA algorithms such as Social-LSTM, Social-GAN, and Social-STGCNN on both Average Displacement Error (ADE) and Final Displacement Error (FDE). Furthermore, our Social-IWSTCNN is 54.8 times faster in data pre-processing speed, and 4.7 times faster in total test speed than the current best SOTA algorithm Social-STGCNN.",0
"In urban traffic scenarios, pedestrian trajectory prediction plays a crucial role in enhancing road safety and improving transportation efficiency. We propose Social-IWSTCNN, a social interaction-weighted spatio-temporal convolutional neural network that leverages pedestriansâ€™ social interactions within crowd scenes to enhance predictions. By capturing short-term contexts from neighboring individuals, our model adaptively refines initial velocity predictions at multiple scales while learning subtle patterns from different time intervals over various densities. Extensive experiments on real-world datasets demonstrate significant improvement against state-of-the-art methods, with better accuracy (mean ADE/FDE decreasing by 28%/47%) and robustness across diverse situations, including rare events such as lane crossings and sudden stops. \[1\]  In addition to enhanced performance, we introduce a more intuitive measure for pedestrian behavior analysis using the concept of dispersion correlation derived from Kendallâ€™s tau rank correlation coefficient. Our method effectively quantifies pairwise consistency and discordances among trajectories, revealing valuable insights into crowd dynamics and shedding light on collective vs individual movement preferences. An interpretable ablation study further confirms the significance of each module or component in our architecture. \[1\]  This work pioneers a novel perspective on considering social effects within the scope of motion forecasting problems. We believe that integrating social signals into advanced predictors holds great potential for future research directions. To facilitate reproducibility and foster innovations, code and evaluation results have been made publicly available. Overall, our framework achieves substantial progress towards practical applications, bringing us closer to building intelligent systems capable of understanding dynamic human environments.  Keywords: Pedestrian trajectory prediction; Social interaction; Spatio-temporal convolutional neural networks (STCNN); Motion forecasting; Dispersion correlation; Crowd dynamics; Reproducible research. \[1\]",1
"Recent advances on Vision Transformer (ViT) and its improved variants have shown that self-attention-based networks surpass traditional Convolutional Neural Networks (CNNs) in most vision tasks. However, existing ViTs focus on the standard accuracy and computation cost, lacking the investigation of the intrinsic influence on model robustness and generalization. In this work, we conduct systematic evaluation on components of ViTs in terms of their impact on robustness to adversarial examples, common corruptions and distribution shifts. We find some components can be harmful to robustness. By using and combining robust components as building blocks of ViTs, we propose Robust Vision Transformer (RVT), which is a new vision transformer and has superior performance with strong robustness. We further propose two new plug-and-play techniques called position-aware attention scaling and patch-wise augmentation to augment our RVT, which we abbreviate as RVT*. The experimental results on ImageNet and six robustness benchmarks show the advanced robustness and generalization ability of RVT compared with previous ViTs and state-of-the-art CNNs. Furthermore, RVT-S* also achieves Top-1 rank on multiple robustness leaderboards including ImageNet-C and ImageNet-Sketch. The code will be available at \url{https://git.io/Jswdk}.",0
"In recent years there have been significant advancements in computer vision using deep learning techniques such as Convolutional Neural Networks (CNN). While these models have achieved state-of-the-art performance on several tasks, they often require large amounts of data and computational resources which can limit their application in real world scenarios. This has led researchers to explore alternative model architectures that can achieve comparable results while requiring less data and computation, such as the Vision Transformer architecture proposed by Dosovitskiy et al. However, training Vision Transformers remains challenging due to instability issues caused by the inherent parallel nature of self attention mechanisms. To address this issue, we propose the use of regularization techniques combined with advanced optimization methods to improve both stability and accuracy during training. Our experimental evaluations demonstrate that our approach leads to better generalization performance compared to previous methods while reducing training time by up to 80%. These results show that our method is a promising step towards robust vision transformer networks that can perform well under resource constrained settings.",1
"Camouflaged object detection (COD), which aims to identify the objects that conceal themselves into the surroundings, has recently drawn increasing research efforts in the field of computer vision. In practice, the success of deep learning based COD is mainly determined by two key factors, including (i) A significantly large receptive field, which provides rich context information, and (ii) An effective fusion strategy, which aggregates the rich multi-level features for accurate COD. Motivated by these observations, in this paper, we propose a novel deep learning based COD approach, which integrates the large receptive field and effective feature fusion into a unified framework. Specifically, we first extract multi-level features from a backbone network. The resulting features are then fed to the proposed dual-branch mixture convolution modules, each of which utilizes multiple asymmetric convolutional layers and two dilated convolutional layers to extract rich context features from a large receptive field. Finally, we fuse the features using specially-designed multi-level interactive fusion modules, each of which employs an attention mechanism along with feature interaction for effective feature fusion. Our method detects camouflaged objects with an effective fusion strategy, which aggregates the rich context information from a large receptive field. All of these designs meet the requirements of COD well, allowing the accurate detection of camouflaged objects. Extensive experiments on widely-used benchmark datasets demonstrate that our method is capable of accurately detecting camouflaged objects and outperforms the state-of-the-art methods.",0
In order to improve object detection accuracy in real world scenes where objects are camouflaged by natural backgrounds we propose a novel approach that combines traditional RGB imaging with thermal infrared imaging which has been previously underutilized due to high noise levels. Our method utilizes convolutional neural networks operating on mixtures of these two types of input data rather than just one type. We show through extensive experimentation that our fusion technique allows us to achieve higher quality predictions than competitive state-of-the-art object detection approaches trained only on traditional visible light images. These benefits are especially noticeable in scenarios involving challenging illumination conditions such as night time or shadowed environments.,1
"In this paper, we address the makeup transfer and removal tasks simultaneously, which aim to transfer the makeup from a reference image to a source image and remove the makeup from the with-makeup image respectively. Existing methods have achieved much advancement in constrained scenarios, but it is still very challenging for them to transfer makeup between images with large pose and expression differences, or handle makeup details like blush on cheeks or highlight on the nose. In addition, they are hardly able to control the degree of makeup during transferring or to transfer a specified part in the input face. In this work, we propose the PSGAN++, which is capable of performing both detail-preserving makeup transfer and effective makeup removal. For makeup transfer, PSGAN++ uses a Makeup Distill Network to extract makeup information, which is embedded into spatial-aware makeup matrices. We also devise an Attentive Makeup Morphing module that specifies how the makeup in the source image is morphed from the reference image, and a makeup detail loss to supervise the model within the selected makeup detail area. On the other hand, for makeup removal, PSGAN++ applies an Identity Distill Network to embed the identity information from with-makeup images into identity matrices. Finally, the obtained makeup/identity matrices are fed to a Style Transfer Network that is able to edit the feature maps to achieve makeup transfer or removal. To evaluate the effectiveness of our PSGAN++, we collect a Makeup Transfer In the Wild dataset that contains images with diverse poses and expressions and a Makeup Transfer High-Resolution dataset that contains high-resolution images. Experiments demonstrate that PSGAN++ not only achieves state-of-the-art results with fine makeup details even in cases of large pose/expression differences but also can perform partial or degree-controllable makeup transfer.",0
"This paper presents our new model called â€œPSGAN++â€ that excels at high quality detail preservation under makeup transfer while greatly outperforming competing baselines on challenging real life videos including motion blur and nonuniform lighting. Inspired by recent advances in GANs, we introduce progressive spatial and channel attention blocks into a single generator network which focuses attention on different feature channels instead of mixing them in parallel like in most generators. Our adversarial discriminator network employs perceptual loss as well as temporal gradient loss and is trained using adversarial training based on the LSGAN framework to enable better stability during optimization and generate high resolution outputs up to 4K (2768x1280) at 30 FPS. Using our full pipeline, user perceived image quality is significantly higher than state of the art methods even on difficult test cases.",1
"Multisource image analysis that leverages complementary spectral, spatial, and structural information benefits fine-grained object recognition that aims to classify an object into one of many similar subcategories. However, for multisource tasks that involve relatively small objects, even the smallest registration errors can introduce high uncertainty in the classification process. We approach this problem from a weakly supervised learning perspective in which the input images correspond to larger neighborhoods around the expected object locations where an object with a given class label is present in the neighborhood without any knowledge of its exact location. The proposed method uses a single-source deep instance attention model with parallel branches for joint localization and classification of objects, and extends this model into a multisource setting where a reference source that is assumed to have no location uncertainty is used to aid the fusion of multiple sources in four different levels: probability level, logit level, feature level, and pixel level. We show that all levels of fusion provide higher accuracies compared to the state-of-the-art, with the best performing method of feature-level fusion resulting in 53% accuracy for the recognition of 40 different types of trees, corresponding to an improvement of 5.7% over the best performing baseline when RGB, multispectral, and LiDAR data are used. We also provide an in-depth comparison by evaluating each model at various parameter complexity settings, where the increased model capacity results in a further improvement of 6.3% over the default capacity setting.",0
"This paper presents a method for fine-grained object recognition using weakly supervised instance attention. By leveraging multiple sources of data and utilizing weak labels, our approach allows for more accurate and efficient classification of objects. In particular, we demonstrate the effectiveness of our method on the challenging task of tree species classification, where precise identification can have significant environmental and ecological implications. Our experiments show that our model outperforms state-of-the-art methods and achieves high accuracy under real-world conditions. Overall, our work highlights the potential of weakly supervised learning techniques for solving complex problems in computer vision and beyond.",1
"Adversarial attack is aimed at fooling the target classifier with imperceptible perturbation. Adversarial examples, which are carefully crafted with a malicious purpose, can lead to erroneous predictions, resulting in catastrophic accidents. To mitigate the effects of adversarial attacks, we propose a novel purification model called CAP-GAN. CAP-GAN takes account of the idea of pixel-level and feature-level consistency to achieve reasonable purification under cycle-consistent learning. Specifically, we utilize the guided attention module and knowledge distillation to convey meaningful information to the purification model. Once a model is fully trained, inputs would be projected into the purification model and transformed into clean-like images. We vary the capacity of the adversary to argue the robustness against various types of attack strategies. On the CIFAR-10 dataset, CAP-GAN outperforms other pre-processing based defenses under both black-box and white-box settings.",0
"Title: Improving Adversarial Robustness with GANs: An Evaluation of Purifying Techniques (Paper presented at CVPR 2023) This study investigates new techniques for improving adversarial robustness using Generative Adversarial Networks (GANs). We evaluate several recent methods based on cycle consistency and attention mechanisms to improve generator performance against diverse attacks. Our analysis finds that these approaches can significantly enhance model robustness while maintaining visual fidelity. Furthermore, we demonstrate how our proposed purification framework outperforms state-of-the-art baselines across multiple datasets and attack types. These findings contribute important insights into the design of more secure deep learning systems. Overall, this work serves as a step towards achieving greater resilience for generative models. How would you respond if someone asked you this question: ""What inspired you to write such a comprehensive research paper?""",1
"The central challenge in automated synthesis planning is to be able to generate and predict outcomes of a diverse set of chemical reactions. In particular, in many cases, the most likely synthesis pathway cannot be applied due to additional constraints, which requires proposing alternative chemical reactions. With this in mind, we present Molecule Edit Graph Attention Network (MEGAN), an end-to-end encoder-decoder neural model. MEGAN is inspired by models that express a chemical reaction as a sequence of graph edits, akin to the arrow pushing formalism. We extend this model to retrosynthesis prediction (predicting substrates given the product of a chemical reaction) and scale it up to large datasets. We argue that representing the reaction as a sequence of edits enables MEGAN to efficiently explore the space of plausible chemical reactions, maintaining the flexibility of modeling the reaction in an end-to-end fashion, and achieving state-of-the-art accuracy in standard benchmarks. Code and trained models are made available online at https://github.com/molecule-one/megan.",0
"This paper presents a novel deep learning approach called Molecule Edit Graph Attention Network (MolEditGAT) that models chemical reactions as sequences of graph edits on molecular representations. Traditional approaches for modeling chemistry focus on predictive tasks based on static input structures such as SMILES strings or molecular fingerprints. In contrast, MolEditGAT operates directly on graphs and captures the spatial relationships between atoms in a reaction system. To accomplish this task, we design an attention mechanism specifically tailored for processing graph data, which enables efficient alignment between reactant and product graphs. We further introduce several regularization techniques, including dropout on edges and layer normalization by design, to enhance generalization performance. Using multiple benchmark datasets across diverse domains like organic synthesis planning, physical organic chemistry, and biochemical transformation prediction, our experiments demonstrate state-of-the-art results compared against strong baseline methods. With its unique ability to capture both atom properties and bond connectivities in a single framework, MolEditGAT holds great promise in advancing machine learning applications in computational chemistry and other areas within materials science where graph reasoning plays a crucial role.",1
"Zero-shot action recognition can recognize samples of unseen classes that are unavailable in training by exploring common latent semantic representation in samples. However, most methods neglected the connotative relation and extensional relation between the action classes, which leads to the poor generalization ability of the zero-shot learning. Furthermore, the learned classifier incline to predict the samples of seen class, which leads to poor classification performance. To solve the above problems, we propose a two-stage deep neural network for zero-shot action recognition, which consists of a feature generation sub-network serving as the sampling stage and a graph attention sub-network serving as the classification stage. In the sampling stage, we utilize a generative adversarial networks (GAN) trained by action features and word vectors of seen classes to synthesize the action features of unseen classes, which can balance the training sample data of seen classes and unseen classes. In the classification stage, we construct a knowledge graph (KG) based on the relationship between word vectors of action classes and related objects, and propose a graph convolution network (GCN) based on attention mechanism, which dynamically updates the relationship between action classes and objects, and enhances the generalization ability of zero-shot learning. In both stages, we all use word vectors as bridges for feature generation and classifier generalization from seen classes to unseen classes. We compare our method with state-of-the-art methods on UCF101 and HMDB51 datasets. Experimental results show that our proposed method improves the classification performance of the trained classifier and achieves higher accuracy.",0
"In order to make machines that can interact more effectively with humans in complex environments, it is important to develop systems that can accurately recognize visual objects and actions from raw data. While current approaches have achieved some success in this area, they often suffer from limitations such as poor performance on novel tasks, lack of scalability, or high computational cost. To address these challenges, we propose a two-stage deep network architecture that combines generative adversarial networks (GANs) and knowledge graphs (KGs). Our model leverages the strengths of both types of models while minimizing their weaknesses. We evaluate our approach using several benchmark datasets and demonstrate its superiority over state-of-the-art methods. Overall, our work represents a significant step towards creating intelligent agents capable of seamlessly integrating perception and reasoning.",1
"Person re-identification (re-ID) tackles the problem of matching person images with the same identity from different cameras. In practical applications, due to the differences in camera performance and distance between cameras and persons of interest, captured person images usually have various resolutions. We name this problem as Cross-Resolution Person Re-identification which brings a great challenge for matching correctly. In this paper, we propose a Deep High-Resolution Pseudo-Siamese Framework (PS-HRNet) to solve the above problem. Specifically, in order to restore the resolution of low-resolution images and make reasonable use of different channel information of feature maps, we introduce and innovate VDSR module with channel attention (CA) mechanism, named as VDSR-CA. Then we reform the HRNet by designing a novel representation head to extract discriminating features, named as HRNet-ReID. In addition, a pseudo-siamese framework is constructed to reduce the difference of feature distributions between low-resolution images and high-resolution images. The experimental results on five cross-resolution person datasets verify the effectiveness of our proposed approach. Compared with the state-of-the-art methods, our proposed PS-HRNet improves 3.4\%, 6.2\%, 2.5\%,1.1\% and 4.2\% at Rank-1 on MLR-Market-1501, MLR-CUHK03, MLR-VIPeR, MLR-DukeMTMC-reID, and CAVIAR datasets, respectively. Our code is available at \url{https://github.com/zhguoqing}.",0
"In recent years there have been significant advances in deep learning algorithms for computer vision tasks such as object detection and image classification, but person re-identification remains a challenging problem due to variations in pose, illumination, and occlusion across different cameras. Recently, high-resolution representations have been proposed that capture more detailed features and alleviate these difficulties, but they often require increased computational resources and storage space. This work presents a novel framework called Deep High-Resolution Representation Learning (DHRRL) which incorporates both fine-grained feature extraction from low-resolution inputs and progressive upsampling techniques to generate high-quality multi-scale feature maps at various resolutions. We evaluate our approach on two benchmark datasets, showing improved performance over state-of-the-art methods while maintaining efficient computation time and reduced memory usage. Our results demonstrate the effectiveness of DHRRL in tackling the cross-resolution person re-identification task under varying camera settings.",1
"Recently the crowd counting has received more and more attention. Especially the technology of high-density environment has become an important research content, and the relevant methods for the existence of extremely dense crowd are not optimal. In this paper, we propose a multi-level attentive Convolutional Neural Network (MLAttnCNN) for crowd counting. We extract high-level contextual information with multiple different scales applied in pooling, and use multi-level attention modules to enrich the characteristics at different layers to achieve more efficient multi-scale feature fusion, which is able to be used to generate a more accurate density map with dilated convolutions and a $1\times 1$ convolution. The extensive experiments on three available public datasets show that our proposed network achieves outperformance to the state-of-the-art approaches.",0
"Accurate crowd counting from video footage can have numerous applications such as resource allocation during disaster relief efforts. In this paper, we propose a multi-level attentive convolutional neural network architecture for crowd counting that addresses both local object density estimation and global context modeling using adaptive attention modules. This proposed method achieves state-of-the-art results on two benchmark datasets for crowd counting. Our approach outperforms existing methods by utilizing multiple levels of context and attention mechanisms within our deep learning framework. The contribution of this work lies in our innovative use of multi-level feature extraction and attention techniques for accurate crowd counting in complex urban environments. By improving accuracy in crowd counting, emergency responders can make more informed decisions regarding resource allocation and evacuation planning during crisis situations.",1
"Sleep staging is fundamental for sleep assessment and disease diagnosis. Although previous attempts to classify sleep stages have achieved high classification performance, several challenges remain open: 1) How to effectively extract salient waves in multimodal sleep data; 2) How to capture the multi-scale transition rules among sleep stages; 3) How to adaptively seize the key role of specific modality for sleep staging. To address these challenges, we propose SalientSleepNet, a multimodal salient wave detection network for sleep staging. Specifically, SalientSleepNet is a temporal fully convolutional network based on the $\rm U^2$-Net architecture that is originally proposed for salient object detection in computer vision. It is mainly composed of two independent $\rm U^2$-like streams to extract the salient features from multimodal data, respectively. Meanwhile, the multi-scale extraction module is designed to capture multi-scale transition rules among sleep stages. Besides, the multimodal attention module is proposed to adaptively capture valuable information from multimodal data for the specific sleep stage. Experiments on the two datasets demonstrate that SalientSleepNet outperforms the state-of-the-art baselines. It is worth noting that this model has the least amount of parameters compared with the existing deep neural network models.",0
"This research presents a novel multimodal approach to sleep staging using wave detection networks. We propose the use of a saliency map as a mechanism to model and predict the presence of important features within each time segment of the input signal. Our method leverages recent advances in deep learning and computer vision to create an automated system that can accurately classify different stages of sleep.  The proposed network architecture, called SalientSleepNet, integrates audio signals, electroencephalography (EEG) recordings, and body movement data into a single framework, enabling more accurate and robust predictions compared to traditional methods. To train the network, we utilize datasets collected from real subjects during clinical studies, ensuring that our results generalize well across diverse populations.  We evaluate the performance of SalientSleepNet on several benchmark datasets and demonstrate improved accuracy over state-of-the-art methods, particularly for the most challenging stages of sleep such as rapid eye movement (REM) and light sleep. In addition to improving overall classification performance, our method provides insightful attention maps highlighting regions of interest within each stage of sleep, facilitating further analysis by human experts.  Our work represents an essential step towards developing reliable and efficient tools for sleep medicine and has the potential to significantly impact clinical diagnosis, treatment planning, and public health. By providing a robust automatic sleep stager, this study helps enable larger-scale studies that require high-quality annotations while reducing the cost and burden associated with manual annotation. Further applications of our method span diverse domains where continuous physiological monitoring is crucial, including sleep disorder diagnostics, cognitive behavior studies, and neurology research.",1
"Recently a number of studies demonstrated impressive performance on diverse vision-language multi-modal tasks such as image captioning and visual question answering by extending the BERT architecture with multi-modal pre-training objectives. In this work we explore a broad set of multi-modal representation learning tasks in the medical domain, specifically using radiology images and the unstructured report. We propose Medical Vision Language Learner (MedViLL) which adopts a Transformer-based architecture combined with a novel multimodal attention masking scheme to maximize generalization performance for both vision-language understanding tasks (image-report retrieval, disease classification, medical visual question answering) and vision-language generation task (report generation). By rigorously evaluating the proposed model on four downstream tasks with two chest X-ray image datasets (MIMIC-CXR and Open-I), we empirically demonstrate the superior downstream task performance of MedViLL against various baselines including task-specific architectures.",0
"Title: Multi-Modal Learning For Medical Image And Text Processing  Abstract: Artificial intelligence has made significant strides in recent years thanks to advancements in pre-training techniques. In particular, vision-language models have proven effective at processing multimodal data from both images and text. These models have been used to solve problems such as image generation, caption generation, visual question answering, and more recently, multi-modal medical image analysis and natural language understanding. This paper explores how we can adapt these pre-trained models for use in medical environments by leveraging large amounts of available imaging and corresponding patient reports to create robust and accurate models that can handle various medical tasks. Our contributions focus on building new architectures and training schemes that enhance performance, introduce new capabilities, and provide improved diagnostics compared to previous methods. We demonstrate our approach through several case studies and experimental results, showing the effectiveness of our proposed methodology for a wide range of clinical applications. Note: You can replace ""This paper"" with any sentence you want if required. Also you might need to modify the title accordingly. Thank you! Let me know if there is anything else I can assist you with today.",1
"Point cloud semantic segmentation is a crucial task in 3D scene understanding. Existing methods mainly focus on employing a large number of annotated labels for supervised semantic segmentation. Nonetheless, manually labeling such large point clouds for the supervised segmentation task is time-consuming. In order to reduce the number of annotated labels, we propose a semi-supervised semantic point cloud segmentation network, named SSPC-Net, where we train the semantic segmentation network by inferring the labels of unlabeled points from the few annotated 3D points. In our method, we first partition the whole point cloud into superpoints and build superpoint graphs to mine the long-range dependencies in point clouds. Based on the constructed superpoint graph, we then develop a dynamic label propagation method to generate the pseudo labels for the unsupervised superpoints. Particularly, we adopt a superpoint dropout strategy to dynamically select the generated pseudo labels. In order to fully exploit the generated pseudo labels of the unsupervised superpoints, we furthermore propose a coupled attention mechanism for superpoint feature embedding. Finally, we employ the cross-entropy loss to train the semantic segmentation network with the labels of the supervised superpoints and the pseudo labels of the unsupervised superpoints. Experiments on various datasets demonstrate that our semi-supervised segmentation method can achieve better performance than the current semi-supervised segmentation method with fewer annotated 3D points. Our code is available at https://github.com/MMCheng/SSPC-Net.",0
"In this work we propose a semi-supervised method for semantic segmentation on point clouds which leverages both supervised data (annotations on labeled scans) as well as unlabeled scan pairs where one scan has pseudo-labels generated from weak labels on images. We introduce two variants, our simple variant that performs only cross modality alignment and learning through self-training, as well as our complete model variant that additionally utilizes adversarial training between synthetic image generation guided by the pseudo-labels, and cycle consistency loss. Our evaluations demonstrate state-of-the-art results on three benchmark datasets for semantic point cloud segmentation, with substantial improvements over other approaches. Additionally, we provide evidence of robustness of our model across different levels of annotation quality in real scenes, making it particularly suitable for applications where annotations may be limited or noisy. Our code and models will be made publicly available upon acceptance. The proposed SSPC-Net (Semi-Supervised Semantic 3D Point Cloud Segmentation Network) is a method that combines both supervised data (annotations on labeled scans) and unlabeled scan pairs with pseudo-labels derived from weak labels obtained from corresponding images. Two variations of the network are presented - a basic version using only cross modality alignment and self-training, and a more comprehensive variant employing additional components such as adversarial training for synthesizing artificial images and cycle consistency loss. Experiments showed improved performance compared to existing methods on three established benchmarks for point cloud semantic segmentation, including enhanced resilience against variable label accuracy encountered in real world environments. Researchers plan to release their models and source code upon approval.",1
"There has been a substantial amount of research involving computer methods and technology for the detection and recognition of diabetic foot ulcers (DFUs), but there is a lack of systematic comparisons of state-of-the-art deep learning object detection frameworks applied to this problem. DFUC2020 provided participants with a comprehensive dataset consisting of 2,000 images for training and 2,000 images for testing. This paper summarises the results of DFUC2020 by comparing the deep learning-based algorithms proposed by the winning teams: Faster R-CNN, three variants of Faster R-CNN and an ensemble method; YOLOv3; YOLOv5; EfficientDet; and a new Cascade Attention Network. For each deep learning method, we provide a detailed description of model architecture, parameter settings for training and additional stages including pre-processing, data augmentation and post-processing. We provide a comprehensive evaluation for each method. All the methods required a data augmentation stage to increase the number of images available for training and a post-processing stage to remove false positives. The best performance was obtained from Deformable Convolution, a variant of Faster R-CNN, with a mean average precision (mAP) of 0.6940 and an F1-Score of 0.7434. Finally, we demonstrate that the ensemble method based on different deep learning methods can enhanced the F1-Score but not the mAP.",0
"Title: Detecting Diabetic Foot Ulcers Using Deep Learning: An In-Depth Analysis  Diabetes mellitus is one of the most common chronic diseases worldwide that leads to complications such as foot ulcers. Early detection and diagnosis play a crucial role in preventing life-threatening conditions such as gangrene and amputation. With advancements in technology, deep learning has emerged as a powerful tool in medical image analysis. This comprehensive evaluation aimed to investigate the effectiveness of deep learning methods in detecting diabetic foot ulcers (DFUs).  A systematic literature search was conducted using electronic databases, including PubMed, Scopus, and Web of Science. A total of 28 studies were included in this review after applying specific selection criteria. These studies utilized different deep learning architectures like convolutional neural networks (CNNs), recurrent neural networks (RNNs), and fully connected neural networks (FCNs) for classification and segmentation tasks.  The results demonstrated that deep learning outperformed conventional image processing techniques in detecting DFUs by achieving high sensitivity, accuracy, precision, recall, F1 score, area under curve, and interclass correlation coefficients. Furthermore, the selected studies provided evidence on how deep learning models can learn subtle features from complex images such as color texture, tissue shape, lesion margins, etc., which are difficult to extract manually.  In conclusion, our findings suggest that deep learning holds great potential in improving clinical workflows and decision support systems for early identification of DFUs. Further research is required to develop reliable, efficient, cost-effective, and user-friendly tools incorporating deep learning algorithms that can assist healthcare professionals in making more accurate decisions. Additionally, prospective comparative studies involving multiple centers across diverse populations are warranted to assess the generalizability and scalability of these promising approaches in real-world scenarios.",1
"Channel pruning and tensor decomposition have received extensive attention in convolutional neural network compression. However, these two techniques are traditionally deployed in an isolated manner, leading to significant accuracy drop when pursuing high compression rates. In this paper, we propose a Collaborative Compression (CC) scheme, which joints channel pruning and tensor decomposition to compress CNN models by simultaneously learning the model sparsity and low-rankness. Specifically, we first investigate the compression sensitivity of each layer in the network, and then propose a Global Compression Rate Optimization that transforms the decision problem of compression rate into an optimization problem. After that, we propose multi-step heuristic compression to remove redundant compression units step-by-step, which fully considers the effect of the remaining compression space (i.e., unremoved compression units). Our method demonstrates superior performance gains over previous ones on various datasets and backbone architectures. For example, we achieve 52.9% FLOPs reduction by removing 48.4% parameters on ResNet-50 with only a Top-1 accuracy drop of 0.56% on ImageNet 2012.",0
In summary the paper shows that state of the art cnn architectures can benefit from a simple yet effective method which combines several recent advances. By using collaborations between convolutional blocks we can improve accuracy while reducing computational requirements such as operations count and parameter usage (size). Finally this enables deployment on embedded systems without a significant drop in performance compared against models designed specifically for these platforms. We hope our work inspires further research into making accurate deep learning models deployable across all devices and computing infrastructures without loss of accuracy. We plan to provide code for both training the proposed methods and deployment to mobile devices upon acceptance of this manuscript. If you have any questions please feel free to ask!,1
"Recently, learning a model that generalizes well on out-of-distribution (OOD) data has attracted great attention in the machine learning community. In this paper, after defining OOD generalization via Wasserstein distance, we theoretically show that a model robust to input perturbation generalizes well on OOD data. Inspired by previous findings that adversarial training helps improve input-robustness, we theoretically show that adversarially trained models have converged excess risk on OOD data, and empirically verify it on both image classification and natural language understanding tasks. Besides, in the paradigm of first pre-training and then fine-tuning, we theoretically show that a pre-trained model that is more robust to input perturbation provides a better initialization for generalization on downstream OOD data. Empirically, after fine-tuning, this better-initialized model from adversarial pre-training also has better OOD generalization.",0
"This paper presents a new approach to improve out-of-distribution (OOD) generalization using adversarial training and pre-training techniques. We propose to first train a model on in-distribution data using standard supervised learning methods. Next, we generate synthetic OOD examples by applying adversarial perturbations to the original data during training. These OOD examples act as ""virtual"" OOD samples that provide additional robustness against future unseen outliers. Finally, we fine-tune the model on both real and generated OOD examples using a combination of supervised and self-supervised losses. Experimental results show significant improvements in OOD generalization compared to previous state-of-the-art methods across multiple datasets and models. Our approach is simple yet effective and can be easily adapted to different settings.",1
"Privacy protection on human biological information has drawn increasing attention in recent years, among which face anonymization plays an importance role. We propose a novel approach which protects identity information of facial images from leakage with slightest modification. Specifically, we disentangle identity representation from other facial attributes leveraging the power of generative adversarial networks trained on a conditional multi-scale reconstruction (CMR) loss and an identity loss. We evaulate the disentangle ability of our model, and propose an effective method for identity anonymization, namely Anonymous Identity Generation (AIG), to reach the goal of face anonymization meanwhile maintaining similarity to the original image as much as possible. Quantitative and qualitative results demonstrate our method's superiority compared with the SOTAs on both visual quality and anonymization success rate.",0
"In this work we propose a novel approach to protecting user privacy on social media platforms through manipulation of decoupled identity representation (DIR). By combining advances in computer vision techniques such as face deblurring with DIR, we can effectively hide usersâ€™ identities while still preserving high levels of image quality. This allows individuals greater control over their personal information online without sacrificing engagement with content. Our results show that our method achieves state-of-the-art performance compared to other anonymization methods. We believe this technique has great potential for real world applications in both industry and research settings.",1
"With the explosive growth of video data, video summarization, which attempts to seek the minimum subset of frames while still conveying the main story, has become one of the hottest topics. Nowadays, substantial achievements have been made by supervised learning techniques, especially after the emergence of deep learning. However, it is extremely expensive and difficult to collect human annotation for large-scale video datasets. To address this problem, we propose a convolutional attentive adversarial network (CAAN), whose key idea is to build a deep summarizer in an unsupervised way. Upon the generative adversarial network, our overall framework consists of a generator and a discriminator. The former predicts importance scores for all frames of a video while the latter tries to distinguish the score-weighted frame features from original frame features. Specifically, the generator employs a fully convolutional sequence network to extract global representation of a video, and an attention-based network to output normalized importance scores. To learn the parameters, our objective function is composed of three loss functions, which can guide the frame-level importance score prediction collaboratively. To validate this proposed method, we have conducted extensive experiments on two public benchmarks SumMe and TVSum. The results show the superiority of our proposed method against other state-of-the-art unsupervised approaches. Our method even outperforms some published supervised approaches.",0
"Abstract Video summarization is an important task that involves generating a condensed version of a video while preserving the essential information. Current approaches mostly rely on supervised learning which requires large amounts of labeled data and manual annotations. This study presents an unsupervised approach based on a convolutional attentive adversarial network (CAAN). CAAN consists of two competing networks: a generator network that produces summary frames, and a discriminator network that predicts whether these frames belong to the original video or the generated ones. The generator learns to generate realistic frames through feature matching whereas the discriminator tries to distinguish them from real frames by exploiting their internal inconsistencies. By optimizing both objectives jointly using an alternating adversarial training scheme, CAAN can effectively capture the spatio-temporal features of videos without any explicit guidance or prior knowledge about the content structure. Experiments conducted on benchmark datasets demonstrate that our proposed model achieves state-of-the-art performance, outperforming other unsupervised methods by a significant margin while producing high quality summaries under diverse evaluation metrics such as FID score, precision/recall curve, etc. These results clearly indicate the effectiveness of our method in terms of providing reliable solutions to challenges faced by current approaches in dealing with complex videos. Our work has great potential applications in surveillance systems, entertainment industry, medical diagnosis, etc., where the efficient analysis of massive volumes of video data plays crucial roles. Overall, our contributions provide new perspectives for future research direction towards fully unsupervis",1
"Existing video polyp segmentation (VPS) models typically employ convolutional neural networks (CNNs) to extract features. However, due to their limited receptive fields, CNNs can not fully exploit the global temporal and spatial information in successive video frames, resulting in false-positive segmentation results. In this paper, we propose the novel PNS-Net (Progressively Normalized Self-attention Network), which can efficiently learn representations from polyp videos with real-time speed (~140fps) on a single RTX 2080 GPU and no post-processing. Our PNS-Net is based solely on a basic normalized self-attention block, equipping with recurrence and CNNs entirely. Experiments on challenging VPS datasets demonstrate that the proposed PNS-Net achieves state-of-the-art performance. We also conduct extensive experiments to study the effectiveness of the channel split, soft-attention, and progressive learning strategy. We find that our PNS-Net works well under different settings, making it a promising solution to the VPS task.",0
"In recent years, video polyp segmentation has been gaining increasing attention due to its potential applications in clinical diagnosis and therapy planning for gastrointestinal diseases such as colon polyps. However, accurate video polyp segmentation remains challenging due to high variability in polyp appearance, motion artifacts, poor contrast, and low frame rates. To address these difficulties, we propose a novel deep learning framework called Progressively Normalized Self-Attention Network (PSANet) that effectively integrates multi-scale feature representation, channel attention, spatial attention, and temporal attention for video polyp segmentation. PSANet outperforms existing state-of-the-art methods by achieving significant improvement on two publicly available datasets: ETIS benchmark dataset and CVC-ClinicDB dataset. Our experimental results show that PSANet attains higher precision, recall, Dice coefficient, and faster convergence rate compared to other approaches. Furthermore, our ablation studies demonstrate the effectiveness and contribution of each component in PSANet. This work provides a promising approach towards automatic video polyp segmentation which can potentially assist physicians in improving diagnostic accuracy and reducing intervention risks.",1
"Graph representation learning has attracted increasing research attention. However, most existing studies fuse all structural features and node attributes to provide an overarching view of graphs, neglecting finer substructures' semantics, and suffering from interpretation enigmas. This paper presents a novel hierarchical subgraph-level selection and embedding based graph neural network for graph classification, namely SUGAR, to learn more discriminative subgraph representations and respond in an explanatory way. SUGAR reconstructs a sketched graph by extracting striking subgraphs as the representative part of the original graph to reveal subgraph-level patterns. To adaptively select striking subgraphs without prior knowledge, we develop a reinforcement pooling mechanism, which improves the generalization ability of the model. To differentiate subgraph representations among graphs, we present a self-supervised mutual information mechanism to encourage subgraph embedding to be mindful of the global graph structural properties by maximizing their mutual information. Extensive experiments on six typical bioinformatics datasets demonstrate a significant and consistent improvement in model quality with competitive performance and interpretability.",0
"Sugar: Subgraph Neural Network with Reinforcement Pooling and Self-supervised Mutual Information Mechanism. We present our model, Sugar, which utilizes graph neural networks (GNNS) combined with reinforcement pooling and self-supervision techniques such as mutual information maximization to improve performance on complex tasks such as node classification and link prediction. GNNS have proven effective in representing highly irregular data types found in graphs. However, these models struggle with overfitting and require large amounts of labeled training data, limiting their applicability to real-world scenarios. Our approach addresses these challenges by introducing several innovations. Firstly, we introduce reinforcement pooling, where instead of simply aggregating features at every layer to create a final representation, each layer selects important nodes or subgraphs based on learned policies driven by reward signals from the task. This allows us to reduce computational complexity while capturing more meaningful representations. Secondly, we incorporate self-supervision through maximizing mutual information between different layers and output logits. By aligning lower-level features with higher-level concepts, we can achieve better generalization and transfer learning across domains without requiring additional labels. Finally, we empirically demonstrate the effectiveness of Sugar on various benchmark datasets and show that it achieves competitive results compared to state-of-the-art methods. With its ability to operate under limited label supervision and scalability across diverse application areas, Sugar holds great potential for becoming the backbone architecture for future GNN research.",1
"Graph neural networks (GNN) have been successful in many fields, and derived various researches and applications in real industries. However, in some privacy sensitive scenarios (like finance, healthcare), training a GNN model centrally faces challenges due to the distributed data silos. Federated learning (FL) is a an emerging technique that can collaboratively train a shared model while keeping the data decentralized, which is a rational solution for distributed GNN training. We term it as federated graph learning (FGL). Although FGL has received increasing attention recently, the definition and challenges of FGL is still up in the air. In this position paper, we present a categorization to clarify it. Considering how graph data are distributed among clients, we propose four types of FGL: inter-graph FL, intra-graph FL and graph-structured FL, where intra-graph is further divided into horizontal and vertical FGL. For each type of FGL, we make a detailed discussion about the formulation and applications, and propose some potential challenges.",0
"This paper presents a new approach called federated graph learning (FGL), which can learn from decentralized data sources using graphs. FGL provides several benefits over traditional centralized machine learning methods such as improved scalability, privacy preservation, and reduced communication overheads. The proposed method leverages advanced graph partitioning techniques that enable distributed training on large datasets while maintaining low computational requirements. This allows for efficient knowledge transfer among different domains, leading to better generalization performance compared to other non-cooperative methods. In addition, FGL utilizes asynchronous message passing based on Bayesian updates, enabling parallel processing across multiple devices without compromising accuracy. Experimental results demonstrate that our approach outperforms state-of-the-art methods in various real-world tasks involving graphs, including node classification and link prediction. This paper makes three main contributions: 1) introducing FGL, a novel framework capable of harnessing the power of decentralized data; 2) proposing effective graph partitioning strategies designed specifically for our model; and 3) providing extensive experimental evaluations validating FGL's effectiveness across diverse domains. Overall, we believe FGL offers a compelling solution for addressing modern challenges related to big data analytics and artificial intelligence at scale. By fostering collaboration between decentralized parties through innovative graph-based methods, FGL sets a promising direction for future research in machine learning and beyond.",1
"Federated Learning (FL) has recently received a lot of attention for large-scale privacy-preserving machine learning. However, high communication overheads due to frequent gradient transmissions decelerate FL. To mitigate the communication overheads, two main techniques have been studied: (i) local update of weights characterizing the trade-off between communication and computation and (ii) gradient compression characterizing the trade-off between communication and precision. To the best of our knowledge, studying and balancing those two trade-offs jointly and dynamically while considering their impacts on convergence has remained unresolved even though it promises significantly faster FL. In this paper, we first formulate our problem to minimize learning error with respect to two variables: local update coefficients and sparsity budgets of gradient compression who characterize trade-offs between communication and computation/precision, respectively. We then derive an upper bound of the learning error in a given wall-clock time considering the interdependency between the two variables. Based on this theoretical analysis, we propose an enhanced FL scheme, namely Fast FL (FFL), that jointly and dynamically adjusts the two variables to minimize the learning error. We demonstrate that FFL consistently achieves higher accuracies faster than similar schemes existing in the literature.",0
"In this paper, we present a novel approach to fast federated learning that addresses two key challenges: model performance and communication efficiency. We propose a methodology that balances these trade-offs using carefully selected hyperparameters and fine-grained control over each iteration's computation and communication behavior. Our framework allows clients to work together to train models without significantly impacting their device resources or network connectivity. Through extensive experiments, we demonstrate the effectiveness of our approach, achieving state-of-the-art results while reducing both training time and computational cost compared to traditional methods. This research offers significant potential benefits for real-world deployment scenarios, where efficient and effective distributed machine learning is essential.",1
"As global trends are shifting towards data-driven industries, the demand for automated algorithms that can convert digital images of scanned documents into machine readable information is rapidly growing. Besides the opportunity of data digitization for the application of data analytic tools, there is also a massive improvement towards automation of processes, which previously would require manual inspection of the documents. Although the introduction of optical character recognition technologies mostly solved the task of converting human-readable characters from images into machine-readable characters, the task of extracting table semantics has been less focused on over the years. The recognition of tables consists of two main tasks, namely table detection and table structure recognition. Most prior work on this problem focuses on either task without offering an end-to-end solution or paying attention to real application conditions like rotated images or noise artefacts inside the document image. Recent work shows a clear trend towards deep learning approaches coupled with the use of transfer learning for the task of table structure recognition due to the lack of sufficiently large datasets. In this paper we present a multistage pipeline named Multi-Type-TD-TSR, which offers an end-to-end solution for the problem of table recognition. It utilizes state-of-the-art deep learning models for table detection and differentiates between 3 different types of tables based on the tables' borders. For the table structure recognition we use a deterministic non-data driven algorithm, which works on all table types. We additionally present two algorithms. One for unbordered tables and one for bordered tables, which are the base of the used table structure recognition algorithm. We evaluate Multi-Type-TD-TSR on the ICDAR 2019 table structure recognition dataset and achieve a new state-of-the-art.",0
"This paper presents a multi-stage pipeline approach for extracting tables from document images. Our method combines state-of-the-art table detection techniques with novel table structure recognition algorithms to accurately identify and parse tabular data from complex documents. We begin by performing Optical Character Recognition (OCR) on scanned pages, which provides us with textual representations that can be used as input for our model. Next, we use machine learning methods to detect potential table regions within each page image based on features such as block layouts, cell boundaries, and row/column structures. Once these regions have been identified, we apply additional rules and heuristics to further refine their locations. Finally, our system utilizes advanced computer vision techniques to analyze the extracted tables and determine their underlying structure, including column headers, rows, and cells. The resulting structured representation allows users to easily manipulate and visualize the data contained within each table. Experiments performed on several benchmark datasets demonstrate the effectiveness of our proposed approach in terms of both accuracy and speed, making it a valuable tool for organizations seeking to automate the extraction of structured data from large collections of legacy documents. Overall, our work represents a significant step forward in the field of document analysis, paving the way for more intelligent systems capable of automatically processing complex real-world documents and unlocking new opportunities for knowledge discovery and decision support across many different domains.",1
"Monocular 3D human pose estimation from a single RGB image has received a lot attentions in the past few year. Pose inference models with competitive performance however require supervision with 3D pose ground truth data or at least known pose priors in their target domain. Yet, these data requirements in many real-world applications with data collection constraints may not be achievable. In this paper, we present a heuristic weakly supervised solution, called HW-HuP to estimate 3D human pose in contexts that no ground truth 3D data is accessible, even for fine-tuning. HW-HuP learns partial pose priors from public 3D human pose datasets and uses easy-to-access observations from the target domain to iteratively estimate 3D human pose and shape in an optimization and regression hybrid cycle. In our design, depth data as an auxiliary information is employed as weak supervision during training, yet it is not needed for the inference. We evaluate HW-HuP performance qualitatively on datasets of both in-bed human and infant poses, where no ground truth 3D pose is provided neither any target prior. We also test HW-HuP performance quantitatively on a publicly available motion capture dataset against the 3D ground truth. HW-HuP is also able to be extended to other input modalities for pose estimation tasks especially under adverse vision conditions, such as occlusion or full darkness. On the Human3.6M benchmark, HW-HuP shows 104.1mm in MPJPE and 50.4mm in PA MPJPE, comparable to the existing state-of-the-art approaches that benefit from full 3D pose supervision.",0
"This work presents a heuristic weakly supervised approach for estimating 3D human pose in novel contexts where no ground truth data is available. By leveraging monocular images, depth maps, and sparse pixel-level annotation, we develop a framework that can learn complex body configurations and estimate 3D poses robustly across different scenarios. Our method builds on previous state-of-the-art approaches by introducing several innovations including: i) a multi-branch network architecture that fuses information from multiple modalities; ii) a geometric consistency loss term that regularizes predictions at the joint level; iii) a self-supervised pretraining stage that improves performance under limited data conditions. Experiments demonstrate the effectiveness of our approach in challenging domains such as unseen environments, outdoor scenes, and new actions, achieving competitive results compared to fully-supervised methods trained on large datasets with pixel-wise annotations. Overall, this study shows promise in enabling robust and scalable 3D pose estimation through minimal supervision and transference learning.",1
"The development of practical applications, such as autonomous driving and robotics, has brought increasing attention to 3D point cloud understanding. While deep learning has achieved remarkable success on image-based tasks, there are many unique challenges faced by deep neural networks in processing massive, unstructured and noisy 3D points. To demonstrate the latest progress of deep learning for 3D point cloud understanding, this paper summarizes recent remarkable research contributions in this area from several different directions (classification, segmentation, detection, tracking, flow estimation, registration, augmentation and completion), together with commonly used datasets, metrics and state-of-the-art performances. More information regarding this survey can be found at: https://github.com/SHI-Labs/3D-Point-Cloud-Learning.",0
"This survey provides a comprehensive overview of recent developments in deep learning techniques for understanding 3D point clouds. We present a detailed analysis of popular architectures used in state-of-the-art approaches for processing large-scale 3D data. The authors discuss promising new applications of these models in fields such as computer vision, robotics, and virtual reality. In addition, we review open challenges and future directions in the field, highlighting areas that require further research and innovation. Our goal is to provide readers with a thorough understanding of cutting-edge developments in deep learning for 3D point cloud understanding and inspire ongoing progress in this exciting area of study.",1
"Recently, satellites with high temporal resolution have fostered wide attention in various practical applications. Due to limitations of bandwidth and hardware cost, however, the spatial resolution of such satellites is considerably low, largely limiting their potentials in scenarios that require spatially explicit information. To improve image resolution, numerous approaches based on training low-high resolution pairs have been proposed to address the super-resolution (SR) task. Despite their success, however, low/high spatial resolution pairs are usually difficult to obtain in satellites with a high temporal resolution, making such approaches in SR impractical to use. In this paper, we proposed a new unsupervised learning framework, called ""MIP"", which achieves SR tasks without low/high resolution image pairs. First, random noise maps are fed into a designed generative adversarial network (GAN) for reconstruction. Then, the proposed method converts the reference image to latent space as the migration image prior. Finally, we update the input noise via an implicit method, and further transfer the texture and structured information from the reference image. Extensive experimental results on the Draper dataset show that MIP achieves significant improvements over state-of-the-art methods both quantitatively and qualitatively. The proposed MIP is open-sourced at http://github.com/jiaming-wang/MIP.",0
"This paper presents a novel method for unsupervised remote sensing super-resolution using migration image prior (MIP) as guidance. In traditional remote sensing imagery, small pixel size can lead to loss of spatial resolution, which limits the ability to identify fine details on the ground. To address this issue, we propose a new algorithm that utilizes MIP to guide the reconstruction process. Our approach takes advantage of the fact that high-frequency components often exist in multiple low-resolution images taken at different times and angles. By leveraging these features, our model effectively improves upon existing methods, achieving state-of-the-art results without relying on labeled data. We evaluate our proposed framework through extensive experiments and demonstrate its effectiveness across various use cases, including satellite imagery and aerial photography. Overall, our work represents an important step forward towards enabling unsupervised remote sensing super-resolution using MIP, paving the way for improved visualization and analysis in many applications such as agriculture, surveying, urban planning, environmental monitoring, and military intelligence gathering.",1
"The bus system is a critical component of sustainable urban transportation. However, due to the significant uncertainties in passenger demand and traffic conditions, bus operation is unstable in nature and bus bunching has become a common phenomenon that undermines the reliability and efficiency of bus services. Despite recent advances in multi-agent reinforcement learning (MARL) on traffic control, little research has focused on bus fleet control due to the tricky asynchronous characteristic -- control actions only happen when a bus arrives at a bus stop and thus agents do not act simultaneously. In this study, we formulate route-level bus fleet control as an asynchronous multi-agent reinforcement learning (ASMR) problem and extend the classical actor-critic architecture to handle the asynchronous issue. Specifically, we design a novel critic network to effectively approximate the marginal contribution for other agents, in which graph attention neural network is used to conduct inductive learning for policy evaluation. The critic structure also helps the ego agent optimize its policy more efficiently. We evaluate the proposed framework on real-world bus services and actual passenger demand derived from smart card data. Our results show that the proposed model outperforms both traditional headway-based control methods and existing MARL methods.",0
"Abstract: This paper proposes a novel approach to reducing bus bunching using asynchronous multi-agent reinforcement learning. Traditional methods relying on fixed schedules have proven insufficient due to their limited ability to adapt to real-time traffic conditions. Our method addresses this challenge by allowing buses to make decisions autonomously based on local observations and peer interactions. We evaluate our model through simulations that show significant improvements over current practices and highlight the effectiveness of our algorithm under different traffic scenarios. The findings have important implications for public transportation systems worldwide. Keywords: bus bunching, multi-agent reinforcement learning, asynchronous, simulation (More than 300 words.)",1
"Significant progress on the crowd counting problem has been achieved by integrating larger context into convolutional neural networks (CNNs). This indicates that global scene context is essential, despite the seemingly bottom-up nature of the problem. This may be explained by the fact that context knowledge can adapt and improve local feature extraction to a given scene. In this paper, we therefore investigate the role of global context for crowd counting. Specifically, a pure transformer is used to extract features with global information from overlapping image patches. Inspired by classification, we add a context token to the input sequence, to facilitate information exchange with tokens corresponding to image patches throughout transformer layers. Due to the fact that transformers do not explicitly model the tried-and-true channel-wise interactions, we propose a token-attention module (TAM) to recalibrate encoded features through channel-wise attention informed by the context token. Beyond that, it is adopted to predict the total person count of the image through regression-token module (RTM). Extensive experiments demonstrate that our method achieves state-of-the-art performance on various datasets, including ShanghaiTech, UCF-QNRF, JHU-CROWD++ and NWPU. On the large-scale JHU-CROWD++ dataset, our method improves over the previous best results by 26.9% and 29.9% in terms of MAE and MSE, respectively.",0
"This paper presents a novel approach using transformer architectures in conjunction with convolutional neural networks (CNNs) for crowd counting tasks. We evaluate our method on popular datasets, showing competitive performance compared to state-of-the art approaches. Our main contributions include: (1) utilizing multi-scale feature extraction from CNNs coupled with self attention mechanisms introduced by transformers; (2) demonstrating that our proposed architecture achieves better results than traditional methods while maintaining computational efficiency; and (3) providing insights into future directions where our framework could potentially lead to further improvements.",1
"We tackle the problem of place recognition from point cloud data and introduce a self-attention and orientation encoding network (SOE-Net) that fully explores the relationship between points and incorporates long-range context into point-wise local descriptors. Local information of each point from eight orientations is captured in a PointOE module, whereas long-range feature dependencies among local descriptors are captured with a self-attention unit. Moreover, we propose a novel loss function called Hard Positive Hard Negative quadruplet loss (HPHN quadruplet), that achieves better performance than the commonly used metric learning loss. Experiments on various benchmark datasets demonstrate superior performance of the proposed network over the current state-of-the-art approaches. Our code is released publicly at https://github.com/Yan-Xia/SOE-Net.",0
"Title: ""SOE-Net: A Self-Attention and Orientation Encoding Network for Improved Point Cloud Based Place Recognition""  Abstract: This research introduces SOE-Net, a novel approach for place recognition using point cloud data generated from LiDAR sensors on autonomous vehicles. The proposed model utilizes self-attention mechanisms to selectively focus on relevant features in the point cloud while disregarding others. Additionally, orientation encoding techniques are used to encode the three-dimensional structure of the environment into the network, which improves performance compared to traditional methods that only rely on geometric information. Through comprehensive experiments on public datasets, we demonstrate that SOE-Net outperforms state-of-the-art approaches by achieving higher accuracy under challenging conditions such as dynamic environments, variable viewpoints, and different sensor configurations. Overall, our results showcase the effectiveness of incorporating self-attention and orientation encoding within a deep learning framework for solving computer vision problems arising in robotics applications.",1
"The development of intelligent tutoring system has greatly influenced the way students learn and practice, which increases their learning efficiency. The intelligent tutoring system must model learners' mastery of the knowledge before providing feedback and advices to learners, so one class of algorithm called ""knowledge tracing"" is surely important. This paper proposed Deep Self-Attentive Knowledge Tracing (DSAKT) based on the data of PTA, an online assessment system used by students in many universities in China, to help these students learn more efficiently. Experimentation on the data of PTA shows that DSAKT outperforms the other models for knowledge tracing an improvement of AUC by 2.1% on average, and this model also has a good performance on the ASSIST dataset.",0
"Abstract: Knowledge tracing is a technique used in education technology to estimate student proficiency levels on different skills based on their interactions with educational material. Existing knowledge tracing methods mainly rely on statistical models that assume linear relationships between studentsâ€™ mastery levels at adjacent time steps which may limit their accuracy. In recent years deep learning has emerged as a promising approach to model nonlinear patterns in complex data such as student interaction logs in online courses. This work presents an application of self-attention mechanism into knowledge tracing framework by employing Transformer architecture. Our experiment results show that using self attention improves accuracy over traditional knowledge tracing algorithms in terms of predicting skill proficiencies across multiple tasks including maths, chemistry, physics and biology domains. Additionally, we investigate the impact of model size and pretraining on our method and discuss possible future directions for incorporating external features to improve performance further.",1
"Twitter is currently a popular online social media platform which allows users to share their user-generated content. This publicly-generated user data is also crucial to healthcare technologies because the discovered patterns would hugely benefit them in several ways. One of the applications is in automatically discovering mental health problems, e.g., depression. Previous studies to automatically detect a depressed user on online social media have largely relied upon the user behaviour and their linguistic patterns including user's social interactions. The downside is that these models are trained on several irrelevant content which might not be crucial towards detecting a depressed user. Besides, these content have a negative impact on the overall efficiency and effectiveness of the model. To overcome the shortcomings in the existing automatic depression detection methods, we propose a novel computational framework for automatic depression detection that initially selects relevant content through a hybrid extractive and abstractive summarization strategy on the sequence of all user tweets leading to a more fine-grained and relevant content. The content then goes to our novel deep learning framework comprising of a unified learning machinery comprising of Convolutional Neural Network (CNN) coupled with attention-enhanced Gated Recurrent Units (GRU) models leading to better empirical performance than existing strong baselines.",0
"Depression is a pervasive mental health disorder that affects millions of individuals worldwide. With the proliferation of social media platforms, there has been growing interest in using online data as a source for detecting depression. In this paper, we present ""DepressionNet,"" a novel deep learning framework designed specifically for depression detection on social media data. Our model utilizes state-of-the-art techniques from natural language processing, computer vision, and graph neural networks to extract meaningful features from user posts, images, and interactions. We then employ a summarization technique to distill these features into a concise and informative representation of each user's mental state. Experimental results on three benchmark datasets demonstrate the superiority of our approach over baseline models and prior work in terms of accuracy, robustness, and interpretability. Overall, DepressionNet represents a significant step forward in automating the identification of depression on social media and highlights the potential benefits of integrating multiple modalities and sources of information.",1
"Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions. Although substantial progresses have been made in automatic FER in the past few decades, previous studies are mainly designed for lab-controlled FER. Real-world occlusions, variant head poses and other issues definitely increase the difficulty of FER on account of these information-deficient regions and complex backgrounds. Different from previous pure CNNs based methods, we argue that it is feasible and practical to translate facial images into sequences of visual words and perform expression recognition from a global perspective. Therefore, we propose Convolutional Visual Transformers to tackle FER in the wild by two main steps. First, we propose an attentional selective fusion (ASF) for leveraging the feature maps generated by two-branch CNNs. The ASF captures discriminative information by fusing multiple features with global-local attention. The fused feature maps are then flattened and projected into sequences of visual words. Second, inspired by the success of Transformers in natural language processing, we propose to model relationships between these visual words with global self-attention. The proposed method are evaluated on three public in-the-wild facial expression datasets (RAF-DB, FERPlus and AffectNet). Under the same settings, extensive experiments demonstrate that our method shows superior performance over other methods, setting new state of the art on RAF-DB with 88.14%, FERPlus with 88.81% and AffectNet with 61.85%. We also conduct cross-dataset evaluation on CK+ show the generalization capability of the proposed method.",0
"Advances in deep learning have revolutionized computer vision by providing accurate, automated solutions for challenging tasks such as facial expression recognition (FER). However, existing FER methods suffer from limitations due to their reliance on hand-engineered features and heuristics that fail to capture complex visual patterns. To address these issues, we propose a novel end-to-end convolutional transformer architecture called Robust Facial Expression Recognizer (RuFER) that utilizes self-attention mechanisms to learn discriminative representations directly from raw image data. RuFER outperforms state-of-the-art FER systems across several benchmark datasets while requiring fewer model parameters and achieving faster inference speeds. Our contributions include: i) introducing self-attention based processing in FER models; ii) designing new attention heads tailored towards spatial and temporal information integration; iii) creating more efficient transformer architectures through data augmentation techniques; iv) evaluating the efficacy of our method using standard evaluation protocols and cross-dataset testing. These findings pave the way for improved real-world deployment of FER systems in applications ranging from mental health monitoring to market research analysis. This study sets forth a foundation for future exploration into advanced deep learning architectures designed specifically for high-level feature extraction in vision tasks.",1
"Automated image captioning is one of the applications of Deep Learning which involves fusion of work done in computer vision and natural language processing, and it is typically performed using Encoder-Decoder architectures. In this project, we have implemented and experimented with various flavors of multi-modal image captioning networks where ResNet101, DenseNet121 and VGG19 based CNN Encoders and Attention based LSTM Decoders were explored. We have studied the effect of beam size and the use of pretrained word embeddings and compared them to baseline CNN encoder and RNN decoder architecture. The goal is to analyze the performance of each approach using various evaluation metrics including BLEU, CIDEr, ROUGE and METEOR. We have also explored model explainability using Visual Attention Maps (VAM) to highlight parts of the images which has maximum contribution for predicting each word of the generated caption.",0
This should summarize what the papers says without revealing any results,1
"In this paper, we present an attention-guided deformable convolutional network for hand-held multi-frame high dynamic range (HDR) imaging, namely ADNet. This problem comprises two intractable challenges of how to handle saturation and noise properly and how to tackle misalignments caused by object motion or camera jittering. To address the former, we adopt a spatial attention module to adaptively select the most appropriate regions of various exposure low dynamic range (LDR) images for fusion. For the latter one, we propose to align the gamma-corrected images in the feature-level with a Pyramid, Cascading and Deformable (PCD) alignment module. The proposed ADNet shows state-of-the-art performance compared with previous methods, achieving a PSNR-$l$ of 39.4471 and a PSNR-$\mu$ of 37.6359 in NTIRE 2021 Multi-Frame HDR Challenge.",0
"This research proposes a novel deep learning architecture for high dynamic range (HDR) imaging called ADNet, which stands for attention-guided deformable convolutional network. HDR images have a wide range of luminance values that exceed the capacity of traditional cameras and displays, resulting in lossy compression and tone mapping techniques. Existing methods for processing HDR images typically use handcrafted features and heuristics that may not accurately capture all aspects of the image. In contrast, ADNet uses a combination of attention mechanisms and deformable convolutions to adaptively learn feature representations that better align with the structure of the scene. Attention allows the model to focus on relevant regions of the image, while deformable convolutions can handle nonrigid transformations such as motion or varying lighting conditions. We evaluate our approach on two benchmark datasets, showing that ADNet outperforms state-of-the-art methods in terms of both quantitative metrics and visual quality. Our work demonstrates the potential of using deep learning techniques for HDR imaging and opens up new possibilities for future research in this area.",1
"Due to the powerful learning ability on high-rank and non-linear features, deep neural networks (DNNs) are being applied to data mining and machine learning in various fields, and exhibit higher discrimination performance than conventional methods. However, the applications based on DNNs are rare in enterprise credit rating tasks because most of DNNs employ the ""end-to-end"" learning paradigm, which outputs the high-rank representations of objects and predictive results without any explanations. Thus, users in the financial industry cannot understand how these high-rank representations are generated, what do they mean and what relations exist with the raw inputs. Then users cannot determine whether the predictions provided by DNNs are reliable, and not trust the predictions providing by such ""black box"" models. Therefore, in this paper, we propose a novel network to explicitly model the enterprise credit rating problem using DNNs and attention mechanisms. The proposed model realizes explainable enterprise credit ratings. Experimental results obtained on real-world enterprise datasets verify that the proposed approach achieves higher performance than conventional methods, and provides insights into individual rating results and the reliability of model training.",0
"In recent years, deep learning has been widely used to solve problems related to finance, such as credit scoring. However, traditional approaches often lack interpretability and can lead to black box models that make it difficult for domain experts to understand their predictions. Therefore, explainability remains an important challenge for financial applications based on machine learning techniques. This paper proposes a new method called ""Explainable Enterprise Credit Rating via Deep Feature Crossing Network"" (EECR) that addresses these limitations by incorporating feature interactions into model building processes.  The proposed EECR framework consists of two main components: a deep neural network that predicts loan defaults and another model that focuses on mapping loan features to latent space representations. By cross-referencing loan default probabilities and corresponding feature vectors from both networks, our approach enables users to visualize the impact of different factors on prediction results. These insights provide valuable cues for practitioners to better evaluate lending risks and develop more informed strategies for underwriting loans. Moreover, since our framework operates directly on numerical data without relying on preprocessing steps like transforming categorical variables, we avoid introducing additional noise or ambiguity associated with manual encoding methods commonly adopted in existing works. Our experiments demonstrate superior performance over alternative models across multiple datasets while maintaining competitive accuracy, demonstrating the promise of our algorithm. Lastly, extensive evaluations using three benchmark sets further verify the effectiveness and validity of our approach towards explaining and interpreting enterprise credit ratings derived through deep learning architectures. We believe that this work provides a foundation for future research aimed at creating even more transparent artificial intelligence systems capable of supporting human decision making tasks in various domains",1
"Puck localization is an important problem in ice hockey video analytics useful for analyzing the game, determining play location, and assessing puck possession. The problem is challenging due to the small size of the puck, excessive motion blur due to high puck velocity and occlusions due to players and boards. In this paper, we introduce and implement a network for puck localization in broadcast hockey video. The network leverages expert NHL play-by-play annotations and uses temporal context to locate the puck. Player locations are incorporated into the network through an attention mechanism by encoding player positions with a Gaussian-based spatial heatmap drawn at player positions. Since event occurrence on the rink and puck location are related, we also perform event recognition by augmenting the puck localization network with an event recognition head and training the network through multi-task learning. Experimental results demonstrate that the network is able to localize the puck with an AUC of $73.1 \%$ on the test set. The puck location can be inferred in 720p broadcast videos at $5$ frames per second. It is also demonstrated that multi-task learning with puck location improves event recognition accuracy.",0
"Recently developed deep learning approaches have enabled efficient and effective puck tracking solutions even under difficult imaging conditions such as occlusions or low resolution footage. In contrast to state-of-the-art methods which often rely on manually defined features, our approach uses Convolutional Neural Networks (CNN) trained end-to-end from raw pixel data to learn representations that can distinguish players, referees and other relevant objects within sport events like ice hockey games captured by regular video cameras. Moreover, the same network is used for both object detection and pose estimation tasks simultaneously by predicting bounding boxes along with their corresponding 2D coordinates as well as 3D joint angles. We conduct comprehensive experiments using publicly available datasets to showcase the efficacy of our methodology which consistently outperforms contemporary techniques while maintaining real-time performance even when dealing with high definition streams. Finally, we demonstrate how our system facilitates the implementation of novel applications relying on accurate knowledge extraction from unconstrained multimedia material.",1
"The performance of object detection, to a great extent, depends on the availability of large annotated datasets. To alleviate the annotation cost, the research community has explored a number of ways to exploit unlabeled or weakly labeled data. However, such efforts have met with limited success so far. In this work, we revisit the problem with a pragmatic standpoint, trying to explore a new balance between detection performance and annotation cost by jointly exploiting fully and weakly annotated data. Specifically, we propose a weakly- and semi-supervised object detection framework (WSSOD), which involves a two-stage learning procedure. An agent detector is first trained on a joint dataset and then used to predict pseudo bounding boxes on weakly-annotated images. The underlying assumptions in the current as well as common semi-supervised pipelines are also carefully examined under a unified EM formulation. On top of this framework, weakly-supervised loss (WSL), label attention and random pseudo-label sampling (RPS) strategies are introduced to relax these assumptions, bringing additional improvement on the efficacy of the detection pipeline. The proposed framework demonstrates remarkable performance on PASCAL-VOC and MSCOCO benchmark, achieving a high performance comparable to those obtained in fully-supervised settings, with only one third of the annotations.",0
"In recent years, object detection has become one of the most important tasks in computer vision. With advancements in deep learning techniques such as convolutional neural networks (CNNs), supervised learning has emerged as the dominant approach for training object detectors. However, acquiring large amounts of labeled data can be prohibitively expensive, limiting the performance gains that could otherwise be achieved through these models. To overcome this challenge, weakly-supervised object detection (WSOD) was introduced as an alternative paradigm, where unlabeled images are used alongside a small number of image-level annotations to train the detector. While promising results have been reported using these methods, they suffer from several limitations due to their reliance on heuristics, handcrafted features, and limited flexibility.  This work proposes a new pipeline for WSOD called WSSOD, which addresses some of the key challenges faced by existing approaches. Our method leverages the strengths of both fully-supervised and weakly-supervised learning, combining the power of CNNs with carefully designed regularization terms that promote spatial consistency across multiple scales. By doing so, we achieve state-of-the-art results on popular benchmark datasets while requiring significantly fewer annotated examples compared to traditional fully-supervised methods. Furthermore, our method is efficient and scalable, making it suitable for real-world applications with limited resources. Overall, we believe that WSSOD represents a significant step forward in the development of powerful yet practical object detection systems.",1
"Recently, text detection has attracted sufficient attention in the field of computer vision and artificial intelligence. Among the existing approaches, regression-based models are limited to handle the texts with arbitrary shapes, while segmentation-based algorithms have high computational costs and suffer from the text adhesion problem. In this paper, we propose a new one-stage text detector, termed as Bold Outline Text Detector (BOTD), which is able to process the arbitrary-shaped text with low model complexity. Different from previous works, BOTD utilizes the Polar Minimum Distance (PMD) to encode the shortest distance between the center point and the contour of the text instance, and generates a Center Mask (CM) for each text instance. After learning the PMD heat map and CM map, the final results can be obtained with a simple Text Reconstruction Module (TRM). Since the CM resides within the text box exactly, the text adhesion problem is avoided naturally. Meanwhile, all the points on the text contour share the same PMD, so the complexity of BOTD is much lower than existing segmentation-based methods. Experimental results on three real-world benchmarks show the state-of-the-art performance of BOTD.",0
"In recent years, there has been an increasing demand for automated tools that can accurately detect and extract relevant text from digital images. This task is particularly challenging due to variations in font styles, sizes, colors, and backgrounds. To address these issues, we propose a novel approach called Bold Outline Text Detector (BOTD) which utilizes deep learning techniques to identify text in images by focusing on bold outlines surrounding text regions. Our method achieves state-of-the-art performance compared to existing methods using publicly available datasets. Additionally, our system allows users to fine-tune parameters such as color threshold values, enabling customization based on specific image characteristics. We believe that BOTD has significant potential applications in areas like document digitization, optical character recognition (OCR), and data extraction.",1
"Human pose estimation has achieved significant progress in recent years. However, most of the recent methods focus on improving accuracy using complicated models and ignoring real-time efficiency. To achieve a better trade-off between accuracy and efficiency, we propose a novel neural architecture search (NAS) method, termed ViPNAS, to search networks in both spatial and temporal levels for fast online video pose estimation. In the spatial level, we carefully design the search space with five different dimensions including network depth, width, kernel size, group number, and attentions. In the temporal level, we search from a series of temporal feature fusions to optimize the total accuracy and speed across multiple video frames. To the best of our knowledge, we are the first to search for the temporal feature fusion and automatic computation allocation in videos. Extensive experiments demonstrate the effectiveness of our approach on the challenging COCO2017 and PoseTrack2018 datasets. Our discovered model family, S-ViPNAS and T-ViPNAS, achieve significantly higher inference speed (CPU real-time) without sacrificing the accuracy compared to the previous state-of-the-art methods.",0
"Recent advancements in deep learning have significantly improved video pose estimation performance using convolutional neural networks (CNNs). However, designing these CNN architectures is still largely a manual process that requires expert knowledge. In our work we introduce ViPNAS (video pose network architecture search), which automatically searches for efficient architectures tailored specifically for the task of human video pose estimation. Our proposed NAS method uses DARTS as its backbone, a recent technique that surpasses traditional methods in accuracy and efficiency. We demonstrate on two challenging benchmark datasets (MuPoTs) that ViPNAS outperforms existing state-of-the-art approaches in both speed and accuracy by a significant margin. This opens up opportunities for real-time applications like virtual reality, augmented reality, and gaming where quick and accurate pose estimates are crucial. With its superior performance, ViPNAS promises new possibilities for researchers and practitioners alike, enabling them to quickly and easily find solutions that meet their needs without the need for extensive tuning and hyperparameter optimization. Overall our work represents a significant step forward towards automating the design of efficient and effective CNN architectures for vision tasks such as pose estimation, with far-reaching implications for other computer vision problems.",1
"The task of predicting smooth and edge-consistent depth maps is notoriously difficult for single image depth estimation. This paper proposes a novel Bilateral Grid based 3D convolutional neural network, dubbed as 3DBG-UNet, that parameterizes high dimensional feature space by encoding compact 3D bilateral grids with UNets and infers sharp geometric layout of the scene. Further, another novel 3DBGES-UNet model is introduced that integrate 3DBG-UNet for inferring an accurate depth map given a single color view. The 3DBGES-UNet concatenates 3DBG-UNet geometry map with the inception network edge accentuation map and a spatial object's boundary map obtained by leveraging semantic segmentation and train the UNet model with ResNet backbone. Both models are designed with a particular attention to explicitly account for edges or minute details. Preserving sharp discontinuities at depth edges is critical for many applications such as realistic integration of virtual objects in AR video or occlusion-aware view synthesis for 3D display applications.The proposed depth prediction network achieves state-of-the-art performance in both qualitative and quantitative evaluations on the challenging NYUv2-Depth data. The code and corresponding pre-trained weights will be made publicly available.",0
"This is an artificial intelligence system designed to assist people by providing accurate answers and relevant information quickly and efficiently. The assistant uses natural language processing techniques to analyze questions and determine their meaning, and then generates responses based on that analysis. Its capabilities may vary depending on how well it has been trained. The user should try asking simple questions at first before moving onto more complex ones. Feedback is welcome but please note that this assistant is a work in progress and may produce unexpected results from time to time. Enjoy your chat!",1
"Many efforts have been devoted to designing sampling, mining, and weighting strategies in high-level deep metric learning (DML) loss objectives. However, little attention has been paid to low-level but essential data transformation. In this paper, we develop a novel mechanism, the independent domain embedding augmentation learning ({IDEAL}) method. It can simultaneously learn multiple independent embedding spaces for multiple domains generated by predefined data transformations. Our IDEAL is orthogonal to existing DML techniques and can be seamlessly combined with prior DML approaches for enhanced performance. Empirical results on visual retrieval tasks demonstrate the superiority of the proposed method. For example, the IDEAL improves the performance of MS loss by a large margin, 84.5\% $\rightarrow$ 87.1\% on Cars-196, and 65.8\% $\rightarrow$ 69.5\% on CUB-200 at Recall$@1$. Our IDEAL with MS loss also achieves the new state-of-the-art performance on three image retrieval benchmarks, \ie, \emph{Cars-196}, \emph{CUB-200}, and \emph{SOP}. It outperforms the most recent DML approaches, such as Circle loss and XBM, significantly. The source code and pre-trained models of our method will be available at\emph{\url{https://github.com/emdata-ailab/IDEAL}}.",0
"In recent years, unsupervised domain adaptation has gained significant attention as a promising approach to handle the problem of changing domains during testing time. In this work, we present a novel framework called IDEAL (Independent Domain Embedding Augmentation Learning) that significantly improves the performance of existing methods by enhancing inter-domain discrepancy minimization while preserving intra-class compactness. We achieve this through a unique combination of two components: latent embedding augmentation (LEA) and independent feature learning (IFL). LEA enables us to learn a new mapping function from one domain to another, effectively transforming input features into latent space without losing important semantic information. IFL helps maintain local discriminative power within each domain by creating separate classifiers for both source and target domains while sharing hidden layers. Our extensive experiments on benchmark datasets validate the effectiveness of our proposed method, which achieves state-of-the-art results across different metrics and outperforms previous approaches by significant margins. This research shows great potential for application in real-world scenarios where adaptive models can significantly improve performance in dynamic environments.",1
"The way features propagate in Fully Convolutional Networks is of momentous importance to capture multi-scale contexts for obtaining precise segmentation masks. This paper proposes a novel series-parallel hybrid paradigm called the Chained Context Aggregation Module (CAM) to diversify feature propagation. CAM gains features of various spatial scales through chain-connected ladder-style information flows and fuses them in a two-stage process, namely pre-fusion and re-fusion. The serial flow continuously increases receptive fields of output neurons and those in parallel encode different region-based contexts. Each information flow is a shallow encoder-decoder with appropriate down-sampling scales to sufficiently capture contextual information. We further adopt an attention model in CAM to guide feature re-fusion. Based on these developments, we construct the Chained Context Aggregation Network (CANet), which employs an asymmetric decoder to recover precise spatial details of prediction maps. We conduct extensive experiments on six challenging datasets, including Pascal VOC 2012, Pascal Context, Cityscapes, CamVid, SUN-RGBD and GATECH. Results evidence that CANet achieves state-of-the-art performance.",0
"""Semantic segmentation is one of the most challenging tasks in computer vision, where we aim to label every pixel in an image with its corresponding class (such as road, sidewalk, pedestrian). In this work, we propose attention-guided chained context aggregation (AGCCA), which significantly improves state-of-the-art performance by simultaneously addressing two critical issues: limited receptive fields and insufficient context modeling. By introducing both channel-wise attentions that dynamically select important features and spatial pyramid dilations for computational efficiency, AGCCA effectively enlarges its receptive field while reducing computation overheads. Moreover, our AGCCA block learns interdependencies between global and local information via guided contextualized feature interactions, allowing for more efficient utilization of context from diverse scales and regions within an image. Finally, extensive experiments on five widely used datasets demonstrate that our proposed AGCCA method outperforms existing approaches across all metrics.""",1
"First-person object-interaction tasks in high-fidelity, 3D, simulated environments such as the AI2Thor virtual home-environment pose significant sample-efficiency challenges for reinforcement learning (RL) agents learning from sparse task rewards. To alleviate these challenges, prior work has provided extensive supervision via a combination of reward-shaping, ground-truth object-information, and expert demonstrations. In this work, we show that one can learn object-interaction tasks from scratch without supervision by learning an attentive object-model as an auxiliary task during task learning with an object-centric relational RL agent. Our key insight is that learning an object-model that incorporates object-attention into forward prediction provides a dense learning signal for unsupervised representation learning of both objects and their relationships. This, in turn, enables faster policy learning for an object-centric relational RL agent. We demonstrate our agent by introducing a set of challenging object-interaction tasks in the AI2Thor environment where learning with our attentive object-model is key to strong performance. Specifically, we compare our agent and relational RL agents with alternative auxiliary tasks to a relational RL agent equipped with ground-truth object-information, and show that learning with our object-model best closes the performance gap in terms of both learning speed and maximum success rate. Additionally, we find that incorporating object-attention into an object-model's forward predictions is key to learning representations which capture object-category and object-state.",0
"This paper presents a method that enables reinforcement learning agents to learn complex object manipulation tasks from raw sensor inputs without task-specific engineering. We achieve this by leveraging the intrinsic reward signals produced by the physics simulation as well as the sparse rewards provided by human demonstrations. Our algorithm runs inside the simulation loop enabling realtime interaction, making use of fast approximation techniques to scale up to large action spaces while still maintaining high precision on low dimensional actions. Experiments show that our agent outperforms model free and model based algorithms across environments which require contact interactions between objects that can be grasped. Additionally we test robustness to perceptual aliasing due to camera noise and changes in visual appearance during physical interactions with objects over time. Finally we investigate how transfer learning techniques may allow agents trained exclusively in simulators to generalize their learned behaviors to similar robotic systems.",1
"Following the recent initiatives for the democratization of AI, deep fake generators have become increasingly popular and accessible, causing dystopian scenarios towards social erosion of trust. A particular domain, such as biological signals, attracted attention towards detection methods that are capable of exploiting authenticity signatures in real videos that are not yet faked by generative approaches. In this paper, we first propose several prominent eye and gaze features that deep fakes exhibit differently. Second, we compile those features into signatures and analyze and compare those of real and fake videos, formulating geometric, visual, metric, temporal, and spectral variations. Third, we generalize this formulation to the deep fake detection problem by a deep neural network, to classify any video in the wild as fake or real. We evaluate our approach on several deep fake datasets, achieving 92.48% accuracy on FaceForensics++, 80.0% on Deep Fakes (in the wild), 88.35% on CelebDF, and 99.27% on DeeperForensics datasets. Our approach outperforms most deep and biological fake detectors with complex network architectures without the proposed gaze signatures. We conduct ablation studies involving different features, architectures, sequence durations, and post-processing artifacts.",0
"This paper presents a novel method for detecting synthetic faces (deep fakes) by tracking gaze direction in videos. The authors propose that deep fakes often fail to accurately model eye movements and thus their gaze patterns differ from those of real humans. By analyzing gaze data using machine learning algorithms, they can identify these subtle differences and distinguish between real and fake faces. Experimental results on several benchmark datasets show significant improvement over state-of-the-art methods, demonstrating the effectiveness of the proposed approach. This work has important implications for media verification and trustworthiness assessment in today's age of digital manipulation.",1
"The technology for Visual Odometry (VO) that estimates the position and orientation of the moving object through analyzing the image sequences captured by on-board cameras, has been well investigated with the rising interest in autonomous driving. This paper studies monocular VO from the perspective of Deep Learning (DL). Unlike most current learning-based methods, our approach, called DeepAVO, is established on the intuition that features contribute discriminately to different motion patterns. Specifically, we present a novel four-branch network to learn the rotation and translation by leveraging Convolutional Neural Networks (CNNs) to focus on different quadrants of optical flow input. To enhance the ability of feature selection, we further introduce an effective channel-spatial attention mechanism to force each branch to explicitly distill related information for specific Frame to Frame (F2F) motion estimation. Experiments on various datasets involving outdoor driving and indoor walking scenarios show that the proposed DeepAVO outperforms the state-of-the-art monocular methods by a large margin, demonstrating competitive performance to the stereo VO algorithm and verifying promising potential for generalization.",0
"Title: ""Practical Deep AVO"" by Arne Sieverding, Alexander Kolve, Joachim Denzler, Frank Rastisberger, Juergen Gall, Markus Kroeninger, Sascha Woestes. This research presents a new approach for visual odometry (VO) that uses deep learning techniques to improve feature selection and refinement for accurate camera pose estimation. While traditional VO methods rely on handcrafted features and classical algorithms, our method utilizes Convolutional Neural Networks (CNNs) to learn discriminative features from raw image data. These learned features are then used in conjunction with a CNN-based estimator for improved accuracy over state-of-the-art VO systems. We demonstrate significant improvements in robustness under challenging conditions such as large viewpoint changes, dynamic scenes, and moving objects through several quantitative evaluations. Our system achieves top performance across all benchmark datasets while maintaining real-time inference speeds, making it suitable for deployment in robotics applications requiring high accuracy and efficiency.",1
"Attention layers are widely used in natural language processing (NLP) and are beginning to influence computer vision architectures. Training very large transformer models allowed significant improvement in both fields, but once trained, these networks show symptoms of over-parameterization. For instance, it is known that many attention heads can be pruned without impacting accuracy. This work aims to enhance current understanding on how multiple heads interact. Motivated by the observation that attention heads learn redundant key/query projections, we propose a collaborative multi-head attention layer that enables heads to learn shared projections. Our scheme decreases the number of parameters in an attention layer and can be used as a drop-in replacement in any transformer architecture. Our experiments confirm that sharing key/query dimensions can be exploited in language understanding, machine translation and vision. We also show that it is possible to re-parametrize a pre-trained multi-head attention layer into our collaborative attention layer. Collaborative multi-head attention reduces the size of the key and query projections by 4 for same accuracy and speed. Our code is public.",0
"Incorporating structured representations of knowledge into neural network architectures has recently gained significant attention as a means to improve their performance on tasks such as question answering, text generation, and machine translation. Among the most popular approaches proposed so far are knowledge injection methods that concatenate pre-trained language models (PLMs) and Transformer networks. However, concatenation can lead to excessively deep networks which may require increased computational resources to train effectively. Additionally, the contextual representation learned by one component of the model might clash with another one, causing poor generalization ability across domains. To address these issues, we introduce Multi-head Attention Networks (MANet), a novel architecture that leverages multi-head attention mechanisms to collaboratively fuse PLMs and Transformers, allowing them to learn shared dependencies while preserving their respective strengths. Experiments conducted on three benchmark datasets demonstrate the superiority of MANet over state-of-the-art baselines, achieving new records on two of them. Furthermore, ablation studies showcase the importance of each component in our design, confirming the effectiveness and robustness of our method. Overall, our work highlights the benefits of collaboration within neural networks and offers insights into how multi-head attention can be applied beyond sequential data processing.",1
"The absence or abnormality of fidgety movements of joints or limbs is strongly indicative of cerebral palsy in infants. Developing computer-based methods for assessing infant movements in videos is pivotal for improved cerebral palsy screening. Most existing methods use appearance-based features and are thus sensitive to strong but irrelevant signals caused by background clutter or a moving camera. Moreover, these features are computed over the whole frame, thus they measure gross whole body movements rather than specific joint/limb motion.   Addressing these challenges, we develop and validate a new method for fidgety movement assessment from consumer-grade videos using human poses extracted from short clips. Human poses capture only relevant motion profiles of joints and limbs and are thus free from irrelevant appearance artifacts. The dynamics and coordination between joints are modeled using spatio-temporal graph convolutional networks. Frames and body parts that contain discriminative information about fidgety movements are selected through a spatio-temporal attention mechanism. We validate the proposed model on the cerebral palsy screening task using a real-life consumer-grade video dataset collected at an Australian hospital through the Cerebral Palsy Alliance, Australia. Our experiments show that the proposed method achieves the ROC-AUC score of 81.87%, significantly outperforming existing competing methods with better interpretability.",0
"This paper presents a spatio-temporal attention-based model for infant movement assessment from videos. The proposed approach leverages convolutional neural networks (CNNs) to capture temporal dependencies as well as spatial relationships among different body joints during movement sequences. To achieve this, we introduce two key components: Temporal Convolution Modules (TCMs), which are designed to extract meaningful features from input video frames by considering their contextual relationship over time; and Spatial Coordinate Regression (SCR) units that map each human joint onto an image frame, enabling end-to-end training and better utilization of spatial information across network layers. Our evaluation on a publicly available dataset demonstrates that our method outperforms state-of-the-art approaches in terms of accuracy and robustness while maintaining low computational complexity. Our research opens up new possibilities for using computer vision techniques to analyze pediatric motor development and provide insights into early childhood health monitoring.",1
"In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.",0
"""Knowledge distillation refers to the process of transferring knowledge from one model or dataset to another, typically resulting in a more compact representation that can still achieve similar performance on tasks."" - this quote comes from this article [https://arxiv.org/abs/2206.07827](https://arxiv.org/abs/2206.07827)  The article provides an overview of the current state of art in knowledge distillation as well as its applications across various domains, such as computer vision, natural language processing (NLP), speech recognition, robotics, recommendation systems, etc. It highlights how KD has become an essential tool for training smaller models without sacrificing their accuracy. Furthermore, it discusses future directions and challenges in using KD for developing efficient AI solutions. Finally, the survey helps readers to gain insights into the latest advancements and open problems in the area.",1
"Chinese word segmentation (CWS) is the basic of Chinese natural language processing (NLP). The quality of word segmentation will directly affect the rest of NLP tasks. Recently, with the artificial intelligence tide rising again, Long Short-Term Memory (LSTM) neural network, as one of easily modeling in sequence, has been widely utilized in various kinds of NLP tasks, and functions well. Attention mechanism is an ingenious method to solve the memory compression problem on LSTM. Furthermore, inspired by the powerful abilities of bidirectional LSTM models for modeling sequence and CRF model for decoding, we propose a Bidirectional LSTM-CRF Attention-based Model in this paper. Experiments on PKU and MSRA benchmark datasets show that our model performs better than the baseline methods modeling by other neural networks.",0
"""In natural language processing, word segmentation involves dividing continuous text into individual words. In Chinese language analysis tasks such as machine translation or sentiment analysis, accurate word segmen",1
"Human motion prediction aims to forecast future human poses given a sequence of past 3D skeletons. While this problem has recently received increasing attention, it has mostly been tackled for single humans in isolation. In this paper we explore this problem from a novel perspective, involving humans performing collaborative tasks. We assume that the input of our system are two sequences of past skeletons for two interacting persons, and we aim to predict the future motion for each of them. For this purpose, we devise a novel cross interaction attention mechanism that exploits historical information of both persons and learns to predict cross dependencies between self poses and the poses of the other person in spite of their spatial or temporal distance. Since no dataset to train such interactive situations is available, we have captured ExPI (Extreme Pose Interaction), a new lab-based person interaction dataset of professional dancers performing acrobatics. ExPI contains 115 sequences with 30k frames and 60k instances with annotated 3D body poses and shapes. We thoroughly evaluate our cross-interaction network on this dataset and show that both in short-term and long-term predictions, it consistently outperforms baselines that independently reason for each person. We plan to release our code jointly with the dataset and the train/test splits to spur future research on the topic.",0
"In this paper we present a methodology for predicting extreme motion of multiple interacting persons. Using advanced techniques from computer vision such as attention mechanisms, our model can capture complex relationships between individuals and anticipate how they may move in response to external stimuli or each other. We compare the performance of our approach against several baseline methods on a benchmark dataset of pedestrian interactions, demonstrating that our model achieves state-of-the-art results with significant margins over previous methods. By leveraging rich features from convolutional neural networks and introducing novel ways to represent cross-interactions, we pave the way towards more accurate predictions in multi-person scenes and applications therein. Keywords: motion prediction; pedestrian interaction; attention mechanism",1
"Deep convolutional networks have attracted great attention in image restoration and enhancement. Generally, restoration quality has been improved by building more and more convolutional block. However, these methods mostly learn a specific model to handle all images and ignore difficulty diversity. In other words, an area in the image with high frequency tend to lose more information during compressing while an area with low frequency tends to lose less. In this article, we adrress the efficiency issue in image SR by incorporating a patch-wise rolling network(PRN) to content-adaptively recover images according to difficulty levels. In contrast to existing studies that ignore difficulty diversity, we adopt different stage of a neural network to perform image restoration. In addition, we propose a rolling strategy that utilizes the parameters of each stage more flexible. Extensive experiments demonstrate that our model not only shows a significant acceleration but also maintain state-of-the-art performance.",0
"This work presents content-adaptive representation learning for fast image super-resolution, which can effectively improve the quality of low-resolution images by using deep neural networks. With traditional methods, high computational cost and complex model designs make real-time processing difficult. However, our proposed method utilizes efficient feature extraction techniques and advanced training strategies that allow for faster processing while maintaining excellent performance. Our approach involves two key components: a shallow network for efficient feature extraction, and a deep network trained on multiple scales to capture contextual information from both local and global regions. Experimental results demonstrate that our method outperforms state-of-the-art methods in terms of visual fidelity and efficiency, making it well suited for applications requiring real-time processing such as video surveillance systems or virtual reality environments. In conclusion, our research shows significant promise in advancing the field of image super-resolution through effective use of content-adaptive representations and efficient computation.",1
"Machine learning on graphs has been extensively studied in both academic and industry. However, as the literature on graph learning booms with a vast number of emerging methods and techniques, it becomes increasingly difficult to manually design the optimal machine learning algorithm for different graph-related tasks. To solve this critical challenge, automated machine learning (AutoML) on graphs which combines the strength of graph machine learning and AutoML together, is gaining attention from the research community. Therefore, we comprehensively survey AutoML on graphs in this paper, primarily focusing on hyper-parameter optimization (HPO) and neural architecture search (NAS) for graph machine learning. We further overview libraries related to automated graph machine learning and in-depth discuss AutoGL, the first dedicated open-source library for AutoML on graphs. In the end, we share our insights on future research directions for automated graph machine learning. This paper is the first systematic and comprehensive review of automated machine learning on graphs to the best of our knowledge.",0
"Title: ""Automated Machine Learning on Graphs: A Survey""  Abstract: With the explosive growth in data volume and complexity, automated machine learning (AutoML) has emerged as a powerful tool that enables users without expertise in machine learning to design, train, and deploy models effectively. In recent years, graphs have become increasingly important in data representation, analysis, and understanding complex relationships among entities. However, traditional AutoML approaches may not always capture the richness of graph structures, leading to suboptimal model performance. This survey paper provides a comprehensive overview of state-of-the-art methods and tools for applying AutoML techniques specifically designed for graph-structured data. We discuss key challenges encountered while processing large scale graphs, such as scalability issues and sparsity problems, which often require specialized algorithms tailored to these unique characteristics. Our review includes both supervised and unsupervised settings along with applications across several domains, including social networks, biological networks, recommender systems, and natural language processing. Our aim is to offer a clear understanding of existing solutions, highlight potential research directions, and encourage further exploration into the rapidly expanding intersection of AutoML and graph-based learning.",1
"Recently, the anchor-free object detection model has shown great potential for accuracy and speed to exceed anchor-based object detection. Therefore, two issues are mainly studied in this article: (1) How to let the backbone network in the anchor-free object detection model learn feature extraction? (2) How to make better use of the feature pyramid network? In order to solve the above problems, Experiments show that our model has a certain improvement in accuracy compared with the current popular detection models on the COCO dataset, the designed attention mechanism module can capture contextual information well, improve detection accuracy, and use sepc network to help balance abstract and detailed information, and reduce the problem of semantic gap in the feature pyramid network. Whether it is anchor-based network model YOLOv3, Faster RCNN, or anchor-free network model Foveabox, FSAF, FCOS. Our optimal model can get 39.5% COCO AP under the background of ResNet50.",0
"In recent years, there has been significant progress in object detection algorithms using convolutional neural networks (CNNs). However, existing methods still have limitations in detecting small objects due to the variations in scales and contexts across different images. This work proposes a novel approach called Attention Guided Feature Pyramid Network (AGSFCOS), which addresses these challenges by incorporating attention mechanisms and scale-equalized feature maps.  Our method first constructs a multi-scale feature map based on the downsampled image and generates a channel group from each level of the feature map. Then, we apply a lightweight CNN on each channel group to predict the classification results. By doing so, our model effectively aggregates features from multiple levels and captures fine details in high resolution. To further enhance performance, we introduce a scale-equalizing pyramid module that learns to adjust the weights of features at different scales dynamically according to their importance. Our experiments show that this module significantly improves detection accuracy compared to traditional approaches.  In summary, our proposed AGSFCOS framework combines attention and scale-equalization techniques to overcome obstacles in current object detection algorithms, resulting in improved performance, particularly in detecting smaller objects. We believe that our findings contribute valuable insights into advancing state-of-the-art object detection research.",1
"Artistic style transfer aims to transfer the style characteristics of one image onto another image while retaining its content. Existing approaches commonly leverage various normalization techniques, although these face limitations in adequately transferring diverse textures to different spatial locations. Self-Attention-based approaches have tackled this issue with partial success but suffer from unwanted artifacts. Motivated by these observations, this paper aims to combine the best of both worlds: self-attention and normalization. That yields a new plug-and-play module that we name Self-Attentive Factorized Instance Normalization (SAFIN). SAFIN is essentially a spatially adaptive normalization module whose parameters are inferred through attention on the content and style image. We demonstrate that plugging SAFIN into the base network of another state-of-the-art method results in enhanced stylization. We also develop a novel base network composed of Wavelet Transform for multi-scale style transfer, which when combined with SAFIN, produces visually appealing results with lesser unwanted textures.",0
"Title: Artful Expression Through Algorithmic Ingenuity  Art has always been an outlet for self-expression, allowing individuals to capture their emotions and perceptions through various mediums. Recently, advances in computer graphics have enabled researchers to create algorithms that mimic traditional art styles on digital images, further pushing the boundaries of artistic expression. One such algorithm, called SAFIN (Arbitrary Style Transfer With Self-Attentive Factorized Instance Normalization), allows users to apply arbitrary artistic styles to images using deep learning techniques. This article presents the architecture and training methodology behind SAFIN, providing readers with insights into how machine learning can replicate human creativity. By leveraging convolutional neural networks and attention mechanisms, the authors demonstrate the effectiveness of SAFIN in transferring various art styles onto digital photographs while preserving important details. As we continue exploring the intersection of technology and artistry, SAFIN serves as a powerful tool for individuals looking to explore new forms of expression beyond the confines of realism.",1
"Object goal navigation aims to steer an agent towards a target object based on observations of the agent. It is of pivotal importance to design effective visual representations of the observed scene in determining navigation actions. In this paper, we introduce a Visual Transformer Network (VTNet) for learning informative visual representation in navigation. VTNet is a highly effective structure that embodies two key properties for visual representations: First, the relationships among all the object instances in a scene are exploited; Second, the spatial locations of objects and image regions are emphasized so that directional navigation signals can be learned. Furthermore, we also develop a pre-training scheme to associate the visual representations with navigation signals, and thus facilitate navigation policy learning. In a nutshell, VTNet embeds object and region features with their location cues as spatial-aware descriptors and then incorporates all the encoded descriptors through attention operations to achieve informative representation for navigation. Given such visual representations, agents are able to explore the correlations between visual observations and navigation actions. For example, an agent would prioritize ""turning right"" over ""turning left"" when the visual representation emphasizes on the right side of activation map. Experiments in the artificial environment AI2-Thor demonstrate that VTNet significantly outperforms state-of-the-art methods in unseen testing environments.",0
"Abstract We introduce VTNet (Visual Transformer Network), which models object goal navigation tasks by explicitly reasoning about spatial relationships among objects in visual scenes. Unlike prior methods that rely on heuristics or predefined rules, our approach learns these complex interactions directly from data without any manual engineering. By employing self attention mechanisms commonly used in natural language processing, we enable efficient computation of both global and local contexts while reasoning about object configurations. Experimental results show that our method outperforms existing stateof-the-art algorithms across diverse datasets. Additionally, ablation studies demonstrate the importance of each component in VTNet, including position encoding and graph convolution operations. Our work opens up new opportunities for studying environment understanding problems using large-scale training datasets and deep learning architectures.",1
"In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an Artificial Intelligence (AI) Health Outcomes Challenge seeking solutions to predict risk in value-based care for incorporation into CMS Innovation Center payment and service delivery models. Recently, modern language models have played key roles in a number of health related tasks. This paper presents, to the best of our knowledge, the first application of these models to patient readmission prediction. To facilitate this, we create a dataset of 1.2 million medical history samples derived from the Limited Dataset (LDS) issued by CMS. Moreover, we propose a comprehensive modeling solution centered on a deep learning framework for this data. To demonstrate the framework, we train an attention-based Transformer to learn Medicare semantics in support of performing downstream prediction tasks thereby achieving 0.91 AUC and 0.91 recall on readmission classification. We also introduce a novel data pre-processing pipeline and discuss pertinent deployment considerations surrounding model explainability and bias.",0
"In recent years, there has been increased interest in developing predictive models that can accurately estimate health risks for individuals using their electronic health records (EHRs). However, most existing methods face several challenges, including limited interpretability and poor ability to handle missing data and outliers. To address these issues, we propose a novel model called Explainable Health Risk Predictor (EHRP) that uses a combination of deep learning techniques and feature engineering. EHRP leverages Transformer-based Medicare claim encoders to encode patient claims into vector representations that capture important clinical concepts. This allows the model to learn complex relationships between different medical conditions and treatment history, while still producing explainable predictions. We evaluated our method on two large and diverse datasets, demonstrating significantly improved accuracy over state-of-the-art baseline models. Our results show that EHRP effectively identifies patients at high risk of adverse events such as hospital readmissions or mortality, highlighting the potential impact of our approach on improving population health management. Overall, this work represents an important step towards creating interpretable and reliable health risk prediction systems that could ultimately support more effective care delivery decisions.",1
"Different from general photo retouching tasks, portrait photo retouching (PPR), which aims to enhance the visual quality of a collection of flat-looking portrait photos, has its special and practical requirements such as human-region priority (HRP) and group-level consistency (GLC). HRP requires that more attention should be paid to human regions, while GLC requires that a group of portrait photos should be retouched to a consistent tone. Models trained on existing general photo retouching datasets, however, can hardly meet these requirements of PPR. To facilitate the research on this high-frequency task, we construct a large-scale PPR dataset, namely PPR10K, which is the first of its kind to our best knowledge. PPR10K contains $1, 681$ groups and $11, 161$ high-quality raw portrait photos in total. High-resolution segmentation masks of human regions are provided. Each raw photo is retouched by three experts, while they elaborately adjust each group of photos to have consistent tones. We define a set of objective measures to evaluate the performance of PPR and propose strategies to learn PPR models with good HRP and GLC performance. The constructed PPR10K dataset provides a good benchmark for studying automatic PPR methods, and experiments demonstrate that the proposed learning strategies are effective to improve the retouching performance. Datasets and codes are available: https://github.com/csjliang/PPR10K.",0
"This paper presents a new dataset called PPR10K (Portraits with Personalized Retouchings), which consists of over 87,000 images with manually created masks that separate human regions from non-human ones such as backgrounds. These masks allow for precise editing of specific areas within portraits, while preserving other elements like hair color, accessories, and facial expressions. The dataset contains both individual portrait photos as well as group shots featuring multiple subjects, all with high quality retouching applied by professional artists. Additionally, the authors provide two challenge tasks including regional editing using detailed labels provided by photographers, which can improve generalization ability on models; a benchmark evaluation on several state-of-the-art methods shows superior performance after training on our richer data compared to earlier works. For future reference, we make available PPR10K as a public resource.",1
"View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on three public datasets shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at https://github.com/Gword/Recursive-NeRF.",0
"Recent advances in neural rendering have led to the development of neural radiance fields (NeRF), which use deep learning techniques to estimate scene representations from images. However, current methods suffer from limitations such as slow convergence rates, limited scalability, and lack of robustness to changing lighting conditions. We propose Recursive-NeRF, a novel framework that addresses these issues by introducing recursive depth estimation and dynamically growing network sizes. Our method achieves efficient inference and better reconstruction quality through iterative fusion of local depth estimates, dynamic expansion of the network size, and regularization using shape prior knowledge. Experiments on challenging datasets demonstrate significant improvements over state-of-the-art approaches in terms of accuracy, efficiency, and robustness. Our findings pave the way for future research in neural scene representation and rendering.",1
"Federated Learning (FL) is a framework which enables distributed model training using a large corpus of decentralized training data. Existing methods aggregate models disregarding their internal representations, which are crucial for training models in vision tasks. System and statistical heterogeneity (e.g., highly imbalanced and non-i.i.d. data) further harm model training. To this end, we introduce a method, called FedProto, which computes client deviations using margins of prototypical representations learned on distributed data, and applies them to drive federated optimization via an attention mechanism. In addition, we propose three methods to analyse statistical properties of feature representations learned in FL, in order to elucidate the relationship between accuracy, margins and feature discrepancy of FL models. In experimental analyses, FedProto demonstrates state-of-the-art accuracy and convergence rate across image classification and semantic segmentation benchmarks by enabling maximum margin training of FL models. Moreover, FedProto reduces uncertainty of predictions of FL models compared to the baseline. To our knowledge, this is the first work evaluating FL models in dense prediction tasks, such as semantic segmentation.",0
"In recent years, federated learning has emerged as a promising approach for training machine learning models on distributed data across multiple devices or users without sharing their private data. However, designing effective feature representations that can achieve high accuracy with limited communication rounds remains challenging. This work proposes Prototype Guided Federated Learning (PGFL), which utilizes prototypes derived from local model predictions during the initial rounds of communication, to guide the optimization process towards better feature representations that capture both global and local patterns. Our method outperforms existing state-of-the-art methods across several benchmark datasets and real world applications. We demonstrate the efficacy of PGFL by comparing results against baseline approaches on image classification tasks performed over various device settings under varying network conditions. Overall, our contributions provide insights into developing more robust and accurate federated learning systems suitable for deployment in practical scenarios.",1
"Decentralized nonconvex optimization has received increasing attention in recent years in machine learning due to its advantages in system robustness, data privacy, and implementation simplicity. However, three fundamental challenges in designing decentralized optimization algorithms are how to reduce their sample, communication, and memory complexities. In this paper, we propose a \underline{g}radient-\underline{t}racking-based \underline{sto}chastic \underline{r}ecursive \underline{m}omentum (GT-STORM) algorithm for efficiently solving nonconvex optimization problems. We show that to reach an $\epsilon^2$-stationary solution, the total number of sample evaluations of our algorithm is $\tilde{O}(m^{1/2}\epsilon^{-3})$ and the number of communication rounds is $\tilde{O}(m^{-1/2}\epsilon^{-3})$, which improve the $O(\epsilon^{-4})$ costs of sample evaluations and communications for the existing decentralized stochastic gradient algorithms. We conduct extensive experiments with a variety of learning models, including non-convex logistical regression and convolutional neural networks, to verify our theoretical findings. Collectively, our results contribute to the state of the art of theories and algorithms for decentralized network optimization.",0
"In decentralized non-convex learning, distributed optimization over large scale networks faces significant challenges due to issues related to sample complexity, communication efficiency, memory usage, and synchronization among agents. Existing methods have been limited by their reliance on either centralized coordination or impractical assumptions regarding smoothness properties of objective functions, which cannot handle many real-world problems such as deep neural networks and robust control systems. This paper introduces a new methodology called GT-Storm that tackles these challenges via three key ingredients, namely Gradient Tracking (GT), Sample Interleaving (SI), and Adaptive Local Weighted Averaging (ALWA). GT mitigates high variance gradient noise by normalizing local gradients against corresponding running averages, SI deals with heterogeneous data sizes across agents by interleaving gradient steps with small random samples from each agent, and ALWA adaptively allocates weights based on the quality of individual contributions towards efficient memory utilization. Extensive experimental results demonstrate substantial improvements achieved by our proposed approach compared to state-of-the-art alternatives, including reduced communication rounds, faster convergence rates, and improved parameter scalability under heavy resource constraints in edge computing scenarios. Our work paves a promising direction for scaling up decentralized non-convex learning techniques to more complex applications where existing approaches are insufficient.",1
"Decomposing knowledge into interchangeable pieces promises a generalization advantage when there are changes in distribution. A learning agent interacting with its environment is likely to be faced with situations requiring novel combinations of existing pieces of knowledge. We hypothesize that such a decomposition of knowledge is particularly relevant for being able to generalize in a systematic manner to out-of-distribution changes. To study these ideas, we propose a particular training framework in which we assume that the pieces of knowledge an agent needs and its reward function are stationary and can be re-used across tasks. An attention mechanism dynamically selects which modules can be adapted to the current task, and the parameters of the selected modules are allowed to change quickly as the learner is confronted with variations in what it experiences, while the parameters of the attention mechanisms act as stable, slowly changing, meta-parameters. We focus on pieces of knowledge captured by an ensemble of modules sparsely communicating with each other via a bottleneck of attention. We find that meta-learning the modular aspects of the proposed system greatly helps in achieving faster adaptation in a reinforcement learning setup involving navigation in a partially observed grid world with image-level input. We also find that reversing the role of parameters and meta-parameters does not work nearly as well, suggesting a particular role for fast adaptation of the dynamically selected modules.",0
"Learning from sequential data is essential for artificial intelligence applications that involve speech recognition, natural language processing, robotics, bioinformatics, and many others. There has been recent interest in using recurrent independent mechanisms (Miconi, 1982; Seung et al., 1996) to model sequential patterns and sequence dependent learning. These models have shown promising results but suffer from scalability issues due to the high number of parameters required for each time step. In this work we propose fast and slow learning of recurrent independent mechanisms as a solution to overcome these limitations. We introduce two variants of the original model: fast RIMs and slow RIMs. Fast RIMs use a linear combination of previous hidden states and current input signal and require significantly fewer parameters per time step than traditional RIMs while still preserving their advantageous properties. Slow RIMs learn more slowly but capture longer term dependencies by taking into account multiple steps. Our experiments on benchmark datasets show that our proposed methods outperform other state-of-the art models across a wide range of tasks. This research provides important insights into understanding how temporal representations can be learned efficiently with limited computational resources which is critical in real world applications where memory usage and computational power is restricted.",1
"We extend panoptic segmentation to the open-world and introduce an open-set panoptic segmentation (OPS) task. This task requires performing panoptic segmentation for not only known classes but also unknown ones that have not been acknowledged during training. We investigate the practical challenges of the task and construct a benchmark on top of an existing dataset, COCO. In addition, we propose a novel exemplar-based open-set panoptic segmentation network (EOPSN) inspired by exemplar theory. Our approach identifies a new class based on exemplars, which are identified by clustering and employed as pseudo-ground-truths. The size of each class increases by mining new exemplars based on the similarities to the existing ones associated with the class. We evaluate EOPSN on the proposed benchmark and demonstrate the effectiveness of our proposals. The primary goal of our work is to draw the attention of the community to the recognition in the open-world scenarios. The implementation of our algorithm is available on the project webpage: https://cv.snu.ac.kr/research/EOPSN.",0
"In order to create more accurate segmentations in open-set environments without relying on predefined object boundaries, we introduce an exemplar based approach for panoptic segmentation that predicts pixel-level masks as well as instance IDs for each object present in the scene. Our method uses semantic embedding vectors learnt from human annotations as references in order to guide the model during training towards better solutions. We show how our proposed network can achieve state-of-the-art results on two different benchmark datasets using only annotated image regions instead of full bounding boxes, demonstrating its generalisation capabilities across domains and setups. Additionally, through ablation studies we provide insights into which components within our architecture play crucial roles for performance improvements and why. By introducing such a method, we hope to reduce the reliance on manually defined prior knowledge by utilizing self-supervised learning from unlabelled data, enabling us to overcome current limitations in open-world applications like robotics or automotive scenarios where previously seen objects may appear in images.",1
"Automatic pain recognition is paramount for medical diagnosis and treatment. The existing works fall into three categories: assessing facial appearance changes, exploiting physiological cues, or fusing them in a multi-modal manner. However, (1) appearance changes are easily affected by subjective factors which impedes objective pain recognition. Besides, the appearance-based approaches ignore long-range spatial-temporal dependencies that are important for modeling expressions over time; (2) the physiological cues are obtained by attaching sensors on human body, which is inconvenient and uncomfortable. In this paper, we present a novel multi-task learning framework which encodes both appearance changes and physiological cues in a non-contact manner for pain recognition. The framework is able to capture both local and long-range dependencies via the proposed attention mechanism for the learned appearance representations, which are further enriched by temporally attended physiological cues (remote photoplethysmography, rPPG) that are recovered from videos in the auxiliary task. This framework is dubbed rPPG-enriched Spatio-Temporal Attention Network (rSTAN) and allows us to establish the state-of-the-art performance of non-contact pain recognition on publicly available pain databases. It demonstrates that rPPG predictions can be used as an auxiliary task to facilitate non-contact automatic pain recognition.",0
"Automatic non-contact pain recognition from video sequences has become increasingly important due to recent advances in machine learning techniques and remote sensing technologies. This task requires predicting the intensity of pain based on a sequence of visual inputs without any physical contact with the individual. In our work, we propose a method that integrates remote physiological measurements into deep neural networks for accurate pain prediction from video sequences. By leveraging remote sensors such as wearables, cameras, microphones, and other devices, we can capture real-time data related to vital signs, facial expressions, body movements, voice tremor, etc. Our approach combines these different modalities to better analyze the relationship between physiological signals and the level of pain experienced by individuals during specific activities. Experimental results using public datasets show that our proposed method outperforms existing state-of-the art methods and achieves high accuracy while maintaining interpretability. Overall, our research opens new directions towards developing non-invasive monitoring systems capable of detecting pain levels automatically from video sequences and remote physiological measurements.",1
"Due to their high retrieval efficiency and low storage cost for cross-modal search task, cross-modal hashing methods have attracted considerable attention. For the supervised cross-modal hashing methods, how to make the learned hash codes preserve semantic information sufficiently contained in the label of datapoints is the key to further enhance the retrieval performance. Hence, almost all supervised cross-modal hashing methods usually depends on defining a similarity between datapoints with the label information to guide the hashing model learning fully or partly. However, the defined similarity between datapoints can only capture the label information of datapoints partially and misses abundant semantic information, then hinders the further improvement of retrieval performance. Thus, in this paper, different from previous works, we propose a novel cross-modal hashing method without defining the similarity between datapoints, called Deep Cross-modal Hashing via \textit{Margin-dynamic-softmax Loss} (DCHML). Specifically, DCHML first trains a proxy hashing network to transform each category information of a dataset into a semantic discriminative hash code, called proxy hash code. Each proxy hash code can preserve the semantic information of its corresponding category well. Next, without defining the similarity between datapoints to supervise the training process of the modality-specific hashing networks , we propose a novel \textit{margin-dynamic-softmax loss} to directly utilize the proxy hashing codes as supervised information. Finally, by minimizing the novel \textit{margin-dynamic-softmax loss}, the modality-specific hashing networks can be trained to generate hash codes which can simultaneously preserve the cross-modal similarity and abundant semantic information well.",0
"In recent years, deep cross-modal hashing techniques have emerged as powerful tools for efficient retrieval of multimedia content across different modalities such as images, videos, and text. These methods learn mappings between high-dimensional feature spaces and lower-dimensional hash code spaces using deep neural networks. However, existing approaches often suffer from limited robustness due to the use of simple distance metrics that fail to capture important relationships among data points. To address these limitations, we propose a novel method called ""Deep Cross-Modal Hashing with Margin-Dynamic Softmax Loss"" (DMH), which significantly improves retrieval accuracy by incorporating margin constraints into the learning process. Our approach adopts dynamic softmax scaling factor normalization techniques to regularize the gradient dynamics during optimization, enabling more effective information flow through the network. Experimental evaluations on several benchmark datasets demonstrate that our DMH approach consistently outperforms state-of-the-art baseline methods in terms of precision, recall, and F1 score measures. Furthermore, extensive analysis shows that our proposed model effectively captures complex relationships between modalities and preserves semantic information, leading to improved performance in retrieval tasks. Overall, our work advances the field of deep cross-modal hashing by introducing a new loss function that promotes better generalization and stability in training, resulting in significant improvements in retrieval quality.",1
"Recent years have witnessed the tremendous research interests in network embedding. Extant works have taken the neighborhood formation as the critical information to reveal the inherent dynamics of network structures, and suggested encoding temporal edge formation sequences to capture the historical influences of neighbors. In this paper, however, we argue that the edge formation can be attributed to a variety of driving factors including the temporal influence, which is better referred to as multiple aspects. As a matter of fact, different node aspects can drive the formation of distinctive neighbors, giving birth to the multi-aspect embedding that relates to but goes beyond a temporal scope. Along this vein, we propose a Mixture of Hawkes-based Temporal Network Embeddings (MHNE) model to capture the aspect-driven neighborhood formation of networks. In MHNE, we encode the multi-aspect embeddings into the mixture of Hawkes processes to gain the advantages in modeling the excitation effects and the latent aspects. Specifically, a graph attention mechanism is used to assign different weights to account for the excitation effects of history events, while a Gumbel-Softmax is plugged in to derive the distribution over the aspects. Extensive experiments on 8 different temporal networks have demonstrated the great performance of the multi-aspect embeddings obtained by MHNE in comparison with the state-of-the-art methods.",0
"In recent years there has been increased interest in analyzing network data from a temporal perspective which models how networks change over time. One challenge associated with modeling dynamic graphs arises due to their high complexity, as they evolve according to multiple asynchronous patterns that may exhibit varying levels of uncertainty. Therefore, we need new techniques able to capture multi-aspect evolutions at different scales. To achieve this goal, we introduce a mixture of Hawkes processes view to embed nodes into a vector space where each component represents a specific aspect such as edge density, node velocity, etc. Our framework can accommodate both homogeneous (e.g., social) and heterogeneous (e.g., citation) networks and leverages knowledge on node features. We validate our approach on several real-world datasets and demonstrate better results than previous state-of-the-art methods.",1
"Adversarial training (AT) is one of the most effective ways for improving the robustness of deep convolution neural networks (CNNs). Just like common network training, the effectiveness of AT relies on the design of basic network components. In this paper, we conduct an in-depth study on the role of the basic ReLU activation component in AT for robust CNNs. We find that the spatially-shared and input-independent properties of ReLU activation make CNNs less robust to white-box adversarial attacks with either standard or adversarial training. To address this problem, we extend ReLU to a novel Sparta activation function (Spatially attentive and Adversarially Robust Activation), which enables CNNs to achieve both higher robustness, i.e., lower error rate on adversarial examples, and higher accuracy, i.e., lower error rate on clean examples, than the existing state-of-the-art (SOTA) activation functions. We further study the relationship between Sparta and the SOTA activation functions, providing more insights about the advantages of our method. With comprehensive experiments, we also find that the proposed method exhibits superior cross-CNN and cross-dataset transferability. For the former, the adversarially trained Sparta function for one CNN (e.g., ResNet-18) can be fixed and directly used to train another adversarially robust CNN (e.g., ResNet-34). For the latter, the Sparta function trained on one dataset (e.g., CIFAR-10) can be employed to train adversarially robust CNNs on another dataset (e.g., SVHN). In both cases, Sparta leads to CNNs with higher robustness than the vanilla ReLU, verifying the flexibility and versatility of the proposed method.",0
"This paper introduces a new activation function called sparsely attentive adversarial robust (Sparta). Our goal was to create an activation that would improve performance on standard computer vision tasks while also making models more robust against common forms of image noise such as small shifts, scale changes, rotations, translations, and pixel swaps, but without increasing model size or computational requirements. We test our activation on several benchmark datasets and demonstrate consistent improvements over popular alternatives like ReLU and LeakyReLU. Finally, we provide ablation studies that show how each component contributes to improved performance and robustness, making our results highly interpretable and actionable for practitioners interested in designing better neural network architectures.",1
"Transformers, composed of multiple self-attention layers, hold strong promises toward a generic learning primitive applicable to different data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art (SOTA) standard accuracy with better parameter efficiency. Since self-attention helps a model systematically align different components present inside the input data, it leaves grounds to investigate its performance under model robustness benchmarks. In this work, we study the robustness of the Vision Transformer (ViT) against common corruptions and perturbations, distribution shifts, and natural adversarial examples. We use six different diverse ImageNet datasets concerning robust classification to conduct a comprehensive performance comparison of ViT models and SOTA convolutional neural networks (CNNs), Big-Transfer. Through a series of six systematically designed experiments, we then present analyses that provide both quantitative and qualitative indications to explain why ViTs are indeed more robust learners. For example, with fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1 accuracy of 28.10% on ImageNet-A which is 4.3x higher than a comparable variant of BiT. Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness. Code for reproducing our experiments is available here: https://git.io/J3VO0.",0
"Vision transformer models have been shown to outperform convolutional neural networks (CNNs) on tasks such as image classification and object detection. One reason why vision transformers perform well could be due to their ability to handle noise and distortions in images better than CNNs. In this paper, we investigate whether vision transformers can learn from noisy data more effectively than CNNs by training both architectures using synthetic noise during pretraining. We find that indeed, vision transformers exhibit greater robustness to image corruptions compared to CNNs, even without fine-tuning specifically for robustness. Our results suggest that part of the success of vision transformers could stem from their capacity to capture and utilize high-level representations that are less sensitive to local perturbations introduced through noise or other types of transformations. These results highlight important differences between CNNs and transformers in terms of how they process complex visual patterns, which has implications for future research aiming to improve generalization and reduce overfitting in deep learning models.",1
"Due to the scarcity of annotated scene flow data, self-supervised scene flow learning in point clouds has attracted increasing attention. In the self-supervised manner, establishing correspondences between two point clouds to approximate scene flow is an effective approach. Previous methods often obtain correspondences by applying point-wise matching that only takes the distance on 3D point coordinates into account, introducing two critical issues: (1) it overlooks other discriminative measures, such as color and surface normal, which often bring fruitful clues for accurate matching; and (2) it often generates sub-par performance, as the matching is operated in an unconstrained situation, where multiple points can be ended up with the same corresponding point. To address the issues, we formulate this matching task as an optimal transport problem. The output optimal assignment matrix can be utilized to guide the generation of pseudo ground truth. In this optimal transport, we design the transport cost by considering multiple descriptors and encourage one-to-one matching by mass equality constraints. Also, constructing a graph on the points, a random walk module is introduced to encourage the local consistency of the pseudo labels. Comprehensive experiments on FlyingThings3D and KITTI show that our method achieves state-of-the-art performance among self-supervised learning methods. Our self-supervised method even performs on par with some supervised learning approaches, although we do not need any ground truth flow for training.",0
"Abstract: Accurately estimating scene flow (i.e., motion fields) from point clouds is fundamental for many computer vision tasks such as robotics, autonomous driving, virtual reality, etc. Recent years have witnessed significant progress in developing supervised learning methods for high quality scene flow estimation. These approaches typically rely on large amounts of labeled data, which can be expensive and time consuming to obtain. In contrast, self-supervised learning has emerged as a powerful paradigm that allows one to learn representations from unlabeled data by designing suitable pretext tasks. While several successful self-supervised representation learning methods exist for images and videos, little work has been done so far to develop effective self-supervised scene flow estimation methods using point clouds. Inspired by optimal transport theory, we propose Self-Point-Flow, a new algorithm that tackles self-supervised scene flow estimation problem using point clouds. Our approach uses random walks on graphs constructed based on two consecutive frames of point cloud data to create correspondences. We then use a novel variant of Sinkhorn-Knopp optimization combined with a differentiable loss function to solve for the correspondence matrix. To further improve our approach, we introduce a cycle consistency constraint into the framework, enabling the network to better capture global geometric relationships between different points in space. Experimental results demonstrate that our method outperforms state-of-the-art supervised baselines on both synthetic and real benchmark datasets while requiring significantly less training data. Additionally, ablation studies show that each component in our algorithm contributes positively to the performance improvement over other competin",1
"Many attempts have been made towards combining RGB and 3D poses for the recognition of Activities of Daily Living (ADL). ADL may look very similar and often necessitate to model fine-grained details to distinguish them. Because the recent 3D ConvNets are too rigid to capture the subtle visual patterns across an action, this research direction is dominated by methods combining RGB and 3D Poses. But the cost of computing 3D poses from RGB stream is high in the absence of appropriate sensors. This limits the usage of aforementioned approaches in real-world applications requiring low latency. Then, how to best take advantage of 3D Poses for recognizing ADL? To this end, we propose an extension of a pose driven attention mechanism: Video-Pose Network (VPN), exploring two distinct directions. One is to transfer the Pose knowledge into RGB through a feature-level distillation and the other towards mimicking pose driven attention through an attention-level distillation. Finally, these two approaches are integrated into a single model, we call VPN++. We show that VPN++ is not only effective but also provides a high speed up and high resilience to noisy Poses. VPN++, with or without 3D Poses, outperforms the representative baselines on 4 public datasets. Code is available at https://github.com/srijandas07/vpnplusplus.",0
"Activity recognition has become an increasingly important research topic as more people rely on technology like smartphones to record and analyze their daily activities. One popular approach to activity recognition is using video pose embeddings (VPEs), which encode the location and posture of human subjects in a video into a fixed-size vector representation. However, existing VPE methods have limitations that make them less effective at recognizing certain types of activities, such as those involving fine motor movements or subtle changes in body position. In our new paper, we propose VPN++, an improved method for generating VPEs that addresses these shortcomings by incorporating additional motion features and refining the embedding process through adversarial training. Our experiments show that VPN++ outperforms state-of-the-art VPE approaches across a wide range of ADL tasks, demonstrating the potential of our method to advance the field of activity recognition research.",1
"Dynamic graph modeling has recently attracted much attention due to its extensive applications in many real-world scenarios, such as recommendation systems, financial transactions, and social networks. Although many works have been proposed for dynamic graph modeling in recent years, effective and scalable models are yet to be developed. In this paper, we propose a novel graph neural network approach, called TCL, which deals with the dynamically-evolving graph in a continuous-time fashion and enables effective dynamic node representation learning that captures both the temporal and topology information. Technically, our model contains three novel aspects. First, we generalize the vanilla Transformer to temporal graph learning scenarios and design a graph-topology-aware transformer. Secondly, on top of the proposed graph transformer, we introduce a two-stream encoder that separately extracts representations from temporal neighborhoods associated with the two interaction nodes and then utilizes a co-attentional transformer to model inter-dependencies at a semantic level. Lastly, we are inspired by the recently developed contrastive learning and propose to optimize our model by maximizing mutual information (MI) between the predictive representations of two future interaction nodes. Benefiting from this, our dynamic representations can preserve high-level (or global) semantics about interactions and thus is robust to noisy interactions. To the best of our knowledge, this is the first attempt to apply contrastive learning to representation learning on dynamic graphs. We evaluate our model on four benchmark datasets for interaction prediction and experiment results demonstrate the superiority of our model.",0
"Title: ""Dynamic Graph Modeling via Contrastive Learning"" Abstract: This paper proposes a novel approach to dynamic graph modeling based on contrastive learning using transformer networks. We introduce a new method called TCL (Transformer-based Contrastive Learning) that enables efficient modeling of changing graph structures over time. Our approach leverages the power of self-supervised pretraining by designing a contrastive learning objective that can capture temporal dependencies within the graph data. To achieve this, we modify the traditional attention mechanism in transformer networks to operate on graphs directly, allowing us to effectively learn representations that encode both node content and their relationships in the graph. Experimental results show significant improvements over state-of-the-art methods in several benchmark datasets for dynamic graph prediction tasks, including link predicting and node attribute forecasting. Overall, our work demonstrates the effectiveness of applying contrastive learning to real-world problems involving evolving graph data and opens up promising research directions for future studies in this area.",1
"Motor imagery (MI)-based brain-computer interface (BCI) systems are being increasingly employed to provide alternative means of communication and control for people suffering from neuro-motor impairments, with a special effort to bring these systems out of the controlled lab environments. Hence, accurately classifying MI from brain signals, e.g., from electroencephalography (EEG), is essential to obtain reliable BCI systems. However, MI classification is still a challenging task, because the signals are characterized by poor SNR, high intra-subject and cross-subject variability. Deep learning approaches have started to emerge as valid alternatives to standard machine learning techniques, e.g., filter bank common spatial pattern (FBCSP), to extract subject-independent features and to increase the cross-subject classification performance of MI BCI systems. In this paper, we first present a review of the most recent studies using deep learning for MI classification, with particular attention to their cross-subject performance. Second, we propose DynamicNet, a Python-based tool for quick and flexible implementations of deep learning models based on convolutional neural networks. We show-case the potentiality of DynamicNet by implementing EEGNet, a well-established architecture for effective EEG classification. Finally, we compare its performance with FBCSP in a 4-class MI classification over public datasets. To explore its cross-subject classification ability, we applied three different cross-validation schemes. From our results, we demonstrate that DynamicNet-implemented EEGNet outperforms FBCSP by about 25%, with a statistically significant difference when cross-subject validation schemes are applied.",0
"In this study we reviewed existing methods using CNNs (Convolutional Neural Networks) that have been used for cross-subject classification in motor imagery EEG signals. Our analysis showed that current state-of-the-art approaches fall short of real world performance requirements because they require massive amounts of training data per subject and lack adaptability to varying subjects without retraining. To address these limitations, our proposed approach,DynamicNet, utilizes transfer learning from pretrained models on large datasets such as ImageNet while incorporating dynamic fine tuning layers that adjust weights based on new subjects. This allows for greater flexibility and reduces the amount of training required compared to traditional methods. We evaluate the effectiveness of our approach through experimentation with publicly available benchmark datasets. Results demonstrate improved accuracy over previous state-of-the-art techniques as well as competitive results against popular BCI classifiers. Overall, our findings indicate the potential viability of DynamicNet as a reliable methodology in noninvasive Brain Computer Interface systems.",1
"Physics-Informed Neural Networks (PINNs) have enabled significant improvements in modelling physical processes described by partial differential equations (PDEs). PINNs are based on simple architectures, and learn the behavior of complex physical systems by optimizing the network parameters to minimize the residual of the underlying PDE. Current network architectures share some of the limitations of classical numerical discretization schemes when applied to non-linear differential equations in continuum mechanics. A paradigmatic example is the solution of hyperbolic conservation laws that develop highly localized nonlinear shock waves. Learning solutions of PDEs with dominant hyperbolic character is a challenge for current PINN approaches, which rely, like most grid-based numerical schemes, on adding artificial dissipation. Here, we address the fundamental question of which network architectures are best suited to learn the complex behavior of non-linear PDEs. We focus on network architecture rather than on residual regularization. Our new methodology, called Physics-Informed Attention-based Neural Networks, (PIANNs), is a combination of recurrent neural networks and attention mechanisms. The attention mechanism adapts the behavior of the deep neural network to the non-linear features of the solution, and break the current limitations of PINNs. We find that PIANNs effectively capture the shock front in a hyperbolic model problem, and are capable of providing high-quality solutions inside and beyond the training set.",0
"Solving partial differential equations (PDEs) is an essential task in many fields such as physics, engineering, mathematics, etc. Many advanced techniques have been proposed so far, but they often face limitations like computational cost and lack of flexibility. To address these issues, we propose a novel method called physics- informed attention-based neural network (PIA-NN), which combines physical knowledge with deep learning principles. We utilize an attention mechanism to identify important features from the PDEs and then use a neural network to learn the solution map. This approach has advantages over existing methods because of its interpretability, adaptivity, and efficiency. Our numerical results demonstrate that our model can accurately solve benchmark problems from different fields and outperforms several state-of-the-art models. Overall, PIA-NN provides a promising direction towards efficient and effective solutions of nonlinear PDEs, opening up new possibilities for scientific discoveries and technological innovations. Abstract:  Solving nonlinear partial differential equations (NPDEs) remains a challenging problem in many scientific fields due to their complex nature and high computation costs associated with traditional methods. In this work, we present a physics-informed attention-based neural network (PIA-NN) framework that integrates prior domain knowledge into a deep learning architecture. Our method uses attention mechanisms to select relevant features from NPDEs before training a neural network to approximate their solutions. Compared to other approaches, PIA-NN exhibits better performance, interpretability, and adaptivity while maintaining computational efficiency.  We evaluate our framework on a variety of benchmark problems across multiple domains, including fluid dynamics, quantum chemistry, and heat transfer. Results showcase the superiority of PIA-NN compared to established methods, both in accuracy and speed. Furthermore, since the learned approximation can be treated as an initial guess for more accurate solvers, our framework enables progress toward improved NPDE resolution strategies that integrate machine learning and classical methods. Overall, our contributions pave the way for tackling real-world applications where NPDEs prevail, boosting innovation opportunities in science and technology.",1
"Optical Character Recognition (OCR) systems have been widely used in various applications for extracting semantic information from images. To give the user more control over their privacy, an on-device solution is needed. The current state-of-the-art models are too heavy and complex to be deployed on-device. We develop an efficient lightweight scene text recognition (STR) system, which has only 0.88M parameters and performs real-time text recognition. Attention modules tend to boost the accuracy of STR networks but are generally slow and not optimized for device inference. So, we propose the use of convolution attention modules to the text recognition networks, which aims to provide channel and spatial attention information to the LSTM module by adding very minimal computational cost. It boosts our word accuracy on ICDAR 13 dataset by almost 2\%. We also introduce a novel orientation classifier module, to support the simultaneous recognition of both horizontal and vertical text. The proposed model surpasses on-device metrics of inference time and memory footprint and achieves comparable accuracy when compared to the leading commercial and other open-source OCR engines. We deploy the system on-device with an inference speed of 2.44 ms per word on the Exynos 990 chipset device and achieve an accuracy of 88.4\% on ICDAR-13 dataset.",0
"Despite many advances in computer vision, scene text recognition remains a challenging task due to variations in lighting conditions, camera angles, font types, image quality, and occlusions. State-of-the-art methods rely on cloud computing and expensive GPUs, which may not always be available or feasible in practice. We introduce STRIDE (Scene Text Recognizer In Device), a novel approach that uses deep learning models trained on synthetic data to achieve accurate scene text recognition directly on consumer smartphones without Internet access. Our key contributions include creating large-scale realistic synthetic datasets using generative adversarial networks (GANs) and developing an efficient algorithm based on hierarchical attention modules (HAM). Experimental results show that our method outperforms existing in-device solutions and approaches state-of-the-art performance. Moreover, we demonstrate the efficiency of our model by running inference on several popular Android devices with significant speedups compared to current industry practices. Overall, our work offers promise for enabling high-quality in-device scene text recognition under diverse operating scenarios, thus benefiting applications such as accessibility, autonomous driving, augmented reality, and mobile document analysis.",1
"Video frame interpolation can up-convert the frame rate and enhance the video quality. In recent years, although the interpolation performance has achieved great success, image blur usually occurs at the object boundaries owing to the large motion. It has been a long-standing problem, and has not been addressed yet. In this paper, we propose to reduce the image blur and get the clear shape of objects by preserving the edges in the interpolated frames. To this end, the proposed Edge-Aware Network (EA-Net) integrates the edge information into the frame interpolation task. It follows an end-to-end architecture and can be separated into two stages, \emph{i.e.}, edge-guided flow estimation and edge-protected frame synthesis. Specifically, in the flow estimation stage, three edge-aware mechanisms are developed to emphasize the frame edges in estimating flow maps, so that the edge-maps are taken as the auxiliary information to provide more guidance to boost the flow accuracy. In the frame synthesis stage, the flow refinement module is designed to refine the flow map, and the attention module is carried out to adaptively focus on the bidirectional flow maps when synthesizing the intermediate frames. Furthermore, the frame and edge discriminators are adopted to conduct the adversarial training strategy, so as to enhance the reality and clarity of synthesized frames. Experiments on three benchmarks, including Vimeo90k, UCF101 for single-frame interpolation and Adobe240-fps for multi-frame interpolation, have demonstrated the superiority of the proposed EA-Net for the video frame interpolation task.",0
"Edge-aware video frame interpolation has recently gained attention due to its ability to produce high quality intermediate frames that can improve video playback on devices such as televisions and smartphones. However, current edge-aware methods often require expensive computational resources and may still suffer from motion inconsistency artifacts. In this work, we present EA-Net, a novel flow-guided convolutional network architecture specifically designed for efficient and accurate edge-aware video frame interpolation. Our approach uses a lightweight network with only 79k parameters and achieves competitive performance compared to state-of-the-art methods with significantly fewer computations. To further enhance our method's efficiency, we introduce a new adaptive upsampling strategy that adjusts spatial resolution based on local image content without increasing computational cost. Experimental results demonstrate that our proposed EA-Net outperforms other state-of-the-art methods in terms of visual quality and accuracy while requiring up to 82% less computation time than popular deep learning frameworks. Overall, our EA-Net framework offers a promising solution for real-time edge-aware video frame interpolation applications on resource-constrained platforms.",1
"Audio and vision are two main modalities in video data. Multimodal learning, especially for audiovisual learning, has drawn considerable attention recently, which can boost the performance of various computer vision tasks. However, in video summarization, existing approaches just exploit the visual information while neglect the audio information. In this paper, we argue that the audio modality can assist vision modality to better understand the video content and structure, and further benefit the summarization process. Motivated by this, we propose to jointly exploit the audio and visual information for the video summarization task, and develop an AudioVisual Recurrent Network (AVRN) to achieve this. Specifically, the proposed AVRN can be separated into three parts: 1) the two-stream LSTM is utilized to encode the audio and visual feature sequentially by capturing their temporal dependency. 2) the audiovisual fusion LSTM is employed to fuse the two modalities by exploring the latent consistency between them. 3) the self-attention video encoder is adopted to capture the global dependency in the video. Finally, the fused audiovisual information, and the integrated temporal and global dependencies are jointly used to predict the video summary. Practically, the experimental results on the two benchmarks, \emph{i.e.,} SumMe and TVsum, have demonstrated the effectiveness of each part, and the superiority of AVRN compared to those approaches just exploiting visual information for video summarization.",0
"This paper presents a comprehensive survey on audio-visual video summarization techniques. The authors provide an overview of existing approaches including model-based methods and data-driven deep learning techniques. They compare different aspects such as quality of visual content extraction, sound integration, compression algorithms used, visual effects and interactive interfaces. Furthermore, they discuss evaluation metrics employed in related work and open challenges faced by researchers. Finally, they provide recommendations for future studies in this field. Overall, the article serves as a valuable resource for readers interested in video summarization.",1
"Transformer-based models consist of interleaved feed-forward blocks - that capture content meaning, and relatively more expensive self-attention blocks - that capture context meaning. In this paper, we explored trade-offs and ordering of the blocks to improve upon the current Transformer architecture and proposed PAR Transformer. It needs 35% lower compute time than Transformer-XL achieved by replacing ~63% of the self-attention blocks with feed-forward blocks, and retains the perplexity on WikiText-103 language modelling benchmark. We further validated our results on text8 and enwiki8 datasets, as well as on the BERT model.",0
"This article presents research on the benefits and drawbacks of paying attention when required versus dividing oneâ€™s focus across multiple tasks at once. Studies have shown that focusing attention on a single task can increase productivity, enhance learning outcomes, and improve memory retention compared to multi-tasking. However, dividing attention has been found to be beneficial in certain situations such as monitoring multiple sources of information, detecting changes or anomalies, and performing routine tasks efficiently without losing track of time and deadlines. Additionally, some individuals may possess natural cognitive abilities to effectively divide their attentional capacity while others need training and practice to strengthen their ability to shift and maintain focused attention. The study concludes that the optimal balance of attention allocation depends on personal characteristics, contextual factors, and task demands and suggests guidelines for managing divided attention in different scenarios.",1
"Accurate and explainable health event predictions are becoming crucial for healthcare providers to develop care plans for patients. The availability of electronic health records (EHR) has enabled machine learning advances in providing these predictions. However, many deep learning based methods are not satisfactory in solving several key challenges: 1) effectively utilizing disease domain knowledge; 2) collaboratively learning representations of patients and diseases; and 3) incorporating unstructured text. To address these issues, we propose a collaborative graph learning model to explore patient-disease interactions and medical domain knowledge. Our solution is able to capture structural features of both patients and diseases. The proposed model also utilizes unstructured text data by employing an attention regulation strategy and then integrates attentive text features into a sequential learning process. We conduct extensive experiments on two important healthcare problems to show the competitive prediction performance of the proposed method compared with various state-of-the-art models. We also confirm the effectiveness of learned representations and model interpretability by a set of ablation and case studies.",0
"This work presents a methodology for temporal event prediction using collaborative graph learning techniques that incorporate auxiliary text data. In healthcare settings, predicting patient events can greatly improve care outcomes and reduce costs. Previous methods have relied on domain-specific features such as medical codes and medications, but may not capture important contextual information from other sources like electronic health records (EHRs).  Our approach utilizes graph neural networks (GNN) to model interactions between entities present in EHRs and auxiliary text documents related to patients. GNNs allow us to efficiently learn representations of these complex relationships while considering both structured and unstructured inputs. We introduce two variants of our model: 1) a simple baseline architecture which learns a single representation shared by all types of data; and 2) a more advanced design which maintains separate node embeddings for different entity types within the same layer. By combining the strengths of both architectures, we achieve state-of-the-art results across multiple datasets measuring diagnostic accuracy, readmission risk, and mortality rate predictions.  Overall, this research demonstrates how effective collaboration between different modalities of data through novel machine learning techniques has great potential to positively impact healthcare decision making. Our study encourages further exploration into bridging traditional clinical data sources with complementary narrative content in order to enhance critical insights from digital patient profiles.",1
"One of the limiting factors in training data-driven, rare-event prediction algorithms is the scarcity of the events of interest resulting in an extreme imbalance in the data. There have been many methods introduced in the literature for overcoming this issue; simple data manipulation through undersampling and oversampling, utilizing cost-sensitive learning algorithms, or by generating synthetic data points following the distribution of the existing data. While synthetic data generation has recently received a great deal of attention, there are real challenges involved in doing so for high-dimensional data such as multivariate time series. In this study, we explore the usefulness of the conditional generative adversarial network (CGAN) as a means to perform data-informed oversampling in order to balance a large dataset of multivariate time series. We utilize a flare forecasting benchmark dataset, named SWAN-SF, and design two verification methods to both quantitatively and qualitatively evaluate the similarity between the generated minority and the ground-truth samples. We further assess the quality of the generated samples by training a classical, supervised machine learning algorithm on synthetic data, and testing the trained model on the unseen, real data. The results show that the classifier trained on the data augmented with the synthetic multivariate time series achieves a significant improvement compared with the case where no augmentation is used. The popular flare forecasting evaluation metrics, TSS and HSS, report 20-fold and 5-fold improvements, respectively, indicating the remarkable statistical similarities, and the usefulness of CGAN-based data generation for complicated tasks such as flare forecasting.",0
"This paper presents a novel approach for synthesizing multivariate time series data that can be used to improve flare forecasting models. Our method leverages state-of-the-art machine learning techniques such as Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE), along with statistical modeling methods like ARIMA and LSTM, to generate realistic time series data that captures complex relationships among different variables. We evaluate our approach on two benchmark datasets and demonstrate improved accuracy over traditional approaches. We believe our proposed methodology has significant potential for advancing flare forecasting capabilities and could lead to more accurate predictions of solar flares.",1
"We propose a novel Synergistic Attention Network (SA-Net) to address the light field salient object detection by establishing a synergistic effect between multi-modal features with advanced attention mechanisms. Our SA-Net exploits the rich information of focal stacks via 3D convolutional neural networks, decodes the high-level features of multi-modal light field data with two cascaded synergistic attention modules, and predicts the saliency map using an effective feature fusion module in a progressive manner. Extensive experiments on three widely-used benchmark datasets show that our SA-Net outperforms 28 state-of-the-art models, sufficiently demonstrating its effectiveness and superiority. Our code will be made publicly available.",0
"This abstract describes a new approach to light field saliency detection that leverages synergistic attention mechanisms. By incorporating both global context and local details into the model, our method achieves state-of-the-art performance on several benchmark datasets. Our key contributions include: (i) introducing a novel multi-scale attention module that captures diverse features across different levels of granularity; (ii) designing a progressive refinement network architecture that iteratively enhances object boundaries using feedback from previous iterations; and (iii) evaluating our framework under multiple evaluation metrics and demonstrating consistent improvements over baseline methods.",1
"In this work we tackle the task of video-based visual emotion recognition in the wild. Standard methodologies that rely solely on the extraction of bodily and facial features often fall short of accurate emotion prediction in cases where the aforementioned sources of affective information are inaccessible due to head/body orientation, low resolution and poor illumination. We aspire to alleviate this problem by leveraging visual context in the form of scene characteristics and attributes, as part of a broader emotion recognition framework. Temporal Segment Networks (TSN) constitute the backbone of our proposed model. Apart from the RGB input modality, we make use of dense Optical Flow, following an intuitive multi-stream approach for a more effective encoding of motion. Furthermore, we shift our attention towards skeleton-based learning and leverage action-centric data as means of pre-training a Spatial-Temporal Graph Convolutional Network (ST-GCN) for the task of emotion recognition. Our extensive experiments on the challenging Body Language Dataset (BoLD) verify the superiority of our methods over existing approaches, while by properly incorporating all of the aforementioned modules in a network ensemble, we manage to surpass the previous best published recognition scores, by a large margin.",0
"This paper proposes a new method for recognizing human emotions from video data that leverages semantic scene characteristics along with multi-stream convolutional architectures in a contextual approach. We explore three different types of features: facial expression features, appearance features, and contextual features related to the scene in which the emotion was observed. Our model incorporates these features into a deep neural network architecture designed specifically for visual emotion recognition in challenging real-world scenarios. Extensive experiments on several benchmark datasets demonstrate that our proposed approach significantly outperforms state-of-the art methods for visual emotion recognition in both laboratory and natural settings. Our work highlights the importance of understanding the complex relationship between human emotional expressions and their surroundings, as well as the value of developing models capable of handling real-world complexity and variability. We believe that our findings have significant implications for applications such as affective computing, social robotics, and mental health monitoring. Overall, we hope that our research contributes to a better understanding of how humans express and experience emotions in everyday life.",1
"This paper investigates the principles of embedding learning to tackle the challenging semi-supervised video object segmentation. Unlike previous practices that focus on exploring the embedding learning of foreground object (s), we consider background should be equally treated. Thus, we propose a Collaborative video object segmentation by Foreground-Background Integration (CFBI) approach. CFBI separates the feature embedding into the foreground object region and its corresponding background region, implicitly promoting them to be more contrastive and improving the segmentation results accordingly. Moreover, CFBI performs both pixel-level matching processes and instance-level attention mechanisms between the reference and the predicted sequence, making CFBI robust to various object scales. Based on CFBI, we introduce a multi-scale matching structure and propose an Atrous Matching strategy, resulting in a more robust and efficient framework, CFBI+. We conduct extensive experiments on two popular benchmarks, i.e., DAVIS and YouTube-VOS. Without applying any simulated data for pre-training, our CFBI+ achieves the performance (J&F) of 82.9% and 82.8%, outperforming all the other state-of-the-art methods. Code: https://github.com/z-x-yang/CFBI.",0
"This paper presents a new algorithm called Collaborative Video Object Segmentation (CVOS) that utilizes multi-scale foreground-background integration to accurately segment objects from moving backgrounds. CVOS leverages a collaboration between different layers of convolutional neural networks (CNNs), as well as multiple frame feature maps. By doing so, CVOS can effectively model both short-term dynamics such as object motion, as well as more global scene context. Our experiments demonstrate that our method outperforms prior approaches on a variety of challenging video sequences, including those with large changes in camera viewpoint and cluttered scenes. In addition, we show how CVOS can be used to improve state-of-the-art methods that rely heavily on object detection, demonstrating its versatility across application domains.",1
"Encoder-decoder models have made great progress on handwritten mathematical expression recognition recently. However, it is still a challenge for existing methods to assign attention to image features accurately. Moreover, those encoder-decoder models usually adopt RNN-based models in their decoder part, which makes them inefficient in processing long $\LaTeX{}$ sequences. In this paper, a transformer-based decoder is employed to replace RNN-based ones, which makes the whole model architecture very concise. Furthermore, a novel training strategy is introduced to fully exploit the potential of the transformer in bidirectional language modeling. Compared to several methods that do not use data augmentation, experiments demonstrate that our model improves the ExpRate of current state-of-the-art methods on CROHME 2014 by 2.23%. Similarly, on CROHME 2016 and CROHME 2019, we improve the ExpRate by 1.92% and 2.28% respectively.",0
"Abstract: In recent years, deep learning methods have shown great promise for handwritten mathematical expression recognition (HMER). One successful approach has been the use of bidirectional recurrent neural networks (RNNs) trained on sequence data generated from converted images of expressions. However, the computational cost and memory requirements of these models can limit their effectiveness, especially when dealing with large datasets. To address this issue, we propose using transformers, which are highly parallelizable architectures that require less memory compared to RNNs. Our proposed model uses a novel training scheme where both forward and backward passes through the network contribute to the final output. Experimental results show significant improvements over previous state-of-the-art HMER approaches based on traditional CNN+RNN architectures. These findings suggest that our bidirectionally trained transformer architecture represents a promising new direction for efficient and accurate handwritten mathematics recognition.",1
"Fast and effective responses are required when a natural disaster (e.g., earthquake, hurricane, etc.) strikes. Building damage assessment from satellite imagery is critical before relief effort is deployed. With a pair of pre- and post-disaster satellite images, building damage assessment aims at predicting the extent of damage to buildings. With the powerful ability of feature representation, deep neural networks have been successfully applied to building damage assessment. Most existing works simply concatenate pre- and post-disaster images as input of a deep neural network without considering their correlations. In this paper, we propose a novel two-stage convolutional neural network for Building Damage Assessment, called BDANet. In the first stage, a U-Net is used to extract the locations of buildings. Then the network weights from the first stage are shared in the second stage for building damage assessment. In the second stage, a two-branch multi-scale U-Net is employed as backbone, where pre- and post-disaster images are fed into the network separately. A cross-directional attention module is proposed to explore the correlations between pre- and post-disaster images. Moreover, CutMix data augmentation is exploited to tackle the challenge of difficult classes. The proposed method achieves state-of-the-art performance on a large-scale dataset -- xBD. The code is available at https://github.com/ShaneShen/BDANet-Building-Damage-Assessment.",0
"This paper presents BDANet, a novel multiscale convolutional neural network architecture that utilizes cross-directional attention mechanisms for building damage assessment from satellite images. The proposed model leverages both global context and local features to accurately identify damaged buildings following natural disasters. In addition, the use of cross-directional attention allows for efficient processing of both horizontal and vertical image information. Experimental results demonstrate the effectiveness of our approach, outperforming state-of-the-art methods on several benchmark datasets. Our method has important implications for rapid response efforts during disaster relief operations by providing accurate and timely assessments of infrastructure damage.",1
"Generating images from a single sample, as a newly developing branch of image synthesis, has attracted extensive attention. In this paper, we formulate this problem as sampling from the conditional distribution of a single image, and propose a hierarchical framework that simplifies the learning of the intricate conditional distributions through the successive learning of the distributions about structure, semantics and texture, making the process of learning and generation comprehensible. On this basis, we design ExSinGAN composed of three cascaded GANs for learning an explainable generative model from a given image, where the cascaded GANs model the distributions about structure, semantics and texture successively. ExSinGAN is learned not only from the internal patches of the given image as the previous works did, but also from the external prior obtained by the GAN inversion technique. Benefiting from the appropriate combination of internal and external information, ExSinGAN has a more powerful capability of generation and competitive generalization ability for the image manipulation tasks compared with prior works.",0
"Title: ""Learning an Explainable Generative Model from a Single Image"" (ExSinGAN)  Abstract: We propose a novel approach to generating high-resolution images by leveraging the power of Generative Adversarial Networks (GAN). Our method, dubbed ExSinGAN, takes advantage of attention mechanisms to learn meaningful representations that capture both local and global contextual relationships present in input images. This allows us to produce highly detailed outputs while maintaining interpretability even when trained on single images. By applying these learned representations to synthesize new content, we achieve state-of-the art results comparable to previous works using more complex architectures, making our model competitive with leading approaches despite only requiring one example as input. Furthermore, we show how our trained models can perform well on both image generation tasks and downstream image classification benchmarks, which demonstrates their utility for various visual tasks beyond generative modelling.",1
"The Reward-Biased Maximum Likelihood Estimate (RBMLE) for adaptive control of Markov chains was proposed to overcome the central obstacle of what is variously called the fundamental ""closed-identifiability problem"" of adaptive control, the ""dual control problem"", or, contemporaneously, the ""exploration vs. exploitation problem"". It exploited the key observation that since the maximum likelihood parameter estimator can asymptotically identify the closed-transition probabilities under a certainty equivalent approach, the limiting parameter estimates must necessarily have an optimal reward that is less than the optimal reward attainable for the true but unknown system. Hence it proposed a counteracting reverse bias in favor of parameters with larger optimal rewards, providing a solution to the fundamental problem alluded to above. It thereby proposed an optimistic approach of favoring parameters with larger optimal rewards, now known as ""optimism in the face of uncertainty"". The RBMLE approach has been proved to be long-term average reward optimal in a variety of contexts. However, modern attention is focused on the much finer notion of ""regret"", or finite-time performance. Recent analysis of RBMLE for multi-armed stochastic bandits and linear contextual bandits has shown that it not only has state-of-the-art regret, but it also exhibits empirical performance comparable to or better than the best current contenders, and leads to strikingly simple index policies. Motivated by this, we examine the finite-time performance of RBMLE for reinforcement learning tasks that involve the general problem of optimal control of unknown Markov Decision Processes. We show that it has a regret of $\mathcal{O}( \log T)$ over a time horizon of $T$ steps, similar to state-of-the-art algorithms. Simulation studies show that RBMLE outperforms other algorithms such as UCRL2 and Thompson Sampling.",0
"In reinforcement learning (RL), accurate models of states and policies enable more efficient learning. Traditional methods estimate these parameters via maximum likelihood estimation under full knowledge of the environment. However, realistically we may never know all aspects about environments even if there were some way we could interactively query them. This paper proposes a new method called reward biased maximum likelihood estimation that uses both environmental rewards and transition dynamics as constraints. By combining multiple sources of signals available during interaction with complex real world problems, our approach can achieve better policy quality on benchmark problems where standard models fail catastrophically early due to model mismatch and exploration issues. Experimental results show significantly improved stability and adaptivity by using estimated transitions and incorporating prior beliefs into the optimization problem. We hope that this work represents an important step towards solving general, continuous state problems in RL that has been stuck at suboptimal solutions for years.",1
"The HGR is a quite challenging task as its performance is influenced by various aspects such as illumination variations, cluttered backgrounds, spontaneous capture, etc. The conventional CNN networks for HGR are following two stage pipeline to deal with the various challenges: complex signs, illumination variations, complex and cluttered backgrounds. The existing approaches needs expert expertise as well as auxiliary computation at stage 1 to remove the complexities from the input images. Therefore, in this paper, we proposes an novel end-to-end compact CNN framework: fine grained feature attentive network for hand gesture recognition (Fit-Hand) to solve the challenges as discussed above. The pipeline of the proposed architecture consists of two main units: FineFeat module and dilated convolutional (Conv) layer. The FineFeat module extracts fine grained feature maps by employing attention mechanism over multiscale receptive fields. The attention mechanism is introduced to capture effective features by enlarging the average behaviour of multi-scale responses. Moreover, dilated convolution provides global features of hand gestures through a larger receptive field. In addition, integrated layer is also utilized to combine the features of FineFeat module and dilated layer which enhances the discriminability of the network by capturing complementary context information of hand postures. The effectiveness of Fit- Hand is evaluated by using subject dependent (SD) and subject independent (SI) validation setup over seven benchmark datasets: MUGD-I, MUGD-II, MUGD-III, MUGD-IV, MUGD-V, Finger Spelling and OUHANDS, respectively. Furthermore, to investigate the deep insights of the proposed Fit-Hand framework, we performed ten ablation study.",0
"Here is my attempt at writing such an abstract for your research paper. If you have any suggestions for improvements, please let me know!  ---  The problem of hand gesture recognition remains challenging, especially in unconstrained environments where large variations exist across users, actions, and backgrounds. To tackle these issues, we propose One for All (OFA), an end-to-end compact solution that utilizes novel components and achieves high accuracy without requiring complex fine-tuning on each specific task. Our key innovations include exploiting spatial transformation networks to achieve accurate bounding boxes in the presence of occlusion; developing a new approach called BGGR, which generates robust pseudo ground truth from raw frames by clustering trajectories rather than relying on laborious annotations; using adversarial training against two discriminators based on real/fake and feature matching to strengthen OFA against distribution shift; generating more generalized features via random Gaussian scale augmentation during training instead of employing costly data augments; leveraging both temporal and spatial context via spatio-temporal attention; and designing a lightweight architecture tailored for efficient inference. Extensive experiments demonstrate OFAâ€™s superiority over state-of-the-art methods, generalizing well across different datasets under diverse evaluation metrics, while offering smaller model size and competitive computational requirements. This work constitutes a significant step towards deployable solutions that can perform reliable gestural interaction with limited resources in real-world settings.  ---",1
"Visual Question Answering (VQA) models have achieved significant success in recent times. Despite the success of VQA models, they are mostly black-box models providing no reasoning about the predicted answer, thus raising questions for their applicability in safety-critical such as autonomous systems and cyber-security. Current state of the art fail to better complex questions and thus are unable to exploit compositionality. To minimize the black-box effect of these models and also to make them better exploit compositionality, we propose a Dynamic Neural Network (DMN), which can understand a particular question and then dynamically assemble various relatively shallow deep learning modules from a pool of modules to form a network. We incorporate compositional temporal attention to these deep learning based modules to increase compositionality exploitation. This results in achieving better understanding of complex questions and also provides reasoning as to why the module predicts a particular answer. Experimental analysis on the two benchmark datasets, VQA2.0 and CLEVR, depicts that our model outperforms the previous approaches for Visual Question Answering task as well as provides better reasoning, thus making it reliable for mission critical applications like safety and security.",0
"Title: Sharing Knowledge through Transparent Artificial Intelligence  Artificial intelligence (AI) has made significant strides over recent years due to advances in machine learning techniques such as deep neural networks. However, there remains a lack of transparency and explainability in these models, which hinders our ability to trust and use them effectively. This research presents a novel approach to enhance explainability in AI systems by leveraging compositional temporal attention mechanisms that enable efficient knowledge sharing among agents. Our method allows us to model complex relationships between different components of AI systems while ensuring human-like reasoning and decision making processes. We evaluate the effectiveness of our proposed framework across several real world scenarios ranging from natural language understanding to computer vision tasks. Results demonstrate a clear improvement in both accuracy and explainability over existing state-of-the-art approaches. By providing transparent insights into how AI decisions are made, our work takes a step towards building more reliable and trustworthy artificial intelligence systems that can truly benefit society.",1
"Cross-modal hashing, favored for its effectiveness and efficiency, has received wide attention to facilitating efficient retrieval across different modalities. Nevertheless, most existing methods do not sufficiently exploit the discriminative power of semantic information when learning the hash codes, while often involving time-consuming training procedure for handling the large-scale dataset. To tackle these issues, we formulate the learning of similarity-preserving hash codes in terms of orthogonally rotating the semantic data so as to minimize the quantization loss of mapping such data to hamming space, and propose an efficient Fast Discriminative Discrete Hashing (FDDH) approach for large-scale cross-modal retrieval. More specifically, FDDH introduces an orthogonal basis to regress the targeted hash codes of training examples to their corresponding semantic labels, and utilizes ""-dragging technique to provide provable large semantic margins. Accordingly, the discriminative power of semantic information can be explicitly captured and maximized. Moreover, an orthogonal transformation scheme is further proposed to map the nonlinear embedding data into the semantic subspace, which can well guarantee the semantic consistency between the data feature and its semantic representation. Consequently, an efficient closed form solution is derived for discriminative hash code learning, which is very computationally efficient. In addition, an effective and stable online learning strategy is presented for optimizing modality-specific projection functions, featuring adaptivity to different training sizes and streaming data. The proposed FDDH approach theoretically approximates the bi-Lipschitz continuity, runs sufficiently fast, and also significantly improves the retrieval performance over the state-of-the-art methods. The source code is released at: https://github.com/starxliu/FDDH.",0
"This is an extended version of the paper, which will present fast discriminative hashing for large-scale cross-modal retrieval. Unlike conventional works that focus on pairwise distances among instances, we aim at optimizing a general kernel function that can adaptively capture both instance similarity and their relations from different modalities (e.g., image and text). In practice, our method achieves state-of-the-art performance and outperforms existing approaches by a wide margin on several benchmark datasets. Our approach is scalable and robust to high dimensionality, making it well suited for large scale applications such as social media analysis and medical data mining. We believe this work provides new insights into understanding and solving real world problems in multimedia computing, and contributes significantly to the field of computer vision and machine learning research.",1
"One commonly used clinical approach towards detecting melanomas recognises the existence of Ugly Duckling nevi, or skin lesions which look different from the other lesions on the same patient. An automatic method of detecting and analysing these lesions would help to standardize studies, compared with manual screening methods. However, it is difficult to obtain expertly-labelled images for ugly duckling lesions. We therefore propose to use self-supervised machine learning to automatically detect outlier lesions. We first automatically detect and extract all the lesions from a wide-field skin image, and calculate an embedding for each detected lesion in a patient image, based on automatically identified features. These embeddings are then used to calculate the L2 distances as a way to measure dissimilarity. Using this deep learning method, Ugly Ducklings are identified as outliers which should deserve more attention from the examining physician. We evaluate through comparison with dermatologists, and achieve a sensitivity rate of 72.1% and diagnostic accuracy of 94.2% on the held-out test set.",0
"This paper seeks to determine whether self-training can effectively identify ""ugly duckling"" skin lesions, which are atypical moles that may indicate melanoma but have no clear diagnostic features. Self-training, a form of machine learning where algorithms train themselves on large amounts of data, has been shown to be effective in identifying mammograms indicating cancerous masses but little research has been done in dermatology. The study involved gathering photographs of moles from patients diagnosed with either benign nevi (common moles) or malignant melanomas. These photos were used to create a dataset and then fed into two different types of self-trained deep neural networks: AlexNet and VGG-Face. Both models produced high levels of accuracy in detecting cancerous moles, outperforming human physicians in some cases. However, there was little difference in performance between the two types of models, suggesting that simpler solutions such as CNN classifiers might perform just as well in certain situations. Overall, the results suggest that self-training using convolutional neural networks could potentially serve as a valuable tool for detecting early stages of skin cancer.",1
"Distance correlation has gained much recent attention in the data science community: the sample statistic is straightforward to compute and asymptotically equals zero if and only if independence, making it an ideal choice to discover any type of dependency structure given sufficient sample size. One major bottleneck is the testing process: because the null distribution of distance correlation depends on the underlying random variables and metric choice, it typically requires a permutation test to estimate the null and compute the p-value, which is very costly for large amount of data. To overcome the difficulty, in this paper we propose a chi-square test for distance correlation. Method-wise, the chi-square test is non-parametric, extremely fast, and applicable to bias-corrected distance correlation using any strong negative type metric or characteristic kernel. The test exhibits a similar testing power as the standard permutation test, and can be utilized for K-sample and partial testing. Theory-wise, we show that the underlying chi-square distribution well approximates and dominates the limiting null distribution in upper tail, prove the chi-square test can be valid and universally consistent for testing independence, and establish a testing power inequality with respect to the permutation test.",0
"This paper examines the chi-square test of distance correlation (CDC), which is used to analyze data that has a nonlinear relationship between variables. CDC is similar to the standard Pearson correlation coefficient, but allows for more complex relationships by considering both linear and higher order terms. However, there is still limited research on the properties of CDC and how it compares to other methods. In this study, we investigate the performance of CDC under different scenarios using simulations. We find that CDC generally performs well compared to competing methods, although it can produce inflated Type I errors when the sample size is small or the true association between variables is weak. Overall, our results suggest that CDC may be a valuable tool for analyzing nonlinear relationships in certain situations, particularly where high dimensionality makes traditional regression models challenging. More work is required to fully understand the strengths and limitations of CDC and determine its optimal use cases in practice.",1
"Self-supervision has demonstrated to be an effective learning strategy when training target tasks on small annotated data-sets. While current research focuses on creating novel pretext tasks to learn meaningful and reusable representations for the target task, these efforts obtain marginal performance gains compared to fully-supervised learning. Meanwhile, little attention has been given to study the robustness of networks trained in a self-supervised manner. In this work, we demonstrate that networks trained via self-supervised learning have superior robustness and generalizability compared to fully-supervised learning in the context of medical imaging. Our experiments on pneumonia detection in X-rays and multi-organ segmentation in CT yield consistent results exposing the hidden benefits of self-supervision for learning robust feature representations.",0
"This abstract evaluates the robustness of self-supervised learning in medical imaging by exploring how well the method performs under different levels of supervision. Self-supervised learning (SSL) refers to machine learning algorithms that learn from data without explicit labels or guidance. In the context of medical imaging, SSL has shown promising results as a means to reduce reliance on manually labeled training data, which can be costly and time-consuming to obtain. However, little attention has been paid to understanding how the performance of SSL methods varies based on the amount of available labeled data. To address this gap, we conducted experiments using several popular SSL algorithms on two publicly available medical image datasets, including one with limited labeled data (268 images), and another with more comprehensive annotations (479 images). We compared the accuracy of these models against their supervised counterparts trained on full labels, demonstrating consistent improvements in both datasets. Furthermore, we found that SSL models trained on large amounts of unlabeled data outperformed those trained solely on small label sets. These findings suggest that self-supervised learning can indeed yield high-performing models in the low data regime while requiring minimal human intervention. Our work highlights the need for continued research into SSL techniques tailored specifically to medical imaging domains where data annotation remains challenging. By further investigating the impact of varying degrees of supervision on SSL algorithms, our study paves the way towards building more efficient and effective healthcare technologies grounded in robust artificial intelligence.",1
"Recent years have witnessed unprecedented success achieved by deep learning models in the field of computer vision. However, their vulnerability towards carefully crafted adversarial examples has also attracted the increasing attention of researchers. Motivated by the observation that adversarial examples are due to the non-robust feature learned from the original dataset by models, we propose the concepts of salient feature(SF) and trivial feature(TF). The former represents the class-related feature, while the latter is usually adopted to mislead the model. We extract these two features with coupled generative adversarial network model and put forward a novel detection and defense method named salient feature extractor (SFE) to defend against adversarial attacks. Concretely, detection is realized by separating and comparing the difference between SF and TF of the input. At the same time, correct labels are obtained by re-identifying SF to reach the purpose of defense. Extensive experiments are carried out on MNIST, CIFAR-10, and ImageNet datasets where SFE shows state-of-the-art results in effectiveness and efficiency compared with baselines. Furthermore, we provide an interpretable understanding of the defense and detection process.",0
"In recent years, deep neural networks (DNNs) have been widely used in various applications due to their high accuracy and robustness. However, DNNs are known to be vulnerable to adversarial attacks that can cause them to make incorrect predictions even with small perturbations to inputs. These attacks can significantly impact the safety and reliability of systems relying on DNNs, making it necessary to develop effective defense methods. One approach to improve the resilience of DNNs against adversarial attacks is through feature extraction using salient features, which emphasize important parts of input images while suppressing unimportant regions.  This work presents a novel method for designing a salient feature extractor as a plug-and-play module for use in adversarial defenses. Our framework employs attention mechanisms inspired by human visual processing to identify discriminative image regions and reduce the effectiveness of attackers. To achieve state-of-the-art performance in both clean and adversarial settings, our extractor adaptively weighs different local contextual features and utilizes multi-scale representations. We evaluate our model extensively across several benchmark datasets, including CIFAR-10, CIFAR-100, SVHN, ImageNet, and Stable Diffusion, demonstrating consistent improvements over existing methods across different architectures and attack scenarios. Additionally, ablation studies validate the contributions made by individual components in our proposed extractor architecture. Overall, our findings suggest that incorporating salient feature extraction can effectively strengthen DNNs against a wide range of adversarial attacks, making them more suitable for real-world applications.",1
"In recent years, neural network-based anomaly detection methods have attracted considerable attention in the hyperspectral remote sensing domain due to the powerful reconstruction ability compared with traditional methods. However, actual probability distribution statistics hidden in the latent space are not discovered by exploiting the reconstruction error because the probability distribution of anomalies is not explicitly modeled. To address the issue, we propose a novel probability distribution representation detector (PDRD) that explores the intrinsic distribution of both the background and the anomalies in original data for hyperspectral anomaly detection in this paper. First, we represent the hyperspectral data with multivariate Gaussian distributions from a probabilistic perspective. Then, we combine the local statistics with the obtained distributions to leverage the spatial information. Finally, the difference between the corresponding distributions of the test pixel and the average expectation of the pixels in the Chebyshev neighborhood is measured by computing the modified Wasserstein distance to acquire the detection map. We conduct the experiments on four real data sets to evaluate the performance of our proposed method. Experimental results demonstrate the accuracy and efficiency of our proposed method compared to the state-of-the-art detection methods.",0
"Exploring the Intrinsic Probability Distribution of hyperspectral anomaly detection reveals important insights into how we can better identify and characterize novel phenomena across different domains. By leveraging advances in probabilistic modeling techniques, our study shows that the intrinsic probability distribution is a powerful tool for capturing spatially varying patterns of heteroscedasticity present in hyperspectral imagery. This allows us to more accurately estimate the likelihood of occurrence for both known and unknown features, improving overall anomaly detection performance while reducing uncertainty. We evaluate our method on several public benchmark datasets spanning natural environments ranging from coastal habitats to urban scenes, demonstrating state-of-the art results while remaining highly interpretive for use in real world applications. Through these findings we aim to contribute towards building greater confidence in automated feature discovery for scientific researchers working across diverse application areas. Explore a new dimension in detecting anomalies! Discover the power hidden within your data through understanding the intrinsic probability distribution of hyperspectral images. Improve accuracy, reduce uncertainty, and unlock new possibilities for identifying novel phenomena. Let cutting-edge techniques guide you toward confident decisions backed by interpretable analysis methods tailored specifically for your unique domain challenges. Join scientists at the forefront of exploration and push the boundaries of knowledge today. Find out why leading experts trust intrinsic probabilities for their next groundbreaking discoveries - it might just surprise you!",1
"This paper aims to address few-shot semantic segmentation. While existing prototype-based methods have achieved considerable success, they suffer from uncertainty and ambiguity caused by limited labelled examples. In this work, we propose attentional prototype inference (API), a probabilistic latent variable framework for few-shot semantic segmentation. We define a global latent variable to represent the prototype of each object category, which we model as a probabilistic distribution. The probabilistic modeling of the prototype enhances the model's generalization ability by handling the inherent uncertainty caused by limited data and intra-class variations of objects. To further enhance the model, we introduce a local latent variable to represent the attention map of each query image, which enables the model to attend to foreground objects while suppressing background. The optimization of the proposed model is formulated as a variational Bayesian inference problem, which is established by amortized inference networks.We conduct extensive experiments on three benchmarks, where our proposal obtains at least competitive and often better performance than state-of-the-art methods. We also provide comprehensive analyses and ablation studies to gain insight into the effectiveness of our method for few-shot semantic segmentation.",0
"This work presents an approach that leverages attention mechanisms to enable few-shot semantic segmentation. Our method takes advantage of recent advancements in few-shot learning by utilizing meta learnable attention modules (MLAMs) to infer attentional prototypes from support images. These prototypes serve as strong priors for novel classes at inference time, allowing our framework to make accurate predictions even for unseen classes. We evaluate our model on three popular benchmark datasets, including PASCAL VOC, COCO, and Cityscapes, and demonstrate state-of-the-art results across all three. Additionally, we perform an ablation study to investigate the contribution of each component in our framework. Our findings suggest that MLAMs play a crucial role in producing high quality attentional prototypes, leading to significant improvements over baseline methods. Overall, this research offers new insights into few-shot semantic segmentation, paving the way for more effective algorithms in this important area of computer vision.",1
"Video Question Answering (VideoQA) is a challenging video understanding task since it requires a deep understanding of both question and video. Previous studies mainly focus on extracting sophisticated visual and language embeddings, fusing them by delicate hand-crafted networks. However, the relevance of different frames, objects, and modalities to the question are varied along with the time, which is ignored in most of existing methods. Lacking understanding of the the dynamic relationships and interactions among objects brings a great challenge to VideoQA task. To address this problem, we propose a novel Relation-aware Hierarchical Attention (RHA) framework to learn both the static and dynamic relations of the objects in videos. In particular, videos and questions are embedded by pre-trained models firstly to obtain the visual and textual features. Then a graph-based relation encoder is utilized to extract the static relationship between visual objects. To capture the dynamic changes of multimodal objects in different video frames, we consider the temporal, spatial, and semantic relations, and fuse the multimodal features by hierarchical attention mechanism to predict the answer. We conduct extensive experiments on a large scale VideoQA dataset, and the experimental results demonstrate that our RHA outperforms the state-of-the-art methods.",0
"In recent years, we have seen rapid development on computer vision task such as video question answering (VQA). VQA aims at understanding natural language queries posed by users against videos using deep learning techniques. Various methods have been proposed to tackle different aspects of VQA task including attention mechanism, multimodal fusion and relation reasoning among others. However most of these approaches operate independently without considering their interactions and dependencies with each other. This paper presents the first attempt to study relation reasoning under hierarchical attention framework which simultaneously models interdependencies across multiple modalities like image/audio features and linguistics tokens under both intra-modality and cross-modality contexts. We propose a novel model that performs hierarchical attention on both local visual regions and global questions, followed by temporal attention over video segments, enabling our method to attend to salient subtasks adaptively based on current understanding of query semantics instead of uniform scan through entire input. Additionally, a multi-level distillation strategy is introduced for better performance generalization. To evaluate the effectiveness of the proposed approach, we conduct extensive experiments and ablation studies on three popular datasets: GVCeleb, TGIF-LSVRC2016 and MovieQA respectively where all of them demonstrate the superiority compared to state-of-the-art baselines. Overall, the results suggest that the proposed approach is capable of jointly modeling inter-dependency of different cues, leading to more accurate VQA answers.",1
"The recent growth of web video sharing platforms has increased the demand for systems that can efficiently browse, retrieve and summarize video content. Query-aware multi-video summarization is a promising technique that caters to this demand. In this work, we introduce a novel Query-Aware Hierarchical Pointer Network for Multi-Video Summarization, termed DeepQAMVS, that jointly optimizes multiple criteria: (1) conciseness, (2) representativeness of important query-relevant events and (3) chronological soundness. We design a hierarchical attention model that factorizes over three distributions, each collecting evidence from a different modality, followed by a pointer network that selects frames to include in the summary. DeepQAMVS is trained with reinforcement learning, incorporating rewards that capture representativeness, diversity, query-adaptability and temporal coherence. We achieve state-of-the-art results on the MVS1K dataset, with inference time scaling linearly with the number of input video frames.",0
"This paper presents a novel approach to multi-video summarization that leverages query awareness to capture important events relevant to user queries. Our method, called ""DeepQAMVS"", employs a hierarchical pointer network architecture designed specifically for summarizing videos given a query. We introduce two key components: 1) a temporal attention mechanism that allows our model to focus on semantically meaningful segments within each video, and 2) an inter-modal attention module that integrates visual features from multiple videos based on relevance to the user's query. An extensive evaluation on several benchmark datasets demonstrates the effectiveness and efficiency of our proposed framework compared to state-of-the-art methods. Our work advances the field of multimedia summarization by incorporating real-world constraints such as user feedback and input context into the learning process.",1
"The assignment of importance scores to particular frames or (short) segments in a video is crucial for summarization, but also a difficult task. Previous work utilizes only one source of visual features. In this paper, we suggest a novel model architecture that combines three feature sets for visual content and motion to predict importance scores. The proposed architecture utilizes an attention mechanism before fusing motion features and features representing the (static) visual content, i.e., derived from an image classification model. Comprehensive experimental evaluations are reported for two well-known datasets, SumMe and TVSum. In this context, we identify methodological issues on how previous work used these benchmark datasets, and present a fair evaluation scheme with appropriate data splits that can be used in future work. When using static and motion features with parallel attention mechanism, we improve state-of-the-art results for SumMe, while being on par with the state of the art for the other dataset.",0
"""In today's fast-paced world, video summarization has become increasingly important due to the vast amount of video content available online. Existing approaches typically rely on manually engineered features, which can limit their effectiveness. In this work, we propose a novel approach that uses multiple feature sets extracted from visual content as well as textual descriptions to improve the quality of video summaries. We then introduce parallel attention mechanisms to fuse these diverse features together in order to better capture key aspects of the input videos. Our experimental results demonstrate significant improvements over state-of-the-art methods on benchmark datasets.""",1
"Crowdsourcing has attracted much attention for its convenience to collect labels from non-expert workers instead of experts. However, due to the high level of noise from the non-experts, an aggregation model that learns the true label by incorporating the source credibility is required. In this paper, we propose a novel framework based on graph neural networks for aggregating crowd labels. We construct a heterogeneous graph between workers and tasks and derive a new graph neural network to learn the representations of nodes and the true labels. Besides, we exploit the unknown latent interaction between the same type of nodes (workers or tasks) by adding a homogeneous attention layer in the graph neural networks. Experimental results on 13 real-world datasets show superior performance over state-of-the-art models.",0
"This can be achieved by exploiting latent worker/task correlation through GCNs. The model jointly learns both task-specific features on worker node representations as well as worker-worker graph structures using GCN layers. By doing so, label aggregation could take into account both inter-node dependencies (between tasks) and intra-node dependencies (among workers assigned to the same task). In addition, the learned task similarity information from GCN-based representation learning improves crowdsourced label quality. Experiments show that our proposed approach outperforms several baseline methods under different settings.",1
"Financial markets are a source of non-stationary multidimensional time series which has been drawing attention for decades. Each financial instrument has its specific changing over time properties, making their analysis a complex task. Improvement of understanding and development of methods for financial time series analysis is essential for successful operation on financial markets. In this study we propose a volume-based data pre-processing method for making financial time series more suitable for machine learning pipelines. We use a statistical approach for assessing the performance of the method. Namely, we formally state the hypotheses, set up associated classification tasks, compute effect sizes with confidence intervals, and run statistical tests to validate the hypotheses. We additionally assess the trading performance of the proposed method on historical data and compare it to a previously published approach. Our analysis shows that the proposed volume-based method allows successful classification of the financial time series patterns, and also leads to better classification performance than a price action-based method, excelling specifically on more liquid financial instruments. Finally, we propose an approach for obtaining feature interactions directly from tree-based models on example of CatBoost estimator, as well as formally assess the relatedness of the proposed approach and SHAP feature interactions with a positive outcome.",0
"Investors rely on data analysis techniques such as machine learning (ML) models that can make accurate predictions about asset prices. However, these ML algorithms often suffer from poor interpretability due to their black box nature which makes them less trustworthy. This research proposes an explainable strategy using feature attribution methods for pattern extraction in trading decisions, thus leading towards a transparent framework where both humans and machines can collaborate effectively. By providing insight into model workings through simple visual representations, investment professionals can have more confidence in making informed decisions based on reliable quantitative results generated by automated systems.",1
"Pseudo-LiDAR-based methods for monocular 3D object detection have received considerable attention in the community due to the performance gains exhibited on the KITTI3D benchmark, in particular on the commonly reported validation split. This generated a distorted impression about the superiority of Pseudo-LiDAR-based (PL-based) approaches over methods working with RGB images only. Our first contribution consists in rectifying this view by pointing out and showing experimentally that the validation results published by PL-based methods are substantially biased. The source of the bias resides in an overlap between the KITTI3D object detection validation set and the training/validation sets used to train depth predictors feeding PL-based methods. Surprisingly, the bias remains also after geographically removing the overlap. This leaves the test set as the only reliable set for comparison, where published PL-based methods do not excel. Our second contribution brings PL-based methods back up in the ranking with the design of a novel deep architecture which introduces a 3D confidence prediction module. We show that 3D confidence estimation techniques derived from RGB-only 3D detection approaches can be successfully integrated into our framework and, more importantly, that improved performance can be obtained with a newly designed 3D confidence measure, leading to state-of-the-art performance on the KITTI3D benchmark.",0
"Despite advances made recently in monocular 2D object detection algorithms, pseudo-LiDAR methods have gained popularity due to their ability to use depth estimation techniques from a single RGB image. While promising results have been obtained using such methods, there remain unaddressed questions about whether they truly capture sufficient depth information from these images. This paper therefore investigates how confidence may be impacting the performance of pseudo-LiDAR methods for monocular 3D object detection. By comparing several state-of-the-art approaches on standard benchmark datasets, our findings suggest that insufficient consideration has been given to estimating depth uncertainty, leading to underestimation of errors and a lack of robustness to occlusions. Our research thus highlights a critical gap in current work on pseudo-LiDAR methods which must be addressed if significant improvements are desired in monocular 3D object detection tasks. We provide recommendations on incorporating uncertainty into future models, demonstrating through ablation studies that explicit modelling of depth uncertainty can improve both accuracy and robustness. Our contributions further extend to open source implementation of several baselines used in the study, enabling easier reproducibility and experimentation by other researchers in the field.",1
"In this work, we present a deep learning-based approach for image tampering localization fusion. This approach is designed to combine the outcomes of multiple image forensics algorithms and provides a fused tampering localization map, which requires no expert knowledge and is easier to interpret by end users. Our fusion framework includes a set of five individual tampering localization methods for splicing localization on JPEG images. The proposed deep learning fusion model is an adapted architecture, initially proposed for the image restoration task, that performs multiple operations in parallel, weighted by an attention mechanism to enable the selection of proper operations depending on the input signals. This weighting process can be very beneficial for cases where the input signal is very diverse, as in our case where the output signals of multiple image forensics algorithms are combined. Evaluation in three publicly available forensics datasets demonstrates that the performance of the proposed approach is competitive, outperforming the individual forensics techniques as well as another recently proposed fusion framework in the majority of cases.",0
"Here is a sample abstract: --------------------------- This article presents a novel method for detecting tampering in images using attention mechanisms. Our approach combines two key components: operation-wise attention and fusion. We first use a pretrained convolutional neural network (CNN) to extract features from the input image. Then, we apply three different operations â€“ pixel-wise, patch-wise and holistic operations - on these features. These operations capture local features at different levels of detail which can then be combined through attention modules. Finally, we fuse these outputs into one result. Experimental results show that our method achieves state-of-the-art performance compared to other algorithms designed for tamper detection task. This demonstrates the effectiveness of applying attention mechanisms along with fusion methods for tampered image identification. Overall, our framework is robust against multiple attack types such as copy & paste, splicing, addition, deletion and geometric transformations while maintaining a high level accuracy for non-tampered data sets. As future work, we plan to extend our system to incorporate more complex attacks including video tampering. By providing accurate tamper localization, our method could enable better security systems for media authentication by highlighting regions where tamper occurred.",1
"Source code summarization aims to generate natural language summaries from structured code snippets for better understanding code functionalities. However, automatic code summarization is challenging due to the complexity of the source code and the language gap between the source code and natural language summaries. Most previous approaches either rely on retrieval-based (which can take advantage of similar examples seen from the retrieval database, but have low generalization performance) or generation-based methods (which have better generalization performance, but cannot take advantage of similar examples). This paper proposes a novel retrieval-augmented mechanism to combine the benefits of both worlds. Furthermore, to mitigate the limitation of Graph Neural Networks (GNNs) on capturing global graph structure information of source code, we propose a novel attention-based dynamic graph to complement the static graph representation of the source code, and design a hybrid message passing GNN for capturing both the local and global structural information. To evaluate the proposed approach, we release a new challenging benchmark, crawled from diversified large-scale open-source C projects (total 95k+ unique functions in the dataset). Our method achieves the state-of-the-art performance, improving existing methods by 1.42, 2.44 and 1.29 in terms of BLEU-4, ROUGE-L and METEOR.",0
"Abstract: In this paper we present a novel approach for code summarization that combines retrieval and generation models using hybrid graph neural networks (GNNs). Our method first performs retrieval by finding similar code snippets from a large corpus based on contextual similarity, then uses these examples as guidance during the actual summary generation stage. We propose two types of hybrid architectures, one using a single Graph Convolutional Network (GCN) model for both retrieval and generation tasks, and another utilizing separate GCNs for each task with late fusion of their outputs. Experimental results on several benchmark datasets demonstrate significant improvements over state-of-the-art baselines across multiple evaluation metrics. Overall, our work highlights the effectiveness of combining retrieval and generation techniques for code summarization, especially when powered by advanced deep learning models such as GNNs. Further implications and future directions are discussed towards developing more comprehensive summarization tools for real-world use cases involving complex software systems.",1
"We introduce a unified framework to jointly model images, text, and human attention traces. Our work is built on top of the recent Localized Narratives annotation framework [30], where each word of a given caption is paired with a mouse trace segment. We propose two novel tasks: (1) predict a trace given an image and caption (i.e., visual grounding), and (2) predict a caption and a trace given only an image. Learning the grounding of each word is challenging, due to noise in the human-provided traces and the presence of words that cannot be meaningfully visually grounded. We present a novel model architecture that is jointly trained on dual tasks (controlled trace generation and controlled caption generation). To evaluate the quality of the generated traces, we propose a local bipartite matching (LBM) distance metric which allows the comparison of two traces of different lengths. Extensive experiments show our model is robust to the imperfect training data and outperforms the baselines by a clear margin. Moreover, we demonstrate that our model pre-trained on the proposed tasks can be also beneficial to the downstream task of COCO's guided image captioning. Our code and project page are publicly available.",0
"This research presents a novel approach to understanding how humans direct their attention during language comprehension tasks. Using natural language processing techniques, we analyze large datasets of eye tracking data collected from human participants engaged in reading and listening tasks. Our method involves modeling attentional traces that capture patterns of gaze fixations as individuals process written or spoken language. These models allow us to predict where people look next based on current linguistic context and previous fixations. By analyzing these predictions against actual eye movement behavior, we can evaluate the accuracy and reliability of our models in capturing human attentional processes. Ultimately, our work seeks to advance our understanding of how people connect meaning (what they need to know) with perception (where they look). This insight has important implications for applications such as automated tutoring systems, virtual reality environments, and interactive storytelling.",1
"Deepfakes are computer manipulated videos where the face of an individual has been replaced with that of another. Software for creating such forgeries is easy to use and ever more popular, causing serious threats to personal reputation and public security. The quality of classifiers for detecting deepfakes has improved with the releasing of ever larger datasets, but the understanding of why a particular video has been labelled as fake has not kept pace.   In this work we develop, extend and compare white-box, black-box and model-specific techniques for explaining the labelling of real and fake videos. In particular, we adapt SHAP, GradCAM and self-attention models to the task of explaining the predictions of state-of-the-art detectors based on EfficientNet, trained on the Deepfake Detection Challenge (DFDC) dataset. We compare the obtained explanations, proposing metrics to quantify their visual features and desirable characteristics, and also perform a user survey collecting users' opinions regarding the usefulness of the explainers.",0
"As deepfakes become more prevalent and difficult to detect, research has focused on developing methods for identifying these manipulated videos. In our paper, we examine two common approaches for explaining deepfake detection models: feature attribution maps and class activation maps (CAM). We evaluate both explainers using human annotations of whether the generated explanations accurately depict what's happening in the corresponding input video frames. Our results show that while feature attribution maps can sometimes provide accurate explanations, they often highlight irrelevant features and cannot adequately capture complex actions. On the other hand, CAM provides better spatial coverage but may generate spurious regions due to background noise. These findings suggest that current explainers have limitations and there is room for improvement in generating high-quality explanations for deepfake detection models.",1
"Soft biometrics analysis is seen as an important research topic, given its relevance to various applications. However, even though it is frequently seen as a solved task, it can still be very hard to perform in wild conditions, under varying image conditions, uncooperative poses, and occlusions. Considering the gender trait as our topic of study, we report an extensive analysis of the feasibility of its inference regarding image (resolution, luminosity, and blurriness) and subject-based features (face and body keypoints confidence). Using three state-of-the-art datasets (PETA, PA-100K, RAP) and five Person Attribute Recognition models, we correlate feature analysis with gender inference accuracy using the Shapley value, enabling us to perceive the importance of each image/subject-based feature. Furthermore, we analyze face-based gender inference and assess the pose effect on it. Our results suggest that: 1) image-based features are more influential for low-quality data; 2) an increase in image quality translates into higher subject-based feature importance; 3) face-based gender inference accuracy correlates with image quality increase; and 4) subjects' frontal pose promotes an implicit attention towards the face. The reported results are seen as a basis for subsequent developments of inference approaches in uncontrolled outdoor environments, which typically correspond to visual surveillance conditions.",0
"Title: Challenges Remain in Inferring Gender from Online Communications  Gender inference remains a difficult task even in natural language processing (NLP). Many NLP tasks rely on accurate gender information, yet few studies have explored how accurately gender can be inferred outside of controlled lab settings. This study examines whether gender inference is indeed a solved problem by looking at online communication data that reflects more realistic contexts. We find evidence of persistent challenges in accurately predicting gender based solely on text data without additional contextual knowledge. Our results suggest that further research may still be necessary to improve the accuracy of automated gender inference systems, particularly in open domains where there is greater variability in both language use and cultural background. Overall, our work highlights the importance of considering the limitations of current gender inference techniques and suggests potential areas for future improvement in this area of NLP.",1
"Facial expressions recognition (FER) of 3D face scans has received a significant amount of attention in recent years. Most of the facial expression recognition methods have been proposed using mainly 2D images. These methods suffer from several issues like illumination changes and pose variations. Moreover, 2D mapping from 3D images may lack some geometric and topological characteristics of the face. Hence, to overcome this problem, a multi-modal 2D + 3D feature-based method is proposed. We extract shallow features from the 3D images, and deep features using Convolutional Neural Networks (CNN) from the transformed 2D images. Combining these features into a compact representation uses covariance matrices as descriptors for both features instead of single-handedly descriptors. A covariance matrix learning is used as a manifold layer to reduce the deep covariance matrices size and enhance their discrimination power while preserving their manifold structure. We then use the Bag-of-Features (BoF) paradigm to quantize the covariance matrices after flattening. Accordingly, we obtained two codebooks using shallow and deep features. The global codebook is then used to feed an SVM classifier. High classification performances have been achieved on the BU-3DFE and Bosphorus datasets compared to the state-of-the-art methods.",0
"This paper introduces a novel method for quantizing features extracted from deep neural networks used for facial expression recognition tasks in three dimensions. By utilizing both deep and shallow covariance features we aim to improve upon existing approaches that only use one type of feature. We propose using the Gram matrix, which captures second order statistical relationships between different layers of the network, as our first set of features and we argue that they contain important information for classification. As our second set of features, we introduce a new approach based on shallow covariances between pairs of neurons in each layer of the network and show how these can complement deeper representations to further boost performance. Our experiments demonstrate state-of-the-art accuracy across multiple databases using very compact feature representations making them suitable for deployment on resource constrained devices.",1
"Most Video Super-Resolution (VSR) methods enhance a video reference frame by aligning its neighboring frames and mining information on these frames. Recently, deformable alignment has drawn extensive attention in VSR community for its remarkable performance, which can adaptively align neighboring frames with the reference one. However, we experimentally find that deformable alignment methods still suffer from fast motion due to locally loss-driven offset prediction and lack explicit motion constraints. Hence, we propose a Matching-based Flow Estimation (MFE) module to conduct global semantic feature matching and estimate optical flow as coarse offset for each location. And a Flow-guided Deformable Module (FDM) is proposed to integrate optical flow into deformable convolution. The FDM uses the optical flow to warp the neighboring frames at first. And then, the warped neighboring frames and the reference one are used to predict a set of fine offsets for each coarse offset. In general, we propose an end-to-end deep network called Flow-guided Deformable Alignment Network (FDAN), which reaches the state-of-the-art performance on two benchmark datasets while is still competitive in computation and memory consumption.",0
"This paper presents a new method for video super-resolution called FDAN (Flow-guided Deformable Alignment Network). The proposed approach leverages flow estimation to guide deformable alignment and utilizes convolutional neural networks to predict high-quality intermediate frames from low-resolution input frames. Through extensive experiments on publicly available datasets, we demonstrate that FDAN achieves state-of-the-art performance in terms of both quantitative metrics and subjective visual quality evaluations. Our results show that incorporating flow guidance significantly improves alignment accuracy compared to methods without flow estimation. Additionally, our ablation studies provide insights into the design choices made within the FDAN architecture. Overall, the proposed FDAN framework sets a new benchmark for video super-resolution performance.",1
"Knowledge distillation (KD) has recently emerged as an efficacious scheme for learning compact deep neural networks (DNNs). Despite the promising results achieved, the rationale that interprets the behavior of KD has yet remained largely understudied. In this paper, we introduce a novel task-oriented attention model, termed as KDExplainer, to shed light on the working mechanism underlying the vanilla KD. At the heart of KDExplainer is a Hierarchical Mixture of Experts (HME), in which a multi-class classification is reformulated as a multi-task binary one. Through distilling knowledge from a free-form pre-trained DNN to KDExplainer, we observe that KD implicitly modulates the knowledge conflicts between different subtasks, and in reality has much more to offer than label smoothing. Based on such findings, we further introduce a portable tool, dubbed as virtual attention module (VAM), that can be seamlessly integrated with various DNNs to enhance their performance under KD. Experimental results demonstrate that with a negligible additional cost, student models equipped with VAM consistently outperform their non-VAM counterparts across different benchmarks. Furthermore, when combined with other KD methods, VAM remains competent in promoting results, even though it is only motivated by vanilla KD. The code is available at https://github.com/zju-vipa/KDExplainer.",0
"In recent years, knowledge distillation has emerged as a popular technique for training smaller neural networks on the predictions of larger models without losing accuracy. However, explaining how knowledge distillation works remains a challenge due to its complex nature and lack of interpretability. This paper presents KDExplainer, a task-oriented attention model that can explain the workings of knowledge distillation using natural language explanations.  KDExplainer utilizes attention mechanisms to identify which parts of the teacher network are most important for training the student network during the distillation process. By visualizing the attention weights generated by the model, we show that our approach effectively explains why certain features or inputs matter more than others in the knowledge distillation process. Furthermore, we demonstrate the effectiveness of KDExplainer through extensive evaluation against human annotations and other baseline methods, showing significant improvements over existing state-of-the-art approaches.  Overall, KDExplainer offers a novel solution to the problem of explaining knowledge distillation and represents a step forward towards making machine learning models more interpretable. Our work paves the way for future research into developing transparent and efficient machine learning systems that can provide insights into their decision-making processes, ultimately improving trustworthiness in artificial intelligence.",1
"Object detection is widely studied in computer vision filed. In recent years, certain representative deep learning based detection methods along with solid benchmarks are proposed, which boosts the development of related researchs. However, existing detection methods still suffer from undesirable performance under challenges such as camouflage, blur, inter-class similarity, intra-class variance and complex environment. To address this issue, we propose LGA-RCNN which utilizes a loss-guided attention (LGA) module to highlight representative region of objects. Then, those highlighted local information are fused with global information for precise classification and localization.",0
"An effective object detection system relies on accurate bounding box prediction and quality semantic segmentation maps. To address this challenge we propose LGA-RCNN (Loss-Guided Attention for Region-based Convolutional Neural Networks), which utilizes attention mechanisms to emphasize important features at different scales. Our method uses both global and local attention modules to balance intra-class variations and inter-class separability. We show that our model achieves state-of-the-art performance across various benchmark datasets while maintaining efficient inference speed. Additionally, ablation studies demonstrate the effectiveness of each component in our framework.",1
"Inspired by the classic Sauvola local image thresholding approach, we systematically study it from the deep neural network (DNN) perspective and propose a new solution called SauvolaNet for degraded document binarization (DDB). It is composed of three explainable modules, namely, Multi-Window Sauvola (MWS), Pixelwise Window Attention (PWA), and Adaptive Sauolva Threshold (AST). The MWS module honestly reflects the classic Sauvola but with trainable parameters and multi-window settings. The PWA module estimates the preferred window sizes for each pixel location. The AST module further consolidates the outputs from MWS and PWA and predicts the final adaptive threshold for each pixel location. As a result, SauvolaNet becomes end-to-end trainable and significantly reduces the number of required network parameters to 40K -- it is only 1\% of MobileNetV2. In the meantime, it achieves the State-of-The-Art (SoTA) performance for the DDB task -- SauvolaNet is at least comparable to, if not better than, SoTA binarization solutions in our extensive studies on the 13 public document binarization datasets. Our source code is available at https://github.com/Leedeng/SauvolaNet.",0
"In recent years, there has been growing interest in the field of document binarization, which involves converting scanned documents into digital images that can easily be read by computers. One popular method for performing document binarization is through the use of Sauvola networks, which are capable of accurately separating text from backgrounds with high accuracy even under challenging conditions such as degradation and noise. However, traditional Sauvola networks have limited adaptability, meaning they cannot effectively handle changes in illumination, contrast, or other factors within individual pages.  This paper presents a new approach called ""SauvolaNet"" that addresses these limitations. By incorporating an adaptive mechanism into the existing Sauvola network architecture, our proposed model learns how to adjust itself on a per-pixel basis to better account for variations in image quality. This allows SauvolaNet to achieve state-of-the-art performance on a range of publicly available benchmark datasets containing degraded documents, including historical texts with faded characters and modern documents with low resolution.  To evaluate SauvolaNet's effectiveness, we conducted extensive experiments using both quantitative metrics and visual inspections. Our results demonstrate that our algorithm consistently outperforms several strong baseline methods across all four standard evaluation criteria. Furthermore, we found that our model achieves higher F-measure scores than previous adaptive approaches while running significantly faster, making it suitable for real-world applications requiring efficient processing.  Overall, this work represents an important step forward in improving the robustness and flexibility of document binarization techniques, paving the way for more accurate OCR recognition systems in various domains, from historical archives to contemporary offices.",1
"Recently, researchers have utilized neural networks to accurately solve partial differential equations (PDEs), enabling the mesh-free method for scientific computation. Unfortunately, the network performance drops when encountering a high nonlinearity domain. To improve the generalizability, we introduce the novel approach of employing multi-task learning techniques, the uncertainty-weighting loss and the gradients surgery, in the context of learning PDE solutions. The multi-task scheme exploits the benefits of learning shared representations, controlled by cross-stitch modules, between multiple related PDEs, which are obtainable by varying the PDE parameterization coefficients, to generalize better on the original PDE. Encouraging the network pay closer attention to the high nonlinearity domain regions that are more challenging to learn, we also propose adversarial training for generating supplementary high-loss samples, similarly distributed to the original training distribution. In the experiments, our proposed methods are found to be effective and reduce the error on the unseen data points as compared to the previous approaches in various PDE examples, including high-dimensional stochastic PDEs.",0
"In recent years, physics-informed neural networks (PINNs) have emerged as a powerful tool for solving partial differential equations (PDEs). However, training PINNs can be challenging due to the high nonlinearity of many physical systems and the limited availability of data. To address these issues, we propose a new method called adversarial multi-task learning enhanced PINNs (AML-PINNs), which significantly improves the accuracy and efficiency of PINNs by leveraging multiple tasks and an adversarial loss function.  Our approach involves formulating a set of auxiliary tasks that complement the primary task of solving the PDEs. These auxiliary tasks may involve predicting derivatives of the solution with respect to different variables, or enforcing consistency constraints on the solutions at known points. By combining all these tasks into a single optimization problem, our model effectively regularizes the training process and encourages more accurate predictions.  Furthermore, we introduce an adversarial loss term inspired by generative adversarial networks (GANs) to enhance the robustness and generalization capabilities of our AML-PINNs model. Specifically, we train an additional discriminator network alongside the main PINNs model to identify any discrepancies between the predicted solution and the true solution. This helps to drive our model towards better convergence rates and more accurate solutions.  We demonstrate the effectiveness of our proposed method through extensive experiments using several benchmark test cases from fluid mechanics and solid mechanics. Our results show significant improvements over traditional PINNs models in terms of accuracy, stability, and efficiency, validating the superior performance of AML-PINNs in solving complex PDE problems. Overall, our work represents a significant step forward in advancing the state-of-the-art in PINNs research and provides new opportunities for exploring multi-task deep learning techniques in scientific computing applications.",1
"In this paper, we proposed a novel Style-based Point Generator with Adversarial Rendering (SpareNet) for point cloud completion. Firstly, we present the channel-attentive EdgeConv to fully exploit the local structures as well as the global shape in point features. Secondly, we observe that the concatenation manner used by vanilla foldings limits its potential of generating a complex and faithful shape. Enlightened by the success of StyleGAN, we regard the shape feature as style code that modulates the normalization layers during the folding, which considerably enhances its capability. Thirdly, we realize that existing point supervisions, e.g., Chamfer Distance or Earth Mover's Distance, cannot faithfully reflect the perceptual quality of the reconstructed points. To address this, we propose to project the completed points to depth maps with a differentiable renderer and apply adversarial training to advocate the perceptual realism under different viewpoints. Comprehensive experiments on ShapeNet and KITTI prove the effectiveness of our method, which achieves state-of-the-art quantitative performance while offering superior visual quality.",0
"This paper proposes a novel approach for generating complete point clouds from partial scan data using adversarial rendering techniques. Our method leverages style-based synthesis, where we use pretrained generative models like DALLâ€¢E2 as prior knowledge sources that guide our completion process through gradient updates. We introduce a new regularization term based on feature matching, which allows us to maintain consistency within individual objects while improving their overall fidelity during inference. Additionally, we present a neural network architecture specifically designed for point cloud tasks, namely point renderers (PxRs), capable of encoding and decoding features at arbitrary resolutions. Extensive experiments demonstrate how our method significantly outperforms state-of-the-art point cloud completion algorithms both quantitatively and qualitatively across four challenging benchmark datasets: Shapenet Parts, ScanNet Core Dataset, SUN RGBD Indoor Scenes, and NYUv2 Depth Datasets. Lastly, ablation studies verify the impact of our major contributions towards achieving these results.",1
"Detecting facial forgery images and videos is an increasingly important topic in multimedia forensics. As forgery images and videos are usually compressed into different formats such as JPEG and H264 when circulating on the Internet, existing forgery-detection methods trained on uncompressed data often suffer from significant performance degradation in identifying them. To solve this problem, we propose a novel anti-compression facial forgery detection framework, which learns a compression-insensitive embedding feature space utilizing both original and compressed forgeries. Specifically, our approach consists of three ideas: (i) extracting compression-insensitive features from both uncompressed and compressed forgeries using an adversarial learning strategy; (ii) learning a robust partition by constructing a metric loss that can reduce the distance of the paired original and compressed images in the embedding space; (iii) improving the accuracy of tampered localization with an attention-transfer module. Experimental results demonstrate that, the proposed method is highly effective in handling both compressed and uncompressed facial forgery images.",0
"Facial forgery detection has become increasingly important as more online content relies on manipulated images. This paper proposes metric learning for anti-compression facial forgery detection, which enhances existing methods by detecting subtle variations due to compression that are often overlooked. We explore three variants of triplet loss: soft margin triplet loss, focused triplet loss, and hierarchical centroid triplet loss. Our results show significant improvements in detection accuracy across multiple datasets, including deepfake videos generated using convolutional neural networks (CNNs). These findings contribute to the growing field of digital media authentication and provide new insights into how artificial intelligence can assist human moderators in identifying fraudulent content. Additionally, our proposed approach shows promise for broader applications in real-world scenarios, such as biometric recognition systems that require robustness against image manipulation attacks. By addressing the limitations of current facial forgery detection techniques, we aim to pave the way towards a safer digital environment where trustworthy information reigns supreme.",1
"Pedestrian detection is an important but challenging problem in computer vision, especially in human-centric tasks. Over the past decade, significant improvement has been witnessed with the help of handcrafted features and deep features. Here we present a comprehensive survey on recent advances in pedestrian detection. First, we provide a detailed review of single-spectral pedestrian detection that includes handcrafted features based methods and deep features based approaches. For handcrafted features based methods, we present an extensive review of approaches and find that handcrafted features with large freedom degrees in shape and space have better performance. In the case of deep features based approaches, we split them into pure CNN based methods and those employing both handcrafted and CNN based features. We give the statistical analysis and tendency of these methods, where feature enhanced, part-aware, and post-processing methods have attracted main attention. In addition to single-spectral pedestrian detection, we also review multi-spectral pedestrian detection, which provides more robust features for illumination variance. Furthermore, we introduce some related datasets and evaluation metrics, and compare some representative methods. We conclude this survey by emphasizing open problems that need to be addressed and highlighting various future directions. Researchers can track an up-to-date list at https://github.com/JialeCao001/PedSurvey.",0
"This survey provides a comprehensive overview of handcrafted feature extraction methods for pedestrian detection as well as recent advances made by integrating convolutional neural networks (CNNs). We first present the conventional approaches that extract features such as HOG, Haar Cascades, LBP, and SIFT from raw images followed by machine learning algorithms like support vector machines (SVM) or boosting techniques. Next we discuss state-of-the-art deep features which have emerged as highly effective alternatives for obtaining better accuracy without requiring explicit object segmentation or region proposal generation. Our main objective is to provide researchers and practitioners with detailed understanding of both classical and modern solutions. By highlighting the strengths, weaknesses, and limitations of each method along with their applications, we aim to facilitate future work in this domain. Finally, our comparative evaluation results on benchmark datasets illustrate the significant improvements achieved through end-to-end CNN architectures compared to traditional feature engineering.",1
"Open set domain recognition has got the attention in recent years. The task aims to specifically classify each sample in the practical unlabeled target domain, which consists of all known classes in the manually labeled source domain and target-specific unknown categories. The absence of annotated training data or auxiliary attribute information for unknown categories makes this task especially difficult. Moreover, exiting domain discrepancy in label space and data distribution further distracts the knowledge transferred from known classes to unknown classes. To address these issues, this work presents an end-to-end model based on attention-based GCN and semantic matching optimization, which first employs the attention mechanism to enable the central node to learn more discriminating representations from its neighbors in the knowledge graph. Moreover, a coarse-to-fine semantic matching optimization approach is proposed to progressively bridge the domain gap. Experimental results validate that the proposed model not only has superiority on recognizing the images of known and unknown classes, but also can adapt to various openness of the target domain.",0
"This research addresses the task of open set domain recognition (OSDR), which refers to the problem of identifying whether an input image was taken from a previously seen camera under different environmental conditions. Existing methods primarily focus on either fine-grained analysis of scene content or coarse context modeling through mid-level features. In contrast, we present a novel approach that effectively combines local feature extraction using graph convolutional networks (GCNs) with semantic matching optimization for OSDR. Our method utilizes attention mechanisms within the GCN architecture to allow the network to focus on discriminative regions of interest while considering both local and global dependencies. These extracted features are then fed into our proposed semantic matching optimizer, which minimizes the difference between reference images in the gallery database and query images. We show that our method achieves superior performance over state-of-the-art techniques across challenging benchmark datasets, demonstrating its effectiveness in addressing the difficult OSDR problem.",1
"Quantitative evaluation has increased dramatically among recent video inpainting work, but the video and mask content used to gauge performance has received relatively little attention. Although attributes such as camera and background scene motion inherently change the difficulty of the task and affect methods differently, existing evaluation schemes fail to control for them, thereby providing minimal insight into inpainting failure modes. To address this gap, we propose the Diagnostic Evaluation of Video Inpainting on Landscapes (DEVIL) benchmark, which consists of two contributions: (i) a novel dataset of videos and masks labeled according to several key inpainting failure modes, and (ii) an evaluation scheme that samples slices of the dataset characterized by a fixed content attribute, and scores performance on each slice according to reconstruction, realism, and temporal consistency quality. By revealing systematic changes in performance induced by particular characteristics of the input content, our challenging benchmark enables more insightful analysis into video inpainting methods and serves as an invaluable diagnostic tool for the field. Our code is available at https://github.com/MichiganCOG/devil .",0
"This paper presents a new benchmark for evaluating image inpainting techniques specifically designed for video applications. We propose a diagnostic evaluation approach that considers both objective measures such as mean squared error (MSE) and subjective assessments using human ratings. Our dataset includes a diverse set of video frames from different sources, which allows us to evaluate the performance of state-of-the-art algorithms under varying conditions. Our results show that our benchmark can effectively identify strengths and weaknesses of different approaches, providing valuable insights into future research directions. Additionally, we provide recommendations on how to design more effective datasets for video inpainting evaluations. Overall, our work advances the field by establishing a comprehensive framework for testing and comparing video inpainting methods.",1
"In this paper, we introduce NBNet, a novel framework for image denoising. Unlike previous works, we propose to tackle this challenging problem from a new perspective: noise reduction by image-adaptive projection. Specifically, we propose to train a network that can separate signal and noise by learning a set of reconstruction basis in the feature space. Subsequently, image denosing can be achieved by selecting corresponding basis of the signal subspace and projecting the input into such space. Our key insight is that projection can naturally maintain the local structure of input signal, especially for areas with low light or weak textures. Towards this end, we propose SSA, a non-local subspace attention module designed explicitly to learn the basis generation as well as the subspace projection. We further incorporate SSA with NBNet, a UNet structured network designed for end-to-end image denosing. We conduct evaluations on benchmarks, including SIDD and DND, and NBNet achieves state-of-the-art performance on PSNR and SSIM with significantly less computational cost.",0
"NBNet (Noise Basis Learning) presents a novel deep learning framework designed to address image denoising using subspace projection techniques. This approach overcomes the challenges associated with traditional methods by leveraging noise prior knowledge to enhance feature representation. Our method can effectively capture local patterns present within natural images while preserving structure, resulting in significant improvements in visual quality compared to existing state-of-the-art approaches. Experimental results demonstrate that our proposed algorithm outperforms competing models on several benchmark datasets commonly used in computer vision research, highlighting its effectiveness as an advanced tool for denoising tasks. In addition, we provide ablation studies showing how each component of our model contributes towards achieving superior performance. Overall, our work provides new insights into noise analysis and offers valuable contributions to the field of image processing research. By exploring the potential of deep neural networks to learn meaningful representations from noisy data, we hope to inspire future advancements towards realizing unsupervised image recovery solutions at parity with supervised counterparts.",1
"Over the past decade, wind energy has gained more attention in the world. However, owing to its indirectness and volatility properties, wind power penetration has increased the difficulty and complexity in dispatching and planning of electric power systems. Therefore, it is needed to make the high-precision wind power prediction in order to balance the electrical power. For this purpose, in this study, the prediction performance of linear regression, k-nearest neighbor regression and decision tree regression algorithms is compared in detail. k-nearest neighbor regression algorithm provides lower coefficient of determination values, while decision tree regression algorithm produces lower mean absolute error values. In addition, the meteorological parameters of wind speed, wind direction, barometric pressure and air temperature are evaluated in terms of their importance on the wind power parameter. The biggest importance factor is achieved by wind speed parameter. In consequence, many useful assessments are made for wind power predictions.",0
"This research compares the performance of different machine learning algorithms on predicting wind turbine power generation. Accurate prediction of power output from wind turbines is essential for efficient operation and maintenance of wind farms. In this study, three commonly used machine learning algorithms, including Decision Trees, Random Forests, and Neural Networks, were trained using historical data collected from a commercial-scale wind farm. The models were tested against real-time measurement data and evaluated based on their accuracy, precision, recall, F1 score, and mean absolute error (MAE). Results showed that all three algorithms achieved high accuracy, with Decision Trees having the lowest MAE followed by Random Forests and Neural Networks. Additionally, the results indicate that each algorithm has its own strengths and weaknesses in terms of model complexity, ease of interpretation, and computational requirements. These findings have important implications for selecting appropriate machine learning algorithms for wind turbine power prediction applications. Overall, this research provides valuable insights into the relative effectiveness of different machine learning techniques in forecasting renewable energy production.",1
"With a mortality rate of 5.4 million lives worldwide every year and a healthcare cost of more than 16 billion dollars in the USA alone, sepsis is one of the leading causes of hospital mortality and an increasing concern in the ageing western world. Recently, medical and technological advances have helped re-define the illness criteria of this disease, which is otherwise poorly understood by the medical society. Together with the rise of widely accessible Electronic Health Records, the advances in data mining and complex nonlinear algorithms are a promising avenue for the early detection of sepsis. This work contributes to the research effort in the field of automated sepsis detection with an open-access labelling of the medical MIMIC-III data set. Moreover, we propose MGP-AttTCN: a joint multitask Gaussian Process and attention-based deep learning model to early predict the occurrence of sepsis in an interpretable manner. We show that our model outperforms the current state-of-the-art and present evidence that different labelling heuristics lead to discrepancies in task difficulty. For instance, when predicting sepsis five hours prior to onset on our new realistic labels, our proposed model achieves an area under the ROC curve of 0.660 and an area under the PR curve of 0.483, whereas the (less interpretable) previous state-of-the-art model (MGP-TCN) achieves 0.635 AUROC and 0.460 AUPR and the popular commercial InSight model achieves 0.490 AUROC and 0.359 AUPR.",0
"In recent years, machine learning has emerged as a powerful tool for predicting sepsis in patients. However, many existing models suffer from low interpretability, which makes it difficult for clinicians to understand why certain predictions were made. To address this issue, we propose a new model called MGP-AttTCN (Maximum Gaussian Process + Attention Transformer Convolutional Neural Network) that combines two complementary approaches - maximum likelihood estimation using Gaussian processes and attention-based deep learning techniques. Our approach allows us to achieve high prediction accuracy while providing interpretable explanations for our predictions. We evaluate our model on a large dataset consisting of electronic health records from critical care units and demonstrate its superior performance compared to state-of-the-art methods. Overall, our work represents a promising step towards building more transparent and explainable machine learning models for healthcare applications.",1
"The fully-convolutional network (FCN) with an encoder-decoder architecture has been the standard paradigm for semantic segmentation. The encoder-decoder architecture utilizes an encoder to capture multi-level feature maps, which are incorporated into the final prediction by a decoder. As the context is crucial for precise segmentation, tremendous effort has been made to extract such information in an intelligent fashion, including employing dilated/atrous convolutions or inserting attention modules. However, these endeavours are all based on the FCN architecture with ResNet or other backbones, which cannot fully exploit the context from the theoretical concept. By contrast, we propose the Swin Transformer as the backbone to extract the context information and design a novel decoder of densely connected feature aggregation module (DCFAM) to restore the resolution and produce the segmentation map. The experimental results on two remotely sensed semantic segmentation datasets demonstrate the effectiveness of the proposed scheme.",0
"This paper presents a novel transformer-based semantic segmentation scheme for fine-resolution remote sensing images. We propose an approach that combines global context attention with local context processing to improve the accuracy of semantic segmentation on high-resolution satellite imagery. Our method uses a multi-scale fusion module to capture features from different image scales, followed by a self-attention mechanism to model long range dependencies between pixels. Experimental results show that our approach outperforms state-of-the-art methods across several metrics, including pixel accuracy, overall accuracy, and mean intersection over union (IOU). Our work demonstrates the effectiveness of using transformers for semantic segmentation tasks in remote sensing, opening up new possibilities for automating the analysis of high-resolution satellite data.",1
"The spatial attention is a straightforward approach to enhance the performance for remote sensing image captioning. However, conventional spatial attention approaches consider only the attention distribution on one fixed coarse grid, resulting in the semantics of tiny objects can be easily ignored or disturbed during the visual feature extraction. Worse still, the fixed semantic level of conventional spatial attention limits the image understanding in different levels and perspectives, which is critical for tackling the huge diversity in remote sensing images. To address these issues, we propose a remote sensing image caption generator with instance-awareness and cross-hierarchy attention. 1) The instances awareness is achieved by introducing a multi-level feature architecture that contains the visual information of multi-level instance-possible regions and their surroundings. 2) Moreover, based on this multi-level feature extraction, a cross-hierarchy attention mechanism is proposed to prompt the decoder to dynamically focus on different semantic hierarchies and instances at each time step. The experimental results on public datasets demonstrate the superiority of proposed approach over existing methods.",0
"This paper presents a novel approach for image caption generation that takes into account both spatial and semantic context within remote sensing images. We introduce instance-aware attention mechanisms which allow the model to focus on individual objects as well as their relationships with other parts of the scene. Our cross-hierarchy attention module enables efficient integration of high-level features from different levels of abstraction for more accurate localization. Experimental results demonstrate significant improvements over previous methods in terms of both quantitative metrics such as METEOR and CIDEr, as well as qualitative analysis through human evaluations. This research provides important advancements towards generating informative and natural language descriptions of complex environmental scenes from satellite imagery.",1
"At present, attention mechanism has been widely applied to the fields of deep learning models. Structural models that based on attention mechanism can not only record the relationships between features position, but also can measure the importance of different features based on their weights. By establishing dynamically weighted parameters for choosing relevant and irrelevant features, the key information can be strengthened, and the irrelevant information can be weakened. Therefore, the efficiency of deep learning algorithms can be significantly elevated and improved. Although transformers have been performed very well in many fields including reinforcement learning, there are still many problems and applications can be solved and made with transformers within this area. MARL (known as Multi-Agent Reinforcement Learning) can be recognized as a set of independent agents trying to adapt and learn through their way to reach the goal. In order to emphasize the relationship between each MDP decision in a certain time period, we applied the hierarchical coding method and validated the effectiveness of this method. This paper proposed a hierarchical transformers MADDPG based on RNN which we call it Hierarchical RNNs-Based Transformers MADDPG(HRTMADDPG). It consists of a lower level encoder based on RNNs that encodes multiple step sizes in each time sequence, and it also consists of an upper sequence level encoder based on transformer for learning the correlations between multiple sequences so that we can capture the causal relationship between sub-time sequences and make HRTMADDPG more efficient.",0
"Deep reinforcement learning (DRL) has shown great success in addressing cooperative problems where multiple agents act together to achieve a common goal. However, real-world scenarios often involve both cooperative and competitive elements among agents, making such approaches less effective. In order to tackle these mixed environments, this work proposes a hierarchical recurrent neural network based transformer architecture called HRNTT-MADDPG that builds upon multi-agent deep deterministic policy gradients (MADDPG). We evaluate our approach on four different benchmark tasks that require varying degrees of cooperation and competition among agents. Experimental results show that our method outperforms state-of-the-art DRL algorithms like MADDPG and TRPO in most cases while maintaining comparable performance in others. Our analysis indicates that the use of external attention mechanisms allows for better handling of mixed situations, leading to improved agent collaboration and coordination in complex environments. Overall, our proposed framework paves the way towards more advanced applications of reinforcement learning in dynamic and uncertain environments involving multiple interacting agents.",1
"Graph-based multi-view clustering aiming to obtain a partition of data across multiple views, has received considerable attention in recent years. Although great efforts have been made for graph-based multi-view clustering, it remains a challenge to fuse characteristics from various views to learn a common representation for clustering. In this paper, we propose a novel Consistent Multiple Graph Embedding Clustering framework(CMGEC). Specifically, a multiple graph auto-encoder(M-GAE) is designed to flexibly encode the complementary information of multi-view data using a multi-graph attention fusion encoder. To guide the learned common representation maintaining the similarity of the neighboring characteristics in each view, a Multi-view Mutual Information Maximization module(MMIM) is introduced. Furthermore, a graph fusion network(GFN) is devised to explore the relationship among graphs from different views and provide a common consensus graph needed in M-GAE. By jointly training these models, the common latent representation can be obtained which encodes more complementary information from multiple views and depicts data more comprehensively. Experiments on three types of multi-view datasets demonstrate CMGEC outperforms the state-of-the-art clustering methods.",0
"This research presents a novel approach for multiple graph embedding that enables multi-view clustering by leveraging the consistency across different graphs. In many real-world applications, data exists as multiple interconnected graphs that capture different perspectives of the same underlying phenomenon. However, traditional graph embedding methods focus on capturing intrinsic properties within each individual graph, ignoring their correlations with other graphs. Our method addresses these limitations by learning joint embeddings across all graphs while enforcing their consistency through constraints. We validate our proposed approach using extensive experiments on benchmark datasets and demonstrate its effectiveness compared to state-of-the-art methods in various scenarios. By integrating complementary knowledge from diverse views and ensuring their coherence, our approach enhances the performance of downstream clustering tasks in complex networks.",1
"Although deep models have greatly improved the accuracy and robustness of image segmentation, obtaining segmentation results with highly accurate boundaries and fine structures is still a challenging problem. In this paper, we propose a simple yet powerful Boundary-Aware Segmentation Network (BASNet), which comprises a predict-refine architecture and a hybrid loss, for highly accurate image segmentation. The predict-refine architecture consists of a densely supervised encoder-decoder network and a residual refinement module, which are respectively used to predict and refine a segmentation probability map. The hybrid loss is a combination of the binary cross entropy, structural similarity and intersection-over-union losses, which guide the network to learn three-level (ie, pixel-, patch- and map- level) hierarchy representations. We evaluate our BASNet on two reverse tasks including salient object segmentation, camouflaged object segmentation, showing that it achieves very competitive performance with sharp segmentation boundaries. Importantly, BASNet runs at over 70 fps on a single GPU which benefits many potential real applications. Based on BASNet, we further developed two (close to) commercial applications: AR COPY & PASTE, in which BASNet is integrated with augmented reality for ""COPYING"" and ""PASTING"" real-world objects, and OBJECT CUT, which is a web-based tool for automatic object background removal. Both applications have already drawn huge amount of attention and have important real-world impacts. The code and two applications will be publicly available at: https://github.com/NathanUA/BASNet.",0
"In recent years, image segmentation has become increasingly important for mobile and web applications such as photo editing, augmented reality (AR), and virtual reality (VR). Accurate image segmentation plays a critical role in these applications by enabling them to process and manipulate images more effectively. However, current methods for image segmentation often suffer from limitations in terms of accuracy and computational efficiency.  This research addresses these challenges by proposing a boundary-aware segmentation network that can perform accurate image segmentation on both mobile devices and desktop computers. Our approach uses the latest advances in deep learning and computer vision to build a model that can effectively detect object boundaries while maintaining high levels of precision. This model is designed specifically for the resource constraints found in mobile and AR/VR environments, making it well suited for use in real-world scenarios.  The proposed method was evaluated through comprehensive experiments conducted on several benchmark datasets and compared against state-of-the-art approaches. Results show that our approach achieves significantly better performance than existing methods across all metrics while maintaining computational efficiency. Additionally, we demonstrate how our approach can be applied to real-world applications such as video frame-by-frame semantic segmentation and real-time AR/VR object detection.  In summary, this work presents a novel boundary-aware segmentation network that provides fast and highly precise results for mobile and web applications. With its superior performance and adaptability to various platforms, our approach holds great potential for further development in many exciting areas including mobile computing, VR/AR, and robotics.",1
"Image deblurring has seen a great improvement with the development of deep neural networks. In practice, however, blurry images often suffer from additional degradations such as downscaling and compression. To address these challenges, we propose an Enhanced Deep Pyramid Network (EDPN) for blurry image restoration from multiple degradations, by fully exploiting the self- and cross-scale similarities in the degraded image.Specifically, we design two pyramid-based modules, i.e., the pyramid progressive transfer (PPT) module and the pyramid self-attention (PSA) module, as the main components of the proposed network. By taking several replicated blurry images as inputs, the PPT module transfers both self- and cross-scale similarity information from the same degraded image in a progressive manner. Then, the PSA module fuses the above transferred features for subsequent restoration using self- and spatial-attention mechanisms. Experimental results demonstrate that our method significantly outperforms existing solutions for blurry image super-resolution and blurry image deblocking. In the NTIRE 2021 Image Deblurring Challenge, EDPN achieves the best PSNR/SSIM/LPIPS scores in Track 1 (Low Resolution) and the best SSIM/LPIPS scores in Track 2 (JPEG Artifacts).",0
"Enhanced deep pyramid network is proposed method for blurry image restoration which includes multi scale features extraction through pyramidal decomposition followed by cascade dilated convolutions at each level . This process allows effective feature representation as well as efficient spatial propagation . To further improve model performance we introduce two additional techniques : residual learning which explicitly enforces similarity on a low dimensional embedding space and auxiliary adversarial training that ensures generated images satisfy certain quality standards beyond MSE loss minimization . Our approach outperforms existing methods on both synthetic and real benchmark datasets such as GoPro , BSD68 / BSD100 [3] , RealBlind etc and can generate very high resolution images (up to ) from extremely blurred inputs ( PSNR gain over other state of art models >3 dB) . The code will be made publicly available after acceptance . Keywords: Blind Deconvolution ; Deep Learning ; Multi Scale Representations ; High Resolution Images Generation ; Pyramidal Architecture Extension .",1
"The problem of grounding VQA tasks has seen an increased attention in the research community recently, with most attempts usually focusing on solving this task by using pretrained object detectors. However, pre-trained object detectors require bounding box annotations for detecting relevant objects in the vocabulary, which may not always be feasible for real-life large-scale applications. In this paper, we focus on a more relaxed setting: the grounding of relevant visual entities in a weakly supervised manner by training on the VQA task alone. To address this problem, we propose a visual capsule module with a query-based selection mechanism of capsule features, that allows the model to focus on relevant regions based on the textual cues about visual information in the question. We show that integrating the proposed capsule module in existing VQA systems significantly improves their performance on the weakly supervised grounding task. Overall, we demonstrate the effectiveness of our approach on two state-of-the-art VQA systems, stacked NMN and MAC, on the CLEVR-Answers benchmark, our new evaluation set based on CLEVR scenes with ground truth bounding boxes for objects that are relevant for the correct answer, as well as on GQA, a real world VQA dataset with compositional questions. We show that the systems with the proposed capsule module consistently outperform the respective baseline systems in terms of answer grounding, while achieving comparable performance on VQA task.",0
"In this work we propose a novel approach for weakly-supervised grounded visual question answering (VQA) called ""Found a reason for me?""(FaRM). Our method is based on capsule networks which have been shown to excel at image understanding tasks like VQA. Unlike previous weakly supervised approaches, ours uses no explicit alignment loss to ensure that the learned representations align well with human notions of correctness; instead we rely solely on self-training via data augmentation and latent variable analysis using unsupervised techniques from machine learning. Experiments show that FaRM matches or exceeds state-of-the art fully-supervised results across several benchmark datasets.",1
"Fashion trend forecasting is of great research significance in providing useful suggestions for both fashion companies and fashion lovers. Although various studies have been devoted to tackling this challenging task, they only studied limited fashion elements with highly seasonal or simple patterns, which could hardly reveal the real complex fashion trends. Moreover, the mainstream solutions for this task are still statistical-based and solely focus on time-series data modeling, which limit the forecast accuracy. Towards insightful fashion trend forecasting, previous work [1] proposed to analyze more fine-grained fashion elements which can informatively reveal fashion trends. Specifically, it focused on detailed fashion element trend forecasting for specific user groups based on social media data. In addition, it proposed a neural network-based method, namely KERN, to address the problem of fashion trend modeling and forecasting. In this work, to extend the previous work, we propose an improved model named Relation Enhanced Attention Recurrent (REAR) network. Compared to KERN, the REAR model leverages not only the relations among fashion elements but also those among user groups, thus capturing more types of correlations among various fashion trends. To further improve the performance of long-range trend forecasting, the REAR method devises a sliding temporal attention mechanism, which is able to capture temporal patterns on future horizons better. Extensive experiments and more analysis have been conducted on the FIT and GeoStyle datasets to evaluate the performance of REAR. Experimental and analytical results demonstrate the effectiveness of the proposed REAR model in fashion trend forecasting, which also show the improvement of REAR compared to the KERN.",0
"This paper presents a novel framework that leverages multiple relations for fashion trend forecasting based on social media data. By utilizing both local and global contextual relationships, our approach can effectively capture subtle differences in style and taste across different regions and time periods, resulting in more accurate predictions. Our method also incorporates temporal dynamics into the relation modeling process by considering how trends evolve over time. Through extensive experiments, we demonstrate the superiority of our proposed solution compared to state-of-the-art baselines in terms of prediction accuracy and stability. In addition, the visualization results further validate the effectiveness of our framework. Overall, our work represents an important step towards developing intelligent systems capable of efficiently monitoring and predicting fashion trends from large volumes of social media data.",1
"Although existing person re-identification (Re-ID) methods have shown impressive accuracy, most of them usually suffer from poor generalization on unseen target domain. Thus, generalizable person Re-ID has recently drawn increasing attention, which trains a model on source domains that generalizes well on unseen target domain without model updating. In this work, we propose a novel adaptive domain-specific normalization approach (AdsNorm) for generalizable person Re-ID. It describes unseen target domain as a combination of the known source ones, and explicitly learns domain-specific representation with target distribution to improve the model's generalization by a meta-learning pipeline. Specifically, AdsNorm utilizes batch normalization layers to collect individual source domains' characteristics, and maps source domains into a shared latent space by using these characteristics, where the domain relevance is measured by a distance function of different domain-specific normalization statistics and features. At the testing stage, AdsNorm projects images from unseen target domain into the same latent space, and adaptively integrates the domain-specific features carrying the source distributions by domain relevance for learning more generalizable aggregated representation on unseen target domain. Considering that target domain is unavailable during training, a meta-learning algorithm combined with a customized relation loss is proposed to optimize an effective and efficient ensemble model. Extensive experiments demonstrate that AdsNorm outperforms the state-of-the-art methods. The code is available at: https://github.com/hzphzp/AdsNorm.",0
"In recent years there has been significant progress in developing deep learning models for person re-identification (ReID). Despite these advances, current state-of-the-art methods still suffer from performance degradation when evaluating on domain disjoint datasets, which hinders their generalizability. To address this issue we propose Adaptive Domain-Specific Normalization (ADSN), which consists of two key components: Adaptive Contrastive Biasing Networks and Task-specific Feature Adjustment. Our method first learns discriminative feature representations by generating contrastive pairs using a style transfer technique that modifies images to simulate different domains and adaptively adjusts the model's parameters based on learned bias weights. Then, our task-specific feature adjustment module finetunes the features extracted by the base network to further enhance intra-class compactness and inter-class separability. Extensive experiments conducted on multiple benchmarks demonstrate the superiority of our approach over existing ReID methods under both same-domain and cross-domain settings. Our results show that ADSN can effectively enhance the robustness and generalization ability of ReID systems across diverse environments, paving the way for improved real-world applications such as video surveillance and tracking scenarios.",1
"Representation learning of static and more recently dynamically evolving graphs has gained noticeable attention. Existing approaches for modelling graph dynamics focus extensively on the evolution of individual nodes independently of the evolution of mesoscale community structures. As a result, current methods do not provide useful tools to study and cannot explicitly capture temporal community dynamics. To address this challenge, we propose GRADE - a probabilistic model that learns to generate evolving node and community representations by imposing a random walk prior over their trajectories. Our model also learns node community membership which is updated between time steps via a transition matrix. At each time step link generation is performed by first assigning node membership from a distribution over the communities, and then sampling a neighbor from a distribution over the nodes for the assigned community. We parametrize the node and community distributions with neural networks and learn their parameters via variational inference. Experiments demonstrate GRADE outperforms baselines in dynamic link prediction, shows favourable performance on dynamic community detection, and identifies coherent and interpretable evolving communities.",0
"This is an abstract around 150 to 300 words long that summarizes a paper titled ""GRADE: Graph Dynamic Embedding."" In this paper, we propose a new approach to graph dynamic embedding which allows nodes in a graph to have both static and dynamic embeddings. Our method leverages graph neural networks (GNNs) to learn node representations while preserving their temporal information. We evaluate our approach on several benchmark datasets and demonstrate its effectiveness compared to state-of-the-art baselines.",1
"Segmenting an entire 3D image often has high computational complexity and requires large memory consumption; by contrast, performing volumetric segmentation in a slice-by-slice manner is efficient but does not fully leverage the 3D data. To address this challenge, we propose a multi-dimensional attention network (MDA-Net) to efficiently integrate slice-wise, spatial, and channel-wise attention into a U-Net based network, which results in high segmentation accuracy with a low computational cost. We evaluate our model on the MICCAI iSeg and IBSR datasets, and the experimental results demonstrate consistent improvements over existing methods.",0
"MDA-Net (Multi-Dimensional Attention-Based Neural Network) has been proposed as a novel deep learning method for the task of image segmentation. This technique utilizes multi-dimensional attention modules that can effectively capture contextual relationships across multiple dimensions such as spatial, channel, and semantic features. By integrating these different types of attention mechanisms into one network architecture, we show that our approach consistently outperforms state-of-the-art methods on several challenging benchmark datasets for 3D image segmentation tasks. We thoroughly evaluate our model by comparing it against existing approaches on five widely used public datasets and demonstrate significant improvements in terms of Dice similarity coefficient, Jaccard index, and boundary F1 score. Our extensive experimental results highlight the effectiveness and generalizability of the MDA-Net framework for accurately segmenting complex 3D images. In summary, the MDA-Net represents a major advance in image segmentation technology and has the potential to significantly impact medical imaging research and applications.",1
"Graph neural networks (GNNs) have received tremendous attention due to their power in learning effective representations for graphs. Most GNNs follow a message-passing scheme where the node representations are updated by aggregating and transforming the information from the neighborhood. Meanwhile, they adopt the same strategy in aggregating the information from different feature dimensions. However, suggested by social dimension theory and spectral embedding, there are potential benefits to treat the dimensions differently during the aggregation process. In this work, we investigate to enable heterogeneous contributions of feature dimensions in GNNs. In particular, we propose a general graph feature gating network (GFGN) based on the graph signal denoising problem and then correspondingly introduce three graph filters under GFGN to allow different levels of contributions from feature dimensions. Extensive experiments on various real-world datasets demonstrate the effectiveness and robustness of the proposed frameworks.",0
"Artificial Intelligence (AI) has become increasingly important due to the ability to create more intelligent systems that can handle complex tasks such as natural language understanding, computer vision and decision making, among others. One of the challenges faced by these models is the difficulty in controlling the behavior of neural networks during training. As they learn from large amounts of data and generate their own representations of problems, they often use many redundant features or learn problem specific suboptimal solutions. This study presents Graph Feature Gating Networks (GFGN), which attempt to address this issue. GFGN use graphs to encode feature dependencies and improve the learning process of deep learning models. By using graph nodes to selectively amplify certain input dimensions while dampening others, the method achieves better generalization performance and reduces overfitting compared to other methods. Results show that GFGN outperforms previous state of art techniques on several benchmark datasets across different domains. The proposed framework holds promise for further advances in representation learning, allowing deep neural networks to achieve better results faster with less computational requirements.",1
"Personalized recommendation system has become pervasive in various video platform. Many effective methods have been proposed, but most of them didn't capture the user's multi-level interest trait and dependencies between their viewed micro-videos well. To solve these problems, we propose a Self-over-Co Attention module to enhance user's interest representation. In particular, we first use co-attention to model correlation patterns across different levels and then use self-attention to model correlation patterns within a specific level. Experimental results on filtered public datasets verify that our presented module is useful.",0
"In recent years, micro-videos have become one of the most popular types of content on social media platforms. They offer users quick and entertaining ways to share their experiences and interests with others. However, identifying which videos are relevant to individual users can be challenging due to the vast amount of data generated by these short clips. To address this issue, researchers propose a novel approach that incorporates high-order interactions into a collaborative filtering model designed for recommending micro-videos based on multiple user interests.  The proposed method takes advantage of the unique characteristics of micro-videos such as their short duration, simple production techniques, and wide range of topics covered. By exploiting both collaborative and content-based features, the model can effectively capture user preferences and content attributes. This results in more accurate recommendations tailored to each individual user's specific needs and tastes.  To validate their approach, the authors conducted extensive experiments using large datasets from major micro-video sharing sites. The results demonstrate significant improvements over state-of-the-art methods, achieving higher accuracy and diversity in recommended items. These findings suggest that capturing complex relationships between different aspects of multimedia data is crucial for effective recommendation systems. Further studies may explore other factors influencing user engagement, such as temporal dynamics and network effects. Overall, this work represents a valuable contribution to the field of personalized video recommendations, enabling better user experience and engagement with micro-content.",1
"Abnormal event detection in video is a complex computer vision problem that has attracted significant attention in recent years. The complexity of the task arises from the commonly-adopted definition of an abnormal event, that is, a rarely occurring event that typically depends on the surrounding context. Following the standard formulation of abnormal event detection as outlier detection, we propose a background-agnostic framework that learns from training videos containing only normal events. Our framework is composed of an object detector, a set of appearance and motion auto-encoders, and a set of classifiers. Since our framework only looks at object detections, it can be applied to different scenes, provided that normal events are defined identically across scenes and that the single main factor of variation is the background. To overcome the lack of abnormal data during training, we propose an adversarial learning strategy for the auto-encoders. We create a scene-agnostic set of out-of-domain pseudo-abnormal examples, which are correctly reconstructed by the auto-encoders before applying gradient ascent on the pseudo-abnormal examples. We further utilize the pseudo-abnormal examples to serve as abnormal examples when training appearance-based and motion-based binary classifiers to discriminate between normal and abnormal latent features and reconstructions. We compare our framework with the state-of-the-art methods on four benchmark data sets, using various evaluation metrics. Compared to existing methods, the empirical results indicate that our approach achieves favorable performance on all data sets. In addition, we provide region-based and track-based annotations for two large-scale abnormal event detection data sets from the literature, namely ShanghaiTech and Subway.",0
"In recent years, there has been significant interest in developing algorithms that can automatically detect abnormal events in video footage. These systems have applications in security, surveillance, and monitoring, among other areas. However, many existing methods rely on handcrafted features or domain-specific knowledge, which can limit their generalizability and robustness. To address these issues, we propose a background-agnostic framework with adversarial training for abnormal event detection in video. This approach leverages both generative models and discriminative models to learn representations that capture important patterns in the data without relying on predefined feature engineering. We evaluate our method using several benchmark datasets and show that it outperforms state-of-the-art approaches across a range of metrics. Our results demonstrate the effectiveness of combining adversarial training with background modeling for improved abnormal event detection in complex scenes. Overall, our work represents an important step towards building more versatile and reliable video analytics systems.",1
"Convolutional Neural Networks (CNNs) have achieved tremendous success in a number of learning tasks including image classification. Recent advanced models in CNNs, such as ResNets, mainly focus on the skip connection to avoid gradient vanishing. DenseNet designs suggest creating additional bypasses to transfer features as an alternative strategy in network design. In this paper, we design Attentive Feature Integration (AFI) modules, which are widely applicable to most recent network architectures, leading to new architectures named AFI-Nets. AFI-Nets explicitly model the correlations among different levels of features and selectively transfer features with a little overhead.AFI-ResNet-152 obtains a 1.24% relative improvement on the ImageNet dataset while decreases the FLOPs by about 10% and the number of parameters by about 9.2% compared to ResNet-152.",0
Abstract: We introduce Attentive Feature Integration Networks (AFINets) for image classification tasks that efficiently integrate features from multiple branches derived from global and local feature representations. AFINets employ attention mechanisms to weigh different feature maps according to their importance which allows for selective integration into more discriminative fused representation that captures both global contextual information as well as fine-grained details across scales. Extensive experiments on several benchmark datasets demonstrate state-of-the-art results compared against current approaches while using fewer parameters.,1
"Existing online multiple object tracking (MOT) algorithms often consist of two subtasks, detection and re-identification (ReID). In order to enhance the inference speed and reduce the complexity, current methods commonly integrate these double subtasks into a unified framework. Nevertheless, detection and ReID demand diverse features. This issue would result in an optimization contradiction during the training procedure. With the target of alleviating this contradiction, we devise a module named Global Context Disentangling (GCD) that decouples the learned representation into detection-specific and ReID-specific embeddings. As such, this module provides an implicit manner to balance the different requirements of these two subtasks. Moreover, we observe that preceding MOT methods typically leverage local information to associate the detected targets and neglect to consider the global semantic relation. To resolve this restriction, we develop a module, referred to as Guided Transformer Encoder (GTE), by combining the powerful reasoning ability of Transformer encoder and deformable attention. Unlike previous works, GTE avoids analyzing all the pixels and only attends to capture the relation between query nodes and a few self-adaptively selected key samples. Therefore, it is computationally efficient. Extensive experiments have been conducted on the MOT16, MOT17 and MOT20 benchmarks to demonstrate the superiority of the proposed MOT framework, namely RelationTrack. The experimental results indicate that RelationTrack has surpassed preceding methods significantly and established a new state-of-the-art performance, e.g., IDF1 of 70.5% and MOTA of 67.2% on MOT20.",0
"Title: Improving Multiple Object Tracking Using Relational Information Multiple object tracking (MOT) is a challenging task that involves identifying and keeping track of multiple objects across frames in a video sequence. Accurate and efficient MOT algorithms have numerous applications ranging from surveillance systems to autonomous driving vehicles. However, traditional approaches often struggle in crowded scenes where objects may overlap or occlude each other, leading to confusion and errors in tracking. To address these limitations, we propose RelationTrack, a novel relation-aware approach to improve MOT performance by incorporating relational information into the tracking process. Our method uses a two-stream architecture to separately learn representations for individual objects and their relationships within the scene. By decoupling the representation learning, our model can better handle complex interactions between objects, such as merging tracks or splitting apart in case of incorrect assignments. We evaluate our approach on several benchmark datasets, demonstrating significantly improved results over state-of-the art methods while maintaining real-time inference speeds. Overall, RelationTrack shows great promise in enhancing the accuracy and robustness of multi-object tracking under difficult scenarios, paving the way towards advanced computer vision applications demanding precise multitarget analysis at scale.",1
"Scene text recognition is a challenging task due to diverse variations of text instances in natural scene images. Conventional methods based on CNN-RNN-CTC or encoder-decoder with attention mechanism may not fully investigate stable and efficient feature representations for multi-oriented scene texts. In this paper, we propose a primitive representation learning method that aims to exploit intrinsic representations of scene text images. We model elements in feature maps as the nodes of an undirected graph. A pooling aggregator and a weighted aggregator are proposed to learn primitive representations, which are transformed into high-level visual text representations by graph convolutional networks. A Primitive REpresentation learning Network (PREN) is constructed to use the visual text representations for parallel decoding. Furthermore, by integrating visual text representations into an encoder-decoder model with the 2D attention mechanism, we propose a framework called PREN2D to alleviate the misalignment problem in attention-based methods. Experimental results on both English and Chinese scene text recognition tasks demonstrate that PREN keeps a balance between accuracy and efficiency, while PREN2D achieves state-of-the-art performance.",0
"Artificial Intelligence (AI) has revolutionized many aspects of our lives, including the development of technologies that can automatically extract text from natural scenes. Text recognition technology is particularly important for tasks such as document digitization, sign recognition, street view applications, automatic image description generation, and multilingual translation systems, among others. Most existing scene text recognition methods use deep learning techniques, which require large amounts of annotated data to achieve high accuracy. However, these approaches often struggle with the limitations imposed by small datasets and unconstrained backgrounds, making them difficult to generalize across multiple domains. To address these challenges, we propose an alternative approach based on primitive representation learning. Our method enables efficient training with limited annotations and can effectively handle complex background environments. We evaluate the performance of our model against several state-of-the-art text recognition algorithms using standard benchmark datasets and demonstrate that our proposed technique outperforms other approaches under varying conditions, including low-resolution images and cluttered backgrounds. Overall, our work highlights the potential of primitive representation learning for improving the robustness and adaptability of scene text recognition models in real-world scenarios.",1
"The key challenge in cross-modal retrieval is to find similarities between objects represented with different modalities, such as image and text. However, each modality embeddings stem from non-related feature spaces, which causes the notorious 'heterogeneity gap'. Currently, many cross-modal systems try to bridge the gap with self-attention. However, self-attention has been widely criticized for its quadratic complexity, which prevents many real-life applications. In response to this, we propose T-EMDE - a neural density estimator inspired by the recently introduced Efficient Manifold Density Estimator (EMDE) from the area of recommender systems. EMDE operates on sketches - representations especially suitable for multimodal operations. However, EMDE is non-differentiable and ingests precomputed, static embeddings. With T-EMDE we introduce a trainable version of EMDE which allows full end-to-end training. In contrast to self-attention, the complexity of our solution is linear to the number of tokens/segments. As such, T-EMDE is a drop-in replacement for the self-attention module, with beneficial influence on both speed and metric performance in cross-modal settings. It facilitates communication between modalities, as each global text/image representation is expressed with a standardized sketch histogram which represents the same manifold structures irrespective of the underlying modality. We evaluate T-EMDE by introducing it into two recent cross-modal SOTA models and achieving new state-of-the-art results on multiple datasets and decreasing model latency by up to 20%.",0
"This paper presents a new method called T-EMDE (Training Extreme Multi-Dimensional Embedding) which uses sketching techniques to create compact representations of images that can be used for cross-modal image search. The proposed approach is evaluated on several benchmark datasets and compared against other state-of-the-art methods. Results show that T-EMDE outperforms existing approaches in terms of accuracy and efficiency while maintaining a high level of scalability. Overall, T-EMDE represents a significant advancement in the field of cross-modal retrieval and has potential applications in various domains such as multimedia databases, computer vision, and natural language processing. The full text of this article can be accessed at <https://www.computer.org/csdl/journals/tcad/2020/7609848>.",1
"Accurate prediction of pedestrian crossing behaviors by autonomous vehicles can significantly improve traffic safety. Existing approaches often model pedestrian behaviors using trajectories or poses but do not offer a deeper semantic interpretation of a person's actions or how actions influence a pedestrian's intention to cross in the future. In this work, we follow the neuroscience and psychological literature to define pedestrian crossing behavior as a combination of an unobserved inner will (a probabilistic representation of binary intent of crossing vs. not crossing) and a set of multi-class actions (e.g., walking, standing, etc.). Intent generates actions, and the future actions in turn reflect the intent. We present a novel multi-task network that predicts future pedestrian actions and uses predicted future action as a prior to detect the present intent and action of the pedestrian. We also designed an attention relation network to incorporate external environmental contexts thus further improve intent and action detection performance. We evaluated our approach on two naturalistic driving datasets, PIE and JAAD, and extensive experiments show significantly improved and more explainable results for both intent detection and action prediction over state-of-the-art approaches. Our code is available at: https://github.com/umautobots/pedestrian_intent_action_detection.",0
"Pedestrians often interact with cars and other road users in dynamic environments that can present challenges to both parties involved in these interactions. To address these difficulties, we have developed models based on real data from intersections in New York City (NYC). Our results suggest that predicting pedestrian crossing behavior involves understanding how intent and action come together to influence final decisions made by individuals at crosswalks. By capturing different types of movements from participants over time, including starting locations as well as stopping points during decision processes, our models show that intentional movements across roads require clear awareness of the surrounding environment. These findings provide new insights into how future humanâ€“automation interaction design may need to consider such factors to support more effective decision making in a wide range of complex systems domains.",1
"Novelty detection is a important research area which mainly solves the classification problem of inliers which usually consists of normal samples and outliers composed of abnormal samples. Auto-encoder is often used for novelty detection. However, the generalization ability of the auto-encoder may cause the undesirable reconstruction of abnormal elements and reduce the identification ability of the model. To solve the problem, we focus on the perspective of better reconstructing the normal samples as well as retaining the unique information of normal samples to improve the performance of auto-encoder for novelty detection. Firstly, we introduce attention mechanism into the task. Under the action of attention mechanism, auto-encoder can pay more attention to the representation of inlier samples through adversarial training. Secondly, we apply the information entropy into the latent layer to make it sparse and constrain the expression of diversity. Experimental results on three public datasets show that the proposed method achieves comparable performance compared with previous popular approaches.",0
"Novelty detection has become increasingly important in recent years as it allows for anomaly detection in data sets, which can then lead to early identification of potential issues before they turn into bigger problems. Auto-encoders have been successfully used in novelty detection due to their ability to learn a compressed representation of the data set while preserving the most relevant features. However, current methods suffer from limitations such as underfitting, overfitting, sensitivity to hyperparameters, etc. In order to address these challenges, we propose a new method that combines channel attention and entropy minimization techniques to improve auto-encoder based novelty detection performance. Our approach uses channel attention to focus on the most informative features of the input data, allowing for better compression and faster convergence during training. Furthermore, our method introduces entropy minimization as a regularizer to reduce overfitting and ensure robustness against changes in parameters. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1 score, AUCROC, and AUCPR compared to state-of-the-art methods on several benchmark datasets across different domains. Overall, our proposed method represents a significant step forward towards achieving more accurate and reliable novelty detection through improved auto-encoder performance.",1
"Counterfactual explanations is one of the post-hoc methods used to provide explainability to machine learning models that have been attracting attention in recent years. Most examples in the literature, address the problem of generating post-hoc explanations for black-box machine learning models after the rejection of a loan application. In contrast, in this work, we investigate mathematical programming formulations for scorecard models, a type of interpretable model predominant within the banking industry for lending. The proposed mixed-integer programming formulations combine objective functions to ensure close, realistic and sparse counterfactuals using multi-objective optimization techniques for a binary, probability or continuous outcome. Moreover, we extend these formulations to generate multiple optimal counterfactuals simultaneously while guaranteeing diversity. Experiments on two real-world datasets confirm that the presented approach can generate optimal diverse counterfactuals addressing desired properties with assumable CPU times for practice use.",0
"Title: Optimal Counterfactual Explanations for Scorecard Models  Model interpretability has become increasingly important as decision making models gain popularity. In many cases, scorecards are used to make predictions but their black box nature makes them difficult to explain. This research focuses on addressing this challenge by developing optimal counterfactuals for decision trees that can increase interpretability of the underlying model. Our approach uses Monte Carlo Tree Search (MCTS) which creates an explanation tree based on the decisions made during the decision-making process. We evaluate our method using three case studies from the financial sector and show how MCTS improves upon current approaches such as individual feature importance measures and permutation feature elimination methods. Results demonstrate improved fidelity in terms of agreement with human judgements while maintaining low computational cost. Overall, this work shows promise in bridging the gap between powerful prediction tools like decision trees and better understanding of complex decision processes.",1
"Intersections where vehicles are permitted to turn and interact with vulnerable road users (VRUs) like pedestrians and cyclists are among some of the most challenging locations for automated and accurate recognition of road users' behavior. In this paper, we propose a deep conditional generative model for interaction detection at such locations. It aims to automatically analyze massive video data about the continuity of road users' behavior. This task is essential for many intelligent transportation systems such as traffic safety control and self-driving cars that depend on the understanding of road users' locomotion. A Conditional Variational Auto-Encoder based model with Gaussian latent variables is trained to encode road users' behavior and perform probabilistic and diverse predictions of interactions. The model takes as input the information of road users' type, position and motion automatically extracted by a deep learning object detector and optical flow from videos, and generates frame-wise probabilities that represent the dynamics of interactions between a turning vehicle and any VRUs involved. The model's efficacy was validated by testing on real--world datasets acquired from two different intersections. It achieved an F1-score above 0.96 at a right--turn intersection in Germany and 0.89 at a left--turn intersection in Japan, both with very busy traffic flows.",0
"Abstract  Interactions between vehicles and vulnerable road users (VRUs) such as pedestrians and cyclists can be dangerous and often result in accidents. Therefore, developing effective systems that can accurately detect and predict these interactions is crucial for improving safety on the roads. In recent years, deep learning methods have been increasingly used for computer vision tasks related to traffic scenarios, but existing approaches still face challenges in detecting subtle cues and contextual information necessary for interaction detection.  To address these issues, we propose a novel approach using deep generative models with attention mechanisms. Our method learns to generate synthetic data by sampling from the latent space of a pre-trained variational autoencoder, which captures complex dependencies among different modalities present in traffic scenes. This allows us to train a detector network that exploits cross-modal attentional links between features extracted from RGB video frames, depth maps, and bird's eye view projections. We further enhance our model's interpretability by employing a self-attention mechanism over the generated samples to emphasize informative regions most relevant for interaction prediction.  Our experimental evaluation shows that our proposed approach outperforms several state-of-the-art baselines across multiple datasets and metrics, demonstrating its effectiveness in identifying interactions involving both VRUs and other vehicles. Furthermore, qualitative analysis reveals that our method generates more realistic and diverse synthetic images than previous GAN-based generation schemes. These results highlight the potential benefits of incorporating deep generative techniques into advanced driver assistance systems, ultimately leading to safer and more efficient transportation networks.",1
"Within Convolutional Neural Network (CNN), the convolution operations are good at extracting local features but experience difficulty to capture global representations. Within visual transformer, the cascaded self-attention modules can capture long-distance feature dependencies but unfortunately deteriorate local feature details. In this paper, we propose a hybrid network structure, termed Conformer, to take advantage of convolutional operations and self-attention mechanisms for enhanced representation learning. Conformer roots in the Feature Coupling Unit (FCU), which fuses local features and global representations under different resolutions in an interactive fashion. Conformer adopts a concurrent structure so that local features and global representations are retained to the maximum extent. Experiments show that Conformer, under the comparable parameter complexity, outperforms the visual transformer (DeiT-B) by 2.3% on ImageNet. On MSCOCO, it outperforms ResNet-101 by 3.7% and 3.6% mAPs for object detection and instance segmentation, respectively, demonstrating the great potential to be a general backbone network. Code is available at https://github.com/pengzhiliang/Conformer.",0
"This work presents a novel approach for visual recognition that combines local features with global representations. We introduce a new architecture called ""Conformer"" which leverages convolutional neural networks (CNNs) to capture both types of information. Our method couples these two sources of knowledge by using attention mechanisms to dynamically weight their contributions based on the input image. By doing so, we achieve state-of-the-art results across several benchmark datasets such as CIFAR-10/100, ImageNet, and COCO. In addition to our main contribution, we analyze the effectiveness of different design choices within our model, including feature pooling methods, activation functions, and regularization techniques. Overall, our findings demonstrate the importance of coupling local and global information for accurate visual recognition tasks.",1
"Establishing a new business may involve Knowledge acquisition in various areas, from personal to business and marketing sources. This task is challenging as it requires examining various data islands to uncover hidden patterns and unknown correlations such as purchasing behavior, consumer buying signals, and demographic and socioeconomic attributes of different locations. This paper introduces a novel framework for extracting and identifying important features from banking and non-banking data sources to address this challenge. We present an attention-based supervised feature selection approach to select important and relevant features which contribute most to the customer's query regarding establishing a new business. We report on the experiment conducted on an openly available dataset created from Kaggle and the UCI machine learning repositories.",0
"This article presents a method for assisting banking customers in establishing new businesses through dynamic feature selection with attention. The proposed approach leverages deep learning techniques, specifically multi-head attention networks, to select features that maximize performance on given tasks. Our model achieves state-of-the-art results on real-world datasets, demonstrating its effectiveness at identifying relevant features for specific applications. Furthermore, we analyze the behavior of our algorithm by visualizing how it distributes attention across different parts of a dataset, providing insights into the decisions made during feature selection. We believe our work represents a significant contribution towards intelligent decision support systems that can effectively guide entrepreneurs in making informed choices about their new ventures.",1
"In recent years, quantitative investment methods combined with artificial intelligence have attracted more and more attention from investors and researchers. Existing related methods based on the supervised learning are not very suitable for learning problems with long-term goals and delayed rewards in real futures trading. In this paper, therefore, we model the price prediction problem as a Markov decision process (MDP), and optimize it by reinforcement learning with expert trajectory. In the proposed method, we employ more than 100 short-term alpha factors instead of price, volume and several technical factors in used existing methods to describe the states of MDP. Furthermore, unlike DQN (deep Q-learning) and BC (behavior cloning) in related methods, we introduce expert experience in training stage, and consider both the expert-environment interaction and the agent-environment interaction to design the temporal difference error so that the agents are more adaptable for inevitable noise in financial data. Experimental results evaluated on share price index futures in China, including IF (CSI 300) and IC (CSI 500), show that the advantages of the proposed method compared with three typical technical analysis and two deep leaning based methods.",0
"In recent years, there has been increasing interest in using machine learning techniques for quantitative trading. One approach that has gained popularity is reinforcement learning (RL), which involves training algorithms to make sequential decisions based on feedback from their environment. This paper proposes a new method for applying RL to quantitative trading called ""Expert Trajectory RL."" This method uses expert trajectories as a guidance signal to improve both sample efficiency and robustness in policy optimization. We evaluate our algorithm through extensive experiments on benchmark datasets and show that it outperforms several state-of-the-art methods in terms of cumulative returns and risk metrics. Our results demonstrate the potential of using RL with expert guidance for effective quantitative trading strategies.",1
"Template-based discriminative trackers are currently the dominant tracking methods due to their robustness and accuracy, and the Siamese-network-based methods that depend on cross-correlation operation between features extracted from template and search images show the state-of-the-art tracking performance. However, general cross-correlation operation can only obtain relationship between local patches in two feature maps. In this paper, we propose a novel tracker network based on a powerful attention mechanism called Transformer encoder-decoder architecture to gain global and rich contextual interdependencies. In this new architecture, features of the template image is processed by a self-attention module in the encoder part to learn strong context information, which is then sent to the decoder part to compute cross-attention with the search image features processed by another self-attention module. In addition, we design the classification and regression heads using the output of Transformer to localize target based on shape-agnostic anchor. We extensively evaluate our tracker TrTr, on VOT2018, VOT2019, OTB-100, UAV, NfS, TrackingNet, and LaSOT benchmarks and our method performs favorably against state-of-the-art algorithms. Training code and pretrained models are available at https://github.com/tongtybj/TrTr.",0
"This paper presents TrTr, a new visual tracking algorithm that uses a transformer architecture to model the appearance and motion of objects in video sequences. Traditional visual tracking methods rely on handcrafted features or deep learning models that have limited capacity for temporal reasoning, resulting in poor performance under challenging conditions such as occlusions, illumination changes, and object deformations. In contrast, TrTr utilizes self attention mechanisms to capture global relationships among image patches within both spatial and temporal dimensions, enabling it to accurately predict the location of target objects even under difficult situations. Our approach outperforms state-of-the-art trackers on multiple benchmarks, demonstrating its effectiveness in real-world scenarios.",1
"Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory.",0
"Title: ""Differential Dynamic Programming Neural Optimizer"" by Xavier et al. (2017) presents a new method for neural network optimization using Differential Dynamic Programming (DDP). The authors aimed to develop a model that can perform globally optimal dynamic programming on high dimensional problems without relying on expensive second derivatives. Their approach uses stochastic gradient descent with randomized directions, which allows for efficient computation while maintaining convergence guarantees similar to batch methods. In addition, the proposed method was found to outperform existing algorithms such as SGD with Nesterov momentum and Adam in terms of both speed and accuracy. Overall, this study provides a promising solution to improve the efficiency of training deep learning models.  Note: Make sure to replace Xavier et al. (2017) with appropriate citations if you need this text for academic purposes. Also ensure to summarize the content accurately according to your understanding after reading the full text, and check with guidelines specific to your field before submitting any written work.",1
"Small target motion detection within complex natural environments is an extremely challenging task for autonomous robots. Surprisingly, the visual systems of insects have evolved to be highly efficient in detecting mates and tracking prey, even though targets are as small as a few pixels in their visual fields. The excellent sensitivity to small target motion relies on a class of specialized neurons called small target motion detectors (STMDs). However, existing STMD-based models are heavily dependent on visual contrast and perform poorly in complex natural environments where small targets generally exhibit extremely low contrast against neighbouring backgrounds. In this paper, we develop an attention and prediction guided visual system to overcome this limitation. The developed visual system comprises three main subsystems, namely, an attention module, an STMD-based neural network, and a prediction module. The attention module searches for potential small targets in the predicted areas of the input image and enhances their contrast against complex background. The STMD-based neural network receives the contrast-enhanced image and discriminates small moving targets from background false positives. The prediction module foresees future positions of the detected targets and generates a prediction map for the attention module. The three subsystems are connected in a recurrent architecture allowing information to be processed sequentially to activate specific areas for small target detection. Extensive experiments on synthetic and real-world datasets demonstrate the effectiveness and superiority of the proposed visual system for detecting small, low-contrast moving targets against complex natural environments.",0
"Abstract: In this paper, we present a new approach to detecting low-contrast small moving targets in video footage that utilizes attention mechanisms and predictive guidance. Our method addresses two common issues encountered in traditional motion detection algorithms: false positives caused by background movement and missed detections due to the limited contrast of the target. By incorporating attention mechanisms into our model, we focus computational resources on areas of the image where motion is most likely occurring, reducing the number of false positives. Additionally, through the use of predictive guidance, we can better anticipate the motion trajectory of the target, which improves detection accuracy even when the target has a limited contrast against the surrounding environment. Experimental results demonstrate significant improvements in detection performance compared to state-of-the-art methods. Overall, our proposed algorithm provides an effective solution for accurately identifying low-contrast small moving targets in real-world scenarios.",1
"Federated learning has attracted attention in recent years for collaboratively training data on distributed devices with privacy-preservation. The limited network capacity of mobile and IoT devices has been seen as one of the major challenges for cross-device federated learning. Recent solutions have been focusing on threshold-based client selection schemes to guarantee the communication efficiency. However, we find this approach can cause biased client selection and results in deteriorated performance. Moreover, we find that the challenge of network limit may be overstated in some cases and the packet loss is not always harmful. In this paper, we explore the loss tolerant federated learning (LT-FL) in terms of aggregation, fairness, and personalization. We use ThrowRightAway (TRA) to accelerate the data uploading for low-bandwidth-devices by intentionally ignoring some packet losses. The results suggest that, with proper integration, TRA and other algorithms can together guarantee the personalization and fairness performance in the face of packet loss below a certain fraction (10%-30%).",0
"In federated learning, clients train on their own data locally before sharing parameters with others. This can result in models that have poor performance due to insufficient or biased training data. To address these issues, loss tolerant federated learning techniques have been proposed which relax constraints on model accuracy. These methods allow models to continue training even if they experience large losses, allowing them to adapt to unseen datasets more effectively. However, previous works have mainly focused on simple machine learning tasks such as binary classification and regression using logistic regression and linear models. In this work we introduce a novel approach to federated learning by incorporating robust loss functions into the optimization process. Our method allows for better adaptation to unseen data while maintaining strong generalization ability across different domains. Additionally, our experiments show improved performance compared to state-of-the-art methods. We believe our contributions will provide new directions for future research in loss tolerant federated learning.",1
"Federated Learning (FL), arising as a novel secure learning paradigm, has received notable attention from the public. In each round of synchronous FL training, only a fraction of available clients are chosen to participate and the selection decision might have a significant effect on the training efficiency, as well as the final model performance. In this paper, we investigate the client selection problem under a volatile context, in which the local training of heterogeneous clients is likely to fail due to various kinds of reasons and in different levels of frequency. Intuitively, too much training failure might potentially reduce the training efficiency, while too much selection on clients with greater stability might introduce bias, and thereby result in degradation of the training effectiveness. To tackle this tradeoff, we in this paper formulate the client selection problem under joint consideration of effective participation and fairness. Further, we propose E3CS, a stochastic client selection scheme on the basis of an adversarial bandit solution, and we further corroborate its effectiveness by conducting real data-based experiments. According to the experimental results, our proposed selection scheme is able to achieve up to 2x faster convergence to a fixed model accuracy while maintaining the same level of final model accuracy, in comparison to the vanilla selection scheme in FL.",0
"Federated learning has emerged as one of the promising techniques to train deep neural networks using distributed datasets without sharing raw data. However, existing approaches assume that clients participate consistently throughout training sessions; they neither account for dynamic membership nor adapt to changing quality of service offered by volatile clients. In practice, such clientele can become unresponsive over time due to diverse reasons ranging from network interruptions to battery exhaustion. This work proposes a stochastic selection technique that makes efficient use of available resources while ensuring low risk of data corruption or privacy breaches. We formulate our optimization problem under a robustness framework, where we minimize impact on model accuracy while maximizing diversity in selecting participants. Our approach outperforms baseline methods by effectively accommodating participation fluctuations during federated training, leading to better generalization performance compared to alternative random selections or heuristics-based strategies. Furthermore, we demonstrate the effectiveness of our solution through experiments conducted on both synthetic and real world benchmarks. These empirical results showcase improvements in convergence speed, stability, and utility preservation under varying environmental conditions across different domains.",1
"Prior human parsing models are limited to parsing humans into classes pre-defined in the training data, which is not flexible to generalize to unseen classes, e.g., new clothing in fashion analysis. In this paper, we propose a new problem named one-shot human parsing (OSHP) that requires to parse human into an open set of reference classes defined by any single reference example. During training, only base classes defined in the training set are exposed, which can overlap with part of reference classes. In this paper, we devise a novel Progressive One-shot Parsing network (POPNet) to address two critical challenges , i.e., testing bias and small sizes. POPNet consists of two collaborative metric learning modules named Attention Guidance Module and Nearest Centroid Module, which can learn representative prototypes for base classes and quickly transfer the ability to unseen classes during testing, thereby reducing testing bias. Moreover, POPNet adopts a progressive human parsing framework that can incorporate the learned knowledge of parent classes at the coarse granularity to help recognize the descendant classes at the fine granularity, thereby handling the small sizes issue. Experiments on the ATR-OS benchmark tailored for OSHP demonstrate POPNet outperforms other representative one-shot segmentation models by large margins and establishes a strong baseline. Source code can be found at https://github.com/Charleshhy/One-shot-Human-Parsing.",0
"Automatically parsing out humans from background has many applications such as video surveillance system, autonomous driving, virtual reality, and augmented reality. Despite recent progresses in computer vision research area with convolutional neural networks (CNNs), human parsing remains challenging due to occlusion, variation of pose and shape, cluttered backgrounds, low resolution and fast movement. In order to efficiently perform human parsing while addressing these issues we introduce a novel approach called Progressive One-shot Human Parsing that accurately detects and segments one person per image in single shot input images. Our method utilizes transfer learning by initializing weights pretrained on large scale datasets like ImageNet before finetuning them using newly collected data specifically tailored for our task at hand, human parsing in single shot input images. We further improve our results by employing Focal Loss which addresses class imbalances common in detection tasks like this. To evaluate the effectiveness of our method, we benchmark against other state-of-the-art methods including Mask R-CNN and DETR. Results show significant improvement over all metrics suggesting high quality predictions made possible only through careful consideration of design choices paired together into our model architecture. Overall, our work shows great potential within computer vision field towards real world application of automatic human parsing under dynamic environments mentioned earlier with high accuracy and speed.",1
"Learning pyramidal feature representations is crucial for recognizing object instances at different scales. Feature Pyramid Network (FPN) is the classic architecture to build a feature pyramid with high-level semantics throughout. However, intrinsic defects in feature extraction and fusion inhibit FPN from further aggregating more discriminative features. In this work, we propose Attention Aggregation based Feature Pyramid Network (A^2-FPN), to improve multi-scale feature learning through attention-guided feature aggregation. In feature extraction, it extracts discriminative features by collecting-distributing multi-level global context features, and mitigates the semantic information loss due to drastically reduced channels. In feature fusion, it aggregates complementary information from adjacent features to generate location-wise reassembly kernels for content-aware sampling, and employs channel-wise reweighting to enhance the semantic consistency before element-wise addition. A^2-FPN shows consistent gains on different instance segmentation frameworks. By replacing FPN with A^2-FPN in Mask R-CNN, our model boosts the performance by 2.1% and 1.6% mask AP when using ResNet-50 and ResNet-101 as backbone, respectively. Moreover, A^2-FPN achieves an improvement of 2.0% and 1.4% mask AP when integrated into the strong baselines such as Cascade Mask R-CNN and Hybrid Task Cascade.",0
"The paper presents a novel architecture called A^2-FPN (Attention Aggregation Based Feature Pyramid Network) which introduces attention aggregation modules into feature pyramids for instance segmentation. By doing so, A^2-FPN improves upon existing features extraction methods such as Faster R-CNN by utilizing self attention mechanisms at different scales. The proposed method significantly reduces computational cost while improving mask quality through effective integration of multi-scale features. Results show that A^2-FPN achieves state-of-the art performance on instance segmentation benchmarks. Furthermore, ablation studies demonstrate that each component contributes substantially towards overall improvement of the networkâ€™s efficiency and effectiveness.",1
"In this paper, we propose a novel text-based talking-head video generation framework that synthesizes high-fidelity facial expressions and head motions in accordance with contextual sentiments as well as speech rhythm and pauses. To be specific, our framework consists of a speaker-independent stage and a speaker-specific stage. In the speaker-independent stage, we design three parallel networks to generate animation parameters of the mouth, upper face, and head from texts, separately. In the speaker-specific stage, we present a 3D face model guided attention network to synthesize videos tailored for different individuals. It takes the animation parameters as input and exploits an attention mask to manipulate facial expression changes for the input individuals. Furthermore, to better establish authentic correspondences between visual motions (i.e., facial expression changes and head movements) and audios, we leverage a high-accuracy motion capture dataset instead of relying on long videos of specific individuals. After attaining the visual and audio correspondences, we can effectively train our network in an end-to-end fashion. Extensive experiments on qualitative and quantitative results demonstrate that our algorithm achieves high-quality photo-realistic talking-head videos including various facial expressions and head motions according to speech rhythms and outperforms the state-of-the-art.",0
"In our modern society, we often take for granted that technology has improved many aspects of our lives, from transportation to communication to entertainment. However, one area where technology has lagged behind is emotions. While computers can now recognize basic facial expressions, there still exists a gap between human expression and machine understanding. To bridge this divide, researchers have explored different techniques such as text-to-image synthesis and generative adversarial networks (GANs). This study proposes a method called Write-A-Speaker which combines text-based input with rhythmic talking-head generation to generate realistic video clips of virtual characters expressing a wide range of emotions. By utilizing large language models like GPT-4, the system generates natural-sounding speech and lip movements synchronized to phonemes, allowing viewers to better comprehend the emotional state conveyed by the character. Preliminary experiments show promising results in terms of user engagement and subjective ratings for generated videos. Overall, the proposed method has the potential to revolutionize how we interact with virtual assistants and improve the accessibility of multimedia content for individuals with hearing impairments or limited audio resources.",1
"One of the successful approaches in semi-supervised learning is based on the consistency regularization. Typically, a student model is trained to be consistent with teacher prediction for the inputs under different perturbations. To be successful, the prediction targets given by teacher should have good quality, otherwise the student can be misled by teacher. Unfortunately, existing methods do not assess the quality of the teacher targets. In this paper, we propose a novel Certainty-driven Consistency Loss (CCL) that exploits the predictive uncertainty in the consistency loss to let the student dynamically learn from reliable targets. Specifically, we propose two approaches, i.e. Filtering CCL and Temperature CCL to either filter out uncertain predictions or pay less attention on them in the consistency regularization. We further introduce a novel decoupled framework to encourage model difference. Experimental results on SVHN, CIFAR-10, and CIFAR-100 demonstrate the advantages of our method over a few existing methods.",0
"In recent years, semi-supervised learning has emerged as a powerful approach for leveraging unlabeled data to improve model performance in various domains such as computer vision and natural language processing. However, one major challenge faced by semi-supervised methods is ensuring consistency across different models trained using limited labeled data from multiple teachers. This paper proposes a novel technique called Certainty Driven Consistency Loss (CDC) that addresses this issue by imposing constraints based on the uncertainty levels of each teacher network. CDC employs a multi-teacher setup where each teacher network generates predictions with their corresponding uncertainty estimates. These uncertainty estimates then guide the training process through a regularization term that encourages high confidence predictions while discouraging low confidence ones. Experiments performed on several benchmark datasets demonstrate that our proposed method significantly outperforms state-of-the-art techniques in terms of accuracy and robustness, providing a promising solution for improving the generalizability of semi-supervised approaches under real-world scenarios. Our work sheds light on the importance of incorporating uncertainty awareness into semi-supervised frameworks, which could potentially lead to more reliable machine learning models across diverse applications.",1
"Existing part-aware person re-identification methods typically employ two separate steps: namely, body part detection and part-level feature extraction. However, part detection introduces an additional computational cost and is inherently challenging for low-quality images. Accordingly, in this work, we propose a simple framework named Batch Coherence-Driven Network (BCD-Net) that bypasses body part detection during both the training and testing phases while still learning semantically aligned part features. Our key observation is that the statistics in a batch of images are stable, and therefore that batch-level constraints are robust. First, we introduce a batch coherence-guided channel attention (BCCA) module that highlights the relevant channels for each respective part from the output of a deep backbone model. We investigate channelpart correspondence using a batch of training images, then impose a novel batch-level supervision signal that helps BCCA to identify part-relevant channels. Second, the mean position of a body part is robust and consequently coherent between batches throughout the training process. Accordingly, we introduce a pair of regularization terms based on the semantic consistency between batches. The first term regularizes the high responses of BCD-Net for each part on one batch in order to constrain it within a predefined area, while the second encourages the aggregate of BCD-Nets responses for all parts covering the entire human body. The above constraints guide BCD-Net to learn diverse, complementary, and semantically aligned part-level features. Extensive experimental results demonstrate that BCDNet consistently achieves state-of-the-art performance on four large-scale ReID benchmarks.",0
"This paper presents a novel approach, called Batch Coherence-Driven Network (BCDN), which focuses on learning discriminative feature representations for part-based person re-identification by enforcing batch consistency across different views of the same identity during training. Our method takes advantage of both local features from body parts and global contextual information captured in full images, thus enhancing feature representation coherency at both fine-grained levels, such as individual body parts and coarse-grained level, such as full image comparisons. Extensive experiments conducted on four widely used datasets demonstrate that our proposed model outperforms state-of-the-art methods under varying experimental settings. We analyze two real-world use cases to showcase how our method can effectively address scenarios beyond traditional evaluation metrics, including handling occlusions and partial matches commonly found in practical applications, demonstrating its superiority over existing approaches. In conclusion, we have introduced BCDN, a highly effective solution for part-based person re-identification, with extensive validation showing significant improvements over current techniques. Overall, BCDN has great potential to drive new advancements within the field of computer vision, specifically regarding human recognition systems.",1
"With recent advances in data collection from multiple sources, multi-view data has received significant attention. In multi-view data, each view represents a different perspective of data. Since label information is often expensive to acquire, multi-view clustering has gained growing interest, which aims to obtain better clustering solution by exploiting complementary and consistent information across all views rather than only using an individual view. Due to inevitable sensor failures, data in each view may contain error. Error often exhibits as noise or feature-specific corruptions or outliers. Multi-view data may contain any or combination of these error types. Blindly clustering multi-view data i.e., without considering possible error in view(s) could significantly degrade the performance. The goal of error-robust multi-view clustering is to obtain useful outcome even if the multi-view data is corrupted. Existing error-robust multi-view clustering approaches with explicit error removal formulation can be structured into five broad research categories - sparsity norm based approaches, graph based methods, subspace based learning approaches, deep learning based methods and hybrid approaches, this survey summarizes and reviews recent advances in error-robust clustering for multi-view data. Finally, we highlight the challenges and provide future research opportunities.",0
"Title: A Survey on Robustness in Cluster Analysis and Learning from Multiple Views ---------------------------- Cluster analysis refers to a broad class of algorithms that partition data points into groups based on their shared properties. Applications of cluster analysis span across many domains including image segmentation, bioinformatics, recommendation systems, computer vision, speech recognition, natural language processing, neuroscience, anthropology, sociology, criminology, psychology, demography, economics, market research, communication studies, and management science. Despite the ubiquity of these methods, clustering techniques often suffer from issues related to errors or noise present in the input data. To address such challenges, there has been significant recent interest in developing error-robust approaches towards multi-view clustering; these methods seek to leverage complementary views of the same underlying clusters from multiple perspectives. This paper presents a survey of existing work in the area of robust cluster analysis and learning from multiple views with a focus on recent advances as well as opportunities for future research.  The main contributions of our survey are several fold: Firstly, we provide a comprehensive review of state-of-the art methodologies aimed at improving the quality of cluster assignments and partitions by taking into account the presence of errors within individual view spaces and/or between different views. Secondly, we critically discuss open questions and future directions for researchers working in the field of error-tolerant multi-view clustering. Finally, we highlight promising applications areas where these new methodological developments can have a substantial impact. We structure the remainder of the paper as follows: In Section 2, we outline relevant background material on cluster analysis, multiple view learning, and error models encountered i",1
"With recent advancements in deep learning methods, automatically learning deep features from the original data is becoming an effective and widespread approach. However, the hand-crafted expert knowledge-based features are still insightful. These expert-curated features can increase the model's generalization and remind the model of some data characteristics, such as the time interval between two patterns. It is particularly advantageous in tasks with the clinically-relevant data, where the data are usually limited and complex. To keep both implicit deep features and expert-curated explicit features together, an effective fusion strategy is becoming indispensable. In this work, we focus on a specific clinical application, i.e., sleep apnea detection. In this context, we propose a contrastive learning-based cross attention framework for sleep apnea detection (named ConCAD). The cross attention mechanism can fuse the deep and expert features by automatically assigning attention weights based on their importance. Contrastive learning can learn better representations by keeping the instances of each class closer and pushing away instances from different classes in the embedding space concurrently. Furthermore, a new hybrid loss is designed to simultaneously conduct contrastive learning and classification by integrating a supervised contrastive loss with a cross-entropy loss. Our proposed framework can be easily integrated into standard deep learning models to utilize expert knowledge and contrastive learning to boost performance. As demonstrated on two public ECG dataset with sleep apnea annotation, ConCAD significantly improves the detection performance and outperforms state-of-art benchmark methods.",0
"Automatic sleep apnea detection from overnight Polysomnography (PSG) recordings has been a challenging task due to the complexity and variability of the signals involved. Recent advances in deep learning have shown promising results for addressing this problem, but these approaches often suffer from high computational costs and limited interpretability. In this work, we propose a novel framework called ConCAD (Contrastive Learning-based Cross Attention) that addresses both of these limitations by leveraging contrastive learning and cross attention mechanisms.  Our approach first extracts features from raw PSG signals using a pretrained convolutional neural network, then applies a contrastive loss function to learn representations that capture the discriminative patterns associated with different types of sleep apneas. This allows us to train our model efficiently without large datasets or expensive annotations. To enhance interpretability, we incorporate a channel-wise attention mechanism that highlights relevant channels based on their contribution to classification. Finally, we evaluate our method on two publicly available datasets, achieving state-of-the-art performance while significantly reducing computational requirements compared to other deep learning methods. Our study demonstrates the effectiveness and efficiency of ConCAD as a viable solution for automatic sleep apnea detection from uninterrupted PSG signals.",1
"Low-precision deep neural network (DNN) training has gained tremendous attention as reducing precision is one of the most effective knobs for boosting DNNs' training time/energy efficiency. In this paper, we attempt to explore low-precision training from a new perspective as inspired by recent findings in understanding DNN training: we conjecture that DNNs' precision might have a similar effect as the learning rate during DNN training, and advocate dynamic precision along the training trajectory for further boosting the time/energy efficiency of DNN training. Specifically, we propose Cyclic Precision Training (CPT) to cyclically vary the precision between two boundary values which can be identified using a simple precision range test within the first few training epochs. Extensive simulations and ablation studies on five datasets and eleven models demonstrate that CPT's effectiveness is consistent across various models/tasks (including classification and language modeling). Furthermore, through experiments and visualization we show that CPT helps to (1) converge to a wider minima with a lower generalization error and (2) reduce training variance which we believe opens up a new design knob for simultaneously improving the optimization and efficiency of DNN training. Our codes are available at: https://github.com/RICE-EIC/CPT.",0
"Artificial intelligence has become increasingly popular as researchers seek new ways to improve efficiency in training deep neural networks (DNNs). One method that has been proposed for improving DNN training speed is cyclic precision (CPT), which involves adjusting the precision used during forward and backward passes in each layer. By using higher precision during the forward pass and lower precision during the backpropagation process, computational resources can be optimized and training times reduced without sacrificing accuracy. This paper presents experimental results demonstrating the effectiveness of CPT at accelerating DNN training while maintaining comparable performance to traditional methods. In conclusion, CPT offers a promising approach towards efficient DNN training that warrants further investigation in future work. Keywords: deep learning, neural network training, precision, optimization.",1
"Thanks to the increasing availability of drug-drug interactions (DDI) datasets and large biomedical knowledge graphs (KGs), accurate detection of adverse DDI using machine learning models becomes possible. However, it remains largely an open problem how to effectively utilize large and noisy biomedical KG for DDI detection. Due to its sheer size and amount of noise in KGs, it is often less beneficial to directly integrate KGs with other smaller but higher quality data (e.g., experimental data). Most of the existing approaches ignore KGs altogether. Some try to directly integrate KGs with other data via graph neural networks with limited success. Furthermore, most previous works focus on binary DDI prediction whereas the multi-typed DDI pharmacological effect prediction is a more meaningful but harder task. To fill the gaps, we propose a new method SumGNN: knowledge summarization graph neural network, which is enabled by a subgraph extraction module that can efficiently anchor on relevant subgraphs from a KG, a self-attention based subgraph summarization scheme to generate a reasoning path within the subgraph, and a multi-channel knowledge and data integration module that utilizes massive external biomedical knowledge for significantly improved multi-typed DDI predictions. SumGNN outperforms the best baseline by up to 5.54\%, and the performance gain is particularly significant in low data relation types. In addition, SumGNN provides interpretable prediction via the generated reasoning paths for each prediction.",0
"Abstract In this paper we present a novel framework called SumGNN that uses graph neural networks (GNNs) to efficiently summarize knowledge graphs and predict interactions between drugs. We demonstrate how our method outperforms state-of-the-art baselines across multiple drug interaction prediction benchmark datasets. Our approach builds on recent advances in GNNs by using a lightweight variant of a GCN architecture that operates directly on graphs represented as matrices. This allows us to generate compressed representations of large knowledge graphs which can then be passed through a multi-layered perceptron network to perform the desired predictions. To evaluate the effectiveness of our approach, we conduct experiments on several real world drug interaction prediction tasks, including predicting interactions between drugs based on their chemical structures as well as predicted drug targets, and show that our model consistently achieves superior performance compared to prior art methods. Keywords: Knowledge graphs; Graph Neural Networks (GNN); Drug interaction prediction; Chemical structure analysis; Target prediction",1
"The goal of this work is to temporally align asynchronous subtitles in sign language videos. In particular, we focus on sign-language interpreted TV broadcast data comprising (i) a video of continuous signing, and (ii) subtitles corresponding to the audio content. Previous work exploiting such weakly-aligned data only considered finding keyword-sign correspondences, whereas we aim to localise a complete subtitle text in continuous signing. We propose a Transformer architecture tailored for this task, which we train on manually annotated alignments covering over 15K subtitles that span 17.7 hours of video. We use BERT subtitle embeddings and CNN video representations learned for sign recognition to encode the two signals, which interact through a series of attention layers. Our model outputs frame-level predictions, i.e., for each video frame, whether it belongs to the queried subtitle or not. Through extensive evaluations, we show substantial improvements over existing alignment baselines that do not make use of subtitle text embeddings for learning. Our automatic alignment model opens up possibilities for advancing machine translation of sign languages via providing continuously synchronized video-text data.",0
"Title: Improving Accessibility through Subtitle Alignment in Sign Language Videos  Abstract: Sign language videos are crucial tools for providing access to content for individuals who are deaf or hard of hearing. However, current subtitling methods often result in poor alignment between sign language signs and their corresponding text descriptions, making it difficult for viewers to follow along. This study presents a novel approach to improve subtitle alignment by using natural language processing techniques to identify key words and phrases from both audio transcriptions and sign language videos. Our method achieves an accuracy rate of over 94%, significantly improving upon existing methods that rely on manual annotation or limited rule-based systems. We demonstrate how our system can enhance overall comprehension and engagement for viewers watching sign language videos online. With further development, this technology could greatly enhance accessibility for millions of users worldwide.",1
"We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT",0
"Title : LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference.  Abstract: This paper presents LeViT (Lightweight Vision Transformer), a new architecture that combines the strengths of convolutional neural networks (ConvNets) and vision transformers (ViT). The goal of LeViT is to achieve faster inference while maintaining high accuracy on challenging image classification tasks such as ImageNet. The key insight behind LeViT is that ViT models rely heavily on self-attention mechanisms which can lead to slow computation times and require large computational resources. By contrast, ConvNets employ repeated local convolutions that efficiently capture spatial hierarchies within images. To combine these two approaches, we introduce Local Attentive Blocks (LABs) that compute attentional weights locally, without considering all pixels in the input image at once. This allows us to replace most of the self-attention operations in ViT with LABs, resulting in a more efficient model that retains strong performance on benchmark datasets. Experiments demonstrate that our approach achieves state-of-the art performance in terms of speed and efficiency, outperforming other modern models on many metrics. These results hold promise for real-world deployment scenarios where inference time and resource usage matter. Overall, LeViT represents an important step forward towards fast and accurate deep learning for computer vision.",1
"For NP-hard combinatorial optimization problems, it is usually difficult to find high-quality solutions in polynomial time. The design of either an exact algorithm or an approximate algorithm for these problems often requires significantly specialized knowledge. Recently, deep learning methods provide new directions to solve such problems. In this paper, an end-to-end deep reinforcement learning framework is proposed to solve this type of combinatorial optimization problems. This framework can be applied to different problems with only slight changes of input (for example, for a traveling salesman problem (TSP), the input is the two-dimensional coordinates of nodes; while for a capacity-constrained vehicle routing problem (CVRP), the input is simply changed to three-dimensional vectors including the two-dimensional coordinates and the customer demands of nodes), masks and decoder context vectors. The proposed framework is aiming to improve the models in literacy in terms of the neural network model and the training algorithm. The solution quality of TSP and the CVRP up to 100 nodes are significantly improved via our framework. Specifically, the average optimality gap is reduced from 4.53\% (reported best \cite{R22}) to 3.67\% for TSP with 100 nodes and from 7.34\% (reported best \cite{R22}) to 6.68\% for CVRP with 100 nodes when using the greedy decoding strategy. Furthermore, our framework uses about 1/3$\sim$3/4 training samples compared with other existing learning methods while achieving better results. The results performed on randomly generated instances and the benchmark instances from TSPLIB and CVRPLIB confirm that our framework has a linear running time on the problem size (number of nodes) during the testing phase, and has a good generalization performance from random instance training to real-world instance testing.",0
"In this paper we present an algorithm for solving routing problems using a Residual Edge-Graph Attention Neural Network (REGANN). Our approach builds on recent advances in graph neural networks by incorporating both edge features and node attributes to enhance the modelâ€™s understanding of the problem space. We demonstrate through extensive experiments that our method achieves state-of-the-art results across several common benchmark datasets used for testing routing algorithms. Our REGANN approach has significant advantages over traditional methods such as Dijkstraâ€™s Algorithm and Breadth First Search due to its ability to handle complex road networks and its robustness under uncertainty. Overall, our work shows great promise for future research in the field of transportation systems optimization and offers new opportunities for real-world applications.",1
"The potential for machine learning systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. Much recent work has focused on developing algorithmic tools to assess and mitigate such unfairness. However, there is little work on enhancing fairness in graph algorithms. Here, we develop a simple, effective and general method, CrossWalk, that enhances fairness of various graph algorithms, including influence maximization, link prediction and node classification, applied to node embeddings. CrossWalk is applicable to any random walk based node representation learning algorithm, such as DeepWalk and Node2Vec. The key idea is to bias random walks to cross group boundaries, by upweighting edges which (1) are closer to the groups' peripheries or (2) connect different groups in the network. CrossWalk pulls nodes that are near groups' peripheries towards their neighbors from other groups in the embedding space, while preserving the necessary structural information from the graph. Extensive experiments show the effectiveness of our algorithm to enhance fairness in various graph algorithms, including influence maximization, link prediction and node classification in synthetic and real networks, with only a very small decrease in performance.",0
"Abstract: In today's world where data is king, large scale graph analytics has become increasingly important as one can extract valuable insights from graphs by analyzing the relationships between nodes representing objects or entities. Graph neural networks (GNN) have gained popularity recently due to their ability to capture node representations that encode complex relationships in a network. However, capturing meaningful representations often requires deep training procedures leading to concerns regarding generalization performance. Furthermore, current GNN models treat all edges within a single layer equally without considering edge weights which may cause issues like overfitting on a small set of influential nodes while underfitting on others. Therefore, we propose CrossWalk, a novel GNN architecture aimed at addressing these challenges. Our model includes weighted attention mechanisms allowing edge importance based on edge weights to propagate through the layers explicitly. We argue that using these weights allows the learning process to focus more effectively on high quality paths which helps improve both node representation generation and fairness simultaneously via fine tuning hyperparameters during training as demonstrated empirically. We verify our claims in several experiments across different datasets.",1
"The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9\% top-1 accuracy, compared to 77.9\% and 79.9\% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are.",0
"ImageNet, one of the most well known datasets used today was introduced in 2014 by Rusu et al., as â€œa large-scale image database [with] 14 million high-resolution images belonging to 22 thousand categories.â€ Image recognition has come a long way since then, thanks in part due to advances made by models such as ResNets, DenseNets, and others that have been trained with attention mechanisms. However research from the past few years suggests that stacks of feedforward layers can perform surprisingly well without using any attention at all. In our latest work, we investigate if attention mechanism is truly necessary when it comes to training feedforward neural networks to solve complex vision tasks. Our results show that models built solely with feedforward layers equipped with skip connections can achieve comparable performance compared to state-of-the art models that use explicit spatial attention mechanisms. To further validate these findings, we conducted experiments on the ImageNet benchmark dataset. We demonstrate that a network constructed only with depthwise separable convolutional layers, Batch Normalization (BN), and Rectified Linear Unit (ReLU) activations â€“ without skip connections - cannot match the performance of a model incorporating residual connections but no attention layer, which raises questions about the importance of attention mechanisms versus other components of the network architecture. These intriguing results lead us to speculate that the benefits of using attention may lie less in providing access to additional information than previously believed. Instead they might serve other purposes, like regularizing the model by encouraging sparser representations and making the optimization process easier. Overall our study indicates that, contrary to popular belief, attention is not always needed for top p",1
"With the rapid development of facial manipulation techniques, face forgery detection has received considerable attention in digital media forensics due to security concerns. Most existing methods formulate face forgery detection as a classification problem and utilize binary labels or manipulated region masks as supervision. However, without considering the correlation between local regions, these global supervisions are insufficient to learn a generalized feature and prone to overfitting. To address this issue, we propose a novel perspective of face forgery detection via local relation learning. Specifically, we propose a Multi-scale Patch Similarity Module (MPSM), which measures the similarity between features of local regions and forms a robust and generalized similarity pattern. Moreover, we propose an RGB-Frequency Attention Module (RFAM) to fuse information in both RGB and frequency domains for more comprehensive local feature representation, which further improves the reliability of the similarity pattern. Extensive experiments show that the proposed method consistently outperforms the state-of-the-arts on widely-used benchmarks. Furthermore, detailed visualization shows the robustness and interpretability of our method.",0
"Facial forgery detection has become increasingly important as advancements in image synthesis technology have made it easier to manipulate images and videos. In recent years, convolutional neural networks (CNNs) have been widely used for facial forgery detection, but they rely on global features that may not capture local relationships within images. To address this limitation, we propose a novel approach called ""Local Relation Learning"" which captures local relations between neighboring patches within facial images. Our method utilizes a pre-trained CNN to extract feature maps from input images, and then applies multi-scale dilations to generate corresponding relation maps at different scales. These relation maps encode local patterns and textural details such as edges, corners, and gradients, which can be used to detect manipulations. We further introduce a contrastive learning framework to learn discriminative features by comparing authentic and tampered regions. Experimental results on four public datasets demonstrate the effectiveness of our proposed method, outperforming state-of-the-art techniques. Overall, our work shows that incorporating local relationship learning into facial forgery detection can lead to improved performance.",1
"Recently, deep convolutional neural networks (DCNN) that leverage the adversarial training framework for image restoration and enhancement have significantly improved the processed images' sharpness. Surprisingly, although these DCNNs produced crispier images than other methods visually, they may get a lower quality score when popular measures are employed for evaluating them. Therefore it is necessary to develop a quantitative metric to reflect their performances, which is well-aligned with the perceived quality of an image. Famous quantitative metrics such as Peak signal-to-noise ratio (PSNR), The structural similarity index measure (SSIM), and Perceptual Index (PI) are not well-correlated with the mean opinion score (MOS) for an image, especially for the neural networks trained with adversarial loss functions.   This paper has proposed a convolutional neural network using an extension architecture of the traditional Siamese network so-called Siamese-Difference neural network. We have equipped this architecture with the spatial and channel-wise attention mechanism to increase our method's performance.   Finally, we employed an auxiliary loss function to train our model. The suggested additional cost function surrogates ranking loss to increase Spearman's rank correlation coefficient while it is differentiable concerning the neural network parameters. Our method achieved superior performance in \textbf{\textit{NTIRE 2021 Perceptual Image Quality Assessment}} Challenge. The implementations of our proposed method are publicly available.",0
"This abstract proposes the use of an attention-based siamese difference neural network architecture combined with a surrogate ranking loss function for perceptual image quality assessment. Existing approaches have relied on heuristics that can be computationally expensive, while prior deep learning based methods often rely on large amounts of labeled data for training which may not always be available. We introduce our attention-based network to address these issues by using attention mechanisms to focus on regions in the input images where differences occur without relying on explicit coordinates. Our surrogate ranking loss functions then ensure that only relative orderings between pairs of images need to be computed during testing, allowing for efficient evaluation of the model even on devices with limited computational resources such as smartphones or embedded systems. Experiments demonstrate strong performance compared against state-of-the-art models trained with significantly more data, confirming the effectiveness of the proposed approach for resource constrained environments. Overall, this work shows promise towards realizing accurate perceptual image quality assessment under limited computational constraints.",1
"Undirected graphical models have been widely used to model the conditional independence structure of high-dimensional random vector data for years. In many modern applications such as EEG and fMRI data, the observations are multivariate random functions rather than scalars. To model the conditional independence of this type of data, functional graphical models are proposed and have attracted an increasing attention in recent years. In this paper, we propose a neighborhood selection approach to estimate Gaussian functional graphical models. We first estimate the neighborhood of all nodes via function-on-function regression, and then we can recover the whole graph structure based on the neighborhood information. By estimating conditional structure directly, we can circumvent the need of a well-defined precision operator which generally does not exist. Besides, we can better explore the effect of the choice of function basis for dimension reduction. We give a criterion for choosing the best function basis and motivate two practically useful choices, which we justified by both theory and experiments and show that they are better than expanding each function onto its own FPCA basis as in previous literature. In addition, the neighborhood selection approach is computationally more efficient than fglasso as it is more easy to do parallel computing. The statistical consistency of our proposed methods in high-dimensional setting are supported by both theory and experiment.",0
"This paper presents a new methodology for learning graphical model structures from high-dimensional data sets using neighborhood selection approach. We propose to leverage the sparsity constraint on neighboring coefficients of different variables as a prior assumption in structure discovery. Specifically, our framework exploits the concept that two connected nodes have similar patterns and behave similarly across different dimensions, which can be captured by applying sparse representation of one node in terms of neighborsâ€™ patches, followed by group Lasso regularization. By exploring the similarity between the rows within each group, we develop a novel algorithm capable of selecting edges among highly correlated nodes without overwhelming computational cost. Our results demonstrate significant improvements compared to state-of-the-art techniques in recovering accurate graph structures for both synthetic and real world datasets.",1
"Capsule network is a type of neural network that uses the spatial relationship between features to classify images. By capturing the poses and relative positions between features, its ability to recognize affine transformation is improved, and it surpasses traditional convolutional neural networks (CNNs) when handling translation, rotation and scaling. The Stacked Capsule Autoencoder (SCAE) is the state-of-the-art capsule network. The SCAE encodes an image as capsules, each of which contains poses of features and their correlations. The encoded contents are then input into the downstream classifier to predict the categories of the images. Existing research mainly focuses on the security of capsule networks with dynamic routing or EM routing, and little attention has been given to the security and robustness of the SCAE. In this paper, we propose an evasion attack against the SCAE. After a perturbation is generated based on the output of the object capsules in the model, it is added to an image to reduce the contribution of the object capsules related to the original category of the image so that the perturbed image will be misclassified. We evaluate the attack using an image classification experiment, and the experimental results indicate that the attack can achieve high success rates and stealthiness. It confirms that the SCAE has a security vulnerability whereby it is possible to craft adversarial samples without changing the original structure of the image to fool the classifiers. We hope that our work will make the community aware of the threat of this attack and raise the attention given to the SCAE's security.",0
"This paper presents an evasion attack against stacked capsule autoencoders (SCAEs). SCAEs are used to learn high-level representations from image data, but their performance can be hampered by adversarial examples crafted specifically to fool them. In order to evaluate how robust these models are under evasion attacks, we generated a dataset of adversarial samples designed to attack the SCAE model. Our results show that although the SCAE performed better than traditional autoencoders on natural images, it was vulnerable to evasion attacks using small perturbations to input images. Furthermore, our experiments revealed that increasing the number of convolutional layers in the encoder did not improve resistance to evasion attacks, suggesting that alternative architectures may need to be explored. These findings have important implications for the use of SCAs as feature extractors in security applications such as malware detection where the presence of small perturbations cannot be ignored. Ultimately, developing more resilient capsule networks remains an open challenge, requiring both improvements to current architectures and new training methods that account for potential adversarial inputs.",1
"Face anti-spoofing approach based on domain generalization(DG) has drawn growing attention due to its robustness forunseen scenarios. Existing DG methods assume that the do-main label is known.However, in real-world applications, thecollected dataset always contains mixture domains, where thedomain label is unknown. In this case, most of existing meth-ods may not work. Further, even if we can obtain the domainlabel as existing methods, we think this is just a sub-optimalpartition. To overcome the limitation, we propose domain dy-namic adjustment meta-learning (D2AM) without using do-main labels, which iteratively divides mixture domains viadiscriminative domain representation and trains a generaliz-able face anti-spoofing with meta-learning. Specifically, wedesign a domain feature based on Instance Normalization(IN) and propose a domain representation learning module(DRLM) to extract discriminative domain features for cluster-ing. Moreover, to reduce the side effect of outliers on cluster-ing performance, we additionally utilize maximum mean dis-crepancy (MMD) to align the distribution of sample featuresto a prior distribution, which improves the reliability of clus tering. Extensive experiments show that the proposed methodoutperforms conventional DG-based face anti-spoofing meth-ods, including those utilizing domain labels. Furthermore, weenhance the interpretability through visualizatio",0
"This work presents generalizable representation learning method based on convolutional neural networks (CNNs) that leverages face detection results by region partitioned feature extraction to simultaneously learn domain invariant representations. We evaluated our approach against several state-of-the-art methods using three datasets and found competitive performance at detecting replay attacks while significantly reducing false acceptance rates for bonafide transactions. Our contributions include proposing effective solution to address intra class variations caused by diverse lightning conditions across different domains while preserving inter-class differences crucial for anti-spoofing tasks. In addition, we introduced novel domain agnostic pre-processing technique applicable to both labeled data as well as unlabeled domain specific dataset augmentations. Overall, our proposed framework outperforms other related methods in many cases while allowing greater flexibility through simple architecture modifications to meet project constraints. As such, these findings support further investigation into deployment of deep learning models on edge devices to improve biometric security. To validate effectiveness of transferring knowledge gained from source models, we conducted ablation study experimentation with promising results. Finally, considering potential applications in real world scenarios involving access control and secure mobile banking systems, future work includes expanding experiments beyond image based attacks and testing under more adversarial environments.",1
"Multi-object tracking (MOT) is an essential task in the computer vision field. With the fast development of deep learning technology in recent years, MOT has achieved great improvement. However, some challenges still remain, such as sensitiveness to occlusion, instability under different lighting conditions, non-robustness to deformable objects, etc. To address such common challenges in most of the existing trackers, in this paper, a tracklet booster algorithm is proposed, which can be built upon any other tracker. The motivation is simple and straightforward: split tracklets on potential ID-switch positions and then connect multiple tracklets into one if they are from the same object. In other words, the tracklet booster consists of two parts, i.e., Splitter and Connector. First, an architecture with stacked temporal dilated convolution blocks is employed for the splitting position prediction via label smoothing strategy with adaptive Gaussian kernels. Then, a multi-head self-attention based encoder is exploited for the tracklet embedding, which is further used to connect tracklets into larger groups. We conduct sufficient experiments on MOT17 and MOT20 benchmark datasets, which demonstrates promising results. Combined with the proposed tracklet booster, existing trackers usually can achieve large improvements on the IDF1 score, which shows the effectiveness of the proposed method.",0
"Increasingly high quality tracklets, defined as a sequence of frames containing the same object, are crucial to many computer vision applications such as video summarization, action recognition, human pose estimation, and video surveillance systems. However, achieving accurate and robust multi-object tracking at scale remains challenging due to occlusions, appearance changes, and drift that can cause track losses in long sequences. To address these issues we propose a new framework called Split and Connect (SnC) which uses multiple tracklet representations to improve overtime accuracy and continuity during tracking. Our SnC model learns to split tracklets into smaller sub-tracklets during periods of uncertainty using an uncertainty estimator module, and then uses those sub-tracklets along with global context to reconnect them later. This allows us to merge disconnected tracklets while maintaining local track coherency even after major occlusion events. Extensive experimental evaluations demonstrate the effectiveness of our approach on three benchmark datasets, outperforming state-of-the art methods in terms of online MOTA metric scores across all datasets and both public and private leaderboards.",1
"We present a novel method for synthesizing both temporally and geometrically consistent street-view panoramic video from a single satellite image and camera trajectory. Existing cross-view synthesis approaches focus on images, while video synthesis in such a case has not yet received enough attention. For geometrical and temporal consistency, our approach explicitly creates a 3D point cloud representation of the scene and maintains dense 3D-2D correspondences across frames that reflect the geometric scene configuration inferred from the satellite view. As for synthesis in the 3D space, we implement a cascaded network architecture with two hourglass modules to generate point-wise coarse and fine features from semantics and per-class latent vectors, followed by projection to frames and an upsampling module to obtain the final realistic video. By leveraging computed correspondences, the produced street-view video frames adhere to the 3D geometric scene structure and maintain temporal consistency. Qualitative and quantitative experiments demonstrate superior results compared to other state-of-the-art synthesis approaches that either lack temporal consistency or realistic appearance. To the best of our knowledge, our work is the first one to synthesize cross-view images to video.",0
"In Sat2Vid: Street-view Panoramic Video Synthesis from a Single Satellite Image we present our model which can synthesize street-level panoramic video by transforming single satellite images into realistically flowing videos that capture urban scenes at high resolutions. Our method creates seamless temporal coherence by blending consecutive frames into each other with smooth transitions resembling camera panning motions. We introduce novel techniques for incorporating dynamic content such as cars, people, and trees, simulating motion along complex trajectories. Additionally, we propose a new representation called space-time geometry graphs which explicitly captures spatiotemporal relationships among objects enabling more accurate scene understanding. Experimental evaluation on challenging datasets demonstrates significantly better visual fidelity over prior works, while outperforming them in user studies for both static image quality and dynamic fluidness. Finally, we provide ablation studies confirming design choices and utility of contributions.",1
"Determining which image regions to concentrate on is critical for Human-Object Interaction (HOI) detection. Conventional HOI detectors focus on either detected human and object pairs or pre-defined interaction locations, which limits learning of the effective features. In this paper, we reformulate HOI detection as an adaptive set prediction problem, with this novel formulation, we propose an Adaptive Set-based one-stage framework (AS-Net) with parallel instances and interaction branches. To attain this, we map a trainable interaction query set to an interaction prediction set with a transformer. Each query adaptively aggregates the interaction-relevant features from global contexts through multi-head co-attention. Besides, the training process is supervised adaptively by matching each ground truth with the interaction prediction. Furthermore, we design an effective instance-aware attention module to introduce instructive features from the instance branch into the interaction branch. Our method outperforms previous state-of-the-art methods without any extra human pose and language features on three challenging HOI detection datasets. Especially, we achieve over $31\%$ relative improvement on a large-scale HICO-DET dataset. Code is available at https://github.com/yoyomimi/AS-Net.",0
"This should read like a normal abstract. You need to summarize what you have done here. Abstract: This paper presents a new approach to visual relationship detection that reformulates human object interaction (HOI) prediction as adaptive set prediction. By leveraging recent advancements in neural network architectures and loss functions, our method learns to predict sets of candidate objects for each target object, allowing it to reason about the presence or absence of interacting objects in complex scenes. Experimental results show significant improvements over state-of-the-art methods on challenging benchmark datasets, demonstrating the effectiveness of our proposed approach. We believe this work represents a step forward towards more robust and interpretable HOI detection models.",1
"Image registration as an important basis in signal processing task often encounter the problem of stability and efficiency. Non-learning registration approaches rely on the optimization of the similarity metrics between the fix and moving images. Yet, those approaches are usually costly in both time and space complexity. The problem can be worse when the size of the image is large or the deformations between the images are severe. Recently, deep learning, or precisely saying, the convolutional neural network (CNN) based image registration methods have been widely investigated in the research community and show promising effectiveness to overcome the weakness of non-learning based methods. To explore the advanced learning approaches in image registration problem for solving practical issues, we present in this paper a method of introducing attention mechanism in deformable image registration problem. The proposed approach is based on learning the deformation field with a Transformer framework (AiR) that does not rely on the CNN but can be efficiently trained on GPGPU devices also. In a more vivid interpretation: we treat the image registration problem as the same as a language translation task and introducing a Transformer to tackle the problem. Our method learns an unsupervised generated deformation map and is tested on two benchmark datasets. The source code of the AiR will be released at Gitlab.",0
"Image registration is an important task that allows us to align multiple images into a common coordinate system. This can enable many downstream applications such as image fusion, panorama creation, structure from motion and many others. Unlike other works that use hand designed features or networks trained on supervised learning, our proposed Attention for Image Registration model uses only unlabeled data to learn representations. In this work we present a novel attention mechanism that enables efficient context propagation across large datasets without explicitly matching every pairwise feature. Our experiments show results competitive with state of the art methods but at a fraction of their computational cost. We further demonstrate the effectiveness of our method by achieving real time performance on an NVIDIA Jetson TX2 GPU while still matching the accuracy of previous methods that required much more powerful hardware. By using only unlabeled data and providing strong accuracy at near interactive speed we hope to democratize image registration making it accessible to users who may have been previously unable to leverage these techniques due to lack of labeled data availability. To summarise our method provides both efficiency, accuracy and applicability compared to current state of the arts methods.",1
"Estimating 3D scene flow from a sequence of monocular images has been gaining increased attention due to the simple, economical capture setup. Owing to the severe ill-posedness of the problem, the accuracy of current methods has been limited, especially that of efficient, real-time approaches. In this paper, we introduce a multi-frame monocular scene flow network based on self-supervised learning, improving the accuracy over previous networks while retaining real-time efficiency. Based on an advanced two-frame baseline with a split-decoder design, we propose (i) a multi-frame model using a triple frame input and convolutional LSTM connections, (ii) an occlusion-aware census loss for better accuracy, and (iii) a gradient detaching strategy to improve training stability. On the KITTI dataset, we observe state-of-the-art accuracy among monocular scene flow methods based on self-supervised learning.",0
"This paper presents a novel method called self-supervised multi-frame monocular scene flow (MFMF) which enables accurate estimation of ego motion, depth maps and dense correspondences between frames without using ground truth data at training time. MFMF uses two neural networks: one network predicts horizontal and vertical optical flows by minimizing the difference between predicted flow estimates from neighboring frames and another network that takes both the raw image and the estimated flow as input and outputs depth map. The system is trained on real video sequences where only camera poses of the first frame are provided but no other supervision. Experimental results show that our model can accurately estimate motion, depth and correspondence even on high resolution images. Our system outperforms previous methods that require extensive annotations during training time. We believe our work opens up new possibilities in computer vision research by providing a simple yet effective solution to the problem of scene flow prediction.",1
"Recent advances in the field of saliency have concentrated on fixation prediction, with benchmarks reaching saturation. However, there is an extensive body of works in psychology and neuroscience that describe aspects of human visual attention that might not be adequately captured by current approaches. Here, we investigate singleton detection, which can be thought of as a canonical example of salience. We introduce two novel datasets, one with psychophysical patterns and one with natural odd-one-out stimuli. Using these datasets we demonstrate through extensive experimentation that nearly all saliency algorithms do not adequately respond to singleton targets in synthetic and natural images. Furthermore, we investigate the effect of training state-of-the-art CNN-based saliency models on these types of stimuli and conclude that the additional training data does not lead to a significant improvement of their ability to find odd-one-out targets. Datasets are available at http://data.nvision2.eecs.yorku.ca/P3O3/.",0
"In summary, our results show that saliency models perform worse on odd-one-out tasks than image retrieval tasks. We present two new datasets containing 24,876 images each (a total of 49,752 images), which we use alongside four previously published datasets. Our evaluations cover both object detection and segmentation approaches across seven popular open-source saliency models; including BaTS-CNNMV, BMS-CNNMV++, CAM, DeepGazeII, DVA+Grad-CAM, LSOS, MTAA. Additionally, our study includes a user study where 10 participants were asked to mark anomalies in the dataset to validate whether humans can detect differences as accurately as models. We hope these findings inspire further research into improving current methods for identifying anomalies by creating more specialized models tailored towards odd-one-out scenarios.",1
"3D morphable models are widely used for the shape representation of an object class in computer vision and graphics applications. In this work, we focus on deep 3D morphable models that directly apply deep learning on 3D mesh data with a hierarchical structure to capture information at multiple scales. While great efforts have been made to design the convolution operator, how to best aggregate vertex features across hierarchical levels deserves further attention. In contrast to resorting to mesh decimation, we propose an attention based module to learn mapping matrices for better feature aggregation across hierarchical levels. Specifically, the mapping matrices are generated by a compatibility function of the keys and queries. The keys and queries are trainable variables, learned by optimizing the target objective, and shared by all data samples of the same object class. Our proposed module can be used as a train-only drop-in replacement for the feature aggregation in existing architectures for both downsampling and upsampling. Our experiments show that through the end-to-end training of the mapping matrices, we achieve state-of-the-art results on a variety of 3D shape datasets in comparison to existing morphable models.",0
"This paper presents a method for learning feature aggregation for deep 3D morphable models (3DMMs), which allows for efficient fitting to new images by selecting appropriate features from a set of precomputed features extracted from training data using classical computer vision techniques such as histograms of oriented gradients (HOG) and local binary patterns (LBP). Our approach enables the use of HOG and LBP features that have been learned separately on different sets of images, as well as feature aggregation based on task-specific criteria. We evaluate our method on two benchmark datasets for facial landmark detection, demonstrating improved performance over previous methods. Additionally, we show that our method can adapt more quickly than traditional feature extraction approaches, resulting in faster model fitting times. Our work has implications for broader applications of deep 3DMMs beyond facial analysis, including object recognition, tracking and pose estimation.",1
"We introduce a new architecture called a conditional invertible neural network (cINN), and use it to address the task of diverse image-to-image translation for natural images. This is not easily possible with existing INN models due to some fundamental limitations. The cINN combines the purely generative INN model with an unconstrained feed-forward network, which efficiently preprocesses the conditioning image into maximally informative features. All parameters of a cINN are jointly optimized with a stable, maximum likelihood-based training procedure. Even though INN-based models have received far less attention in the literature than GANs, they have been shown to have some remarkable properties absent in GANs, e.g. apparent immunity to mode collapse. We find that our cINNs leverage these properties for image-to-image translation, demonstrated on day to night translation and image colorization. Furthermore, we take advantage of our bidirectional cINN architecture to explore and manipulate emergent properties of the latent space, such as changing the image style in an intuitive way.",0
"We present a new approach for image-to-image translation that combines conditional GAN models with invertibility constraints. Our model uses an encoder network to map input images into latent spaces, which can then be transformed using a novel conditioning mechanism based on textual descriptions. These transformations are applied at each layer of the generator network, ensuring smooth transitions across different levels of abstraction.  To enable accurate control over the generated output, we introduce a regularization term based on cycle consistency. This allows our model to learn intrinsic symmetries within input domains, leading to more stable and consistent results compared to previous methods. Furthermore, we demonstrate that our method outperforms state-of-the-art baselines under various metrics including qualitative evaluations.  We believe this work represents a significant step forward in image-to-image translation research and opens up exciting possibilities for future applications such as style transfer, superresolution, and domain adaptation. Overall, we hope this paper provides a valuable contribution to the broader field of computer vision and generative modeling.",1
"Learning feature representation from discriminative local regions plays a key role in fine-grained visual classification. Employing attention mechanisms to extract part features has become a trend. However, there are two major limitations in these methods: First, they often focus on the most salient part while neglecting other inconspicuous but distinguishable parts. Second, they treat different part features in isolation while neglecting their relationships. To handle these limitations, we propose to locate multiple different distinguishable parts and explore their relationships in an explicit way. In this pursuit, we introduce two lightweight modules that can be easily plugged into existing convolutional neural networks. On one hand, we introduce a feature boosting and suppression module that boosts the most salient part of feature maps to obtain a part-specific representation and suppresses it to force the following network to mine other potential parts. On the other hand, we introduce a feature diversification module that learns semantically complementary information from the correlated part-specific representations. Our method does not need bounding boxes/part annotations and can be trained end-to-end. Extensive experimental results show that our method achieves state-of-the-art performances on several benchmark fine-grained datasets. Source code is available at https://github.com/chaomaer/FBSD.",0
This would make a great resource for anyone interested in how artificial intelligence can improve our lives and solve problems that we previously could only imagine solving. Can you please describe some other examples of such use cases? Iâ€™m particularly interested in healthcare but any other sector where AI has been applied successfully would be interesting too. Thanks!,1
"Referring Expression Comprehension (REC) has become one of the most important tasks in visual reasoning, since it is an essential step for many vision-and-language tasks such as visual question answering. However, it has not been widely used in many downstream tasks because it suffers 1) two-stage methods exist heavy computation cost and inevitable error accumulation, and 2) one-stage methods have to depend on lots of hyper-parameters (such as anchors) to generate bounding box. In this paper, we present a proposal-free one-stage (PFOS) model that is able to regress the region-of-interest from the image, based on a textual query, in an end-to-end manner. Instead of using the dominant anchor proposal fashion, we directly take the dense-grid of an image as input for a cross-attention transformer that learns grid-word correspondences. The final bounding box is predicted directly from the image without the time-consuming anchor selection process that previous methods suffer. Our model achieves the state-of-the-art performance on four referring expression datasets with higher efficiency, comparing to previous best one-stage and two-stage methods.",0
"In recent years, referential expression generation has been a topic of interest in natural language processing research. Existing approaches typically involve generating proposals followed by reranking based on complex metrics. However, these methods can suffer from slow inference speed due to proposal search overhead. To address this limitation, we propose a novel one-stage method that directly generates referring expressions without any intermediate proposals. Our approach leverages grid attention modules to integrate contextual features such as object categories and locations into textual descriptions. Experiments show significant improvement over state-of-the-art proposal-based models in both accuracy and efficiency while maintaining competitive performance. This work represents an important step towards real-time referral expression generation for various applications in artificial intelligence.",1
"Contemporary data-driven methods are typically fed with full supervision on large-scale datasets which limits their applicability. However, in the actual systems with limitations such as measurement error and data acquisition problems, people usually obtain incomplete data. Although data completion has attracted wide attention, the underlying data pattern and relativity are still under-developed. Currently, the family of latent variable models allows learning deep latent variables over observed variables by fitting the marginal distribution. As far as we know, current methods fail to perceive the data relativity under partial observation. Aiming at modeling incomplete data, this work uses relational inference to fill in the incomplete data. Specifically, we expect to approximate the real joint distribution over the partial observation and latent variables, thus infer the unseen targets respectively. To this end, we propose Omni-Relational Network (OR-Net) to model the pointwise relativity in two aspects: (i) On one hand, the inner relationship is built among the context points in the partial observation; (ii) On the other hand, the unseen targets are inferred by learning the cross-relationship with the observed data points. It is further discovered that the proposed method can be generalized to different scenarios regardless of whether the physical structure can be observed or not. It is demonstrated that the proposed OR-Net can be well generalized for data completion tasks of various modalities, including function regression, image completion on MNIST and CelebA datasets, and also sequential motion generation conditioned on the observed poses.",0
"This paper presents a new method called OR-Net (Pointwise Relational Inference) that allows us to infer missing data points based on partial observation by reasoning over complex relations between variables. Our approach uses deep neural networks that take noisy input data as input and output complete representations of the underlying entities, events, or objects. We demonstrate how our method can be applied to several real world applications such as image completion, video prediction, question answering and knowledge graph completion. Experimental results show that our model significantly outperforms state-of-the-art baselines across all tasks. Finally we discuss future directions of research for extending relational inference from pointwise predictions to incorporating structural prior constraints.",1
"Detecting unknown and untested scenarios is crucial for scenario-based testing. Scenario-based testing is considered to be a possible approach to validate autonomous vehicles. A traffic scenario consists of multiple components, with infrastructure being one of it. In this work, a method to detect novel traffic scenarios based on their infrastructure images is presented. An autoencoder triplet network provides latent representations for infrastructure images which are used for outlier detection. The triplet training of the network is based on the connectivity graphs of the infrastructure. By using the proposed architecture, expert-knowledge is used to shape the latent space such that it incorporates a pre-defined similarity in the neighborhood relationships of an autoencoder. An ablation study on the architecture is highlighting the importance of the triplet autoencoder combination. The best performing architecture is based on vision transformers, a convolution-free attention-based network. The presented method outperforms other state-of-the-art outlier detection approaches.",0
"This project explores novelty detection (ND) through analyzing traffic scenario infrastructure using triplet autoencoders (AE). An AE can learn latent representations by minimizing reconstruction error; ND methods identify anomalies via distance functions. The authors leverage recent advancements in vision transformer architectures that exhibit strong performance on image datasets while preserving spatial relationships among pixels. They first preprocess raw data images into feature maps then feed them into the AE to obtain encoded vectors. To validate their approach, they contrast two ND formulations: AAE+NN and SAD. Their results indicate that SAD performs better overall but both achieve low reconstruction errors with high anomaly detection accuracy. Future work includes applying their methodology to other sensor modalities and real-world applications such as autonomous driving systems.",1
"Weight sharing, as an approach to speed up architecture performance estimation has received wide attention. Instead of training each architecture separately, weight sharing builds a supernet that assembles all the architectures as its submodels. However, there has been debate over whether the NAS process actually benefits from weight sharing, due to the gap between supernet optimization and the objective of NAS. To further understand the effect of weight sharing on NAS, we conduct a comprehensive analysis on five search spaces, including NAS-Bench-101, NAS-Bench-201, DARTS-CIFAR10, DARTS-PTB, and ProxylessNAS. We find that weight sharing works well on some search spaces but fails on others. Taking a step forward, we further identified biases accounting for such phenomenon and the capacity of weight sharing. Our work is expected to inspire future NAS researchers to better leverage the power of weight sharing.",0
"Artificial neural networks have revolutionized many application areas and researchers have proposed numerous architectures tailored for specific problems. However, designing such custom architectures is challenging as it requires expertise both in computer science (e.g., deep learning) and domain knowledge (e.g., computer vision). This work introduces novel supernets which automatically search among different cell components and discover an efficient architecture that outperforms several SOTA models on image classification benchmark datasets. Furthermore, our method scales better than previous methods by allowing cells from diverse backbones including MobileNetV2 and ResNeSt. Finally we present analysis showing that performance can often be boosted even further through post training quantization. Our results provide guidance towards making neural network search more automated while still achieving state-of-the art accuracy. Full source code for reproducing all experiments is available at https://github.com/facebookresearch/superformers . All models used in this study are available at: https://huggingface.co/models?filter=vision&repo=&sort=-stars&page=1 . We report new SOTA results on CIFAR-10, ImageNet and a subset of VGGFace2 using an open-source model zoo framework.",1
"Recently, referring image segmentation has aroused widespread interest. Previous methods perform the multi-modal fusion between language and vision at the decoding side of the network. And, linguistic feature interacts with visual feature of each scale separately, which ignores the continuous guidance of language to multi-scale visual features. In this work, we propose an encoder fusion network (EFN), which transforms the visual encoder into a multi-modal feature learning network, and uses language to refine the multi-modal features progressively. Moreover, a co-attention mechanism is embedded in the EFN to realize the parallel update of multi-modal features, which can promote the consistent of the cross-modal information representation in the semantic space. Finally, we propose a boundary enhancement module (BEM) to make the network pay more attention to the fine structure. The experiment results on four benchmark datasets demonstrate that the proposed approach achieves the state-of-the-art performance under different evaluation metrics without any post-processing.",0
"In our research, we proposed an innovative model for referring image segmentation by combining two advanced techniques: encoder fusion network (EFN) and co-attention embedding (CE). We focused on improving semantic features representation with EFN and enhancing spatial correspondence between image regions using CE. Our approach effectively utilized these features to localize referred objects in images accurately and efficiently. Our experiments showed that our method outperformed state-of-the-art models on benchmark datasets, demonstrating the effectiveness of our proposed approach. Overall, our work makes significant contributions to advancing image understanding capabilities by tackling complex tasks like referring image segmentation through novel deep learning architectures. This study has great potential applications in many real-world scenarios where precise object detection and segmentation are essential such as self-driving cars, robotics, and medical diagnosis.",1
"Underwater image enhancement is an important low-level computer vision task for autonomous underwater vehicles and remotely operated vehicles to explore and understand the underwater environments. Recently, deep convolutional neural networks (CNNs) have been successfully used in many computer vision problems, and so does underwater image enhancement. There are many deep-learning-based methods with impressive performance for underwater image enhancement, but their memory and model parameter costs are hindrances in practical application. To address this issue, we propose a lightweight adaptive feature fusion network (LAFFNet). The model is the encoder-decoder model with multiple adaptive feature fusion (AAF) modules. AAF subsumes multiple branches with different kernel sizes to generate multi-scale feature maps. Furthermore, channel attention is used to merge these feature maps adaptively. Our method reduces the number of parameters from 2.5M to 0.15M (around 94% reduction) but outperforms state-of-the-art algorithms by extensive experiments. Furthermore, we demonstrate our LAFFNet effectively improves high-level vision tasks like salience object detection and single image depth estimation.",0
"This research proposes a new method for enhancing underwater images using a lightweight adaptive feature fusion network called ""LAFFNet."" Traditionally, underwater image enhancement has been a challenging task due to the degradation caused by the water medium, which can result in low visibility, color shifting, and loss of detail. To address these issues, we have developed a deep learning model that combines multiple features from different layers into a single output, resulting in better performance compared to existing methods. Our model uses minimal computational resources while still achieving state-of-the-art results on various benchmark datasets. In summary, our proposed method demonstrates effectiveness in improving visual quality under harsh underwater environments, paving the way towards further advancements in marine exploration and monitoring applications.",1
"Depth guided any-to-any image relighting aims to generate a relit image from the original image and corresponding depth maps to match the illumination setting of the given guided image and its depth map. To the best of our knowledge, this task is a new challenge that has not been addressed in the previous literature. To address this issue, we propose a deep learning-based neural Single Stream Structure network called S3Net for depth guided image relighting. This network is an encoder-decoder model. We concatenate all images and corresponding depth maps as the input and feed them into the model. The decoder part contains the attention module and the enhanced module to focus on the relighting-related regions in the guided images. Experiments performed on challenging benchmark show that the proposed model achieves the 3 rd highest SSIM in the NTIRE 2021 Depth Guided Any-to-any Relighting Challenge.",0
"Incorporate relevant keywords such as depth guided image relighting, 2D image processing, light estimation, computer graphics into the body and conclusion sections. Use complete sentences instead of phrases or bullets. Avoid using unnecessary jargon terms. Cite at least one other research paper related to your topic if possible. --- ABSTRACT: Despite recent advances in depth guided image relighting techniques, these methods suffer from several limitations. Traditional single stream depth guided image relighting algorithms rely on a fixed pre-computed scene mesh which limits their ability to handle complex scenes accurately. Multi-stream techniques improve upon this by separately estimating structure geometry, surface normals and albedo or lighting parameters but require multiple inputs and increase computational cost. Furthermore, current state-of-the-art methods often make simplifying assumptions regarding light estimation, resulting in suboptimal results. This work presents S3Net, a novel single stream structure that overcomes the challenges faced by previous approaches by explicitly predicting both geometry and texture in a unified deep network architecture. Our model estimates accurate surface normals, diffuse albedos and shading irradiance values directly from input images and corresponding depth maps. We achieve this through adaptive sampling of features across different levels of detail within our proposed differentiable rendering layer. Moreover, we leverage physically grounded priors like smoothness regularization during training to mitigate issues associated with self-occlusion. Experimental evaluation on standard benchmark datasets demonstrates significant improvements in visual fidelity compared to existing single stream models while matching or outperforming multi-stream counterparts in quantitative metrics. Our method enables realti",1
"In this work, we propose TransTrack, a simple but efficient scheme to solve the multiple object tracking problems. TransTrack leverages the transformer architecture, which is an attention-based query-key mechanism. It applies object features from the previous frame as a query of the current frame and introduces a set of learned object queries to enable detecting new-coming objects. It builds up a novel joint-detection-and-tracking paradigm by accomplishing object detection and object association in a single shot, simplifying complicated multi-step settings in tracking-by-detection methods. On MOT17 and MOT20 benchmark, TransTrack achieves 74.5\% and 64.5\% MOTA, respectively, competitive to the state-of-the-art methods. We expect TransTrack to provide a novel perspective for multiple object tracking. The code is available at: \url{https://github.com/PeizeSun/TransTrack}.",0
"In recent years, multiple object tracking (MOT) has become a critical component in many computer vision applications such as autonomous driving, surveillance, and robotics. Traditional MOT algorithms rely on handcrafted features and heuristics, which can lead to suboptimal performance and limited scalability. Recently, deep learning methods have been used to improve the accuracy and robustness of MOT by using end-to-end trainable models. One popular architecture for these tasks is the transformer network. In this work, we propose TransTrack, a new MOT algorithm based on transformers that achieves state-of-the art results on two public datasets. Our approach is composed of three main components: a bounding box predictor, an association module, and a track management module. Each of these modules uses a variant of the transformer architecture and interacts with each other through shared representations. We evaluate our model against several baselines and show that our method outperforms them across all metrics. Our code and trained models will be made available online after acceptance. -----",1
"Explaining the decision of a multi-modal decision-maker requires to determine the evidence from both modalities. Recent advances in XAI provide explanations for models trained on still images. However, when it comes to modeling multiple sensory modalities in a dynamic world, it remains underexplored how to demystify the mysterious dynamics of a complex multi-modal model. In this work, we take a crucial step forward and explore learnable explanations for audio-visual recognition. Specifically, we propose a novel space-time attention network that uncovers the synergistic dynamics of audio and visual data over both space and time. Our model is capable of predicting the audio-visual video events, while justifying its decision by localizing where the relevant visual cues appear, and when the predicted sounds occur in videos. We benchmark our model on three audio-visual video event datasets, comparing extensively to multiple recent multi-modal representation learners and intrinsic explanation models. Experimental results demonstrate the clear superior performance of our model over the existing methods on audio-visual video event recognition. Moreover, we conduct an in-depth study to analyze the explainability of our model based on robustness analysis via perturbation tests and pointing games using human annotations.",0
"This should be a standalone summary that can provide insight into your work without reference to the full text. Please see our guidelines on writing good abstracts here.  Paper Title: Where and When: Space-Time Attention for Audio-Visual Explanations  Abstract: We present a novel deep learning method for audio-visual explanations. Our model addresses two key challenges faced by previous systems: understanding the spatiotemporal dependencies among visual entities and effectively utilizing both audio and visual inputs. We introduce space-time attention modules designed specifically to capture interdependencies across modalities at different spatial locations and time points. Experiments demonstrate significant improvements over baseline methods in terms of both quantitative metrics and human evaluations. We believe that our approach could have important applications in areas such as multimedia analysis, computer vision, and natural language processing.",1
"Graph convolution networks have recently garnered a lot of attention for representation learning on non-Euclidean feature spaces. Recent research has focused on stacking multiple layers like in convolutional neural networks for the increased expressive power of graph convolution networks. However, simply stacking multiple graph convolution layers lead to issues like vanishing gradient, over-fitting and over-smoothing. Such problems are much less when using shallower networks, even though the shallow networks have lower expressive power. In this work, we propose a novel Multipath Graph convolutional neural network that aggregates the output of multiple different shallow networks. We train and test our model on various benchmarks datasets for the task of node property prediction. Results show that the proposed method not only attains increased test accuracy but also requires fewer training epochs to converge. The full implementation is available at https://github.com/rangan2510/MultiPathGCN",0
"Introduction: Graph convolutional neural networks (GCNN) have become increasingly popular due to their ability to process irregular data structures such as graphs, which arise naturally in fields ranging from computer vision to natural language processing. However, most existing GCNN architectures suffer from limited model expressiveness, leading to suboptimal performance on complex tasks that require hierarchical feature learning. In this work, we propose multipath graph convolutional neural networks (MPGNN), a new architecture that leverages multiple graph message passing paths to enable richer representations, improved accuracy, and better scalability compared to state-of-the-art methods.  Methodology: To build MPGNN, we extend traditional GCN layers by introducing a fixed number of parallel branches, each featuring different kernel sizes and/or nonlinear activation functions. This enables our network to effectively learn more robust features at varying levels of abstraction within the hierarchy of nodes in the input graph. By contrasting our results against other strong baselines, we demonstrate that incorporating multiple message passing paths can significantly improve predictive accuracy and make better use of available computational resources. Furthermore, we provide insights into why the proposed architecture works so well through visualization experiments designed to shed light on learned representations.  Conclusion: Our contributions lie at the intersection of deep learning and graph representation learning. We believe that our MPGNN model sets a promising research direction towards making GCN models more powerful and generalizable across diverse domains. Our findings could inspire future efforts seeking to address challenges in knowledge graph completion, image generation, natural language understanding, and many other areas where graph structure plays a fundamental role.",1
"Re-ranking utilizes contextual information to optimize the initial ranking list of person or vehicle re-identification (re-ID), which boosts the retrieval performance at post-processing steps. This paper proposes a re-ranking network to predict the correlations between the probe and top-ranked neighbor samples. Specifically, all the feature embeddings of query and gallery images are expanded and enhanced by a linear combination of their neighbors, with the correlation prediction serves as discriminative combination weights. The combination process is equivalent to moving independent embeddings toward the identity centers, improving cluster compactness. For correlation prediction, we first aggregate the contextual information for probe's k-nearest neighbors via the Transformer encoder. Then, we distill and refine the probe-related features into the Contextual Memory cell via attention mechanism. Like humans that retrieve images by not only considering probe images but also memorizing the retrieved ones, the Contextual Memory produces multi-view descriptions for each instance. Finally, the neighbors are reconstructed with features fetched from the Contextual Memory, and a binary classifier predicts their correlations with the probe. Experiments on six widely-used person and vehicle re-ID benchmarks demonstrate the effectiveness of the proposed method. Especially, our method surpasses the state-of-the-art re-ranking approaches on large-scale datasets by a significant margin, i.e., with an average 3.08% CMC@1 and 7.46% mAP improvements on VERI-Wild, MSMT17, and VehicleID datasets.",0
"In order to achieve high performance on image re-identification tasks, many researchers have turned to convolutional neural networks (CNNs) that rely on attention mechanisms and memory modules. Recent work has shown that these models can effectively learn representations of human identities by attending to discriminative regions and memorizing key features across images. However, existing approaches still face challenges due to limitations in computational efficiency and scalability, as well as limited effectiveness when dealing with datasets containing large variations in pose, illumination, and occlusion. In this paper, we propose a novel approach called ""Moving Towards Centers"" (MTCTransformer), which builds upon previous works that utilize transformer architectures for fine-grained feature matching. By incorporating a self-attention module along with episodic memory techniques, our model is able to efficiently explore and identify subtle differences among distractor images while maintaining robustness against varying conditions encountered in real-world applications. We conduct extensive experiments on six benchmark datasets spanning diverse domains, such as street surveillance, airport security scanners, and fashion item retrieval. Our MTCTransformer significantly outperforms state-of-the-art baselines, achieving new record accuracies on several of the most widely used datasets. Furthermore, ablation studies demonstrate the importance of each component in our design and validate the advantages of our multi-scale architecture for improving generalization under varying scales. We hope that our work provides valuable insights into future developments aimed at enhancing the capabilities of visual recognition systems beyond current limitations. =====",1
"Fairness-aware machine learning for multiple protected at-tributes (referred to as multi-fairness hereafter) is receiving increasing attention as traditional single-protected attribute approaches cannot en-sure fairness w.r.t. other protected attributes. Existing methods, how-ever, still ignore the fact that datasets in this domain are often imbalanced, leading to unfair decisions towards the minority class. Thus, solutions are needed that achieve multi-fairness,accurate predictive performance in overall, and balanced performance across the different classes.To this end, we introduce a new fairness notion,Multi-Max Mistreatment(MMM), which measures unfairness while considering both (multi-attribute) protected group and class membership of instances. To learn an MMM-fair classifier, we propose a multi-objective problem formulation. We solve the problem using a boosting approach that in-training,incorporates multi-fairness treatment in the distribution update and post-training, finds multiple Pareto-optimal solutions; then uses pseudo-weight based decision making to select optimal solution(s) among accurate, balanced, and multi-attribute fair solutions",0
"Multi-fair optimization has recently become a topic of interest due to its ability to balance multiple objectives simultaneously. However, traditional fairness criteria often prioritize one objective over another, leading to suboptimal results. In order to address this issue, we propose multi-fair Pareto boosting (MPB), a novel approach that utilizes Pareto fronts to identify a set of non-dominated solutions that satisfy all objectives simultaneously. Our approach allows us to solve problems involving multiple conflicting objectives while ensuring fairness across different aspects of the problem. We demonstrate the effectiveness of MPB through experiments on a range of problems including machine learning, engineering design, and game theory. Compared to existing methods, our framework achieves better overall performance while maintaining high levels of fairness. This work extends the state-of-the-art by providing a flexible and efficient method for solving complex multi-objective problems with competing fairness constraints.",1
"Federated learning (FL) is a distributed machine learning architecture that leverages a large number of workers to jointly learn a model with decentralized data. FL has received increasing attention in recent years thanks to its data privacy protection, communication efficiency and a linear speedup for convergence in training (i.e., convergence performance increases linearly with respect to the number of workers). However, existing studies on linear speedup for convergence are only limited to the assumptions of i.i.d. datasets across workers and/or full worker participation, both of which rarely hold in practice. So far, it remains an open question whether or not the linear speedup for convergence is achievable under non-i.i.d. datasets with partial worker participation in FL. In this paper, we show that the answer is affirmative. Specifically, we show that the federated averaging (FedAvg) algorithm (with two-sided learning rates) on non-i.i.d. datasets in non-convex settings achieves a convergence rate $\mathcal{O}(\frac{1}{\sqrt{mKT}} + \frac{1}{T})$ for full worker participation and a convergence rate $\mathcal{O}(\frac{\sqrt{K}}{\sqrt{nT}} + \frac{1}{T})$ for partial worker participation, where $K$ is the number of local steps, $T$ is the number of total communication rounds, $m$ is the total worker number and $n$ is the worker number in one communication round if for partial worker participation. Our results also reveal that the local steps in FL could help the convergence and show that the maximum number of local steps can be improved to $T/m$ in full worker participation. We conduct extensive experiments on MNIST and CIFAR-10 to verify our theoretical results.",0
"In order for federated learning (FL) to effectively train machine learning models, it must strike a balance between model accuracy and scalability across multiple devices. However, non-independent identically distributed (non-IID) data distributions among participating worker devices can cause significant heterogeneity that impacts training stability and results in suboptimal performance. This study examines techniques for achieving linear speedup with partial participation of workers within FL. Specifically, we investigate how fine-grained control over which subsets of local datasets are used by each device can improve the convergence rate of the global model. Our approach prioritizes selecting subset sizes that minimize communication costs while maintaining high levels of diversity in data distribution. Experimental evaluation demonstrates that our method outperforms standard techniques, achieving significantly faster training times without sacrificing model accuracy. Our work contributes new insights into effective distributed learning strategies for non-IID environments, helping bridge the gap between research challenges in FL and real-world applications on edge computing platforms and mobile networks. Keywords: Federated Learning, Distributed Computing, Machine Learning, Edge Computing, Mobile Networks",1
"In this paper, we propose a novel framework for multi-target multi-camera tracking (MTMCT) of vehicles based on metadata-aided re-identification (MA-ReID) and the trajectory-based camera link model (TCLM). Given a video sequence and the corresponding frame-by-frame vehicle detections, we first address the isolated tracklets issue from single camera tracking (SCT) by the proposed traffic-aware single-camera tracking (TSCT). Then, after automatically constructing the TCLM, we solve MTMCT by the MA-ReID. The TCLM is generated from camera topological configuration to obtain the spatial and temporal information to improve the performance of MTMCT by reducing the candidate search of ReID. We also use the temporal attention model to create more discriminative embeddings of trajectories from each camera to achieve robust distance measures for vehicle ReID. Moreover, we train a metadata classifier for MTMCT to obtain the metadata feature, which is concatenated with the temporal attention based embeddings. Finally, the TCLM and hierarchical clustering are jointly applied for global ID assignment. The proposed method is evaluated on the CityFlow dataset, achieving IDF1 76.77%, which outperforms the state-of-the-art MTMCT methods.",0
"This paper presents a novel method for tracking vehicles across multiple cameras using metadata-aided re-identification and trajectory-based camera link modeling. Our approach utilizes vehicle descriptions and contextual metadata such as license plate numbers and color to accurately identify and track vehicles through multiple cameras. We employ a two-stage approach that first performs joint detection and classification on each individual camera stream before performing cross-camera linking based on both spatial-temporal correspondences and learned associations from metadata. By incorporating metadata into our system, we achieve improved accuracy over traditional methods which rely solely on visual features. Experimental results demonstrate the effectiveness of our proposed method in challenging scenarios, including occlusions, crowded scenes and camera viewpoint changes. Additionally, we provide insights and analysis of key factors impacting multi-target, multi-camera tracking performance. Overall, our work advances state-of-the-art in automated surveillance systems by enabling more accurate and robust tracking capabilities.",1
"Factorized layers--operations parameterized by products of two or more matrices--occur in a variety of deep learning contexts, including compressed model training, certain types of knowledge distillation, and multi-head self-attention architectures. We study how to initialize and regularize deep nets containing such layers, examining two simple, understudied schemes, spectral initialization and Frobenius decay, for improving their performance. The guiding insight is to design optimization routines for these networks that are as close as possible to that of their well-tuned, non-decomposed counterparts; we back this intuition with an analysis of how the initialization and regularization schemes impact training with gradient descent, drawing on modern attempts to understand the interplay of weight-decay and batch-normalization. Empirically, we highlight the benefits of spectral initialization and Frobenius decay across a variety of settings. In model compression, we show that they enable low-rank methods to significantly outperform both unstructured sparsity and tensor methods on the task of training low-memory residual networks; analogs of the schemes also improve the performance of tensor decomposition techniques. For knowledge distillation, Frobenius decay enables a simple, overcomplete baseline that yields a compact model from over-parameterized training without requiring retraining with or pruning a teacher network. Finally, we show how both schemes applied to multi-head attention lead to improved performance on both translation and unsupervised pre-training.",0
"Title: Optimizing Deep Learning Architectures through Initialization Techniques --------------------------------------------------------------------------  This paper addresses the challenges associated with training deep neural networks (DNNs), specifically focusing on initialization techniques for factorized layers. DNNs have proven highly effective in many domains but suffer from issues such as vanishing gradients, which lead to difficulties in optimization during backpropagation. Traditional weight initializations often struggle to cope with these problems, leading to suboptimal network performance and slow convergence rates.  To address these issues, we propose several novel approaches that leverage recent advances in random matrix theory and eigenvalue analysis. Our methods aim to improve the quality of initialization by reducing the impact of singular values in early epochs, allowing better gradient propagation throughout the model. We evaluate our proposals using comprehensive experiments across multiple datasets and models.  Our results show significant improvements in terms of converged accuracy, faster optimization times, and more stable learning curves compared to existing methods. Additionally, our techniques can work alongside regularizers commonly used in DNNs without losing effectiveness, further enhancing the benefits they provide. These findings highlight the potential advantages of incorporating tailored layer initializations into modern deep learning architectures.  In summary, this research demonstrates how a careful consideration of layer initialization in DNNs can substantially boost their overall performance, resulting in improved generalization capabilities and reduced computational costs during training. Future work may explore applying similar concepts to other types of architectures and considering alternative constraints on random matrices to push performance even farther.",1
"Hypergraph, an expressive structure with flexibility to model the higher-order correlations among entities, has recently attracted increasing attention from various research domains. Despite the success of Graph Neural Networks (GNNs) for graph representation learning, how to adapt the powerful GNN-variants directly into hypergraphs remains a challenging problem. In this paper, we propose UniGNN, a unified framework for interpreting the message passing process in graph and hypergraph neural networks, which can generalize general GNN models into hypergraphs. In this framework, meticulously-designed architectures aiming to deepen GNNs can also be incorporated into hypergraphs with the least effort. Extensive experiments have been conducted to demonstrate the effectiveness of UniGNN on multiple real-world datasets, which outperform the state-of-the-art approaches with a large margin. Especially for the DBLP dataset, we increase the accuracy from 77.4\% to 88.8\% in the semi-supervised hypernode classification task. We further prove that the proposed message-passing based UniGNN models are at most as powerful as the 1-dimensional Generalized Weisfeiler-Leman (1-GWL) algorithm in terms of distinguishing non-isomorphic hypergraphs. Our code is available at \url{https://github.com/OneForward/UniGNN}.",0
"A unified framework for graph and hypergraph neural networks (UniGNN) can improve the accuracy and efficiency of deep learning models by handling both types of data structures in a single framework. In many real-world applications, graphs and hypergraphs occur naturally, but existing approaches often require specialized treatment for each type of structure, leading to duplicated efforts and potential inconsistencies. UniGNN addresses these issues by providing a simple yet effective architecture that is capable of processing both types of data simultaneously while achieving state-of-the-art performance on several benchmark datasets. Our approach uses novel mixing layers to perform computations over multiple graphs/hypergraphs, enabling efficient message passing and capturing rich relationships among nodes. Experimental results demonstrate that UniGNN significantly outperforms other popular methods across various tasks and architectures, making it an attractive choice for numerous application domains such as social network analysis, bioinformatics, and computer vision. This work contributes to advancing the field of graph and hypergraph neural networks by offering a versatile solution applicable to diverse use cases.",1
"Class Activation Mapping (CAM) is a powerful technique used to understand the decision making of Convolutional Neural Network (CNN) in computer vision. Recently, there have been attempts not only to generate better visual explanations, but also to improve classification performance using visual explanations. However, the previous works still have their own drawbacks. In this paper, we propose a novel architecture, LFI-CAM, which is trainable for image classification and visual explanation in an end-to-end manner. LFI-CAM generates an attention map for visual explanation during forward propagation, at the same time, leverages the attention map to improve the classification performance through the attention mechanism. Our Feature Importance Network (FIN) focuses on learning the feature importance instead of directly learning the attention map to obtain a more reliable and consistent attention map. We confirmed that LFI-CAM model is optimized not only by learning the feature importance but also by enhancing the backbone feature representation to focus more on important features of the input image. Experimental results show that LFI-CAM outperforms the baseline models's accuracy on the classification tasks as well as significantly improves on the previous works in terms of attention map quality and stability over different hyper-parameters.",0
"This paper presents LFI-CAM (Learning Feature Importance for Better Visual Explanation), a novel method that uses deep learning techniques to improve the visual explanation capabilities of image classification models. In particular, our approach leverages saliency maps to identify important features in input images that contribute to correct predictions made by convolutional neural networks (CNNs). These feature importance values can then be used to produce more informative explanations that better highlight regions critical for accurate classification. We show through extensive experiments on various benchmark datasets that our method significantly outperforms state-of-the-art methods in terms of both quantitative metrics and subjective user studies. Our work takes a step forward towards creating reliable and trustworthy machine learning systems, especially in safety-critical applications where transparency into decision making is crucial. Overall, we believe that LFI-CAM could serve as a valuable tool for researchers and practitioners who seek to enhance their understanding of how CNNs make decisions and facilitate communication between human users and artificial intelligence algorithms.",1
"Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field.",0
"In this survey paper we aim at reviewing graph learning methods that can perform node classification by employing large amounts of data characterizing both graphsâ€™ topology (e.g., nodesâ€™ degree centrality) and their content (e.g., text corpora associated to them). We discuss different approaches addressing either supervised or semi-supervised problems and classify them based on the type of input features they consider. Afterwards, we analyze state-of-the art graph classification benchmark datasets (i.e. Cora, Citeseer and Pubmed) comparing accuracy scores obtained from several techniques discussed along our work. Finally, current challenges and future research directions towards enhancing graph learning methodologiesâ€™ performance are depicted in this study.",1
"Sign language is commonly used by deaf or speech impaired people to communicate but requires significant effort to master. Sign Language Recognition (SLR) aims to bridge the gap between sign language users and others by recognizing signs from given videos. It is an essential yet challenging task since sign language is performed with the fast and complex movement of hand gestures, body posture, and even facial expressions. Recently, skeleton-based action recognition attracts increasing attention due to the independence between the subject and background variation. However, skeleton-based SLR is still under exploration due to the lack of annotations on hand keypoints. Some efforts have been made to use hand detectors with pose estimators to extract hand key points and learn to recognize sign language via Neural Networks, but none of them outperforms RGB-based methods. To this end, we propose a novel Skeleton Aware Multi-modal SLR framework (SAM-SLR) to take advantage of multi-modal information towards a higher recognition rate. Specifically, we propose a Sign Language Graph Convolution Network (SL-GCN) to model the embedded dynamics and a novel Separable Spatial-Temporal Convolution Network (SSTCN) to exploit skeleton features. RGB and depth modalities are also incorporated and assembled into our framework to provide global information that is complementary to the skeleton-based methods SL-GCN and SSTCN. As a result, SAM-SLR achieves the highest performance in both RGB (98.42\%) and RGB-D (98.53\%) tracks in 2021 Looking at People Large Scale Signer Independent Isolated SLR Challenge. Our code is available at https://github.com/jackyjsy/CVPR21Chal-SLR",0
"Title: Skeleton Aware Multi-Modal Sign Language Recognition Abstract This paper presents a new method for recognizing sign language using multi-modal input from both skeletal tracking data and video footage. We propose a novel architecture that combines multiple modalities of information to improve recognition accuracy. Our approach utilizes convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to process the visual features extracted from the video feed as well as the joint positions from the skeletal tracking data. Experimental results demonstrate significant improvement in accuracy compared to baseline models that use only one modality at a time. In addition, our model achieves competitive performance on benchmark datasets while operating in real-time. Overall, our proposed framework paves the way for more robust sign language recognition systems that can adapt to varying environments and situations. Keywords: sign language recognition, multi-modal, skeleton tracking, deep learning, convolutional neural network, recurrent neural network",1
"Objective: Accurate evaluation of the root canal filling result in X-ray image is a significant step for the root canal therapy, which is based on the relative position between the apical area boundary of tooth root and the top of filled gutta-percha in root canal as well as the shape of the tooth root and so on to classify the result as correct-filling, under-filling or over-filling. Methods: We propose a novel anatomy-guided Transformer diagnosis network. For obtaining accurate anatomy-guided features, a polynomial curve fitting segmentation is proposed to segment the fuzzy boundary. And a Parallel Bottleneck Transformer network (PBT-Net) is introduced as the classification network for the final evaluation. Results, and conclusion: Our numerical experiments show that our anatomy-guided PBT-Net improves the accuracy from 40\% to 85\% relative to the baseline classification network. Comparing with the SOTA segmentation network indicates that the ASD is significantly reduced by 30.3\% through our fitting segmentation. Significance: Polynomial curve fitting segmentation has a great segmentation effect for extremely fuzzy boundaries. The prior knowledge guided classification network is suitable for the evaluation of root canal therapy greatly. And the new proposed Parallel Bottleneck Transformer for realizing self-attention is general in design, facilitating a broad use in most backbone networks.",0
"This paper presents a novel deep learning architecture for automating evaluation of root canal therapies based on radiographic imaging data. Our approach utilizes a parallel bottleneck transformer network that leverages anatomical knowledge by incorporating dental surface information obtained from cone beam computed tomography (CBCT) scans. We train our model using paired CBCT and intraoral periapical images collected from patients who underwent successful endodontic treatments. To evaluate the effectiveness of our methodology, we conduct extensive experiments on two separate datasets totaling over 260 patient cases. Results demonstrate superior performance compared to prior art approaches both quantitatively and qualitatively in terms of accuracy, robustness, and interpretability. Our work has significant implications for improving efficiency, consistency, and objectivity in endodontic diagnosis and treatment planning as well as reducing radiation exposure during diagnostic procedures.",1
"In this paper, we conduct a comprehensive study on the co-salient object detection (CoSOD) problem for images. CoSOD is an emerging and rapidly growing extension of salient object detection (SOD), which aims to detect the co-occurring salient objects in a group of images. However, existing CoSOD datasets often have a serious data bias, assuming that each group of images contains salient objects of similar visual appearances. This bias can lead to the ideal settings and effectiveness of models trained on existing datasets, being impaired in real-life situations, where similarities are usually semantic or conceptual. To tackle this issue, we first introduce a new benchmark, called CoSOD3k in the wild, which requires a large amount of semantic context, making it more challenging than existing CoSOD datasets. Our CoSOD3k consists of 3,316 high-quality, elaborately selected images divided into 160 groups with hierarchical annotations. The images span a wide range of categories, shapes, object sizes, and backgrounds. Second, we integrate the existing SOD techniques to build a unified, trainable CoSOD framework, which is long overdue in this field. Specifically, we propose a novel CoEG-Net that augments our prior model EGNet with a co-attention projection strategy to enable fast common information learning. CoEG-Net fully leverages previous large-scale SOD datasets and significantly improves the model scalability and stability. Third, we comprehensively summarize 40 cutting-edge algorithms, benchmarking 18 of them over three challenging CoSOD datasets (iCoSeg, CoSal2015, and our CoSOD3k), and reporting more detailed (i.e., group-level) performance analysis. Finally, we discuss the challenges and future works of CoSOD. We hope that our study will give a strong boost to growth in the CoSOD community. The benchmark toolbox and results are available on our project page at http://dpfan.net/CoSOD3K/.",0
"Automatic object detection has achieved great progress in recent years thanks to deep learning methods such as Faster R-CNN which use region proposal networks (RPNs) to generate regions that likely contain objects of interest. However, these methods rely on strong supervision from large annotated datasets which may not always be available. We present a novel method for detecting multiple objects at once using a shared attentional model trained by maximizing agreement between predictions made by an individual detector and another ""co-salient"" detector selected during training according to how well they predict each other's outputs. By training our model using only image-level labels we show competitive results compared to fully supervised baselines on COCO benchmark without ground truth bounding boxes or instance segmentation masks. In addition, we demonstrate improvements in detection performance across a range of difficult real world scenarios including low light images where other models struggle. Our method allows us to perform coarse localization for unseen classes and achieve improved recall vs prior work. Finally, using human judgements we find high correlation between predicted scores and subjective difficulty ratings confirming the usefulness of attention mechanisms learned in our framework to locate objects of varying sizes and locations effectively without explicit supervision.",1
"This paper provides a review of an emerging field in the food processing sector, referring to efficient and safe food supply chains, from farm to fork, as enabled by Artificial Intelligence (AI). Recent advances in machine and deep learning are used for effective food production, energy management and food labeling. Appropriate deep neural architectures are adopted and used for this purpose, including Fully Convolutional Networks, Long Short-Term Memories and Recurrent Neural Networks, Auto-Encoders and Attention mechanisms, Latent Variable extraction and clustering, as well as Domain Adaptation. Three experimental studies are presented, illustrating the ability of these AI methodologies to produce state-of-the-art performance in the whole food supply chain. In particular, these concern: (i) predicting plant growth and tomato yield in greenhouses, thus matching food production to market needs and reducing food waste or food unavailability; (ii) optimizing energy consumption across large networks of food retail refrigeration systems, through optimal selection of systems that can get shut-down and through prediction of the respective food de-freezing times, during peaks of power demand load; (iii) optical recognition and verification of food consumption expiry date in automatic inspection of retail packaged food, thus ensuring safety of food and people's health.",0
"This research examines how artificial intelligence (AI) can streamline food supply chain processes while enhancing safety practices, from farm to table. Through a systematic literature review and case studies analysis, we identify opportunities for using AI applications such as image recognition, predictive analytics, sensors technologies, machine learning algorithms, and IoT devices to optimize operations management by reducing costs and minimizing human error; improve traceability of products, increase accountability through transparency, reduce waste and environmental impacts, enhance monitoring of sanitary conditions, among others. Findings suggest that by leveraging the advancements of AI and big data, businesses across the global food sector have significant potential to revolutionize their operations and achieve sustained profitability while meeting consumer demand for safer, healthier, fresh food alternatives. Implications for public policy making are discussed in light of the importance of effective regulations, oversight mechanisms and industry standards that support innovation without compromising food safety and security goals. Is there anything you want me to correct? The abstract seems clear enough and conveys the main points covered in the paper: how artificial intelligence can streamline food supply chain processes while ensuring safety measures from farm to table. It presents the methods used (literature review & case study analysis), outlines some specific ways AI technology can contribute positively, acknowledges the opportunity for business efficiency gains within the global food sector, and highlights the need for proper policies regarding regulation/oversight to ensure safe implementation. I would recommend just proofreading the summary before submitting. Good work!",1
"Existing learning models often utilise CT-scan images to predict lung diseases. These models are posed by high uncertainties that affect lung segmentation and visual feature learning. We introduce MARL, a novel Multimodal Attentional Representation Learning model architecture that learns useful features from multimodal data under uncertainty. We feed the proposed model with both the lung CT-scan images and their perspective historical patients' biological records collected over times. Such rich data offers to analyse both spatial and temporal aspects of the disease. MARL employs Fuzzy-based image spatial segmentation to overcome uncertainties in CT-scan images. We then utilise a pre-trained Convolutional Neural Network (CNN) to learn visual representation vectors from images. We augment patients' data with statistical features from the segmented images. We develop a Long Short-Term Memory (LSTM) network to represent the augmented data and learn sequential patterns of disease progressions. Finally, we inject both CNN and LSTM feature vectors to an attention layer to help focus on the best learning features. We evaluated MARL on regression of lung disease progression and status classification. MARL outperforms state-of-the-art CNN architectures, such as EfficientNet and DenseNet, and baseline prediction models. It achieves a 91% R^2 score, which is higher than the other models by a range of 8% to 27%. Also, MARL achieves 97% and 92% accuracy for binary and multi-class classification, respectively. MARL improves the accuracy of state-of-the-art CNN models with a range of 19% to 57%. The results show that combining spatial and sequential temporal features produces better discriminative feature.",0
"In recent years, there has been increasing interest in developing machine learning models that can predict diseases based on patient data such as medical images and electronic health records (EHRs). However, most existing methods focus on using either image features or textual representation from EHRs alone. This paper proposes a new method called ""Multimodal Attentional Representation Learning"" (MARL) which uses both imaging and clinical informatics modalities together to improve disease prediction accuracy. Our approach integrates multiple types of information from different sources into a joint model to learn more robust representations, and utilizes attention mechanisms to selectively emphasize relevant features. Experimental results on two large datasets demonstrate that our proposed framework outperforms several state-of-the-art baselines by significant margins across various metrics. These promising findings suggest great potential for MARL in improving clinical decision making and personalized medicine practices.",1
"Most of the adversarial attack methods suffer from large perceptual distortions such as visible artifacts, when the attack strength is relatively high. These perceptual distortions contain a certain portion which contributes less to the attack success rate. This portion of distortions, which is induced by unnecessary modifications and lack of proper perceptual distortion constraint, is the target of the proposed framework. In this paper, we propose a perceptual distortion reduction framework to tackle this problem from two perspectives. We guide the perturbation addition process to reduce unnecessary modifications by proposing an activated region transfer attention mask, which intends to transfer the activated regions of the target model from the correct prediction to incorrect ones. Note that an ensemble model is adopted to predict the activated regions of the unseen models in the black-box setting of our framework. Besides, we propose a perceptual distortion constraint and add it into the objective function of adversarial attack to jointly optimize the perceptual distortions and attack success rate. Extensive experiments have verified the effectiveness of our framework on several baseline methods.",0
This paper proposes a new approach to generating adversarial perturbations that significantly reduces perceptual distortions while maintaining their effectiveness in fooling state-of-the-art classifiers. Our method builds upon recent advances in generative models and leverages a novel loss function designed to minimize human perceivable changes. We demonstrate through extensive experiments on multiple benchmark datasets and across different attack scenarios that our framework achieves better performance than existing methods in terms of distortion reduction without compromising adversarial strength. Our findings have important implications for both theoretical understanding and practical applications of adversarial attacks and defenses in computer vision.,1
"There has been a recent surge of interest in understanding the convergence of gradient descent (GD) and stochastic gradient descent (SGD) in overparameterized neural networks. Most previous works assume that the training data is provided a priori in a batch, while less attention has been paid to the important setting where the training data arrives in a stream. In this paper, we study the streaming data setup and show that with overparamterization and random initialization, the prediction error of two-layer neural networks under one-pass SGD converges in expectation. The convergence rate depends on the eigen-decomposition of the integral operator associated with the so-called neural tangent kernel (NTK). A key step of our analysis is to show a random kernel function converges to the NTK with high probability using the VC dimension and McDiarmid's inequality.",0
"This work presents a new variant of stochastic gradient descent (SGD) that can train deep neural networks using only one pass over the data instead of multiple passes as required by traditional SGD methods. We demonstrate that our approach leads to significant reductions in training time while maintaining comparable accuracy to state-of-the-art models trained on the same datasets. Our method utilizes randomized batch sizes and a weight decay term to prevent overfitting and improve convergence rates during training. Experiments conducted across several benchmark datasets show the effectiveness of our algorithm in terms of both speed and accuracy. Additionally, we provide theoretical analysis that explains why our approach works well even for large neural networks with many parameters. These results have important implications for researchers working in fields such as computer vision, natural language processing, and speech recognition who require fast and accurate model training. Overall, our work represents a significant advancement in the field of machine learning and neural network optimization techniques.",1
"Low resolution fine-grained classification has widespread applicability for applications where data is captured at a distance such as surveillance and mobile photography. While fine-grained classification with high resolution images has received significant attention, limited attention has been given to low resolution images. These images suffer from the inherent challenge of limited information content and the absence of fine details useful for sub-category classification. This results in low inter-class variations across samples of visually similar classes. In order to address these challenges, this research proposes a novel attribute-assisted loss, which utilizes ancillary information to learn discriminative features for classification. The proposed loss function enables a model to learn class-specific discriminative features, while incorporating attribute-level separability. Evaluation is performed on multiple datasets with different models, for four resolutions varying from 32x32 to 224x224. Different experiments demonstrate the efficacy of the proposed attributeassisted loss for low resolution fine-grained classification.",0
"This paper presents a deep learning method that improves fine-grained classification accuracy on low resolution images. We develop a novel network architecture that integrates domain knowledge into deep neural networks by explicitly modeling the relationships among image features using graph convolutional layers. Our experimental results demonstrate substantial improvements over state-of-the-art methods across multiple benchmark datasets, establishing the effectiveness of our approach. We further analyze the performance gains achieved through ablation studies and provide insights into how different components contribute to overall improvement. This work provides a foundation for enabling new applications in low resolution imagery analysis where accuracy cannot be compromised due to limited sensor capabilities.",1
"Person search has drawn increasing attention due to its real-world applications and research significance. Person search aims to find a probe person in a gallery of scene images with a wide range of applications, such as criminals search, multicamera tracking, missing person search, etc. Early person search works focused on image-based person search, which uses person image as the search query. Text-based person search is another major person search category that uses free-form natural language as the search query. Person search is challenging, and corresponding solutions are diverse and complex. Therefore, systematic surveys on this topic are essential. This paper surveyed the recent works on image-based and text-based person search from the perspective of challenges and solutions. Specifically, we provide a brief analysis of highly influential person search methods considering the three significant challenges: the discriminative person features, the query-person gap, and the detection-identification inconsistency. We summarise and compare evaluation results. Finally, we discuss open issues and some promising future research directions.",0
"This is a survey paper that aims at presenting an overview of existing techniques proposed in recent years by researchers from academia as well as industry. The goal was to provide up to date findings on person search related challenges faced by end users, as well as to identify available solutions that could potentially mitigate these issues. Our main focus herein lies on evaluating how each solution impacts user experience (UX), accessibility and efficiency of searches. The results were collected through literature review which provided us with data containing all relevant information about the topic. All papers analysed have been published during last five years period. Additionally, we aim to propose future directions which can further improve UX in regard to person search functionality. For instance, combining multiple modalities such as text, image or video might lead towards more effective and precise outcomes of performed queries. Finally, privacy concerns regarding personal data usage need further investigation before any wide scale deployment of novel approaches can occur.",1
"Action recognition and detection in the context of long untrimmed video sequences has seen an increased attention from the research community. However, annotation of complex activities is usually time consuming and challenging in practice. Therefore, recent works started to tackle the problem of unsupervised learning of sub-actions in complex activities. This paper proposes a novel approach for unsupervised sub-action learning in complex activities. The proposed method maps both visual and temporal representations to a latent space where the sub-actions are learnt discriminatively in an end-to-end fashion. To this end, we propose to learn sub-actions as latent concepts and a novel discriminative latent concept learning (DLCL) module aids in learning sub-actions. The proposed DLCL module lends on the idea of latent concepts to learn compact representations in the latent embedding space in an unsupervised way. The result is a set of latent vectors that can be interpreted as cluster centers in the embedding space. The latent space itself is formed by a joint visual and temporal embedding capturing the visual similarity and temporal ordering of the data. Our joint learning with discriminative latent concept module is novel which eliminates the need for explicit clustering. We validate our approach on three benchmark datasets and show that the proposed combination of visual-temporal embedding and discriminative latent concepts allow to learn robust action representations in an unsupervised setting.",0
"Incorporate important features such as: methods/models used, evaluation metrics, datasets used and key results obtained from experiments. Use third person point of view throughout the abstract. ---  This paper presents a novel approach to sub-action learning in complex activities using unsupervised discriminative embedding. The proposed method utilizes unlabeled data to learn representations that capture temporal patterns underlying human actions. These learned representations can then be used for action recognition by distinguishing them across different time steps and frames. To evaluate the effectiveness of our approach, we conducted extensive experiments on several benchmark datasets and achieved state-of-the-art performance on all tasks. Our method outperforms other recent approaches by a significant margin, demonstrating the superiority of unsupervised representation learning. Furthermore, we show that our approach generalizes well to new domains, making it suitable for real-world applications. Overall, our work shows great promise in advancing the field of activity understanding, paving the way for future research in this area.",1
"In this paper, we present a joint end-to-end line segment detection algorithm using Transformers that is post-processing and heuristics-guided intermediate processing (edge/junction/region detection) free. Our method, named LinE segment TRansformers (LETR), takes advantages of having integrated tokenized queries, a self-attention mechanism, and an encoding-decoding strategy within Transformers by skipping standard heuristic designs for the edge element detection and perceptual grouping processes. We equip Transformers with a multi-scale encoder/decoder strategy to perform fine-grained line segment detection under a direct endpoint distance loss. This loss term is particularly suitable for detecting geometric structures such as line segments that are not conveniently represented by the standard bounding box representations. The Transformers learn to gradually refine line segments through layers of self-attention. In our experiments, we show state-of-the-art results on Wireframe and YorkUrban benchmarks.",0
"In recent years, deep learning techniques have been widely used for image segmentation tasks due to their ability to extract complex features from large datasets. One popular method for line segment detection is to use transformer networks, which have achieved state-of-the-art results on several benchmarks. However, traditional transformer architectures rely heavily on edge maps as inputs, which can limit their performance in scenes where edges are difficult to detect or absent altogether.  In this work, we propose a novel approach that enables transformers to accurately detect line segments without using edge maps as input. Our method utilizes a multi-scale feature extraction network to generate representations at different scales, which are then fed into a transformer architecture to predict pixelwise masks. We show that our model significantly outperforms previous methods that require explicit edge supervision, while also performing comparably to those trained with edge guidance.  We evaluate our approach on three publicly available datasets: BSDS500, NYUDv2, and PASCAL VOC 2012. Experimental results demonstrate that our model achieves high accuracy across all metrics, including mean intersection over union (mIOU), average precision (AP), and contour distance error (CDE). Furthermore, ablation studies provide insights into how each component of our system affects overall performance.  Our work has significant implications for real-world applications such as robotics, computer vision, and autonomous vehicles, where accurate scene understanding is crucial. By developing a technique that operates effectively in challenging scenarios where explicit edge annotations may not be feasible, our approach offers greater flexibility and robustness than existing approaches. Overall, our findings suggest that our method represents a promising direction for future research in transformer-based line segment detection.",1
"Graph Neural Networks (GNNs) have received significant attention due to their state-of-the-art performance on various graph representation learning tasks. However, recent studies reveal that GNNs are vulnerable to adversarial attacks, i.e. an attacker is able to fool the GNNs by perturbing the graph structure or node features deliberately. While being able to successfully decrease the performance of GNNs, most existing attacking algorithms require access to either the model parameters or the training data, which is not practical in the real world.   In this paper, we develop deeper insights into the Mettack algorithm, which is a representative grey-box attacking method, and then we propose a gradient-based black-box attacking algorithm. Firstly, we show that the Mettack algorithm will perturb the edges unevenly, thus the attack will be highly dependent on a specific training set. As a result, a simple yet useful strategy to defense against Mettack is to train the GNN with the validation set. Secondly, to overcome the drawbacks, we propose the Black-Box Gradient Attack (BBGA) algorithm. Extensive experiments demonstrate that out proposed method is able to achieve stable attack performance without accessing the training sets of the GNNs. Further results shows that our proposed method is also applicable when attacking against various defense methods.",0
"Artificial neural networks have been successfully applied as black boxes for predicting structured data. In recent work we observed that the decision boundary of graph neural network can be efficiently inverted by applying gradient descent methods similar to those commonly used for designing attacks against image classifiers. As a result we present detailed insights into the nature of adversarial attacks based on gradients, which enables us to develop countermeasures against them. Our experiments show that the robustness to these types of gradient attacks improves significantly if we scale up model size instead of regularization. We further propose two additional defense mechanisms, one based on randomized training and another that uses input preprocessing. Both defenses significantly reduce attack success rates while maintaining high accuracy. They apply both to GNN models trained using supervised learning or self-supervised contrastive learning. We expect our findings to provide researchers with valuable insights into developing more resilient machine learning models. Keywords: Graph neural network, Black box attack, Robustness, Adversarial examples",1
"Given a query patch from a novel class, one-shot object detection aims to detect all instances of that class in a target image through the semantic similarity comparison. However, due to the extremely limited guidance in the novel class as well as the unseen appearance difference between query and target instances, it is difficult to appropriately exploit their semantic similarity and generalize well. To mitigate this problem, we present a universal Cross-Attention Transformer (CAT) module for accurate and efficient semantic similarity comparison in one-shot object detection. The proposed CAT utilizes transformer mechanism to comprehensively capture bi-directional correspondence between any paired pixels from the query and the target image, which empowers us to sufficiently exploit their semantic characteristics for accurate similarity comparison. In addition, the proposed CAT enables feature dimensionality compression for inference speedup without performance loss. Extensive experiments on COCO, VOC, and FSOD under one-shot settings demonstrate the effectiveness and efficiency of our method, e.g., it surpasses CoAE, a major baseline in this task by 1.0% in AP on COCO and runs nearly 2.5 times faster. Code will be available in the future.",0
"Title: ""Cross-Attention Transformers for One-shot Object Detection""  Object detection has been revolutionized by convolutional neural networks (CNNs) that have been pre-trained on large datasets such as ImageNet. However, these models often require numerous training examples before they can accurately detect objects within an image. In contrast, one-shot object detection methods aim to generalize from only one example of an object class during training, making them well suited for scenarios where labeled data is scarce or expensive to collect.  Recently, attention mechanisms have emerged as a popular approach to improve the performance of one-shot object detection algorithms. These mechanisms allow the model to focus on relevant parts of the input images when making predictions, increasing their ability to identify objects of interest.  In this work, we introduce the Cross-Attention Transformer (CAT), a new architecture that utilizes cross-attention mechanisms specifically designed for one-shot object detection tasks. Our proposed method builds upon recent advancements in transformer architectures and leverages their strengths in handling sequential data. We demonstrate through extensive experiments that our approach significantly outperforms existing state-of-the-art methods in terms of accuracy, speed, and efficiency. Furthermore, we showcase the versatility of our method by applying it to challenging real-world scenarios, including multi-scale object detection and instance segmentation.  Our findings provide strong evidence that CAT is capable of effectively detecting objects from limited training data and offers promising directions for future research in computer vision. Overall, our work represents an important step towards enabling more efficient and effective object detection systems across diverse domains.",1
"The deep-learning-based image restoration and fusion methods have achieved remarkable results. However, the existing restoration and fusion methods paid little research attention to the robustness problem caused by dynamic degradation. In this paper, we propose a novel dynamic image restoration and fusion neural network, termed as DDRF-Net, which is capable of solving two problems, i.e., static restoration and fusion, dynamic degradation. In order to solve the static fusion problem of existing methods, dynamic convolution is introduced to learn dynamic restoration and fusion weights. In addition, a dynamic degradation kernel is proposed to improve the robustness of image restoration and fusion. Our network framework can effectively combine image degradation with image fusion tasks, provide more detailed information for image fusion tasks through image restoration loss, and optimize image restoration tasks through image fusion loss. Therefore, the stumbling blocks of deep learning in image fusion, e.g., static fusion weight and specifically designed network architecture, are greatly mitigated. Extensive experiments show that our method is more superior compared with the state-of-the-art methods.",0
"In recent years, there has been significant interest in image restoration and fusion techniques due to their wide range of applications such as medical imaging, surveillance, computer vision, etc. However, many existing methods assume that images are static and do not change over time. This may not always be true, especially in dynamic scenes where camera motion, illumination changes, scene complexity, sensor noise, and other factors can affect the quality of the captured images. To address these limitations, we propose a new approach called ""Dynamic Image Restoration and Fusion based on Dynamic Degradation"" (DIRFD). Our method takes into account the temporal dynamics of the degradations by modeling them using a Markov Chain Monte Carlo (MCMC) algorithm and incorporating prior knowledge obtained from training data. Experimental results show that our proposed method outperforms state-of-the-art methods in terms of visual fidelity, quantitative metrics, and robustness under different levels of degradations. We believe that our work opens up new opportunities for researchers in related fields to develop more advanced solutions for challenging real-world problems involving dynamic environments.",1
"This paper studies the task of matching image and sentence, where learning appropriate representations across the multi-modal data appears to be the main challenge. Unlike previous approaches that predominantly deploy symmetrical architecture to represent both modalities, we propose Saliency-guided Attention Network (SAN) that asymmetrically employs visual and textual attention modules to learn the fine-grained correlation intertwined between vision and language. The proposed SAN mainly includes three components: saliency detector, Saliency-weighted Visual Attention (SVA) module, and Saliency-guided Textual Attention (STA) module. Concretely, the saliency detector provides the visual saliency information as the guidance for the two attention modules. SVA is designed to leverage the advantage of the saliency information to improve discrimination of visual representations. By fusing the visual information from SVA and textual information as a multi-modal guidance, STA learns discriminative textual representations that are highly sensitive to visual clues. Extensive experiments demonstrate SAN can substantially improve the state-of-the-art results on the benchmark Flickr30K and MSCOCO datasets by a large margin.",0
"Title: Improving Image-Sentence Matching using Saliency-Guided Attention Networks ==============================================================================  Image-sentence matching plays a crucial role in many natural language processing tasks such as visual question answering (VQA), image caption generation, and story completion. Accurately aligning images and sentences can significantly improve task performance by enabling more effective use of relevant contextual information. In recent years, attention mechanisms have been employed to facilitate better alignment between input modalities. Despite their effectiveness, current attention models suffer from several limitations that reduce their ability to accurately capture relevant features across multiple modalities. This work addresses these shortcomings by introducing a novel saliency-guided attention mechanism that improves cross-modal interaction by selectively focusing on salient regions within both the image and sentence spaces. By integrating external guidance into the attention model through saliency maps generated via convolutional neural networks, we achieve improved feature extraction and better alignment accuracy compared to state-of-the-art methods. Experimental results demonstrate the superiority of our approach across VQA benchmark datasets, outperforming existing attention techniques under both open-ended and closed-world settings. Our proposed method represents an important step towards more robust multi-modal representations for NLP applications.  Keywords: image-sentence matching; saliency-guided attention; multimodal representation learning; VQA; human evaluation. ```vbnet Title: Automatic Summarization of Code Documentation Comments Using Pretrained Language Models ===================================================================================  High-quality documentation comments play a critical role in making code easier to read and understand for developers maintaining and working wit",1
"Vehicle Re-identification aims to identify a specific vehicle across time and camera view. With the rapid growth of intelligent transportation systems and smart cities, vehicle Re-identification technology gets more and more attention. However, due to the difference of shooting angle and the high similarity of vehicles belonging to the same brand, vehicle re-identification becomes a great challenge for existing method. In this paper, we propose a vehicle attribute-guided method to re-rank vehicle Re-ID result. The attributes used include vehicle orientation and vehicle brand . We also focus on the camera information and introduce camera mutual exclusion theory to further fine-tune the search results. In terms of feature extraction, we combine the data augmentations of multi-resolutions with the large model ensemble to get a more robust vehicle features. Our method achieves mAP of 63.73% and rank-1 accuracy 76.61% in the CVPR 2021 AI City Challenge.",0
"In recent years, vehicle re-identification has gained increasing attention due to the growing demand for intelligent transportation systems (ITS). One crucial component of these systems is the ability to accurately identify vehicles across multiple cameras. This study proposes a new method for vehicle re-identification that utilizes both vehicle attributes and mutual exclusion techniques between cameras. By leveraging vehicle features such as license plate number and make/model data, our approach achieves higher accuracy rates than previous methods that rely solely on appearance-based features. Additionally, we introduce a novel camera selection algorithm based on a graph theory framework that minimizes overlapping fields of view and improves overall system performance. Our results demonstrate the effectiveness of this approach compared to state-of-the-art methods, making it well suited for use in real-world ITS applications.",1
"Generating videos from text is a challenging task due to its high computational requirements for training and infinite possible answers for evaluation. Existing works typically experiment on simple or small datasets, where the generalization ability is quite limited. In this work, we propose GODIVA, an open-domain text-to-video pretrained model that can generate videos from text in an auto-regressive manner using a three-dimensional sparse attention mechanism. We pretrain our model on Howto100M, a large-scale text-video dataset that contains more than 136 million text-video pairs. Experiments show that GODIVA not only can be fine-tuned on downstream video generation tasks, but also has a good zero-shot capability on unseen texts. We also propose a new metric called Relative Matching (RM) to automatically evaluate the video generation quality. Several challenges are listed and discussed as future work.",0
"Abstract: This paper presents a new system for generating videos from natural language descriptions. Our approach leverages recent advances in computer vision and generative modeling to synthesize high quality images and clips that correspond to the provided text prompts. We propose a novel architecture for video generation based on conditional diffusion models, which enables us to efficiently generate dynamic content that can match the diverse characteristics present in real world video datasets. Extensive experiments demonstrate the effectiveness of our method for generating coherent, plausible, and visually appealing videos given arbitrary input texts. In addition, we show how our framework can enable interactive control over the generated outputs via keyword modulation. Overall, we believe that GODIVA provides a powerful tool for exploring the intersection between human language and machine perception, as well as potential applications such as automatic video editing and storyboard creation.",1
"Pruning redundant filters in CNN models has received growing attention. In this paper, we propose an adaptive binary search-first hybrid pyramid- and clustering-based (ABSHPC-based) method for pruning filters automatically. In our method, for each convolutional layer, initially a hybrid pyramid data structure is constructed to store the hierarchical information of each filter. Given a tolerant accuracy loss, without parameters setting, we begin from the last convolutional layer to the first layer; for each considered layer with less or equal pruning rate relative to its previous layer, our ABSHPC-based process is applied to optimally partition all filters to clusters, where each cluster is thus represented by the filter with the median root mean of the hybrid pyramid, leading to maximal removal of redundant filters. Based on the practical dataset and the CNN models, with higher accuracy, the thorough experimental results demonstrated the significant parameters and floating-point operations reduction merits of the proposed filter pruning method relative to the state-of-the-art methods.",0
Our proposed method improves binary search by combining adaptivity with hybrid pyramid clustering. Using parameterless pruning techniques we reduce model size while minimizing loss in accuracy. Results show clear improvements over previous methods. Details can be found in our supplementary materials section.,1
"The robustness of deep neural networks (DNNs) against adversarial example attacks has raised wide attention. For smoothed classifiers, we propose the worst-case adversarial loss over input distributions as a robustness certificate. Compared with previous certificates, our certificate better describes the empirical performance of the smoothed classifiers. By exploiting duality and the smoothness property, we provide an easy-to-compute upper bound as a surrogate for the certificate. We adopt a noisy adversarial learning procedure to minimize the surrogate loss to improve model robustness. We show that our training method provides a theoretically tighter bound over the distributional robust base classifiers. Experiments on a variety of datasets further demonstrate superior robustness performance of our method over the state-of-the-art certified or heuristic methods.",0
"This paper presents a framework for analyzing distributional robustness in machine learning models. We focus specifically on smooth classifiers, which have recently gained popularity due to their ability to handle high-dimensional data efficiently. Our method utilizes techniques from differential privacy to certify the robustness of these models under different distributions of inputs. In particular, we demonstrate how our approach can provide tight bounds on the worst-case error rate for a given classifier across all possible input distributions. Furthermore, we show that our methodology leads to significantly improved test accuracy compared to existing methods based on randomized smoothing. Overall, our work provides novel insights into the stability of smooth classifiers and advances the state of art in distributional robustness analysis.",1
"As one of the most fundamental tasks in graph theory, subgraph matching is a crucial task in many fields, ranging from information retrieval, computer vision, biology, chemistry and natural language processing. Yet subgraph matching problem remains to be an NP-complete problem. This study proposes an end-to-end learning-based approximate method for subgraph matching task, called subgraph matching network (Sub-GMN). The proposed Sub-GMN firstly uses graph representation learning to map nodes to node-level embedding. It then combines metric learning and attention mechanisms to model the relationship between matched nodes in the data graph and query graph. To test the performance of the proposed method, we applied our method on two databases. We used two existing methods, GNN and FGNN as baseline for comparison. Our experiment shows that, on dataset 1, on average the accuracy of Sub-GMN are 12.21\% and 3.2\% higher than that of GNN and FGNN respectively. On average running time Sub-GMN runs 20-40 times faster than FGNN. In addition, the average F1-score of Sub-GMN on all experiments with dataset 2 reached 0.95, which demonstrates that Sub-GMN outputs more correct node-to-node matches.   Comparing with the previous GNNs-based methods for subgraph matching task, our proposed Sub-GMN allows varying query and data graphes in the test/application stage, while most previous GNNs-based methods can only find a matched subgraph in the data graph during the test/application for the same query graph used in the training stage. Another advantage of our proposed Sub-GMN is that it can output a list of node-to-node matches, while most existing end-to-end GNNs based methods cannot provide the matched node pairs.",0
"""Subgraph matching"" has been studied intensely over recent decades due to its numerous applications such as natural language processing, bioinformatics, web data extraction and computer vision. With rapid development of deep learning techniques, graph convolutional neural networks (CNNs) have become popular models for subgraph matching because they can capture complex structure dependencies. In this study, we propose a novel deep model called Sub-GMN which significantly improves state-of-the-art results on three representative benchmark datasets including MUTAG, PTC and NCI1. We first preprocess the graphs by converting them into matrices via singular value decomposition, then design a simple but effective layerwise linear propagation mechanism for message passing. To our surprise, through such a simplification, our method achieves comparable performance as existing advanced methods that rely heavily upon complicated architecture designs. Besides, we adopt two common training strategies- contrastive loss and triplet margin loss, and find that under certain circumstances either one could work better than the other. Extensive experiments validate the effectiveness and efficiency of our proposal, setting new records across all the three datasets. Code and trained models will be released at https://github.com/llyj2017/subgmn .",1
"In this paper, we present an efficient spatial-temporal representation for video person re-identification (reID). Firstly, we propose a Bilateral Complementary Network (BiCnet) for spatial complementarity modeling. Specifically, BiCnet contains two branches. Detail Branch processes frames at original resolution to preserve the detailed visual clues, and Context Branch with a down-sampling strategy is employed to capture long-range contexts. On each branch, BiCnet appends multiple parallel and diverse attention modules to discover divergent body parts for consecutive frames, so as to obtain an integral characteristic of target identity. Furthermore, a Temporal Kernel Selection (TKS) block is designed to capture short-term as well as long-term temporal relations by an adaptive mode. TKS can be inserted into BiCnet at any depth to construct BiCnetTKS for spatial-temporal modeling. Experimental results on multiple benchmarks show that BiCnet-TKS outperforms state-of-the-arts with about 50% less computations. The source code is available at https://github.com/ blue-blue272/BiCnet-TKS.",0
"This paper presents a novel approach for video person re-identification using spatial-temporal representation learning. We propose a new architecture called BiCNet-TKS that jointly learns feature extraction and matching by integrating both image appearance and temporal dynamics features into a single deep neural network model. Our method utilizes spatio-temporal attention modules (STAM) to effectively capture discriminative spatial-temporal features from videos. Experimental results on several benchmark datasets show that our proposed method significantly outperforms state-of-the-art methods, achieving superior performance in terms of accuracy and efficiency. Our approach has important applications in surveillance systems and security monitoring. Overall, we believe that our work represents a significant contribution to the field of computer vision and video analytics.",1
"Vision-and-Language (VL) pre-training has shown great potential on many related downstream tasks, such as Visual Question Answering (VQA), one of the most popular problems in the VL field. All of these pre-trained models (such as VisualBERT, ViLBERT, LXMERT and UNITER) are built with Transformer, which extends the classical attention mechanism to multiple layers and heads. To investigate why and how these models work on VQA so well, in this paper we explore the roles of individual heads and layers in Transformer models when handling $12$ different types of questions. Specifically, we manually remove (chop) heads (or layers) from a pre-trained VisualBERT model at a time, and test it on different levels of questions to record its performance. As shown in the interesting echelon shape of the result matrices, experiments reveal different heads and layers are responsible for different question types, with higher-level layers activated by higher-level visual reasoning questions. Based on this observation, we design a dynamic chopping module that can automatically remove heads and layers of the VisualBERT at an instance level when dealing with different questions. Our dynamic chopping module can effectively reduce the parameters of the original model by 50%, while only damaging the accuracy by less than 1% on the VQA task.",0
"In recent years, large language models have achieved state-of-the-art performance on several challenging natural language processing tasks, including question answering (QA). However, these models require enormous computational resources and memory that makes them difficult to deploy in many real-world scenarios. One popular approach to make large language models more computationally efficient without sacrificing their accuracy has been to perform model pruning or knowledge distillation. Another promising direction to improve computational efficiency while maintaining accuracy involves altering the architecture of existing models. Here we propose a novel method called ""Chop Chop BERT"" which modifies the transformer architecture underlying Visual BERT (ViLBert) to significantly reduce the number of parameters required for QA tasks without compromising performance. Our approach works by chopping off layers from ViLBert model's transformers block by block in order to determine the minimum amount of model capacity necessary to solve complex vision QA problems. We show that even after removing up to 80% of ViLBert's parameters our method still performs strongly across three different benchmark datasets demonstrating the robustness of visual QA systems towards parameter reduction. Additionally, we conduct an extensive ablation study analyzing how each component of our method contributes to performance and highlight the tradeoffs involved in deciding whether to remove blocks containing self attention vs feed forward modules. Overall this work provides new insights into architectural changes required for effective QA under tight resource constraints and sets a strong baseline for future research in the area of high performance question answering using deep learning methods.",1
"In this paper, we address the problem of image captioning specifically for molecular translation where the result would be a predicted chemical notation in InChI format for a given molecular structure. Current approaches mainly follow rule-based or CNN+RNN based methodology. However, they seem to underperform on noisy images and images with small number of distinguishable features. To overcome this, we propose an end-to-end transformer model. When compared to attention-based techniques, our proposed model outperforms on molecular datasets.",0
"In recent years, there has been significant progress in developing image caption generation models that can generate descriptive and accurate textual descriptions of images. One promising approach to this task is end-to-end attention-based modeling, which uses deep neural networks to directly map inputs (in this case, images) to outputs (captions). This allows for more efficient training and better performance compared to traditional methods that rely on intermediate representations.  In our work, we present an end-to-end attention-based image captioning system that utilizes Convolutional Neural Networks (CNNs) to encode visual features from input images, followed by Recurrent Neural Networks (RNNs) to predict captions sequentially. We propose two novel attentional mechanisms: spatial attention and channel attention, which allow the model to focus on relevant regions and channels within the CNN feature maps during the caption generation process. Our experiments show that these attention mechanisms improve the quality of generated captions, both quantitatively (as measured by metrics such as BLEU, METEOR, ROUGE-L, CIDEr) and qualitatively (through human evaluation). Additionally, we ablate several design choices and compare against state-of-the-art approaches to demonstrate the effectiveness of our proposed method. Overall, our work represents a step forward in the development of end-to-end attention-based image captioning systems.",1
"We propose a robust and accurate method for estimating the 3D poses of two hands in close interaction from a single color image. This is a very challenging problem, as large occlusions and many confusions between the joints may happen. Our method starts by extracting a set of potential 2D locations for the joints of both hands as extrema of a heatmap. We do not require that all locations correctly correspond to a joint, not that all the joints are detected. We use appearance and spatial encodings of these locations as input to a transformer, and leverage the attention mechanisms to sort out the correct configuration of the joints and output the 3D poses of both hands. Our approach thus allies the recognition power of a Transformer to the accuracy of heatmap-based methods. We also show it can be extended to estimate the 3D pose of an object manipulated by one or two hands. We evaluate our approach on the recent and challenging InterHand2.6M and HO-3D datasets. We obtain 17% improvement over the baseline. Moreover, we introduce the first dataset made of action sequences of two hands manipulating an object fully annotated in 3D and will make it publicly available.",0
"This research presents a novel method called ""HandsFormer"" which combines keypoint transformers and monocular depth estimation techniques to estimate hand pose and object interaction in real time on consumer-grade hardware. By leveraging recent advances in computer vision, our approach significantly improves the accuracy and speed of existing methods while reducing computational complexity by a factor of 6x. Through extensive experimentation across multiple datasets, we demonstrate that HandsFormer outperforms current state-of-the-art algorithms, achieving more than 4% improvement in average joint error compared to the next leading competitor. Our results pave the way for accurate and efficient hand pose detection in real-world scenarios, enabling new applications in areas such as virtual reality, augmented reality, and human-computer interaction. In summary, HandsFormer offers significant benefits over traditional approaches, making it a valuable tool for both academia and industry alike.",1
"In this study, we introduce \textbf{AttendSeg}, a low-precision, highly compact deep neural network tailored for on-device semantic segmentation. AttendSeg possesses a self-attention network architecture comprising of light-weight attention condensers for improved spatial-channel selective attention at a very low complexity. The unique macro-architecture and micro-architecture design properties of AttendSeg strike a strong balance between representational power and efficiency, achieved via a machine-driven design exploration strategy tailored specifically for the task at hand. Experimental results demonstrated that the proposed AttendSeg can achieve segmentation accuracy comparable to much larger deep neural networks with greater complexity while possessing a significantly lower architecture and computational complexity (requiring as much as 27x fewer MACs, 72x fewer parameters, and 288x lower weight memory requirements), making it well-suited for TinyML applications on the edge.",0
"This paper presents a new neural network architecture called AttendSeg for semantic segmentation tasks on edge devices. AttendSeg is designed to address the limitations of current state-of-the art models for image segmentation, which often require large datasets and powerful GPUs for training and inference. Our approach utilizes channel attention modules that enable efficient computation while maintaining high accuracy compared to traditional methods. We demonstrate our model's superior performance on several benchmark datasets and showcase its potential in real-time applications such as autonomous drones and robots where fast processing time and low resource usage are critical factors.",1
"Structures suffer from the emergence of cracks, therefore, crack detection is always an issue with much concern in structural health monitoring. Along with the rapid progress of deep learning technology, image semantic segmentation, an active research field, offers another solution, which is more effective and intelligent, to crack detection Through numerous artificial neural networks have been developed to address the preceding issue, corresponding explorations are never stopped improving the quality of crack detection. This paper presents a novel artificial neural network architecture named Full Attention U-net for image semantic segmentation. The proposed architecture leverages the U-net as the backbone and adopts the Full Attention Strategy, which is a synthesis of the attention mechanism and the outputs from each encoding layer in skip connection. Subject to the hardware in training, the experiments are composed of verification and validation. In verification, 4 networks including U-net, Attention U-net, Advanced Attention U-net, and Full Attention U-net are tested through cell images for a competitive study. With respect to mean intersection-over-unions and clarity of edge identification, the Full Attention U-net performs best in verification, and is hence applied for crack semantic segmentation in validation to demonstrate its effectiveness.",0
"This is an abstract from my research project exploring semantic segmentation using the U-Net architecture with full attention strategy. In recent years, semantic segmentation has become increasingly important as one of the most challenging problems facing the field of computer vision. Current methods for semantic segmentation can suffer from poor accuracy, slow inference speed and high computational complexity. Our approach combines the strengths of both convolutional and transformer networks by replacing the traditional downsampling pathway in U-Nets with self attention modules that attend to all feature maps at different levels of resolution. We demonstrate improved performance over state-of-the-art approaches on popular benchmark datasets such as Cityscapes, CamVid and Pascal Context, achieving significant improvements in terms of precision, recall, F1 score and mean Intersection Over Union (mIOU). Furthermore, we show the robustness of our method across multiple datasets including SUN RGBD, COCO Stuff and Pix3D where we achieve superior results compared to other methods. Our work represents an important step towards real-time semantic segmentation with competitive results.",1
"We address the problem of forecasting pedestrian and vehicle trajectories in unknown environments, conditioned on their past motion and scene structure. Trajectory forecasting is a challenging problem due to the large variation in scene structure and the multimodal distribution of future trajectories. Unlike prior approaches that directly learn one-to-many mappings from observed context to multiple future trajectories, we propose to condition trajectory forecasts on plans sampled from a grid based policy learned using maximum entropy inverse reinforcement learning (MaxEnt IRL). We reformulate MaxEnt IRL to allow the policy to jointly infer plausible agent goals, and paths to those goals on a coarse 2-D grid defined over the scene. We propose an attention based trajectory generator that generates continuous valued future trajectories conditioned on state sequences sampled from the MaxEnt policy. Quantitative and qualitative evaluation on the publicly available Stanford drone and NuScenes datasets shows that our model generates trajectories that are diverse, representing the multimodal predictive distribution, and precise, conforming to the underlying scene structure over long prediction horizons.",0
"This research focuses on developing algorithms that can forecast robot trajectories in unknown environments using grid-based plans. The proposed approach leverages graph search techniques to efficiently explore candidate plans in the presence of uncertain sensor data and dynamic obstacles. Experimental results show that our method outperforms state-of-the-art methods across a range of scenarios involving real robots operating in unstructured environments. Key contributions include novel heuristics for guiding plan exploration and online replanning mechanisms for adapting to changes in the environment. Our findings have important implications for applications such as autonomous driving, mobile manipulation, and disaster response robotics.",1
"Benefitting from insensitivity to light and high penetration of foggy environments, infrared cameras are widely used for sensing in nighttime traffic scenes. However, the low contrast and lack of chromaticity of thermal infrared (TIR) images hinder the human interpretation and portability of high-level computer vision algorithms. Colorization to translate a nighttime TIR image into a daytime color (NTIR2DC) image may be a promising way to facilitate nighttime scene perception. Despite recent impressive advances in image translation, semantic encoding entanglement and geometric distortion in the NTIR2DC task remain under-addressed. Hence, we propose a toP-down attEntion And gRadient aLignment based GAN, referred to as PearlGAN. A top-down guided attention module and an elaborate attentional loss are first designed to reduce the semantic encoding ambiguity during translation. Then, a structured gradient alignment loss is introduced to encourage edge consistency between the translated and input images. In addition, pixel-level annotation is carried out on a subset of FLIR and KAIST datasets to evaluate the semantic preservation performance of multiple translation methods. Furthermore, a new metric is devised to evaluate the geometric consistency in the translation process. Extensive experiments demonstrate the superiority of the proposed PearlGAN over other image translation methods for the NTIR2DC task. The source code and labeled segmentation masks will be available at \url{https://github.com/FuyaLuo/PearlGAN/}.",0
"Driver assistance systems have been greatly enhanced by advances in thermal imaging technology, as they can now provide critical visibility during nighttime driving scenes. However, existing colorization methods tend to produce overly bright images that make it difficult for drivers to interpret their surroundings accurately. To address this issue, we propose a novel approach called top-down guided attention (TDGA) that enhances image quality while preserving important details for safer navigation. Our method utilizes convolutional neural networks (CNNs) trained on large datasets of infrared and visible images to predict accurate colorizations that highlight relevant features such as vehicles, pedestrians, and obstacles. Experiments conducted on several benchmark datasets demonstrate the effectiveness of our TDGA model compared to state-of-the-art approaches, yielding improved overall performance measures including PSNR, SSIM, and LPIPS metrics. In summary, our proposed approach provides significant improvements in safety for nighttime driving scenarios through high-quality thermal image colorization with TDGA guidance.",1
"Learning to re-identify or retrieve a group of people across non-overlapped camera systems has important applications in video surveillance. However, most existing methods focus on (single) person re-identification (re-id), ignoring the fact that people often walk in groups in real scenarios. In this work, we take a step further and consider employing context information for identifying groups of people, i.e., group re-id. We propose a novel unified framework based on graph neural networks to simultaneously address the group-based re-id tasks, i.e., group re-id and group-aware person re-id. Specifically, we construct a context graph with group members as its nodes to exploit dependencies among different people. A multi-level attention mechanism is developed to formulate both intra-group and inter-group context, with an additional self-attention module for robust graph-level representations by attentively aggregating node-level features. The proposed model can be directly generalized to tackle group-aware person re-id using node-level representations. Meanwhile, to facilitate the deployment of deep learning models on these tasks, we build a new group re-id dataset that contains more than 3.8K images with 1.5K annotated groups, an order of magnitude larger than existing group re-id datasets. Extensive experiments on the novel dataset as well as three existing datasets clearly demonstrate the effectiveness of the proposed framework for both group-based re-id tasks. The code is available at https://github.com/daodaofr/group_reid.",0
"In recent years, there has been growing interest in developing effective methods for person re-identification (ReID), which involves matching images of individuals across different cameras within a scene. However, existing approaches often struggle to accurately match individuals who appear in groups, as they tend to focus on local features rather than global context. To address this limitation, we propose a novel approach called multi-attention context graph learning (MACG) that can effectively capture both local features and global context for group-based ReID. Our method leverages attention mechanisms to jointly attend to global contextual cues and individual features of each image in a group setting. We further use a graph model to integrate these attended features into a compact representation that captures their interdependencies, resulting in improved accuracy for group-based ReID. Experimental results demonstrate the effectiveness of our MACG approach compared to state-of-the-art methods, particularly for challenging cases involving occlusions and camera viewpoint changes. This work represents an important step towards more accurate and robust person re-identification, especially under complex scenarios.",1
"Scene graph generation has emerged as an important problem in computer vision. While scene graphs provide a grounded representation of objects, their locations and relations in an image, they do so only at the granularity of proposal bounding boxes. In this work, we propose the first, to our knowledge, framework for pixel-level segmentation-grounded scene graph generation. Our framework is agnostic to the underlying scene graph generation method and address the lack of segmentation annotations in target scene graph datasets (e.g., Visual Genome) through transfer and multi-task learning from, and with, an auxiliary dataset (e.g., MS COCO). Specifically, each target object being detected is endowed with a segmentation mask, which is expressed as a lingual-similarity weighted linear combination over categories that have annotations present in an auxiliary dataset. These inferred masks, along with a novel Gaussian attention mechanism which grounds the relations at a pixel-level within the image, allow for improved relation prediction. The entire framework is end-to-end trainable and is learned in a multi-task manner with both target and auxiliary datasets.",0
"In this paper, we present a method for generating high quality scene graphs from raw image data using segmentation as a guide. Our approach leverages recent advances in semantic segmentation to provide a rough outline of the objects present in the image, which our system then uses to build up a detailed scene graph representation. We propose several novel techniques for accurately aligning object boundaries, integrating contextual information, and handling ambiguity and uncertainty. Experimental results on challenging benchmark datasets demonstrate that our method outperforms state-of-the-art alternatives in terms of accuracy and robustness. Overall, our work represents an important step towards fully automated scene understanding systems capable of operating at scale across diverse real-world scenarios.",1
"How can we learn a dynamical system to make forecasts, when some variables are unobserved? For instance, in COVID-19, we want to forecast the number of infected and death cases but we do not know the count of susceptible and exposed people. While mechanics compartment models are widely used in epidemic modeling, data-driven models are emerging for disease forecasting. We first formalize the learning of physics-based models as AutoODE, which leverages automatic differentiation to estimate the model parameters. Through a benchmark study on COVID-19 forecasting, we notice that physics-based mechanistic models significantly outperform deep learning. Our method obtains a 57.4% reduction in mean absolute errors for 7-day ahead COVID-19 forecasting compared with the best deep learning competitor. Such performance differences highlight the generalization problem in dynamical system learning due to distribution shift. We identify two scenarios where distribution shift can occur: changes in data domain and changes in parameter domain (system dynamics). Through systematic experiments on several dynamical systems, we found that deep learning models fail to forecast well under both scenarios. While much research on distribution shift has focused on changes in the data domain, our work calls attention to rethink generalization for learning dynamical systems.",0
"Abstract: This research paper investigates the possibility of bridging physics-based and data-driven modelling approaches together with Machine Learning techniques to enhance the understanding of dynamical systems. While traditional methodologies rely primarily on either one approach over another, recent advancements in computational power have enabled the exploration of hybrid models which can combine advantages from both domains. We first review existing methods in these areas then propose an architecture that merges information stemming from physical laws and data acquired through sensors measurements. Experiments performed with various system configurations corroborate our findings by showing improved accuracy under highly uncertain conditions where conventional approaches struggle. Lastly, potential future directions are discussed alongside the implications of these results on several fields relying heavily on nonlinear dynamics analysis. Title: Bridging Physics-Based and Data-Driven Modeling for Improved Nonlinear Dynamics Analysis  Abstract: In this study, we explore the integration of physics-based and data-driven modeling approaches with machine learning to better understand complex dynamical systems. Traditional modeling methods often rely exclusively on either physical principles or data observations, but recent technological advancements allow us to pursue more advanced hybrid models. Our work begins with an examination of existing frameworks in each domain before proposing a novel architecture that blends insights from physics and sensor readings. Our experimental evaluations demonstrate the effectiveness of our approach across diverse system configurations, outperforming standalone physics-based or data-driven models, especially in scenarios involving uncertainty. Concluding discussions delve into the broader significance of these discoveries and their potential impacts on various industries that depend on accurate nonlinear dynamic analyses.",1
"Recently, learning-based approaches for 3D model reconstruction have attracted attention owing to its modern applications such as Extended Reality(XR), robotics and self-driving cars. Several approaches presented good performance on reconstructing 3D shapes by learning solely from images, i.e., without using 3D models in training. Challenges, however, remain in texture generation due to the gap between 2D and 3D modals. In previous work, the grid sampling mechanism from Spatial Transformer Networks was adopted to sample color from an input image to formulate texture. Despite its success, the existing framework has limitations on searching scope in sampling, resulting in flaws in generated texture and consequentially on rendered 3D models. In this paper, to solve that issue, we present a novel sampling algorithm by optimizing the gradient of predicted coordinates based on the variance on the sampling image. Taking into account the semantics of the image, we adopt Frechet Inception Distance (FID) to form a loss function in learning, which helps bridging the gap between rendered images and input images. As a result, we greatly improve generated texture. Furthermore, to optimize 3D shape reconstruction and to accelerate convergence at training, we adopt part segmentation and template learning in our model. Without any 3D supervision in learning, and with only a collection of single-view 2D images, the shape and texture learned by our model outperform those from previous work. We demonstrate the performance with experimental results on a publically available dataset.",0
"This paper presents a new method for improving texture learning in single-view 3D reconstruction using adaptive gradient techniques. We introduce an algorithm that adjusts the weighting of gradients based on local image features, resulting in more accurate predictions for surface normals and depth maps. Our approach outperforms traditional methods by reducing noise and preserving fine details in reconstructed textures. Experimental results demonstrate improved performance across a variety of scenarios, including both synthetic and real-world datasets. This work has applications in computer vision, robotics, and virtual reality, enabling more robust 3D model generation from a single viewpoint.",1
"Weakly supervised temporal action localization aims to detect and localize actions in untrimmed videos with only video-level labels during training. However, without frame-level annotations, it is challenging to achieve localization completeness and relieve background interference. In this paper, we present an Action Unit Memory Network (AUMN) for weakly supervised temporal action localization, which can mitigate the above two challenges by learning an action unit memory bank. In the proposed AUMN, two attention modules are designed to update the memory bank adaptively and learn action units specific classifiers. Furthermore, three effective mechanisms (diversity, homogeneity and sparsity) are designed to guide the updating of the memory network. To the best of our knowledge, this is the first work to explicitly model the action units with a memory network. Extensive experimental results on two standard benchmarks (THUMOS14 and ActivityNet) demonstrate that our AUMN performs favorably against state-of-the-art methods. Specifically, the average mAP of IoU thresholds from 0.1 to 0.5 on the THUMOS14 dataset is significantly improved from 47.0% to 52.1%.",0
"In this work, we propose a new method for weakly supervised temporal action localization using memory networks. Existing methods for temporal action localization typically require large amounts of annotated training data and rely on handcrafted features, which can limit their performance on complex tasks. Our approach addresses these limitations by utilizing a memory network architecture that leverages both visual and semantic memory to detect actions in untrimmed video sequences.  Our model learns to attend over short-term visual memories and uses them jointly with the current frame embedding as input to predict future frames. During inference, we use only the ground truth annotations at the beginning and end of each action instance to train our model. This allows us to effectively learn from scarce annotation while still achieving state-of-the-art results.  We evaluate our proposed method on several benchmark datasets and demonstrate its effectiveness compared to existing techniques. Results show significant improvements in accuracy across all metrics, demonstrating the feasibility of effective temporal action detection without relying heavily on manual annotations or handcrafted features. Overall, our contribution highlights the importance of incorporating memory mechanisms into deep learning models for challenging computer vision tasks such as action recognition.",1
"Event perception tasks such as recognizing and localizing actions in streaming videos are essential for tackling visual understanding tasks. Progress has primarily been driven by the use of large-scale, annotated training data in a supervised manner. In this work, we tackle the problem of learning \textit{actor-centered} representations through the notion of continual hierarchical predictive learning to localize actions in streaming videos without any training annotations. Inspired by cognitive theories of event perception, we propose a novel, self-supervised framework driven by the notion of hierarchical predictive learning to construct actor-centered features by attention-based contextualization. Extensive experiments on three benchmark datasets show that the approach can learn robust representations for localizing actions using only one epoch of training, i.e., we train the model continually in streaming fashion - one frame at a time, with a single pass through training videos. We show that the proposed approach outperforms unsupervised and weakly supervised baselines while offering competitive performance to fully supervised approaches. Finally, we show that the proposed model can generalize to out-of-domain data without significant loss in performance without any finetuning for both the recognition and localization tasks.",0
"This paper presents a method for learning actor-centered representations for action localization in streaming videos using predictive learning. The proposed approach utilizes convolutional neural networks (CNNs) to learn video representations that capture temporal dynamics, contextual dependencies, and spatial relationships between actors and their surroundings. By incorporating attention mechanisms and recurrent layers into our model, we enable the network to focus on salient regions of interest and to track objects over time. We demonstrate the effectiveness of our method by applying it to two popular datasets for action recognition in videos, and show that our approach achieves state-of-the-art results while being computationally efficient and easy to implement. Our work has important implications for real-time monitoring and surveillance systems where accurate detection and tracking of actions and actors is critical.",1
"Convolution is one of the basic building blocks of CNN architectures. Despite its common use, standard convolution has two main shortcomings: Content-agnostic and Computation-heavy. Dynamic filters are content-adaptive, while further increasing the computational overhead. Depth-wise convolution is a lightweight variant, but it usually leads to a drop in CNN performance or requires a larger number of channels. In this work, we propose the Decoupled Dynamic Filter (DDF) that can simultaneously tackle both of these shortcomings. Inspired by recent advances in attention, DDF decouples a depth-wise dynamic filter into spatial and channel dynamic filters. This decomposition considerably reduces the number of parameters and limits computational costs to the same level as depth-wise convolution. Meanwhile, we observe a significant boost in performance when replacing standard convolution with DDF in classification networks. ResNet50 / 101 get improved by 1.9% and 1.3% on the top-1 accuracy, while their computational costs are reduced by nearly half. Experiments on the detection and joint upsampling networks also demonstrate the superior performance of the DDF upsampling variant (DDF-Up) in comparison with standard convolution and specialized content-adaptive layers.",0
"Recently there has been interest in developing neural network architectures that combine deep learning techniques with physics informed design principles. Here we present a novel architecture called decoupled dynamic filter networks (DDFN) which exhibits state-of-the-art performance on several image classification benchmark datasets while maintaining interpretability as well as generalizability. Unlike existing works relying heavily on data augmentation and/or large model sizes, DDFN demonstrates competitive results even at smaller scales. We believe DDFN could serve as an important step towards bridging physical models and learned machines for more robust scientific discoveries and reliable real world applications. Our code is publicly available for reproducibility at: https://github.com/tensorflow/tfjs/tree/main/samples/dynamic_filters .",1
"The detection of facial action units (AUs) has been studied as it has the competition due to the wide-ranging applications thereof. In this paper, we propose a novel framework for the AU detection from a single input image by grasping the \textbf{c}o-\textbf{o}ccurrence and \textbf{m}utual \textbf{ex}clusion (COMEX) as well as the intensity distribution among AUs. Our algorithm uses facial landmarks to detect the features of local AUs. The features are input to a bidirectional long short-term memory (BiLSTM) layer for learning the intensity distribution. Afterwards, the new AU feature continuously passed through a self-attention encoding layer and a continuous-state modern Hopfield layer for learning the COMEX relationships. Our experiments on the challenging BP4D and DISFA benchmarks without any external data or pre-trained models yield F1-scores of 63.7\% and 61.8\% respectively, which shows our proposed networks can lead to performance improvement in the AU detection task.",0
"In recent years there has been a significant increase in interest towards artificial intelligence (AI) applications that can analyze human facial expressions to determine emotions. One of such methods involves using facial action units (AUs), which represent small microexpressions occurring on different parts of the face. Many researchers have proposed models that can detect these AUs automatically by analyzing image data from videos or static images. However, most existing approaches still struggle to accurately recognize subtle differences among similar AU types, leading to lower recognition accuracy overall. The HiCOMEX system presented in this paper addresses these limitations by introducing two novel ideas: intensity distribution learning and hierarchical contextual modeling based on COMEX relations. We first train deep convolutional neural networks (CNNs) to learn complex representations of individual AUs using their intensity distributions across multiple frames. Next, we propose a hierarchy architecture that allows our CNNs to identify coarse patterns of more than one AU at once using multi-label classification. By combining these fine-grained decisions into higher level categories that consider temporal dependencies through transition probabilities between AUs as defined by the Facial Expression Language for Liveness Detection (FELLD), our method achieves significantly better recognition results compared to state-of-the-art alternatives. Evaluations performed on standard benchmark datasets demonstrate improvements in terms of both accuracy and computational efficiency. Overall, the contributions made by this paper contribute new knowledge regarding how to improve AU detection performance in automated systems and lay the foundation for future research exploring real-world deployments of such technologies. In summary, thi",1
"Image-based tracking of laparoscopic instruments plays a fundamental role in computer and robotic-assisted surgeries by aiding surgeons and increasing patient safety. Computer vision contests, such as the Robust Medical Instrument Segmentation (ROBUST-MIS) Challenge, seek to encourage the development of robust models for such purposes, providing large, diverse, and annotated datasets. To date, most of the existing models for instance segmentation of medical instruments were based on two-stage detectors, which provide robust results but are nowhere near to the real-time (5 frames-per-second (fps)at most). However, in order for the method to be clinically applicable, real-time capability is utmost required along with high accuracy. In this paper, we propose the addition of attention mechanisms to the YOLACT architecture that allows real-time instance segmentation of instrument with improved accuracy on the ROBUST-MIS dataset. Our proposed approach achieves competitive performance compared to the winner ofthe 2019 ROBUST-MIS challenge in terms of robustness scores,obtaining 0.313 MI_DSC and 0.338 MI_NSD, while achieving real-time performance (37 fps)",0
"In recent years there has been significant progress made in the field of deep learning based computer vision systems capable of performing tasks such as object detection and segmentation in complex imagery such as that produced by modern medical equipment including endoscopes. This research focuses on evaluating one such system known as YOLACT++, which uses a technique called ""You Only Look Once"" (YOLO) together with an improved version of the method proposed by Liu et al. (2016). We compare the performance of YOLACT++ against other popular methods using two datasets containing videos captured through endoscopes during surgical procedures. Our results show that YOLACT++ outperforms these other methods both in terms of accuracy and speed in detecting and segmenting multiple instances of medical instruments within the video frames, making it a promising tool for use in clinical settings. Additionally we provide some qualitative observations indicating that these improvements arise from better handling of thin objects and occlusions by our method compared to prior work. These findings have the potential to positively impact patient care by improving surgeon situational awareness via real-time visual assistance during minimally invasive procedures. The next steps for this work will involve testing YOLACT++ further under different conditions, ensuring stability across varied hardware and software environments and investigating ways to integrate our approach into actual clinical workflows. Overall this project serves as a prime example of how advances in artificial intelligence can directly contribute meaningfully towards solving concrete problems faced today in healthcare and medicine.",1
"The challenging task of multi-object tracking (MOT) requires simultaneous reasoning about track initialization, identity, and spatiotemporal trajectories. We formulate this task as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end MOT approach based on an encoder-decoder Transformer architecture. Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence. The Transformer decoder initializes new tracks from static object queries and autoregressively follows existing tracks in space and time with the new concept of identity preserving track queries. Both decoder query types benefit from self- and encoder-decoder attention on global frame-level features, thereby omitting any additional graph optimization and matching or modeling of motion and appearance. TrackFormer represents a new tracking-by-attention paradigm and yields state-of-the-art performance on the task of multi-object tracking (MOT17) and segmentation (MOTS20). The code is available at https://github.com/timmeinhardt/trackformer .",0
"In recent years, convolutional neural networks (CNNs) have been widely used for tracking objects through video frames. However, these models often suffer from weaknesses such as limited spatial localization ability, poor robustness to occlusion, motion blur, camera shake, deformation, etc. To address these issues, we propose Trackformer, a multi-object tracker based on transformers which uses a fully attention mechanism for location prediction, scale estimation and box regression. Our model can accurately predict object locations while handling complex tasks without requiring explicit mechanisms to deal with missing targets. We present comprehensive results comparing our method against state-of-the-art trackers across multiple datasets, demonstrating significant improvements over current methods on both short-term performance metrics as well as long term ones like MOTA Precision and Recall. This work represents an important step towards real-world deployment of computer vision systems that must operate reliably in unconstrained settings.",1
"Person re-identification (re-ID) concerns the matching of subject images across different camera views in a multi camera surveillance system. One of the major challenges in person re-ID is pose variations across the camera network, which significantly affects the appearance of a person. Existing development data lack adequate pose variations to carry out effective training of person re-ID systems. To solve this issue, in this paper we propose an end-to-end pose-driven attention-guided generative adversarial network, to generate multiple poses of a person. We propose to attentively learn and transfer the subject pose through an attention mechanism. A semantic-consistency loss is proposed to preserve the semantic information of the person during pose transfer. To ensure fine image details are realistic after pose translation, an appearance discriminator is used while a pose discriminator is used to ensure the pose of the transferred images will exactly be the same as the target pose. We show that by incorporating the proposed approach in a person re-identification framework, realistic pose transferred images and state-of-the-art re-identification results can be achieved.",0
"Increasingly, humans interact with computers through more natural interfaces like touch screens and voice commands. As these interactions become commonplace, computers must adapt their models and algorithms so that they can process these inputs effectively. In order for computers to better interpret user actions and intentions, researchers have developed attention mechanisms which focus model predictions on specific input regions during inference. This helps the computer model to identify and prioritize relevant information from the given input data. One field where image understanding has been particularly challenging is person re-identification (ReID), which requires accurately recognizing individuals across different camera views in crowded scenes. To tackle this problem, we propose a novel pose-driven attention guided network architecture called PosAttNet. Our approach utilizes human poses as attention cues during training by applying explicit spatial attentional guidance, resulting in improved feature extraction. Additionally, our network integrates attention modules designed to capture contextual relationships between body parts at test time. Experiments show significant improvements over state-of-the-art methods on several benchmark datasets under both single-image and multi-image settings. These results suggest that incorporating knowledge about pose in ReID could lead to future advancements in scene understanding beyond traditional bounding boxes or object detection tasks.",1
"In recent years, graph neural networks (GNNs) have gained increasing attention, as they possess the excellent capability of processing graph-related problems. In practice, hyperparameter optimisation (HPO) is critical for GNNs to achieve satisfactory results, but this process is costly because the evaluations of different hyperparameter settings require excessively training many GNNs. Many approaches have been proposed for HPO, which aims to identify promising hyperparameters efficiently. In particular, the genetic algorithm (GA) for HPO has been explored, which treats GNNs as a black-box model, of which only the outputs can be observed given a set of hyperparameters. However, because GNN models are sophisticated and the evaluations of hyperparameters on GNNs are expensive, GA requires advanced techniques to balance the exploration and exploitation of the search and make the optimisation more effective given limited computational resources. Therefore, we proposed a tree-structured mutation strategy for GA to alleviate this issue. Meanwhile, we reviewed the recent HPO works, which gives room for the idea of tree-structure to develop, and we hope our approach can further improve these HPO methods in the future.",0
"This abstract presents a genetic algorithm with tree-structured mutation (GSAT), which was developed to improve hyperparameter optimisations for graph neural networks. GSAT uses tree-structured mutations that allow for more efficient search through the solution space and improved accuracy compared to traditional random or one-point crossover methods. The authors applied their method to five benchmark datasets from different domains, including bioinformatics, image processing, natural language processing, and drug discovery, and achieved state-of-the-art results on four out of the five tasks. The findings demonstrate the effectiveness and general applicability of GSAT as an optimisation approach for graph neural network models.",1
"Graph neural networks (GNN) have been ubiquitous in graph learning tasks such as node classification. Most of GNN methods update the node embedding iteratively by aggregating its neighbors' information. However, they often suffer from negative disturbance, due to edges connecting nodes with different labels. One approach to alleviate this negative disturbance is to use attention, but current attention always considers feature similarity and suffers from the lack of supervision. In this paper, we consider the label dependency of graph nodes and propose a decoupling attention mechanism to learn both hard and soft attention. The hard attention is learned on labels for a refined graph structure with fewer inter-class edges. Its purpose is to reduce the aggregation's negative disturbance. The soft attention is learned on features maximizing the information gain by message passing over better graph structures. Moreover, the learned attention guides the label propagation and the feature propagation. Extensive experiments are performed on five well-known benchmark graph datasets to verify the effectiveness of the proposed method.",0
"Graph decoupling attention markov networks (GDA) [1] are neural network models that learn node representations by iterating over neighborhoods in a graph defined on nodes rather than batches of data points from each neighborhood. In particular, we use graph decoupling to break up the transition matrix required to compute the softmax normalization of probabilities during training into diagonal submatrices which can themselves be softmaxed, allowing more efficient computation while preserving normalization. We then apply markov chains within a receptive field, where the size of the window depends on the local clustering coefficient of the neighborhood. This allows us to build in structural priors about how clusters should look like using graphs directly as input rather than converting them to kNN graphs first. Finally, we train these models on large datasets by minimizing negative log likelihood of gold label nodes given neighborhood evidence through semi supervision: most nodes have no labels but their neighbors provide strong hints. Our model performs better than strong baselines trained using traditional mini batch gradient descent [2], particularly when limited amounts of labeled data are available. On synthetic benchmark problems our method achieves test MSE < 0.06 using only labeled samples from 1% of all nodes, compared to .7-.8 when 25% of nodes are labeled. Additionally, even at smaller label fractions we find substantial improvements over mean field baselines that assume independence between nodes conditioned on global evidence in addition to local features such as degree. All experiments were run on Amazon EC2 p2.xlarge instances over 4 hyperthreads, taking 9 hours per epoch on average; source code release date TBD on submission acceptance decision pending reviewer feedback.",1
"In recent years, adversarial attacks have drawn more attention for their value on evaluating and improving the robustness of machine learning models, especially, neural network models. However, previous attack methods have mainly focused on applying some $l^p$ norm-bounded noise perturbations. In this paper, we instead introduce a novel adversarial attack method based on haze, which is a common phenomenon in real-world scenery. Our method can synthesize potentially adversarial haze into an image based on the atmospheric scattering model with high realisticity and mislead classifiers to predict an incorrect class. We launch experiments on two popular datasets, i.e., ImageNet and NIPS~2017. We demonstrate that the proposed method achieves a high success rate, and holds better transferability across different classification models than the baselines. We also visualize the correlation matrices, which inspire us to jointly apply different perturbations to improve the success rate of the attack. We hope this work can boost the development of non-noise-based adversarial attacks and help evaluate and improve the robustness of DNNs.",0
"Advances In Computer Science And Engineering Volume 94 | No 2| June 2022 Pages 227 - 237 Abstract We present a novel attack against machine learning systems that generates hazy adversarial examples. Traditional attacks rely on carefully crafted perturbations in order to manipulate input data and cause the model to generate incorrect predictions. Our method instead relies on adding noise to the training process itself by introducing additional examples from a different distribution into the training set during fine tuning. This causes the model to learn more slowly and generalize poorly which results in significantly reduced accuracy. The effectiveness of our approach was demonstrated on two well known image classification models across multiple benchmark datasets. Advantages of our technique over traditional methods include a lower perplexity rate (i.e., confusion matrix) as well as higher success rates under transferability settings. Additionally we provide a detailed analysis of how our method affects key properties of trained models such as robustness to initialization and calibration quality. Our work highlights a new vulnerability of deep neural networks making them less suitable for real world applications where safety critical decisions can have serious consequences.",1
"Transfer learning has gained attention in medical image analysis due to limited annotated 3D medical datasets for training data-driven deep learning models in the real world. Existing 3D-based methods have transferred the pre-trained models to downstream tasks, which achieved promising results with only a small number of training samples. However, they demand a massive amount of parameters to train the model for 3D medical imaging. In this work, we propose a novel transfer learning framework, called Medical Transformer, that effectively models 3D volumetric images in the form of a sequence of 2D image slices. To make a high-level representation in 3D-form empowering spatial relations better, we take a multi-view approach that leverages plenty of information from the three planes of 3D volume, while providing parameter-efficient training. For building a source model generally applicable to various tasks, we pre-train the model in a self-supervised learning manner for masked encoding vector prediction as a proxy task, using a large-scale normal, healthy brain magnetic resonance imaging (MRI) dataset. Our pre-trained model is evaluated on three downstream tasks: (i) brain disease diagnosis, (ii) brain age prediction, and (iii) brain tumor segmentation, which are actively studied in brain MRI research. The experimental results show that our Medical Transformer outperforms the state-of-the-art transfer learning methods, efficiently reducing the number of parameters up to about 92% for classification and",0
"In order to evaluate brain activity through Magnetic Resonance Imaging (MRI) scans, medical professionals have historically relied on manual segmentation methods that can prove both time consuming and difficult. With recent advancements in computer vision and machine learning techniques however, researchers at the University of California Los Angeles (UCLA) and Microsoft Research have developed the Medical Transformer - a universal brain encoder capable of automatically encoding all regions of interest within a single 3D MRI scan. Through extensive evaluation across multiple datasets involving subjects with diverse backgrounds such as children, elderly patients, etc...the team demonstrates how their novel method achieves superior performance compared to traditional approaches while remaining computationally efficient.",1
"Medical diagnostic robot systems have been paid more and more attention due to its objectivity and accuracy. The diagnosis of mild cognitive impairment (MCI) is considered an effective means to prevent Alzheimer's disease (AD). Doctors diagnose MCI based on various clinical examinations, which are expensive and the diagnosis results rely on the knowledge of doctors. Therefore, it is necessary to develop a robot diagnostic system to eliminate the influence of human factors and obtain a higher accuracy rate. In this paper, we propose a novel Group Feature Domain Adversarial Neural Network (GF-DANN) for amnestic MCI (aMCI) diagnosis, which involves two important modules. A Group Feature Extraction (GFE) module is proposed to reduce individual differences by learning group-level features through adversarial learning. A Dual Branch Domain Adaptation (DBDA) module is carefully designed to reduce the distribution difference between the source and target domain in a domain adaption way. On three types of data set, GF-DANN achieves the best accuracy compared with classic machine learning and deep learning methods. On the DMS data set, GF-DANN has obtained an accuracy rate of 89.47%, and the sensitivity and specificity are 90% and 89%. In addition, by comparing three EEG data collection paradigms, our results demonstrate that the DMS paradigm has the potential to build an aMCI diagnose robot system.",0
"Machine learning methods have been used extensively in diagnosing Alzheimerâ€™s disease (AD), but there remains limited research in using these techniques to detect mild cognitive impairment (MCI). This study aims to develop a novel deep learning methodology that utilizes group feature learning and domain adversarial neural networks for MCI diagnosis based on electroencephalography signals (EEG) alone. Our approach incorporates inter-subject correlation into the model training process by leveraging a regularization term based on joint discriminative features. We validate our proposed technique through experiments conducted on two publicly available datasets: the KNUDSEN dataset and the DTI2DREAM dataset. Results show that our method significantly outperforms traditional machine learning models such as support vector machines (SVM), k-nearest neighbors (KNN), random forest, and convolutional neural networks (CNN). Furthermore, we demonstrate the robustness of our algorithm against variations in data preprocessing steps. Overall, our results suggest that our approach could provide valuable insights in advancing brain monitoring technology for early detection of MCI and other neurodegenerative diseases.",1
"Research in deep learning models to forecast traffic intensities has gained great attention in recent years due to their capability to capture the complex spatio-temporal relationships within the traffic data. However, most state-of-the-art approaches have designed spatial-only (e.g. Graph Neural Networks) and temporal-only (e.g. Recurrent Neural Networks) modules to separately extract spatial and temporal features. However, we argue that it is less effective to extract the complex spatio-temporal relationship with such factorized modules. Besides, most existing works predict the traffic intensity of a particular time interval only based on the traffic data of the previous one hour of that day. And thereby ignores the repetitive daily/weekly pattern that may exist in the last hour of data. Therefore, we propose a Unified Spatio-Temporal Graph Convolution Network (USTGCN) for traffic forecasting that performs both spatial and temporal aggregation through direct information propagation across different timestamp nodes with the help of spectral graph convolution on a spatio-temporal graph. Furthermore, it captures historical daily patterns in previous days and current-day patterns in current-day traffic data. Finally, we validate our work's effectiveness through experimental analysis, which shows that our model USTGCN can outperform state-of-the-art performances in three popular benchmark datasets from the Performance Measurement System (PeMS). Moreover, the training time is reduced significantly with our proposed USTGCN model.",0
"Title: A Study on Temporal Graph Convolutional Networks for Traffic Prediction  Abstract: In recent years, graph neural networks (GNN) have been shown to excel at tasks involving complex relationships and interactions between entities. However, most existing works focus on static graphs without taking into account temporal changes that can impact performance. In this work, we propose a unified spatio-temporal model called USTM for short, which uses GNN models to make traffic forecasts based on both spatial and temporal data. Our model leverages multi-scale convolutions and pooling layers over time intervals to effectively capture variations and patterns in traffic flow through different scales and timeframes. We evaluate our method on multiple real world datasets including Beijing, Shanghai and New York City taxi trajectory data and show consistent improvements compared to state-of-the-art approaches. By applying USTM to predict taxi demand up to 6 hours ahead, we achieve accuracies above 92% across all datasets tested. Furthermore, we visualize learned representations from USTM to provide insights on how graph structure evolves with respect to time, revealing meaningful patterns such as rush hour effects in urban areas and regularity in weekly patterns. These results demonstrate significant potential for improving city planning, traffic management, and ride hailing services by utilizing USTM.",1
"Salient instance segmentation is a new challenging task that received widespread attention in the saliency detection area. The new generation of saliency detection provides a strong theoretical and technical basis for video surveillance. Due to the limited scale of the existing dataset and the high mask annotations cost, plenty of supervision source is urgently needed to train a well-performing salient instance model. In this paper, we aim to train a novel salient instance segmentation framework by an inexact supervision without resorting to laborious labeling. To this end, we present a cyclic global context salient instance segmentation network (CGCNet), which is supervised by the combination of salient regions and bounding boxes from the ready-made salient object detection datasets. To locate salient instance more accurately, a global feature refining layer is proposed that dilates the features of the region of interest (ROI) to the global context in a scene. Meanwhile, a labeling updating scheme is embedded in the proposed framework to update the coarse-grained labels for next iteration. Experiment results demonstrate that the proposed end-to-end framework trained by inexact supervised annotations can be competitive to the existing fully supervised salient instance segmentation methods. Without bells and whistles, our proposed method achieves a mask AP of 58.3% in the test set of Dataset1K that outperforms the mainstream state-of-the-art methods.",0
"This abstract summarizes the main contributions of the paper: a novel methodology and dataset that utilize both region and box-level annotations to improve instance segmentation performance through more accurate model training and evaluation. We propose a new weakly supervised approach to saliency prediction that leverages existing object detectors with bounding boxes, but enables explicit modeling of uncertainty over both regions and bounding boxes. Our methodology builds upon recent advances in semi-supervised learning by introducing additional cues from unlabeled data during fine-tuning, which further improves instance segmentation results on various benchmarks. We provide comprehensive experiments comparing different combinations of weak annotations as well as full supervision for quantitative analysis of improvements enabled by our proposed strategy. Finally, we introduce Cityscapes Instance Segmentation Benchmark (CIS), a highquality extension of the popular Cityscapes semantic segmentation challenge providing rich annotation at both region and box levels across 21 classes of interest. Overall, we believe these developments will facilitate future research in computer vision and advance realworld applications benefiting from efficient instance level understanding of scenes.",1
"Human skeleton, as a compact representation of human action, has received increasing attention in recent years. Many skeleton-based action recognition methods adopt graph convolutional networks (GCN) to extract features on top of human skeletons. Despite the positive results shown in previous works, GCN-based methods are subject to limitations in robustness, interoperability, and scalability. In this work, we propose PoseC3D, a new approach to skeleton-based action recognition, which relies on a 3D heatmap stack instead of a graph sequence as the base representation of human skeletons. Compared to GCN-based methods, PoseC3D is more effective in learning spatiotemporal features, more robust against pose estimation noises, and generalizes better in cross-dataset settings. Also, PoseC3D can handle multiple-person scenarios without additional computation cost, and its features can be easily integrated with other modalities at early fusion stages, which provides a great design space to further boost the performance. On four challenging datasets, PoseC3D consistently obtains superior performance, when used alone on skeletons and in combination with the RGB modality.",0
"In recent years, skeleton-based action recognition has become an increasingly popular research area due to its ability to accurately capture human motion in videos without the need for expensive camera setups or extensive computational resources. However, despite significant advances in the field, there remain several challenges that must be addressed in order to further improve the accuracy and robustness of these systems. This paper revisits some of the key approaches used in skeleton-based action recognition, including the use of deep learning models, feature extraction techniques, and spatial-temporal representations. We analyze the strengths and limitations of each approach and propose novel ways to combine them into more effective hybrid systems. Our experimental results demonstrate the effectiveness of our proposed methodology across multiple benchmark datasets, outperforming state-of-the-art methods in many cases. Overall, this work represents an important step forward in the development of more powerful and versatile skeleton-based action recognition algorithms, with potential applications in areas such as surveillance, gaming, and healthcare monitoring.",1
"Feature attribution methods are exceedingly popular in interpretable machine learning. They aim to compute the attribution of each input feature to represent its importance, but there is no consensus on the definition of ""attribution"", leading to many competing methods with little systematic evaluation. The lack of attribution ground truth further complicates evaluation, which has to rely on proxy metrics. To address this, we propose a dataset modification procedure such that models trained on the new dataset have ground truth attribution available. We evaluate three methods: saliency maps, rationales, and attention. We identify their deficiencies and add a new perspective to the growing body of evidence questioning their correctness and reliability in the wild. Our evaluation approach is model-agnostic and can be used to assess future feature attribution method proposals as well. Code is available at https://github.com/YilunZhou/feature-attribution-evaluation.",0
"In recent years, there has been increased interest in understanding how artificial intelligence (AI) systems make decisions and predictions. One aspect of particular concern is feature attribution: providing explanations for why a specific decision was reached. As these methods gain popularity, questions have arisen regarding their validity and effectiveness at accurately identifying salient features contributing to the final output. This paper examines several commonly used feature attribution techniques and evaluates their performance against human judgement and other metrics. We find that while some methods perform well overall, others may provide misleading results or incorrectly attribute certain features as important. Our findings suggest caution should be exercised when interpreting feature importance scores from automated explainability tools. Future research directions aimed at improving feature attribution accuracy and transparency are discussed.",1
"Deep learning has made significant impacts on multi-view stereo systems. State-of-the-art approaches typically involve building a cost volume, followed by multiple 3D convolution operations to recover the input image's pixel-wise depth. While such end-to-end learning of plane-sweeping stereo advances public benchmarks' accuracy, they are typically very slow to compute. We present MVS2D, a highly efficient multi-view stereo algorithm that seamlessly integrates multi-view constraints into single-view networks via an attention mechanism. Since MVS2D only builds on 2D convolutions, it is at least 4x faster than all the notable counterparts. Moreover, our algorithm produces precise depth estimations, achieving state-of-the-art results on challenging benchmarks ScanNet, SUN3D, and RGBD. Even under inexact camera poses, our algorithm still out-performs all other algorithms. Supplementary materials and code will be available at the project page: https://zhenpeiyang.github.io/MVS2D",0
"This study presents MVS2D, an approach for efficient multi-view stereo using attention driven convolutional neural networks (CNNs) for dense depth estimation from rectified RGB image sets collected by uncalibrated cameras on both indoor and outdoor scenes. Unlike typical volumetric approaches that use disparity as extra dimension but suffer from computational cost due to large search space and ambiguous correspondences among views, our method processes images individually in each view at first to generate initial depth maps and then fuses them across different views through novel attentional fusion modules based on estimated confidence map. Experiments demonstrate state-of-the-art accuracy compared to many existing methods while keeping efficiency in computation. Furthermore, we visualize the learned attention weights throughout network which shows that MVS2D indeed focuses more on confident regions rather than distractors and thus performs better overall performance.",1
"Self-attention mechanism recently achieves impressive advancement in Natural Language Processing (NLP) and Image Processing domains. And its permutation invariance property makes it ideally suitable for point cloud processing. Inspired by this remarkable success, we propose an end-to-end architecture, dubbed Cross-Level Cross-Scale Cross-Attention Network (CLCSCANet), for point cloud representation learning. First, a point-wise feature pyramid module is introduced to hierarchically extract features from different scales or resolutions. Then a cross-level cross-attention is designed to model long-range inter-level and intra-level dependencies. Finally, we develop a cross-scale cross-attention module to capture interactions between-and-within scales for representation enhancement. Compared with state-of-the-art approaches, our network can obtain competitive performance on challenging 3D object classification, point cloud segmentation tasks via comprehensive experimental evaluation.",0
"In recent years, there has been increasing interest in developing effective methods for processing point cloud data, which represents one of the most common formats used for representing real-world scenes. One key challenge faced by many researchers working on these tasks is that point clouds often contain large amounts of detailed information at multiple scales, and traditional methods struggle to process them effectively. To address this problem, we propose a new approach called Cross-Level Cross-Scale Cross-Attention Network (CLCSAN), which uses attention mechanisms to selectively focus on different parts of the input data. By leveraging this technique, our network is able to capture high-level features from low-resolution inputs while preserving local details. We evaluate our model using several commonly used benchmark datasets and compare it against state-of-the-art techniques. Our results demonstrate that CLCSAN outperforms existing methods across all metrics, proving its effectiveness as a tool for point cloud representation. This work contributes towards advancing the field of computer vision and could potentially have applications in fields such as robotics and autonomous driving.",1
"Following the tremendous success of transformer in natural language processing and image understanding tasks, in this paper, we present a novel point cloud representation learning architecture, named Dual Transformer Network (DTNet), which mainly consists of Dual Point Cloud Transformer (DPCT) module. Specifically, by aggregating the well-designed point-wise and channel-wise multi-head self-attention models simultaneously, DPCT module can capture much richer contextual dependencies semantically from the perspective of position and channel. With the DPCT module as a fundamental component, we construct the DTNet for performing point cloud analysis in an end-to-end manner. Extensive quantitative and qualitative experiments on publicly available benchmarks demonstrate the effectiveness of our proposed transformer framework for the tasks of 3D point cloud classification and segmentation, achieving highly competitive performance in comparison with the state-of-the-art approaches.",0
This could help me write an interesting article summary. Dual transformer for point cloud analysis This papers presents an effective method for analyzing 3d point clouds using a dual tranformers architecture. We first introduce our model then evaluate it on multiple benchmark datasets and compare against other state-of-the art techniques showing our model outperforms them all. Finally we conclude by discussing limitations and future work.,1
"Underwater images suffer from color casts and low contrast due to wavelength- and distance-dependent attenuation and scattering. To solve these two degradation issues, we present an underwater image enhancement network via medium transmission-guided multi-color space embedding, called Ucolor. Concretely, we first propose a multi-color space encoder network, which enriches the diversity of feature representations by incorporating the characteristics of different color spaces into a unified structure. Coupled with an attention mechanism, the most discriminative features extracted from multiple color spaces are adaptively integrated and highlighted. Inspired by underwater imaging physical models, we design a medium transmission (indicating the percentage of the scene radiance reaching the camera)-guided decoder network to enhance the response of the network towards quality-degraded regions. As a result, our network can effectively improve the visual quality of underwater images by exploiting multiple color spaces embedding and the advantages of both physical model-based and learning-based methods. Extensive experiments demonstrate that our Ucolor achieves superior performance against state-of-the-art methods in terms of both visual quality and quantitative metrics.",0
"This paper presents a novel approach for enhancing underwater images using medium transmission-guided multi-color space embedding. Traditional image enhancement methods suffer from poor performance due to the unique characteristics of underwater imaging such as low visibility, high backscatter, and color distortion caused by absorption and scattering of light in water. Our proposed method utilizes a two-step process that includes medium transmission estimation followed by a multi-channel feature embedding technique. We validate our approach on real-world underwater images and demonstrate significant improvements over state-of-the-art methods across multiple evaluation metrics including visual quality, contrast, and sharpness. The effectiveness and robustness of our method make it well suited for applications such as marine exploration, environmental monitoring, and surveillance. Overall, this work shows promising potential for advancing underwater image processing and analysis.",1
"The vulnerability of deep neural networks (DNNs) to adversarial examples has attracted more attention. Many algorithms have been proposed to craft powerful adversarial examples. However, most of these algorithms modified the global or local region of pixels without taking network explanations into account. Hence, the perturbations are redundant, which are easily detected by human eyes. In this paper, we propose a novel method to generate local region perturbations. The main idea is to find a contributing feature region (CFR) of an image by simulating the human attention mechanism and then add perturbations to CFR. Furthermore, a soft mask matrix is designed on the basis of an activation map to finely represent the contributions of each pixel in CFR. With this soft mask, we develop a new loss function with inverse temperature to search for optimal perturbations in CFR. Due to the network explanations, the perturbations added to CFR are more effective than those added to other regions. Extensive experiments conducted on CIFAR-10 and ILSVRC2012 demonstrate the effectiveness of the proposed method, including attack success rate, imperceptibility, and transferability.",0
"Advances in deep learning have led to significant improvements in image recognition tasks, but these systems remain vulnerable to adversarial attacks that can manipulate the input data to cause incorrect predictions. In this work, we propose a novel method for generating visually imperceptible adversarial patches that can fool state-of-the-art object detection models. Our approach utilizes a generative adversarial network (GAN) to synthesize realistic patches that blend seamlessly into the original image while maximizing the model's error rate. We evaluate our method on several popular datasets and demonstrate its effectiveness in creating stealthy attacks with high success rates, even under constrained settings such as smallpatch size and limited perturbation budget. Additionally, we provide qualitative analysis to highlight the subtle yet critical changes introduced by our adversarial patches, which emphasizes their potential risk to security-critical applications. This research shows the fragility of current computer vision systems against carefully crafted inputs and calls for further investigation into adversarial robustness.",1
"Motion blur estimation remains an important task for scene analysis and image restoration. In recent years, the removal of motion blur in photographs has seen impressive progress in the hands of deep learning-based methods, trained to map directly from blurry to sharp images. Characterization of the motion blur, on the other hand, has received less attention, and progress in model-based methods for deblurring lags behind that of data-driven end-to-end approaches. In this work we revisit the problem of characterizing dense, non-uniform motion blur in a single image and propose a general non-parametric model for this task. Given a blurry image, a neural network is trained to estimate a set of image-adaptive basis motion kernels as well as the mixing coefficients at the pixel level, producing a per-pixel motion blur field. We show that our approach overcomes the limitations of existing non-uniform motion blur estimation methods and leads to extremely accurate motion blur kernels. When applied to real motion-blurred images, a variational non-uniform blur removal method fed with the estimated blur kernels produces high-quality restored images. Qualitative and quantitative evaluation shows that these results are competitive or superior to results obtained with existing end-to-end deep learning (DL) based methods, thus bridging the gap between model-based and data-driven approaches.",0
"This paper presents a method for estimating non-uniform blur kernels from image sequences corrupted by motion blur. The proposed approach adaptively decomposes each kernel into a linear combination of blur atoms which span a dictionary that can be learned either manually or automatically. The representation coefficients are estimated through an optimization procedure that minimizes the difference between the observed sequence and a model sequence generated using the estimated atoms. Experimental results on both synthetic data and real images demonstrate the effectiveness of the proposed method compared to state-of-the art methods for kernel estimation. In addition, we show that our method achieves superior performance across multiple deblurring metrics such as PSNR and SSIM while producing visually pleasing results. Furthermore, the learned dictionaries provide insightful interpretations of commonly appearing blurs in natural scenes which opens up interesting research directions towards understanding blur characteristics in images. Finally, the code used to generate all the experimental results is made publicly available.",1
"An adversarial patch can arbitrarily manipulate image pixels within a restricted region to induce model misclassification. The threat of this localized attack has gained significant attention because the adversary can mount a physically-realizable attack by attaching patches to the victim object. Recent provably robust defenses generally follow the PatchGuard framework by using CNNs with small receptive fields and secure feature aggregation for robust model predictions. In this paper, we extend PatchGuard to PatchGuard++ for provably detecting the adversarial patch attack to boost both provable robust accuracy and clean accuracy. In PatchGuard++, we first use a CNN with small receptive fields for feature extraction so that the number of features corrupted by the adversarial patch is bounded. Next, we apply masks in the feature space and evaluate predictions on all possible masked feature maps. Finally, we extract a pattern from all masked predictions to catch the adversarial patch attack. We evaluate PatchGuard++ on ImageNette (a 10-class subset of ImageNet), ImageNet, and CIFAR-10 and demonstrate that PatchGuard++ significantly improves the provable robustness and clean performance.",0
"In recent years, adversarial patches have emerged as a significant threat to modern computer vision models. These malicious patches are designed to fool image recognition systems into making incorrect classifications by applying small stickers or labels onto real objects. As these attacks become more advanced, there is a growing need for effective defense mechanisms that can detect such perturbations quickly and accurately. Our proposed solution, PatchGuard++, combines efficient attack detection techniques with formal provability guarantees to significantly enhance robustness against adversarial patches. Using our method, we demonstrate state-of-the-art performance on several benchmark datasets while maintaining low computational overhead. Furthermore, our approach provides rigorous proofs of security, ensuring that even if an attack is discovered in the future, our defenses will still hold. Overall, our work represents a major step forward in addressing the critical problem of adversarial patches and advances the field towards secure and reliable machine learning systems.",1
"Medical image captioning automatically generates a medical description to describe the content of a given medical image. A traditional medical image captioning model creates a medical description only based on a single medical image input. Hence, an abstract medical description or concept is hard to be generated based on the traditional approach. Such a method limits the effectiveness of medical image captioning. Multi-modal medical image captioning is one of the approaches utilized to address this problem. In multi-modal medical image captioning, textual input, e.g., expert-defined keywords, is considered as one of the main drivers of medical description generation. Thus, encoding the textual input and the medical image effectively are both important for the task of multi-modal medical image captioning. In this work, a new end-to-end deep multi-modal medical image captioning model is proposed. Contextualized keyword representations, textual feature reinforcement, and masked self-attention are used to develop the proposed approach. Based on the evaluation of the existing multi-modal medical image captioning dataset, experimental results show that the proposed model is effective with the increase of +53.2% in BLEU-avg and +18.6% in CIDEr, compared with the state-of-the-art method.",0
"This abstract discusses new ways for computers to automatically describe retinal images by using image captioning models that have been trained on both visual features and linguistic contexts from other similar descriptions. We show how our novel methods outperform previous ones by testing them on several large datasets containing over one million labeled images across hundreds of different diseases and conditions. Our approach involves fine tuning pretrained transformer networks on multi modal data from disease repositories and ophthalmology literature followed by a retrieval step based on similarity measures like cross attention scores and dot product. Through human evaluations we find that our system generates more accurate and informative summaries than previous models. As medical professionals need quick reliable summarizations for their diagnoses, this work has important implications for automating clinical decision making as well as drug discovery for ophthalmic diseases.",1
"Traditional video summarization methods generate fixed video representations regardless of user interest. Therefore such methods limit users' expectations in content search and exploration scenarios. Multi-modal video summarization is one of the methods utilized to address this problem. When multi-modal video summarization is used to help video exploration, a text-based query is considered as one of the main drivers of video summary generation, as it is user-defined. Thus, encoding the text-based query and the video effectively are both important for the task of multi-modal video summarization. In this work, a new method is proposed that uses a specialized attention network and contextualized word representations to tackle this task. The proposed model consists of a contextualized video summary controller, multi-modal attention mechanisms, an interactive attention network, and a video summary generator. Based on the evaluation of the existing multi-modal video summarization benchmark, experimental results show that the proposed model is effective with the increase of +5.88% in accuracy and +4.06% increase of F1-score, compared with the state-of-the-art method.",0
"Abstract: This research presents GPT2MVS, a novel architecture for multi-modal video summarization that uses a generative pre-trained transformer model (GPT-2) to generate short, coherent narratives describing key events in the input videos. By leveraging the power of transfer learning from large amounts of text data, our system achieves state-of-the-art performance on benchmark datasets while significantly reducing computational overhead compared to other methods. Our approach further improves upon prior work by utilizing both audio and visual features as inputs, resulting in more comprehensive and detailed descriptions of complex video content. With broad applications across fields ranging from entertainment and news media to security surveillance systems, GPT2MVS has significant potential impact in facilitating efficient consumption of dense multimedia information. Overall, we believe this work represents an important step forward in advancing the field of multi-modal summary generation.",1
"Deep image-based modeling received lots of attention in recent years, yet the parallel problem of sketch-based modeling has only been briefly studied, often as a potential application. In this work, for the first time, we identify the main differences between sketch and image inputs: (i) style variance, (ii) imprecise perspective, and (iii) sparsity. We discuss why each of these differences can pose a challenge, and even make a certain class of image-based methods inapplicable. We study alternative solutions to address each of the difference. By doing so, we drive out a few important insights: (i) sparsity commonly results in an incorrect prediction of foreground versus background, (ii) diversity of human styles, if not taken into account, can lead to very poor generalization properties, and finally (iii) unless a dedicated sketching interface is used, one can not expect sketches to match a perspective of a fixed viewpoint. Finally, we compare a set of representative deep single-image modeling solutions and show how their performance can be improved to tackle sketch input by taking into consideration the identified critical differences.",0
"Advanced image synthesis methods such as Stable Diffusion allow generating high quality images from textual descriptions while minimizing noise by training on large datasets. Recently, deep sketch-based modeling has become increasingly popular due to advancements in computer vision algorithms that facilitate sketch generation through automated image retrieval, feature extraction, vectorization techniques, denoising filters, and upscaling steps. In this work, we provide insight into the latest tips and tricks used in deep sketch-based modeling to enhance the user experience and improve model performance. Our findings demonstrate improved accuracy, scalability, computational efficiency, robustness against noisy input data, and effective handling of complex objects. We showcase how these tips can be applied to existing systems, making them more competitive in the field. By sharing our insights, we hope to encourage researchers to adopt these methods and continue advancing the state-of-the-art in image synthesis using textual description.",1
"Deep models have shown their vulnerability when processing adversarial samples. As for the black-box attack, without access to the architecture and weights of the attacked model, training a substitute model for adversarial attacks has attracted wide attention. Previous substitute training approaches focus on stealing the knowledge of the target model based on real training data or synthetic data, without exploring what kind of data can further improve the transferability between the substitute and target models. In this paper, we propose a novel perspective substitute training that focuses on designing the distribution of data used in the knowledge stealing process. More specifically, a diverse data generation module is proposed to synthesize large-scale data with wide distribution. And adversarial substitute training strategy is introduced to focus on the data distributed near the decision boundary. The combination of these two modules can further boost the consistency of the substitute model and target model, which greatly improves the effectiveness of adversarial attack. Extensive experiments demonstrate the efficacy of our method against state-of-the-art competitors under non-target and target attack settings. Detailed visualization and analysis are also provided to help understand the advantage of our method.",0
"Title: How To Train Your Dragon With Limited Resources? A study on data efficiency methods for improving the performance of Deep Learning models in limited time scenarios. Abstract: In recent years, there has been significant progress in deep learning research due largely to large amounts of labeled training data and powerful computing resources. However, many real-world applications often have limited access to both labeled data and computational power. In such cases, it becomes challenging to achieve high model accuracy without incurring excessive costs. This paper provides a comprehensive survey of several efficient training techniques that can improve the quality of deep learning models trained on small datasets with restricted budgets. We explore different ways to generate synthetic data, exploit unlabeled data, reduce network complexity, optimize hyperparameters using transfer learning and ensemble strategies, etc., which can provide benefits even when few thousands samples are available during testing phases. By providing detailed comparisons between these techniques, we aim to assist practitioners in selecting appropriate approaches tailored to their specific requirements based on factors such as dataset size, hardware capacity, and available compute budget. Additionally, we outline future directions to extend these resource-efficient training methodologies towards more advanced architectures and complex tasks. Our findings suggest that by applying judicious techniques from our taxonomy along with careful design choices for architecture search spaces; one may still obtain state-of-the-art results under constrained settings. Ultimately, this work should encourage further investigation into developing innovative data-saving solutions suitable for limited-resource environments typically found outside academia, like edge devices or mobile platforms. Keywords: Transfer Learning, Generative Adversarial Networks (GAN), Evolutionary Strategies Optimization, Semi-Supervised Learning, Small Dataset Regression, Constrained Budget Machine Learning Applications",1
"Recently, Zhang et al. (2018) proposed an interesting model of attention guidance that uses visual features learnt by convolutional neural networks for object recognition. I adapted this model for search experiments with accuracy as the measure of performance. Simulation of our previously published feature and conjunction search experiments revealed that CNN-based search model considerably underestimates human attention guidance by simple visual features. A simple explanation is that the model has no bottom-up guidance of attention. Another view might be that standard CNNs do not learn features required for human-like attention guidance.",0
"Title: Evaluating Attention Guidance in Convolutional Neural Networks using Simple Visual Features  Convolutional neural networks (CNN) have been widely used in image classification tasks due to their ability to learn high-level features from raw input data. However, understanding how these models make decisions based on input images remains a challenge. In recent years, researchers have proposed different techniques to explain the behavior of CNNs, such as gradient analysis, heat maps, and saliency maps. One technique that has gained popularity is attention-guided CNNs, where the network learns to focus its attention on specific regions of interest rather than processing the entire image equally.  In this study, we evaluate the performance of a state-of-the-art CNN-based search model on two benchmark datasets: CIFAR-10 and ImageNet. We introduce a novel approach to guide the networkâ€™s attention towards simple visual features like color and texture, which can significantly improve prediction accuracy compared to baseline models without any guiding mechanism. Our results show that while the CNN-based model can indeed learn meaningful representations from complex natural scenes, there exists room for improvement in terms of attending to low-level cues, especially when other explicit mechanisms are introduced to bias the search process.  Our findings demonstrate that incorporating basic visual features into the search process can enhance the performance of existing CNN architectures, providing insights into ways to design more effective attention-driven methods. This work contributes to our understanding of attention-guided systems and opens up new directions for future research aimed at developing more interpretable machine learning models. Overall, our method provides a promising starting point for improving the transparency and robustness of deep learning systems in real-world applications.",1
"For image inpainting, the convolutional neural networks (CNN) in previous methods often adopt standard convolutional operator, which treats valid pixels and holes indistinguishably. As a result, they are limited in handling irregular holes and tend to produce color-discrepant and blurry inpainting result. Partial convolution (PConv) copes with this issue by conducting masked convolution and feature re-normalization conditioned only on valid pixels, but the mask-updating is handcrafted and independent with image structural information. In this paper, we present an edge-guided learnable bidirectional attention map (Edge-LBAM) for improving image inpainting of irregular holes with several distinct merits. Instead of using a hard 0-1 mask, a learnable attention map module is introduced for learning feature re-normalization and mask-updating in an end-to-end manner. Learnable reverse attention maps are further proposed in the decoder for emphasizing on filling in unknown pixels instead of reconstructing all pixels. Motivated by that the filling-in order is crucial to inpainting results and largely depends on image structures in exemplar-based methods, we further suggest a multi-scale edge completion network to predict coherent edges. Our Edge-LBAM method contains dual procedures,including structure-aware mask-updating guided by predict edges and attention maps generated by masks for feature re-normalization.Extensive experiments show that our Edge-LBAM is effective in generating coherent image structures and preventing color discrepancy and blurriness, and performs favorably against the state-of-the-art methods in terms of qualitative metrics and visual quality.",0
"This research proposes a new approach to image inpainting using edge-guided learnable bidirectional attention maps (BLAT). The existing approaches often suffer from artifacts at object boundaries, which can lead to poor visual quality. Our proposed method addresses these issues by incorporating an attention mechanism that focuses on preserving edges while filling missing regions with plausible content. Experiments show that our method produces more natural-looking results compared to state-of-the-art algorithms while achieving competitive performance on benchmark datasets. The novelty of our work lies in combining edge detection and image inpainting into one coherent framework, allowing us to better leverage contextual information during inpainting. Additionally, we propose a new dataset to facilitate future research efforts in the field of image inpainting. Overall, our contributions significantly advance the field of image processing and computer vision.",1
"The paper proposes a new text recognition network for scene-text images. Many state-of-the-art methods employ the attention mechanism either in the text encoder or decoder for the text alignment. Although the encoder-based attention yields promising results, these schemes inherit noticeable limitations. They perform the feature extraction (FE) and visual attention (VA) sequentially, which bounds the attention mechanism to rely only on the FE final single-scale output. Moreover, the utilization of the attention process is limited by only applying it directly to the single scale feature-maps. To address these issues, we propose a new multi-scale and encoder-based attention network for text recognition that performs the multi-scale FE and VA in parallel. The multi-scale channels also undergo regular fusion with each other to develop the coordinated knowledge together. Quantitative evaluation and robustness analysis on the standard benchmarks demonstrate that the proposed network outperforms the state-of-the-art in most cases.",0
"A new approach for scene text recognition called Parallel Scale-wise Attention Network (PSANet) was introduced in our paper. This method enhances the attention mechanism to perform parallel scale estimation and scale selection based on different aspect ratios at multiple scales. PSANet uses two branches: a thickness branch that estimates scale by detecting text boundaries using Hough Transform, and a scale branch that selects the optimal scale among all possible scales. In contrast to traditional single-scale methods, the proposed method can simultaneously handle texts of varying sizes with high accuracy while achieving better speed due to efficient feature reuse. We evaluated our model on three benchmark datasets and achieved state-of-the-art performance in terms of both accuracy and efficiency. Our work offers significant advancements in the field of scene text recognition and paves the way for future research in computer vision.",1
"We present a machine learning method to predict extreme hydrologic events from spatially and temporally varying hydrological and meteorological data. We used a timestep reduction technique to reduce the computational and memory requirements and trained a bidirection LSTM network to predict soil water and stream flow from time series data observed and simulated over eighty years in the Wabash River Watershed. We show that our simple model can be trained much faster than complex attention networks such as GeoMAN without sacrificing accuracy. Based on the predicted values of soil water and stream flow, we predict the occurrence and severity of extreme hydrologic events such as droughts. We also demonstrate that extreme events can be predicted in geographical locations separate from locations observed during the training process. This spatially-inductive setting enables us to predict extreme events in other areas in the US and other parts of the world using our model trained with the Wabash Basin data.",0
"In recent years, the frequency and severity of extreme hydrological events such as floods and droughts have increased due to climate change and other natural factors. Accurate predictions of these events can help communities prepare for potential disasters, reduce damage and loss of life, and improve resilience. This study focuses on predicting extreme hydrologic events using data from the Wabash River watershed, which spans across Indiana and Illinois. We use advanced machine learning techniques based on spatial interpolation of historical precipitation and streamflow records to develop models that make inductive predictions of future extreme events. Our results show promising accuracy in predicting both heavy rainfall events and low flow conditions, highlighting the utility of our approach for effective preparedness measures in the region. Ultimately, further research is necessary to refine these models and evaluate their performance under different climatic scenarios. However, our findings suggest significant potential for improving forecasting capabilities in regions affected by extreme weather conditions.",1
"Data-driven methods for battery lifetime prediction are attracting increasing attention for applications in which the degradation mechanisms are poorly understood and suitable training sets are available. However, while advanced machine learning and deep learning methods promise high performance with minimal data preprocessing, simpler linear models with engineered features often achieve comparable performance, especially for small training sets, while also providing physical and statistical interpretability. In this work, we use a previously published dataset to develop simple, accurate, and interpretable data-driven models for battery lifetime prediction. We first present the ""capacity matrix"" concept as a compact representation of battery electrochemical cycling data, along with a series of feature representations. We then create a number of univariate and multivariate models, many of which achieve comparable performance to the highest-performing models previously published for this dataset. These models also provide insights into the degradation of these cells. Our approaches can be used both to quickly train models for a new dataset and to benchmark the performance of more advanced machine learning methods.",0
"Battery lifetime is an important factor in many applications such as electric vehicles (EVs) and smartphones where energy storage determines reliability and cost. Although research has been conducted on predicting battery lifetimes using machine learning techniques, these approaches often lack interpretability which hinders our understanding of how different factors affect the degradation process. In this study, we propose to use statistical learning methods that can provide both high accuracy and interpretability in predicting battery lifetime. We demonstrate the effectiveness of our approach by testing it on real-world datasets obtained from EVs and smartphones, achieving promising results compared to existing models. Our method provides insights into how different factors impact battery performance, allowing for better design decisions and improved maintenance strategies for batteries. Overall, our work contributes towards developing reliable and sustainable solutions for managing battery-powered systems while prioritizing transparency and explainability in model predictions.",1
"Hyperspectral image (HSI) clustering, which aims at dividing hyperspectral pixels into clusters, has drawn significant attention in practical applications. Recently, many graph-based clustering methods, which construct an adjacent graph to model the data relationship, have shown dominant performance. However, the high dimensionality of HSI data makes it hard to construct the pairwise adjacent graph. Besides, abundant spatial structures are often overlooked during the clustering procedure. In order to better handle the high dimensionality problem and preserve the spatial structures, this paper proposes a novel unsupervised approach called spatial-spectral clustering with anchor graph (SSCAG) for HSI data clustering. The SSCAG has the following contributions: 1) the anchor graph-based strategy is used to construct a tractable large graph for HSI data, which effectively exploits all data points and reduces the computational complexity; 2) a new similarity metric is presented to embed the spatial-spectral information into the combined adjacent graph, which can mine the intrinsic property structure of HSI data; 3) an effective neighbors assignment strategy is adopted in the optimization, which performs the singular value decomposition (SVD) on the adjacent graph to get solutions efficiently. Extensive experiments on three public HSI datasets show that the proposed SSCAG is competitive against the state-of-the-art approaches.",0
"In this paper we propose a new method called spatial spectral clustering using anchor graph (SSCAG). This technique uses both spatial information and spectral data from hyperspectral images to achieve better accuracy in image segmentation tasks. Our approach utilizes an anchor graph constructed by selecting points with distinctive features that serve as landmarks for defining neighborhoods within each pixelâ€™s local region. Using these neighborhoods allows us to capture both spatial and spectral details and enhances the separation of homogeneous regions within the image. We evaluate our proposed method on several challenging benchmark datasets and show significant improvements over traditional methods. Overall, SSCAG provides a simple yet effective framework for integrating richer spatial-spectral representations into hyperspectral imagery analysis, offering opportunities for advancing related research problems such as semantic scene understanding, target detection, and environmental monitoring.",1
"False data injection attacks (FDIA) are becoming an active avenue of research as such attacks are more frequently encountered in power systems. Contrary to the detection of these attacks, less attention has been paid to identifying the attacked units of the grid. To this end, this work jointly studies detecting and localizing the stealth FDIA in modern power grids. Exploiting the inherent graph topology of power systems as well as the spatial correlations of smart meters' data, this paper proposes an approach based on the graph neural network (GNN) to identify the presence and location of the FDIA. The proposed approach leverages the auto-regressive moving average (ARMA) type graph convolutional filters which offer better noise robustness and frequency response flexibility compared to the polynomial type graph convolutional filters such as Chebyshev. To the best of our knowledge, this is the first work based on GNN that automatically detects and localizes FDIA in power systems. Extensive simulations and visualizations show that the proposed approach outperforms the available methods in both detection and localization FDIA for different IEEE test systems. Thus, the targeted areas in power grids can be identified and preventive actions can be taken before the attack impacts the grid.",0
"Attackers can manipulate smart grid data by launching stealth false data injection (FDI) attacks that alter sensor readings without being detected. To address this challenge, we propose a joint detection and localization approach based on graph neural networks (GNNs). We use GNNs to model the relationships among nodes in the grid network, which allows us to identify anomalies indicative of FDI attacks. Our method learns attack signatures from historical data, enabling real-time detection of novel attacks. Moreover, our approach locally pinpoints the locations of attack sources, allowing operators to isolate affected regions quickly. Experimental results demonstrate the effectiveness of our framework, achieving high accuracy rates across several scenarios. Overall, our work advances the state of art in securing critical infrastructure systems against cyber threats.",1
"We introduce Video Transformer (VidTr) with separable-attention for video classification. Comparing with commonly used 3D networks, VidTr is able to aggregate spatio-temporal information via stacked attentions and provide better performance with higher efficiency. We first introduce the vanilla video transformer and show that transformer module is able to perform spatio-temporal modeling from raw pixels, but with heavy memory usage. We then present VidTr which reduces the memory cost by 3.3$\times$ while keeping the same performance. To further compact the model, we propose the standard deviation based topK pooling attention, which reduces the computation by dropping non-informative features. VidTr achieves state-of-the-art performance on five commonly used dataset with lower computational requirement, showing both the efficiency and effectiveness of our design. Finally, error analysis and visualization show that VidTr is especially good at predicting actions that require long-term temporal reasoning. The code and pre-trained weights will be released.",0
"Title: VidTr: Video Transformer Without Convolutions We present VidTr, a novel video architecture that leverages transformers without convolutional layers. Our method is inspired by recent breakthroughs in image processing tasks using vision transformer models like ViT (Vision Transformer) and DeiT (Data Efficient Image Transformers). Unlike these works which process static images as input, our architecture processes temporal inputs such as frames from videos. To achieve good results on high resolution datasets, we introduce tokenization techniques specifically designed for VidTr. Furthermore, we propose different model variations based on multi-layer perceptrons (MLPs), linear layers and non-linear layers (ReLU activations). In experiments, we demonstrate that VidTr achieves state-of-the-art performance across various video understanding benchmarks such as UCF101, HMDB51, Something-Something V2 and Kinetics400. Additionally, we provide analysis showing how our approach compares favorably against existing models across accuracy metrics. By extending transformer architectures to video data, VidTr represents a powerful alternative in achieving competitive performance compared to traditional methods reliant on convolutions.",1
"This work is devoted to the finite sample prediction risk analysis of a class of linear predictors of a response $Y\in \mathbb{R}$ from a high-dimensional random vector $X\in \mathbb{R}^p$ when $(X,Y)$ follows a latent factor regression model generated by a unobservable latent vector $Z$ of dimension less than $p$. Our primary contribution is in establishing finite sample risk bounds for prediction with the ubiquitous Principal Component Regression (PCR) method, under the factor regression model, with the number of principal components adaptively selected from the data -- a form of theoretical guarantee that is surprisingly lacking from the PCR literature. To accomplish this, we prove a master theorem that establishes a risk bound for a large class of predictors, including the PCR predictor as a special case. This approach has the benefit of providing a unified framework for the analysis of a wide range of linear prediction methods, under the factor regression setting. In particular, we use our main theorem to recover known risk bounds for the minimum-norm interpolating predictor, which has received renewed attention in the past two years, and a prediction method tailored to a subclass of factor regression models with identifiable parameters. This model-tailored method can be interpreted as prediction via clusters with latent centers.   To address the problem of selecting among a set of candidate predictors, we analyze a simple model selection procedure based on data-splitting, providing an oracle inequality under the factor model to prove that the performance of the selected predictor is close to the optimal candidate. We conclude with a detailed simulation study to support and complement our theoretical results.",0
"Title: ""Adapting to Change in Latent Factor Regression""  This study explores novel methods for predicting outcomes in situations where there may be unforeseen changes or shifts in underlying relationships. Specifically, we focus on the field of latent factor regression, which models hidden variables that cannot be directly observed but may influence observable data. Traditional approaches to latent factor regression assume static relationships between factors, but real-world scenarios often involve change and adaptation over time. Therefore, we propose two new techniques designed to handle such dynamic environments: adaptive principal component regression (PCR) and a more general framework for online learning with expert advice (OLE). Our empirical evaluations demonstrate that both methods can effectively track changing relationships and improve prediction accuracy compared to standard approaches under varying conditions. These findings have important implications for researchers working with complex systems and big data, as well as practitioners seeking to make accurate predictions in rapidly evolving contexts.",1
"Image quality assessment (IQA) aims to assess the perceptual quality of images. The outputs of the IQA algorithms are expected to be consistent with human subjective perception. In image restoration and enhancement tasks, images generated by generative adversarial networks (GAN) can achieve better visual performance than traditional CNN-generated images, although they have spatial shift and texture noise. Unfortunately, the existing IQA methods have unsatisfactory performance on the GAN-based distortion partially because of their low tolerance to spatial misalignment. To this end, we propose the reference-oriented deformable convolution, which can improve the performance of an IQA network on GAN-based distortion by adaptively considering this misalignment. We further propose a patch-level attention module to enhance the interaction among different patch regions, which are processed independently in previous patch-based methods. The modified residual block is also proposed by applying modifications to the classic residual block to construct a patch-region-based baseline called WResNet. Equipping this baseline with the two proposed modules, we further propose Region-Adaptive Deformable Network (RADN). The experiment results on the NTIRE 2021 Perceptual Image Quality Assessment Challenge dataset show the superior performance of RADN, and the ensemble approach won fourth place in the final testing phase of the challenge. Code is available at https://github.com/IIGROUP/RADN.",0
"Image quality assessment (QA) plays a critical role in evaluating how well digital images meet certain visual fidelity standards. With increasing demand on image processing applications such as computational photography and photo sharing platforms, new approaches have been proposed using convolutional neural networks (CNNs), which can achieve promising results by learning representative features directly from raw data without prior knowledge engineering. Although these methods already outperform traditional metrics such as mean squared error (MSE) or structural similarity index (SSIM), their performance often depends highly on their global parameters shared across all regions within one image, resulting in insufficient adaptiveness to local content changes. To tackle this challenge, we present a novel region-adaptive deformable network (RADNet) that dynamically partitions input images into multiple subregions, each with an independent subnetwork responsible for computing perceptual quality scores according to local contents. This design allows RADNet to capture spatial variations at different levels of granularity while maintaining computational efficiency during inference time through parallel processing. Extensive experimental studies showcase the superior QA accuracy obtained over other state-of-the-art competitors under varying scenarios, including complex real-world scenes with diverse textures and challenging conditions, paving the way towards advanced large-scale image QA tasks.",1
"Mesh representation by random walks has been shown to benefit deep learning. Randomness is indeed a powerful concept. However, it comes with a price: some walks might wander around non-characteristic regions of the mesh, which might be harmful to shape analysis, especially when only a few walks are utilized. We propose a novel walk-attention mechanism that leverages the fact that multiple walks are used. The key idea is that the walks may provide each other with information regarding the meaningful (attentive) features of the mesh. We utilize this mutual information to extract a single descriptor of the mesh. This differs from common attention mechanisms that use attention to improve the representation of each individual descriptor. Our approach achieves SOTA results for two basic 3D shape analysis tasks: classification and retrieval. Even a handful of walks along a mesh suffice for learning.",0
"As cities become increasingly dense, pedestrians face safety concerns while navigating busy streets and crosswalks. In recent years, research has focused on developing intelligent systems that can enhance the visibility of pedestrians at road intersections. This study presents AttWalk - an attentive system designed to analyze deep meshes of real-world streetscapes and identify suitable locations for deploying intelligent traffic lights or smart crosswalks. Our proposed method employs computer vision techniques, including object detection, tracking, depth perception, and edge detection, enabling efficient detection and analysis of human motion patterns near junctions. We evaluate our approach using simulated scenarios and report promising results in terms of accuracy, efficiency, and adaptability. Overall, this work offers valuable insights into improving urban infrastructure planning by addressing the needs of pedestrian safety within modern transportation networks.",1
"The automatic design of architectures for neural networks, Neural Architecture Search, has gained a lot of attention over the recent years, as the thereby created networks repeatedly broke state-of-the-art results for several disciplines. The network search spaces are often finite and designed by hand, in a way that a fixed and small number of decisions constitute a specific architecture. Given these circumstances, inter-choice dependencies are likely to exist and affect the network search, but are unaccounted for in the popular one-shot methods. We extend the Single-Path One-Shot search-networks with additional weights that depend on combinations of choices and analyze their effect. Experiments in NAS-Bench 201 and SubImageNet based search spaces show an improved super-network performance in only-convolutions settings and that the overhead is nearly negligible for sequential network designs.",0
"Here is an example of how you can write an abstract for your paper:  Abstract: In recent years, there has been increasing interest in developing algorithms that leverage inter-choice dependencies in recommendation tasks. Traditional methods have often struggled to capture these complex relationships between items, resulting in suboptimal recommendations. Our approach addresses this challenge by introducing the concept of inter-choice dependent super-networks (IDSNs). These IDSNs explicitly model item correlations across different user choices, allowing us to more accurately predict which items users are most likely to interact with next. We evaluate our method on several real-world datasets and show that it significantly outperforms baseline models in terms of accuracy and diversity metrics. This work represents a step forward towards building more effective and personalized recommendation systems that better meet user needs.",1
"Text-based image captioning (TextCap) which aims to read and reason images with texts is crucial for a machine to understand a detailed and complex scene environment, considering that texts are omnipresent in daily life. This task, however, is very challenging because an image often contains complex texts and visual information that is hard to be described comprehensively. Existing methods attempt to extend the traditional image captioning methods to solve this task, which focus on describing the overall scene of images by one global caption. This is infeasible because the complex text and visual information cannot be described well within one caption. To resolve this difficulty, we seek to generate multiple captions that accurately describe different parts of an image in detail. To achieve this purpose, there are three key challenges: 1) it is hard to decide which parts of the texts of images to copy or paraphrase; 2) it is non-trivial to capture the complex relationship between diverse texts in an image; 3) how to generate multiple captions with diverse content is still an open problem. To conquer these, we propose a novel Anchor-Captioner method. Specifically, we first find the important tokens which are supposed to be paid more attention to and consider them as anchors. Then, for each chosen anchor, we group its relevant texts to construct the corresponding anchor-centred graph (ACG). Last, based on different ACGs, we conduct multi-view caption generation to improve the content diversity of generated captions. Experimental results show that our method not only achieves SOTA performance but also generates diverse captions to describe images.",0
"Abstract:  In recent years, there has been significant progress in developing algorithms that can generate natural language descriptions (captions) of images. However, current text-based image captioning models often suffer from limited content diversity, resulting in repetitive and uniform outputs. This paper presents a novel approach towards generating more diverse and accurate textual representations of images. We propose exploring multiple perspectives on the input data by using different pretrained language models and attention mechanisms, combined with visual attention modules, in order to enhance diversity while still maintaining accuracy. Our method leverages the strengths of both image recognition models and text generation techniques, allowing us to capture fine-grained details and nuances present in the given image. Extensive experiments demonstrate the effectiveness of our framework in achieving superior performance compared to existing methods across several benchmark datasets. Additionally, we provide human evaluation results to showcase the improved quality and diversity of generated captions. Overall, this work represents a step forward towards creating richer and more meaningful image descriptions.",1
"Model Predictive Controllers (MPC) require a good model for the controlled process. In this paper I infer inductive biases about a physical system. I use these biases to derive a new neural network architecture that can model this real system that has noise and inertia. The main inductive biases exploited here are: the delayed impact of some inputs on the system and the separability between the temporal component and how the inputs interact to produce the output of a system. The inputs are independently delayed using shifted convolutional kernels. Feature interactions are modelled using a fully connected network that does not have access to temporal information. The available data and the problem setup allow the usage of Self Supervised Learning in order to train the models. The baseline architecture is an Attention based Reccurent network adapted to work with MPC like inputs. The proposed networks are faster, better at exploiting larger data volumes and are almost as good as baseline networks in terms of prediction performance. The proposed architecture family called Delay can be used in a real scenario to control systems with delayed responses with respect to its controls or inputs. Ablation studies show that the presence of delay kernels are vital to obtain any learning in proposed architecture. Code and some experimental data are available online.",0
"This paper presents a study on how inductive biases can be used effectively in self supervised learning techniques. Specifically, we focus on applying these methods to modeling a physical heating system. We begin by discussing existing literature and previous research in this area, then describe our approach to using inductive biases to improve performance. Our results show that this method leads to significant improvements over traditional training methods. Finally, we conclude with a discussion of potential future directions for this work.",1
"Object handover is a common human collaboration behavior that attracts attention from researchers in Robotics and Cognitive Science. Though visual perception plays an important role in the object handover task, the whole handover process has been specifically explored. In this work, we propose a novel rich-annotated dataset, H2O, for visual analysis of human-human object handovers. The H2O, which contains 18K video clips involving 15 people who hand over 30 objects to each other, is a multi-purpose benchmark. It can support several vision-based tasks, from which, we specifically provide a baseline method, RGPNet, for a less-explored task named Receiver Grasp Prediction. Extensive experiments show that the RGPNet can produce plausible grasps based on the giver's hand-object states in the pre-handover phase. Besides, we also report the hand and object pose errors with existing baselines and show that the dataset can serve as the video demonstrations for robot imitation learning on the handover task. Dataset, model and code will be made public.",0
"This would give away that you know something about the specifics of the project! Please instead write a generic abstract which could apply broadly across any research area. You may use whatever key phrases, terms etc come naturally as part of your explanation, but try not to reference human object handover directly if possible (i.e., don't mention ""H2O"" specifically).",1
"Recently, directly detecting 3D objects from 3D point clouds has received increasing attention. To extract object representation from an irregular point cloud, existing methods usually take a point grouping step to assign the points to an object candidate so that a PointNet-like network could be used to derive object features from the grouped points. However, the inaccurate point assignments caused by the hand-crafted grouping scheme decrease the performance of 3D object detection.   In this paper, we present a simple yet effective method for directly detecting 3D objects from the 3D point cloud. Instead of grouping local points to each object candidate, our method computes the feature of an object from all the points in the point cloud with the help of an attention mechanism in the Transformers \cite{vaswani2017attention}, where the contribution of each point is automatically learned in the network training. With an improved attention stacking scheme, our method fuses object features in different stages and generates more accurate object detection results. With few bells and whistles, the proposed method achieves state-of-the-art 3D object detection performance on two widely used benchmarks, ScanNet V2 and SUN RGB-D. The code and models are publicly available at \url{https://github.com/zeliu98/Group-Free-3D}",0
â€‹,1
"Best group subset selection aims to choose a small part of non-overlapping groups to achieve the best interpretability on the response variable. It is practically attractive for group variable selection; however, due to the computational intractability in high dimensionality setting, it doesn't catch enough attention. To fill the blank of efficient algorithms for best group subset selection, in this paper, we propose a group-splicing algorithm that iteratively detects effective groups and excludes the helpless ones. Moreover, coupled with a novel Bayesian group information criterion, an adaptive algorithm is developed to determine the true group subset size. It is certifiable that our algorithms enable identifying the optimal group subset in polynomial time under mild conditions. We demonstrate the efficiency and accuracy of our proposal by comparing state-of-the-art algorithms on both synthetic and real-world datasets.",0
"Include keywords related to polynomial time algorithms, computational biology and machine learning. No longer than 2 pages.",1
"Depth estimation from a stereo image pair has become one of the most explored applications in computer vision, with most of the previous methods relying on fully supervised learning settings. However, due to the difficulty in acquiring accurate and scalable ground truth data, the training of fully supervised methods is challenging. As an alternative, self-supervised methods are becoming more popular to mitigate this challenge. In this paper, we introduce the H-Net, a deep-learning framework for unsupervised stereo depth estimation that leverages epipolar geometry to refine stereo matching. For the first time, a Siamese autoencoder architecture is used for depth estimation which allows mutual information between the rectified stereo images to be extracted. To enforce the epipolar constraint, the mutual epipolar attention mechanism has been designed which gives more emphasis to correspondences of features which lie on the same epipolar line while learning mutual information between the input stereo pair. Stereo correspondences are further enhanced by incorporating semantic information to the proposed attention mechanism. More specifically, the optimal transport algorithm is used to suppress attention and eliminate outliers in areas not visible in both cameras. Extensive experiments on KITTI2015 and Cityscapes show that our method outperforms the state-ofthe-art unsupervised stereo depth estimation methods while closing the gap with the fully supervised approaches.",0
"An unsupervised method has been developed that can estimate depth from single-view images using attention-based models guided by epipolar geometry constraints. This approach leverages knowledge of camera motion obtained through epipolar lines to better align feature representations across views while minimizing errors caused by occlusions and large baseline disparities commonly present in natural scenes. Experimental results on publicly available datasets demonstrate the effectiveness of our method compared to other state-of-the-art techniques, particularly under challenging conditions where ground truth data is limited or absent. Our findings highlight the importance of incorporating geometric constraints during training to improve performance without requiring costly annotations, making our method well suited for real-world applications involving visual navigation, robotics, and autonomous driving.",1
"Graph convolutional networks (GCNs) have been very successful in modeling non-Euclidean data structures, like sequences of body skeletons forming actions modeled as spatio-temporal graphs. Most GCN-based action recognition methods use deep feed-forward networks with high computational complexity to process all skeletons in an action. This leads to a high number of floating point operations (ranging from 16G to 100G FLOPs) to process a single sample, making their adoption in restricted computation application scenarios infeasible. In this paper, we propose a temporal attention module (TAM) for increasing the efficiency in skeleton-based action recognition by selecting the most informative skeletons of an action at the early layers of the network. We incorporate the TAM in a light-weight GCN topology to further reduce the overall number of computations. Experimental results on two benchmark datasets show that the proposed method outperforms with a large margin the baseline GCN-based method while having 2.9 times less number of computations. Moreover, it performs on par with the state-of-the-art with up to 9.6 times less number of computations.",0
"Artificial intelligence (AI) has been rapidly evolving over the past decade, leading to numerous advancements in fields such as computer vision and natural language processing. With respect to skeleton-based human action recognition, state-of-the-art approaches tend to rely on graph convolutional networks (GCNs). Despite their effectiveness, these GCN models suffer from two main limitations: they struggle to capture temporal dependencies in data due to the use of fixed graphs; and they can become computationally expensive during training and inference. This work introduces Temporal Attention-Augmented Graph Convolutional Networks (TA-GCNs), which aim to address both challenges by incorporating temporal attention mechanisms into traditional GCN architectures. TA-GCNs enable efficient modeling of temporal dynamics while reducing computational complexity through an adaptive sampling mechanism that dynamically selects informative samples along the time axis. Our experiments demonstrate the superiority of our approach compared to other state-of-the-art methods across several benchmark datasets, thereby confirming the effectiveness of our proposed solution for skeleton-based human action recognition tasks.",1
"Graph convolutional networks (GCNs) achieved promising performance in skeleton-based human action recognition by modeling a sequence of skeletons as a spatio-temporal graph. Most of the recently proposed GCN-based methods improve the performance by learning the graph structure at each layer of the network using a spatial attention applied on a predefined graph Adjacency matrix that is optimized jointly with model's parameters in an end-to-end manner. In this paper, we analyze the spatial attention used in spatio-temporal GCN layers and propose a symmetric spatial attention for better reflecting the symmetric property of the relative positions of the human body joints when executing actions. We also highlight the connection of spatio-temporal GCN layers employing additive spatial attention to bilinear layers, and we propose the spatio-temporal bilinear network (ST-BLN) which does not require the use of predefined Adjacency matrices and allows for more flexible design of the model. Experimental results show that the three models lead to effectively the same performance. Moreover, by exploiting the flexibility provided by the proposed ST-BLN, one can increase the efficiency of the model.",0
"Spatial Attention Mechanisms have been shown to significantly improve performance in numerous computer vision tasks such as object detection and image classification. However, their application on graph convolutional networks (GCN) has not yet been explored extensively for skeleton-based human action recognition. In this work, we aim to address that gap by proposing a novel approach which incorporates Spatio-Temporal Graph Convolutional Networks (STGCN) along with Spatial Attention Mechanism (SAM). Our proposed model enables STGCN to automatically select important key-points from sequences of human joint data and focus on the most informative spatiotemporal features leading to improved accuracy over existing state-of-the-art methods. Furthermore, through extensive experimentation using two publicly available datasets, we demonstrate the effectiveness of our approach and provide insight into the importance of incorporating SAM in GCN based models for human action recognition task. Overall, this work presents a new direction towards achieving better results on challenging real-world applications of human activity analysis.",1
"Aerial scene recognition is a fundamental visual task and has attracted an increasing research interest in the last few years. Most of current researches mainly deploy efforts to categorize an aerial image into one scene-level label, while in real-world scenarios, there often exist multiple scenes in a single image. Therefore, in this paper, we propose to take a step forward to a more practical and challenging task, namely multi-scene recognition in single images. Moreover, we note that manually yielding annotations for such a task is extraordinarily time- and labor-consuming. To address this, we propose a prototype-based memory network to recognize multiple scenes in a single image by leveraging massive well-annotated single-scene images. The proposed network consists of three key components: 1) a prototype learning module, 2) a prototype-inhabiting external memory, and 3) a multi-head attention-based memory retrieval module. To be more specific, we first learn the prototype representation of each aerial scene from single-scene aerial image datasets and store it in an external memory. Afterwards, a multi-head attention-based memory retrieval module is devised to retrieve scene prototypes relevant to query multi-scene images for final predictions. Notably, only a limited number of annotated multi-scene images are needed in the training phase. To facilitate the progress of aerial scene recognition, we produce a new multi-scene aerial image (MAI) dataset. Experimental results on variant dataset configurations demonstrate the effectiveness of our network. Our dataset and codes are publicly available.",0
"In this work, we explore aerial scene understanding by developing multi-scene recognition techniques using prototype-based memory networks (ProtoMN). Our approach allows us to accurately identify objects across multiple scenes within a given image. We train our model on large scale datasets and demonstrate outstanding performance compared to state-of-the art methods. Furthermore, we provide detailed analysis of the proposed methodâ€™s effectiveness through comprehensive experiments and visualizations. Overall, this research has the potential to improve the capabilities of aerial scene understanding technology in real world applications such as search & rescue missions and surveillance operations.",1
"Federated learning (FL) is an emerging paradigm for facilitating multiple organizations' data collaboration without revealing their private data to each other. Recently, vertical FL, where the participating organizations hold the same set of samples but with disjoint features and only one organization owns the labels, has received increased attention. This paper presents several feature inference attack methods to investigate the potential privacy leakages in the model prediction stage of vertical FL. The attack methods consider the most stringent setting that the adversary controls only the trained vertical FL model and the model predictions, relying on no background information. We first propose two specific attacks on the logistic regression (LR) and decision tree (DT) models, according to individual prediction output. We further design a general attack method based on multiple prediction outputs accumulated by the adversary to handle complex models, such as neural networks (NN) and random forest (RF) models. Experimental evaluations demonstrate the effectiveness of the proposed attacks and highlight the need for designing private mechanisms to protect the prediction outputs in vertical FL.",0
"This paper presents Feature Inference Attack (FIA), a novel attack against vertically partitioned federated learning systems, where one party shares data features but computes locally before sending predictions back to others. Our work shows that under certain conditions it is possible to infer valuable information from model predictions alone without requiring any further access. We discuss our motivations behind developing FIA, as well as technical details including algorithm design, evaluation metrics used to measure success rate, how we tested effectiveness across different threat models, and discussion of mitigation strategies. Finally, we provide insight into future directions for research in securing machine learning in vertical FL scenarios.",1
"Recently the vision transformer (ViT) architecture, where the backbone purely consists of self-attention mechanism, has achieved very promising performance in visual classification. However, the high performance of the original ViT heavily depends on pretraining using ultra large-scale datasets, and it significantly underperforms on ImageNet-1K if trained from scratch. This paper makes the efforts toward addressing this problem, by carefully considering the role of visual tokens. First, for classification head, existing ViT only exploits class token while entirely neglecting rich semantic information inherent in high-level visual tokens. Therefore, we propose a new classification paradigm, where the second-order, cross-covariance pooling of visual tokens is combined with class token for final classification. Meanwhile, a fast singular value power normalization is proposed for improving the second-order pooling. Second, the original ViT employs the naive embedding of fixed-size image patches, lacking the ability to model translation equivariance and locality. To alleviate this problem, we develop a light-weight, hierarchical module based on off-the-shelf convolutions for visual token embedding. The proposed architecture, which we call So-ViT, is thoroughly evaluated on ImageNet-1K. The results show our models, when trained from scratch, outperform the competing ViT variants, while being on par with or better than state-of-the-art CNN models. Code is available at https://github.com/jiangtaoxie/So-ViT",0
"""So-ViT"" stands for ""Spatial Oversampling and Visual Tokenization"" which is our novel technique that addresses two key limitations found within current transformer based architectures used in image analysis; firstly by mitigating the issue of sparse attention masks generated via self attention mechanisms and secondly by reducing token length. Our approach takes inspiration from ViTs (Visual Transformers) while incorporating spatial oversampling into multihead block architecture. We propose an improved visual tokenizer model capable of processing large inputs without having a significant impact on computational complexity, through downsampling. By combining these enhancements we show the ability to achieve state of the art results on multiple benchmark datasets across both object detection as well as semantic segmentation tasks. -----",1
"Vehicle Re-Identification (Re-ID) aims to identify the same vehicle across different cameras, hence plays an important role in modern traffic management systems. The technical challenges require the algorithms must be robust in different views, resolution, occlusion and illumination conditions. In this paper, we first analyze the main factors hindering the Vehicle Re-ID performance. We then present our solutions, specifically targeting the dataset Track 2 of the 5th AI City Challenge, including (1) reducing the domain gap between real and synthetic data, (2) network modification by stacking multi heads with attention mechanism, (3) adaptive loss weight adjustment. Our method achieves 61.34% mAP on the private CityFlow testset without using external dataset or pseudo labeling, and outperforms all previous works at 87.1% mAP on the Veri benchmark. The code is available at https://github.com/cybercore-co-ltd/track2_aicity_2021.",0
"A vehicle re-identification model should accurately distinguish vehicles under different conditions while maintaining robustness against changes such as lighting variations and occlusions caused by objects like pedestrians and other cars. Our proposed method utilizes discriminative features extracted from high-level layers of convolutional neural networks (CNNs) pretrained on large datasets of natural images for general object recognition. To demonstrate our approachâ€™s effectiveness, we compare it with state-of-the-art methods that rely on handcrafted descriptors or feature learning in specialized architectures tailored specifically for vehicle re-identification tasks. Experimental results using publicly available benchmarks show a clear improvement over existing methods across varying metrics, particularly under challenging conditions where most competitors struggle significantly. Overall, our strong baseline offers new insights into leveraging generic CNN models for high-quality vehicle re-identification without any domain knowledge transfer or fine-tuning, making it an attractive alternative for real-world deployment.",1
"Graph Laplacian (GL)-based semi-supervised learning is one of the most used approaches for classifying nodes in a graph. Understanding and certifying the adversarial robustness of machine learning (ML) algorithms has attracted large amounts of attention from different research communities due to its crucial importance in many security-critical applied domains. There is great interest in the theoretical certification of adversarial robustness for popular ML algorithms. In this paper, we provide the first adversarial robust certification for the GL classifier. More precisely we quantitatively bound the difference in the classification accuracy of the GL classifier before and after an adversarial attack. Numerically, we validate our theoretical certification results and show that leveraging existing adversarial defenses for the $k$-nearest neighbor classifier can remarkably improve the robustness of the GL classifier.",0
"In this paper, we study the problem of certifying the correctness of deep learning models that operate over graph data structures, specifically in the context of Laplacian machine learning algorithms. We focus on the task of predicting node labels based on graph signals, which can represent different types of information such as node attributes, edge weights, or other features extracted from the graph topology. Our work addresses the challenges posed by the irregularity and sparsity of graphs and provides efficient methods for obtaining accurate and meaningful certificates. This paper makes several contributions: (i) A new formulation of Laplacian regularization in deep learning that leads to robust model training and improved generalization performance; (ii) An effective approach to computing lower bounds on the test error of Laplacian neural networks using randomized spectral techniques; (iii) A methodology for generating synthetic adversarial examples that reveal insights into the stability of the learned models under small perturbations; and (iv) Numerical experiments demonstrating the efficacy of our proposed framework across diverse datasets and architectures. Overall, these results have implications for both theoretical understanding of geometric graph learning and practical application of deep models on complex real-world graphs.",1
"Video classification researches that have recently attracted attention are the fields of temporal modeling and 3D efficient architecture. However, the temporal modeling methods are not efficient or the 3D efficient architecture is less interested in temporal modeling. For bridging the gap between them, we propose an efficient temporal modeling 3D architecture, called VoV3D, that consists of a temporal one-shot aggregation (T-OSA) module and depthwise factorized component, D(2+1)D. The T-OSA is devised to build a feature hierarchy by aggregating temporal features with different temporal receptive fields. Stacking this T-OSA enables the network itself to model short-range as well as long-range temporal relationships across frames without any external modules. Inspired by kernel factorization and channel factorization, we also design a depthwise spatiotemporal factorization module, named, D(2+1)D that decomposes a 3D depthwise convolution into two spatial and temporal depthwise convolutions for making our network more lightweight and efficient. By using the proposed temporal modeling method (T-OSA), and the efficient factorized component (D(2+1)D), we construct two types of VoV3D networks, VoV3D-M and VoV3D-L. Thanks to its efficiency and effectiveness of temporal modeling, VoV3D-L has 6x fewer model parameters and 16x less computation, surpassing a state-of-the-art temporal modeling method on both Something-Something and Kinetics-400. Furthermore, VoV3D shows better temporal modeling ability than a state-of-the-art efficient 3D architecture, X3D having comparable model capacity. We hope that VoV3D can serve as a baseline for efficient video classification.",0
"This paper presents two novel techniques for improving video classification efficiency: diverse temporal aggregation and depthwise spatiotemporal factorization. We demonstrate that these methods significantly reduce computational cost while maintaining high accuracy on challenging benchmarks. Our approach incorporates both spatial and temporal feature representations by exploring different ways of combining features across multiple frames. By decomposing convolutional layers into smaller depthwise groups, we can further improve performance without increasing model complexity. Extensive experiments show that our proposed approaches achieve state-of-the-art results on popular video recognition datasets, making them well suited for real-world applications such as action detection and activity understanding. Overall, our work highlights the importance of efficient design strategies for computer vision models.",1
"In the Gastric Histopathology Image Classification (GHIC) tasks, which are usually weakly supervised learning missions, there is inevitably redundant information in the images. Therefore, designing networks that can focus on effective distinguishing features has become a popular research topic. In this paper, to accomplish the tasks of GHIC superiorly and to assist pathologists in clinical diagnosis, an intelligent Hierarchical Conditional Random Field based Attention Mechanism (HCRF-AM) model is proposed. The HCRF-AM model consists of an Attention Mechanism (AM) module and an Image Classification (IC) module. In the AM module, an HCRF model is built to extract attention regions. In the IC module, a Convolutional Neural Network (CNN) model is trained with the attention regions selected and then an algorithm called Classification Probability-based Ensemble Learning is applied to obtain the image-level results from patch-level output of the CNN. In the experiment, a classification specificity of 96.67% is achieved on a gastric histopathology dataset with 700 images. Our HCRF-AM model demonstrates high classification performance and shows its effectiveness and future potential in the GHIC field.",0
"This paper presents a novel approach that combines hierarchical conditional random field (HCRF) and attention mechanisms to classify gastric histopathology images accurately. Our approach builds upon recent advances in deep learning and computer vision techniques that have achieved state-of-the-art results on many tasks such as object detection and image classification. The HCRF captures global dependencies among different features of the histological image while our proposed attention mechanism takes into account local contextual information specific to individual patches within the image. By leveraging these complementary representations, we aim to improve the accuracy and robustness of the model. We evaluate our method on two public datasets comprising over 2700 images and compare its performance against other state-of-the-art approaches. Our experimental results show that the proposed method achieves better accuracy than most existing methods demonstrating its effectiveness at classifying gastric histopathology images. Overall, our work contributes to improving automated diagnostics in pathology by enabling more accurate and efficient evaluation of gastric biopsy samples.",1
"Advances in remote sensing technology have led to the capture of massive amounts of data. Increased image resolution, more frequent revisit times, and additional spectral channels have created an explosion in the amount of data that is available to provide analyses and intelligence across domains, including agriculture. However, the processing of this data comes with a cost in terms of computation time and money, both of which must be considered when the goal of an algorithm is to provide real-time intelligence to improve efficiencies. Specifically, we seek to identify nutrient deficient areas from remotely sensed data to alert farmers to regions that require attention; detection of nutrient deficient areas is a key task in precision agriculture as farmers must quickly respond to struggling areas to protect their harvests. Past methods have focused on pixel-level classification (i.e. semantic segmentation) of the field to achieve these tasks, often using deep learning models with tens-of-millions of parameters. In contrast, we propose a much lighter graph-based method to perform node-based classification. We first use Simple Linear Iterative Cluster (SLIC) to produce superpixels across the field. Then, to perform segmentation across the non-Euclidean domain of superpixels, we leverage a Graph Convolutional Neural Network (GCN). This model has 4-orders-of-magnitude fewer parameters than a CNN model and trains in a matter of minutes.",0
"This study presents a novel method for detecting nutrient deficiency stress using aerial imagery data. The proposed approach utilizes superpixels, which are groups of adjacent pixels that share similar properties, and graph convolutional neural networks (GCNN) to identify regions affected by nutrient deficiencies in crops. The use of superpixels allows us to efficiently segment large images into smaller, more manageable sections while preserving spatial relationships between neighboring pixels.  The GCNN framework provides robustness against noise and overfitting common in high resolution remote sensing data. By leveraging this state-of-the-art deep learning technique on a region proposals provided by superpixel segments, we can accurately classify nutrient deficient areas within aerial imagery. Experiments were conducted using two datasets collected under different nutritional conditions, one dataset was collected under normal healthy condition, another dataset includes plants subjected to Iron deficiency, Zinc deficiency, Phosphorous deficiency etc., where the focus was on quantitative evaluation based on mean accuracy metrics. We found that our proposed model outperformed traditional methods used in the field such as color indexing, vegetation indices and spectral bands selection methods. Furthermore, the results showed a significant improvement in accuracy compared to previous works utilizing deep learning techniques in agriculture domain. Our work can be further extended towards crop yield prediction applications, precision farming management systems, early disease diagnosis, irrigation scheduling, fertilizer recommendation and other tasks related to Agriculture science using advanced computer vision and Machine Learning technologies. In summary, we have presented a highly effective solution for nutrient deficiency detection through aerial imagery analysis using modern Computer Vision, Remote Sensing and Machine Learning approaches paving way for smart sustainable farming practices",1
"In this paper, we study the problem of text line recognition. Unlike most approaches targeting specific domains such as scene-text or handwritten documents, we investigate the general problem of developing a universal architecture that can extract text from any image, regardless of source or input modality. We consider two decoder families (Connectionist Temporal Classification and Transformer) and three encoder modules (Bidirectional LSTMs, Self-Attention, and GRCLs), and conduct extensive experiments to compare their accuracy and performance on widely used public datasets of scene and handwritten text. We find that a combination that so far has received little attention in the literature, namely a Self-Attention encoder coupled with the CTC decoder, when compounded with an external language model and trained on both public and internal data, outperforms all the others in accuracy and computational complexity. Unlike the more common Transformer-based models, this architecture can handle inputs of arbitrary length, a requirement for universal line recognition. Using an internal dataset collected from multiple sources, we also expose the limitations of current public datasets in evaluating the accuracy of line recognizers, as the relatively narrow image width and sequence length distributions do not allow to observe the quality degradation of the Transformer approach when applied to the transcription of long lines.",0
"This paper presents an overview of recent advances in text line recognition models. We discuss how traditional methods have been limited by their reliance on handcrafted features and predefined parameter settings, which hinder their ability to adapt to varying lighting conditions and image qualities. We then introduce novel deep learning approaches that leverage convolutional neural networks (CNNs) to automatically learn discriminative feature representations directly from images, thereby eliminating the need for manual engineering of features and reducing the dependence on ad-hoc parameters. Our experimental results demonstrate the superior performance of these models across multiple benchmark datasets under different challenges such as low resolution, noise, and complex background scenes. Finally, we explore future directions for research in this area and suggest potential applications in fields such as computer vision, document analysis, and digital archiving. Overall, our work represents a significant step forward in the development of accurate and robust text line recognition systems.",1
"Human gaze is known to be an intention-revealing signal in human demonstrations of tasks. In this work, we use gaze cues from human demonstrators to enhance the performance of agents trained via three popular imitation learning methods -- behavioral cloning (BC), behavioral cloning from observation (BCO), and Trajectory-ranked Reward EXtrapolation (T-REX). Based on similarities between the attention of reinforcement learning agents and human gaze, we propose a novel approach for utilizing gaze data in a computationally efficient manner, as part of an auxiliary loss function, which guides a network to have higher activations in image regions where the human's gaze fixated. This work is a step towards augmenting any existing convolutional imitation learning agent's training with auxiliary gaze data. Our auxiliary coverage-based gaze loss (CGL) guides learning toward a better reward function or policy, without adding any additional learnable parameters and without requiring gaze data at test time. We find that our proposed approach improves the performance by 95% for BC, 343% for BCO, and 390% for T-REX, averaged over 20 different Atari games. We also find that compared to a prior state-of-the-art imitation learning method assisted by human gaze (AGIL), our method achieves better performance, and is more efficient in terms of learning with fewer demonstrations. We further interpret trained CGL agents with a saliency map visualization method to explain their performance. At last, we show that CGL can help alleviate a well-known causal confusion problem in imitation learning.",0
"This paper presents a new method for efficiently guiding imitation learning agents using human gaze as input. In recent years, imitation learning has emerged as a promising approach to enabling artificial intelligence systems to learn complex tasks by observing and replicating human behavior. However, current methods require large amounts of data and computational resources, making them impractical for real-world applications. Our proposed method addresses these limitations by leveraging human gaze as a compact form of feedback that can effectively guide imitation learning algorithms towards successful task execution. We demonstrate through extensive experiments on both virtual reality and robotic platforms that our method significantly reduces the amount of training required while still achieving comparable levels of performance to state-of-the-art approaches. These results have important implications for deploying imitation learning in settings where time, computation, and data resources are limited, such as assistive robots, self-driving cars, and education. Overall, we believe this work represents a step forward in advancing the science of artificial intelligence, bringing us closer to creating intelligent agents that can seamlessly integrate into society and collaborate with humans.",1
"Despite the recent attention to DeepFakes, one of the most prevalent ways to mislead audiences on social media is the use of unaltered images in a new but false context. To address these challenges and support fact-checkers, we propose a new method that automatically detects out-of-context image and text pairs. Our key insight is to leverage the grounding of image with text to distinguish out-of-context scenarios that cannot be disambiguated with language alone. We propose a self-supervised training strategy where we only need a set of captioned images. At train time, our method learns to selectively align individual objects in an image with textual claims, without explicit supervision. At test time, we check if both captions correspond to the same object(s) in the image but are semantically different, which allows us to make fairly accurate out-of-context predictions. Our method achieves 85% out-of-context detection accuracy. To facilitate benchmarking of this task, we create a large-scale dataset of 200K images with 450K textual captions from a variety of news websites, blogs, and social media posts. The dataset and source code is publicly available at https://shivangi-aneja.github.io/projects/cosmos/.",0
"This work addresses the problem of out-of-context (OOC) misinformation on social media platforms such as Twitter. OOC misinformation refers to the spread of false claims that are taken out of context, which can cause confusion, harm, and even violence. Existing approaches for detecting misinformation rely heavily on large amounts of labeled data, making them difficult to deploy at scale. To address this challenge, we propose using self-supervised learning techniques to learn representations of tweets that capture their contextuality without relying on explicit labels. We introduce a new dataset called CODA consisting of over 47K annotated tweets from two domains (politics and science), along with expert annotations regarding the level of context present in each one. Using this dataset, we train several pretext tasks including text classification, next sentence prediction, and masked language model fine-tuning. Our results show that our models achieve state-of-the-art performance across multiple metrics for both in-domain and cross-domain detection of OOC misinformation. Furthermore, by applying these learned representations to unseen tweets in real time, we demonstrate the effectiveness of our approach in catching current instances of OOC misinformation circulating online. Our findings have important implications for developing scalable solutions for combatting misinformation on social media platforms.",1
"With the overwhelming popularity of Knowledge Graphs (KGs), researchers have poured attention to link prediction to fill in missing facts for a long time. However, they mainly focus on link prediction on binary relational data, where facts are usually represented as triples in the form of (head entity, relation, tail entity). In practice, n-ary relational facts are also ubiquitous. When encountering such facts, existing studies usually decompose them into triples by introducing a multitude of auxiliary virtual entities and additional triples. These conversions result in the complexity of carrying out link prediction on n-ary relational data. It has even proven that they may cause loss of structure information. To overcome these problems, in this paper, we represent each n-ary relational fact as a set of its role and role-value pairs. We then propose a method called NaLP to conduct link prediction on n-ary relational data, which explicitly models the relatedness of all the role and role-value pairs in an n-ary relational fact. We further extend NaLP by introducing type constraints of roles and role-values without any external type-specific supervision, and proposing a more reasonable negative sampling mechanism. Experimental results validate the effectiveness and merits of the proposed methods.",0
"This paper proposes a novel approach for link prediction that leverages both similarity and relatedness evaluation measures. Our methodology integrates multiple sources of evidence including: (a) graphical features capturing neighborhood patterns such as clustering coefficient; (b) structural features based on node properties such as degree centrality; and (c) semantic features which quantify how closely connected two nodes are based on their attribute values. We evaluate our framework using four real world datasets across domains demonstrating improved accuracy compared to state-of-the-art methods. Furthermore, we perform sensitivity analysis to examine impact of different feature sets used in predicting links among n-ary relations. Finally, the contributions presented herein have immediate implications for applications ranging from social network analysis to knowledge graphs, supply chain management, etc., where effective link prediction remains critical.",1
"Fatigue detection is valued for people to keep mental health and prevent safety accidents. However, detecting facial fatigue, especially mild fatigue in the real world via machine vision is still a challenging issue due to lack of non-lab dataset and well-defined algorithms. In order to improve the detection capability on facial fatigue that can be used widely in daily life, this paper provided an audiovisual dataset named DLFD (daily-life fatigue dataset) which reflected people's facial fatigue state in the wild. A framework using 3D-ResNet along with non-local attention mechanism was training for extraction of local and long-range features in spatial and temporal dimensions. Then, a compacted loss function combining mean squared error and cross-entropy was designed to predict both continuous and categorical fatigue degrees. Our proposed framework has reached an average accuracy of 90.8% on validation set and 72.5% on test set for binary classification, standing a good position compared to other state-of-the-art methods. The analysis of feature map visualization revealed that our framework captured facial dynamics and attempted to build a connection with fatigue state. Our experimental results in multiple metrics proved that our framework captured some typical, micro and dynamic facial features along spatiotemporal dimensions, contributing to the mild fatigue detection in the wild.",0
"Facial analysis has become increasingly important in various fields such as healthcare, psychology, and computer graphics. However, existing methods struggle to accurately detect daily facial fatigue due to factors like lighting variations and expression differences. In this work, we propose a novel approach using machine vision to tackle this problem by employing a nonlocal 3D attention mechanism. Our method is able to capture both local details and global context, enabling robust detection of subtle changes in facial appearance caused by fatigue. We evaluated our model on two challenging datasets and achieved state-of-the-art performance. This study shows great promise for real-world applications where timely recognition of facial fatigue can greatly benefit individuals and society at large.",1
"Neural networks-based learning of the distribution of non-dispatchable renewable electricity generation from sources such as photovoltaics (PV) and wind as well as load demands has recently gained attention. Normalizing flow density models have performed particularly well in this task due to the training through direct log-likelihood maximization. However, research from the field of image generation has shown that standard normalizing flows can only learn smeared-out versions of manifold distributions and can result in the generation of noisy data. To avoid the generation of time series data with unrealistic noise, we propose a dimensionality-reducing flow layer based on the linear principal component analysis (PCA) that sets up the normalizing flow in a lower-dimensional space. We train the resulting principal component flow (PCF) on data of PV and wind power generation as well as load demand in Germany in the years 2013 to 2015. The results of this investigation show that the PCF preserves critical features of the original distributions, such as the probability density and frequency behavior of the time series. The application of the PCF is, however, not limited to renewable power generation but rather extends to any data set, time series, or otherwise, which can be efficiently reduced using PCA.",0
"In recent years, generative models have become increasingly popular for their ability to generate synthetic data that can be used for training machine learning algorithms, testing, and generating novel ideas. However, traditional generative models such as Generative Adversarial Networks (GAN) require large amounts of computational resources and time to train, which limits their applicability to many real-world problems where fast scenario generation is required. To address these limitations, we propose a new approach called Principal Component Density Estimation using Normalizing Flows (PCDENF), which combines the concept of density estimation and normalizing flows to efficiently estimate distributions from limited data samples. Our method is able to generate high-quality, diverse scenarios at a fraction of the time and compute cost compared to other state-of-the-art approaches. We evaluate our method on several benchmark datasets and demonstrate its effectiveness through experiments and comparisons against existing methods. Overall, PCDENF represents a significant advance in the field of scenario generation and has the potential to significantly impact industries ranging from healthcare to finance to entertainment.",1
"Image-text matching is an important multi-modal task with massive applications. It tries to match the image and the text with similar semantic information. Existing approaches do not explicitly transform the different modalities into a common space. Meanwhile, the attention mechanism which is widely used in image-text matching models does not have supervision. We propose a novel attention scheme which projects the image and text embedding into a common space and optimises the attention weights directly towards the evaluation metrics. The proposed attention scheme can be considered as a kind of supervised attention and requiring no additional annotations. It is trained via a novel Discrete-continuous action space policy gradient algorithm, which is more effective in modelling complex action space than previous continuous action space policy gradient. We evaluate the proposed methods on two widely-used benchmark datasets: Flickr30k and MS-COCO, outperforming the previous approaches by a large margin.",0
"In recent years, image-text matching has become increasingly important due to advancements in natural language processing (NLP) and computer vision (CV). However, traditional methods often suffer from issues such as limited capacity to handle complex relationships between images and texts. Therefore, there remains a need for improved models that can effectively tackle image-text matching tasks. To address these challenges, we propose a novel model called Discrete-Continuous Action Space Policy Gradient-Based Attention (DCPGBA), which uses policy gradients and attention mechanisms to learn a mapping function between image and text representations. Our model effectively captures both discrete and continuous aspects of the input data by utilizing a hybrid action space, allowing it to capture both local and global dependencies. Through extensive experiments on three benchmark datasets, our results show that DCPGBA outperforms state-of-the-art models, demonstrating its effectiveness in solving complex image-text matching problems. Overall, this research contributes towards building more advanced NLP and CV systems capable of handling diverse multimedia content.",1
"Vehicle re-identification (Re-ID) distinguishes between the same vehicle and other vehicles in images. It is challenging due to significant intra-instance differences between identical vehicles from different views and subtle inter-instance differences of similar vehicles. Researchers have tried to address this problem by extracting features robust to variations of viewpoints and environments. More recently, they tried to improve performance by using additional metadata such as key points, orientation, and temporal information. Although these attempts have been relatively successful, they all require expensive annotations. Therefore, this paper proposes a novel deep neural network called a multi-attention-based soft partition (MUSP) network to solve this problem. This network does not use metadata and only uses multiple soft attentions to identify a specific vehicle area. This function was performed by metadata in previous studies. Experiments verified that MUSP achieved state-of-the-art (SOTA) performance for the VehicleID dataset without any additional annotations and was comparable to VeRi-776 and VERI-Wild.",0
"In recent years, vehicle reidentification has become increasingly important due to the rapid proliferation of cameras monitoring public spaces such as cities, airports, shopping malls, parking lots, etc. These camera systems generate large amounts of video footage that need to be analyzed quickly and accurately to identify vehicles which might pose a threat to security or commit crimes. The task of vehicle reidentification can be challenging because the appearance of vehicles can change drastically due to lighting conditions, occlusions (e.g., by other vehicles), and changes in viewpoint. This paper proposes a new approach called soft partition network based on multi-attention mechanisms to tackle these issues effectively. Our method utilizes multiple attention modules to capture different aspects of the input image such as local features, global contexts, spatial relationships, and channel information. We fuse these attention modules using a novel soft partition strategy inspired by human vision perception. Experimental results demonstrate that our method achieves state-of-the-art performance on several benchmark datasets compared with existing methods. Overall, our proposed approach represents a significant step towards improving vehicle reidentification accuracy, thereby enhancing surveillance capabilities in public places.",1
"We propose a novel guided interactive segmentation (GIS) algorithm for video objects to improve the segmentation accuracy and reduce the interaction time. First, we design the reliability-based attention module to analyze the reliability of multiple annotated frames. Second, we develop the intersection-aware propagation module to propagate segmentation results to neighboring frames. Third, we introduce the GIS mechanism for a user to select unsatisfactory frames quickly with less effort. Experimental results demonstrate that the proposed algorithm provides more accurate segmentation results at a faster speed than conventional algorithms. Codes are available at https://github.com/yuk6heo/GIS-RAmap.",0
"In this work, we present a novel approach for interactive video object segmentation using reliability-based attention maps. Our method utilizes deep learning techniques to accurately extract objects from moving scenes, while also providing users with the ability to interactively refine segmentations as desired. The key component of our system is a new type of attention map that captures uncertainties arising due to motion blur, occlusions, and other sources of variability. These uncertainty estimates serve as guidance during interaction, enabling the algorithm to focus on areas where human input is most likely to improve results. We evaluate our approach on several challenging datasets and demonstrate significant improvements over state-of-the-art methods, both qualitatively and quantitatively. Additionally, we showcase examples of user interactions highlighting the effectiveness of our approach in producing high quality segmentations. Overall, our research presents a powerful tool for efficient and accurate segmentation of complex videos and opens up promising opportunities for real-world applications such as autonomous driving, robotics, and augmented reality.",1
"Finding communities in networks is a problem that remains difficult, in spite of the amount of attention it has recently received. The Stochastic Block-Model (SBM) is a generative model for graphs with ""communities"" for which, because of its simplicity, the theoretical understanding has advanced fast in recent years. In particular, there have been various results showing that simple versions of spectral clustering using the Normalized Laplacian of the graph can recover the communities almost perfectly with high probability. Here we show that essentially the same algorithm used for the SBM and for its extension called Degree-Corrected SBM, works on a wider class of Block-Models, which we call Preference Frame Models, with essentially the same guarantees. Moreover, the parametrization we introduce clearly exhibits the free parameters needed to specify this class of models, and results in bounds that expose with more clarity the parameters that control the recovery error in this model class.",0
"In recent years, there has been significant interest in developing new methods for unsupervised learning tasks such as data segmentation and feature extraction. One popular approach is through the use of graph partitioning techniques like spectral clustering, which involves representing data points as vertices in a weighted graph and then grouping these vertices into clusters based on their connectivity patterns.  While many different types of graphs have been used in conjunction with spectral clustering algorithms, one particular family of graphs has garnered attention due to their ability to encode important geometric features of the underlying dataset. These graphs, known as kernel principal component analysis (KPCA) graphs, provide a powerful tool for identifying relevant structure within the data.  In this work, we investigate how KPCA graphs can be combined with spectral clustering methods to generate meaningful partitions of complex datasets. We present a general framework for using KPCA graphs in combination with other kinds of affinity matrices for spectral clustering, and demonstrate the effectiveness of our methodology across a variety of applications including image segmentation, anomaly detection, and dimensionality reduction. Our results highlight the versatility and power of combining KPCA graphs with spectral clustering techniques for unsupervised learning problems.",1
"Snake robots, comprised of sequentially connected joint actuators, have recently gained increasing attention in the industrial field, like life detection in narrow space. Such robots can navigate through the complex environment via the cooperation of multiple motors located on the backbone. However, controlling the robots in an unknown environment is challenging, and conventional control strategies can be energy inefficient or even fail to navigate to the destination. In this work, a snake locomotion gait policy is developed via deep reinforcement learning (DRL) for energy-efficient control. We apply proximal policy optimization (PPO) to each joint motor parameterized by angular velocity and the DRL agent learns the standard serpenoid curve at each timestep. The robot simulator and task environment are built upon PyBullet. Comparing to conventional control strategies, the snake robots controlled by the trained PPO agent can achieve faster movement and more energy-efficient locomotion gait. This work demonstrates that DRL provides an energy-efficient solution for robot control.",0
"This study proposes a method for designing energy efficient gaits for snakes using deep reinforcement learning (RL). Snakes use their unique ability to move through constricted spaces by deforming their bodies and moving in ways that cannot easily be modeled mathematically. In order to develop effective locomotion strategies, researchers must account for these complex biomechanical factors and determine which movements are energetically advantageous under different circumstances. Our approach uses realistic simulations based on physical models of snakes to evaluate the efficiency of different gait policies. We train an RL agent to find optimal policy parameters by trial-and-error while observing states, actions, rewards, and next states. By exploring a wide range of possible behaviors, our algorithm identifies energy-efficient policies tailored to specific environments. Evaluations show that our algorithm outperforms state-of-the-art approaches and produces high quality solutions quickly and consistently across multiple scenarios. These results provide important insights into the mechanics of snake movement as well as novel techniques for controlling simulated agents based on complex biological systems. Overall, this work represents an important step towards understanding how animals make decisions about energy allocation during motion.",1
"Image captioning has made substantial progress with huge supporting image collections sourced from the web. However, recent studies have pointed out that captioning datasets, such as COCO, contain gender bias found in web corpora. As a result, learning models could heavily rely on the learned priors and image context for gender identification, leading to incorrect or even offensive errors. To encourage models to learn correct gender features, we reorganize the COCO dataset and present two new splits COCO-GB V1 and V2 datasets where the train and test sets have different gender-context joint distribution. Models relying on contextual cues will suffer from huge gender prediction errors on the anti-stereotypical test data. Benchmarking experiments reveal that most captioning models learn gender bias, leading to high gender prediction errors, especially for women. To alleviate the unwanted bias, we propose a new Guided Attention Image Captioning model (GAIC) which provides self-guidance on visual attention to encourage the model to capture correct gender visual evidence. Experimental results validate that GAIC can significantly reduce gender prediction errors with a competitive caption quality. Our codes and the designed benchmark datasets are available at https://github.com/datamllab/Mitigating_Gender_Bias_In_Captioning_System.",0
"Title: ""Addressing concerns with gender bias in automatic image caption generation systems""",1
"As the request for deep learning solutions increases, the need for explainability is even more fundamental. In this setting, particular attention has been given to visualization techniques, that try to attribute the right relevance to each input pixel with respect to the output of the network. In this paper, we focus on Class Activation Mapping (CAM) approaches, which provide an effective visualization by taking weighted averages of the activation maps. To enhance the evaluation and the reproducibility of such approaches, we propose a novel set of metrics to quantify explanation maps, which show better effectiveness and simplify comparisons between approaches. To evaluate the appropriateness of the proposal, we compare different CAM-based visualization methods on the entire ImageNet validation set, fostering proper comparisons and reproducibility.",0
"Title: ""Revisiting Class Activation Mapping (CAM) for explainability - A novel metric and experimental analysis""  Abstract: Deep neural networks have achieved state-of-the-art results across several domains, including image classification tasks. However, their black box nature makes explaining their decisions challenging, which is crucial in critical applications such as medical diagnosis or self-driving cars. Class Activation Maps (CAMs), proposed by Zhou et al. (2016) has been widely adopted as a method for explaining image classifiers that operate on convolutional neural network backbones. This work revisits CAMs, analyzing both qualitative and quantitative aspects through experiments based on public datasets. Specifically, we propose a new metric called Pixel Weighted Accuracy (PWA) that addresses limitations of existing metrics. Our experiments aim to answer three research questions: RQ1 - How well can current evaluation methods capture important features? RQ2 - What impact do hyperparameters have on generated maps? And RQ3 - Can current CAM variants produce competitive or better results than traditional global average pooling for segmentation problems? Results show mixed answers but suggest room for improvement regarding feature visualization quality provided by current evaluations. Finally, we discuss our findings and future directions for CAMs usage and improvements.",1
"Localizing persons and recognizing their actions from videos is a challenging task towards high-level video understanding. Recent advances have been achieved by modeling direct pairwise relations between entities. In this paper, we take one step further, not only model direct relations between pairs but also take into account indirect higher-order relations established upon multiple elements. We propose to explicitly model the Actor-Context-Actor Relation, which is the relation between two actors based on their interactions with the context. To this end, we design an Actor-Context-Actor Relation Network (ACAR-Net) which builds upon a novel High-order Relation Reasoning Operator and an Actor-Context Feature Bank to enable indirect relation reasoning for spatio-temporal action localization. Experiments on AVA and UCF101-24 datasets show the advantages of modeling actor-context-actor relations, and visualization of attention maps further verifies that our model is capable of finding relevant higher-order relations to support action detection. Notably, our method ranks first in the AVA-Kineticsaction localization task of ActivityNet Challenge 2020, out-performing other entries by a significant margin (+6.71mAP). Training code and models will be available at https://github.com/Siyu-C/ACAR-Net.",0
"Incorporate into your abstract key ideas from your paper: spatiotemporal action localization (STAL), region proposals, relation network (RN) module, actor-context-actor (ACA) relational reasoning, feature maps, and graph convolutional networks (GCNs).",1
"Real-scanned point clouds are often incomplete due to viewpoint, occlusion, and noise. Existing point cloud completion methods tend to generate global shape skeletons and hence lack fine local details. Furthermore, they mostly learn a deterministic partial-to-complete mapping, but overlook structural relations in man-made objects. To tackle these challenges, this paper proposes a variational framework, Variational Relational point Completion network (VRCNet) with two appealing properties: 1) Probabilistic Modeling. In particular, we propose a dual-path architecture to enable principled probabilistic modeling across partial and complete clouds. One path consumes complete point clouds for reconstruction by learning a point VAE. The other path generates complete shapes for partial point clouds, whose embedded distribution is guided by distribution obtained from the reconstruction path during training. 2) Relational Enhancement. Specifically, we carefully design point self-attention kernel and point selective kernel module to exploit relational point features, which refines local shape details conditioned on the coarse completion. In addition, we contribute a multi-view partial point cloud dataset (MVP dataset) containing over 100,000 high-quality scans, which renders partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model. Extensive experiments demonstrate that VRCNet outperforms state-of-theart methods on all standard point cloud completion benchmarks. Notably, VRCNet shows great generalizability and robustness on real-world point cloud scans.",0
This novel approach to point completion allows more efficient computation and storage by training on multiple data sets. Our method uses variational inference to learn a distribution over possible completions that can adapt to incomplete inputs. We train our model on large scale datasets which leads to state of art performance in terms of accuracy. The paper concludes by discussing potential applications such as computer vision and natural language processing.,1
"As autonomous driving systems mature, motion forecasting has received increasing attention as a critical requirement for planning. Of particular importance are interactive situations such as merges, unprotected turns, etc., where predicting individual object motion is not sufficient. Joint predictions of multiple objects are required for effective route planning. There has been a critical need for high-quality motion data that is rich in both interactions and annotation to develop motion planning models. In this work, we introduce the most diverse interactive motion dataset to our knowledge, and provide specific labels for interacting objects suitable for developing joint prediction models. With over 100,000 scenes, each 20 seconds long at 10 Hz, our new dataset contains more than 570 hours of unique data over 1750 km of roadways. It was collected by mining for interesting interactions between vehicles, pedestrians, and cyclists across six cities within the United States. We use a high-accuracy 3D auto-labeling system to generate high quality 3D bounding boxes for each road agent, and provide corresponding high definition 3D maps for each scene. Furthermore, we introduce a new set of metrics that provides a comprehensive evaluation of both single agent and joint agent interaction motion forecasting models. Finally, we provide strong baseline models for individual-agent prediction and joint-prediction. We hope that this new large-scale interactive motion dataset will provide new opportunities for advancing motion forecasting models.",0
"Title: Interactive Motion Forecasting for Autonomous Driving Abstract: This research aimed to develop a large scale interactive motion forecasting system that can accurately predict the future movements of objects in real time. To achieve this, we created the Waymo Open Motion Dataset which contains data from millions of vehicle trips. This dataset allowed us to train and evaluate our forecasting models on a wide range of driving scenarios. Our proposed model outperformed existing methods by a significant margin, demonstrating the effectiveness of using interaction as a key feature in motion prediction. We believe our method has great potential for use in autonomous driving systems, where accurate predictions can greatly improve safety and efficiency. Overall, this research represents an important step forward in advancing the state of art in interactive motion forecasting.",1
"Automatically detecting violence from surveillance footage is a subset of activity recognition that deserves special attention because of its wide applicability in unmanned security monitoring systems, internet video filtration, etc. In this work, we propose an efficient two-stream deep learning architecture leveraging Separable Convolutional LSTM (SepConvLSTM) and pre-trained MobileNet where one stream takes in background suppressed frames as inputs and other stream processes difference of adjacent frames. We employed simple and fast input pre-processing techniques that highlight the moving objects in the frames by suppressing non-moving backgrounds and capture the motion in-between frames. As violent actions are mostly characterized by body movements these inputs help produce discriminative features. SepConvLSTM is constructed by replacing convolution operation at each gate of ConvLSTM with a depthwise separable convolution that enables producing robust long-range Spatio-temporal features while using substantially fewer parameters. We experimented with three fusion methods to combine the output feature maps of the two streams. Evaluation of the proposed methods was done on three standard public datasets. Our model outperforms the accuracy on the larger and more challenging RWF-2000 dataset by more than a 2% margin while matching state-of-the-art results on the smaller datasets. Our experiments lead us to conclude, the proposed models are superior in terms of both computational efficiency and detection accuracy.",0
"Title: ""Efficient Approach for Violence Detection using Video Analysis""  This paper presents an efficient two-stream convolutional neural network architecture utilizing Long Short Term Memory (LSTM) layers for accurate violence detection from video sequences. The proposed model can effectively classify violent content by extracting spatio-temporal features through parallel processing of visual frames and audio signals. Our approach uses separable convolutions which significantly reduce computation costs while maintaining high accuracy compared to existing methods. Experimental results demonstrate that our method outperforms state-of-the-art models on standard benchmark datasets, achieving top performance with faster inference speed. This research paves the way for real-time violence detection systems in surveillance applications and online monitoring platforms.",1
"It's worth noting that the owner-member relationship between wheels and vehicles has an significant contribution to the 3D perception of vehicles, especially in the embedded environment. However, there are currently two main challenges about the above relationship prediction: i) The traditional heuristic methods based on IoU can hardly deal with the traffic jam scenarios for the occlusion. ii) It is difficult to establish an efficient applicable solution for the vehicle-mounted system. To address these issues, we propose an innovative relationship prediction method, namely DeepWORD, by designing a graph convolution network (GCN). Specifically, we utilize the feature maps with local correlation as the input of nodes to improve the information richness. Besides, we introduce the graph attention network (GAT) to dynamically amend the prior estimation deviation. Furthermore, we establish an annotated owner-member relationship dataset called WORD as a large-scale benchmark, which will be available soon. The experiments demonstrate that our solution achieves state-of-the-art accuracy and real-time in practice.",0
"In recent years, autonomous driving has gained significant attention as a promising application field of Artificial Intelligence (AI) technology. One crucial component of autono",1
"Deep learning-based facial recognition systems have experienced increased media attention due to exhibiting unfair behavior. Large enterprises, such as IBM, shut down their facial recognition and age prediction systems as a consequence. Age prediction is an especially difficult application with the issue of fairness remaining an open research problem (e.g. predicting age for different ethnicity equally accurate). One of the main causes of unfair behavior in age prediction methods lies in the distribution and diversity of the training data. In this work, we present two novel approaches for dataset curation and data augmentation in order to increase fairness through distribution aware curation and increase diversity through distribution aware augmentation. To achieve this, we created an out-of-distribution technique which is used to select the data most relevant to the deep neural network's (DNN) task when balancing the data among age, ethnicity, and gender. Our approach shows promising results. Our best-trained DNN model outperformed all academic and industrial baselines in terms of fairness by up to 4.92 times. When it comes to generalization, the increase in diversity also enhanced the DNN's performance, outperforming state-of-the-art approaches of prior research on the Age Estimation Benchmark dataset AFAD by 30.40% and the Amazon AWS and Microsoft Azure public cloud systems by 31.88% and 10.95%, respectively.",0
"Age estimation from facial images raises concerns due to disparate impacts on certain subgroups such as older individuals or ethnic minorities that are frequently underrepresented in publicly available training data for machine learning algorithms. In this work, we tackle these issues by improving out-of-distribution (OOD) detection performance so models can better handle unknown inputs without relying exclusively on uncertain predictions. Our method utilizes normalizing flows and adversarial examples, which boosts OOD detector accuracy by over 2% compared to state-of-the-art techniques. Through extensive experiments across multiple datasets and evaluation metrics, our findings demonstrate that equipping current models with more robust OOD detection can mitigate unfairness associated with incorrect predictions for aging face tasks.",1
"We propose a novel framework for cross-modal zero-shot learning (ZSL) in the context of sketch-based image retrieval (SBIR). Conventionally, the SBIR schema mainly considers simultaneous mappings among the two image views and the semantic side information. Therefore, it is desirable to consider fine-grained classes mainly in the sketch domain using highly discriminative and semantically rich feature space. However, the existing deep generative modeling-based SBIR approaches majorly focus on bridging the gaps between the seen and unseen classes by generating pseudo-unseen-class samples. Besides, violating the ZSL protocol by not utilizing any unseen-class information during training, such techniques do not pay explicit attention to modeling the discriminative nature of the shared space. Also, we note that learning a unified feature space for both the multi-view visual data is a tedious task considering the significant domain difference between sketches and color images. In this respect, as a remedy, we introduce a novel framework for zero-shot SBIR. While we define a cross-modal triplet loss to ensure the discriminative nature of the shared space, an innovative cross-modal attention learning strategy is also proposed to guide feature extraction from the image domain exploiting information from the respective sketch counterpart. In order to preserve the semantic consistency of the shared space, we consider a graph CNN-based module that propagates the semantic class topology to the shared space. To ensure an improved response time during inference, we further explore the possibility of representing the shared space in terms of hash codes. Experimental results obtained on the benchmark TU-Berlin and the Sketchy datasets confirm the superiority of CrossATNet in yielding state-of-the-art results.",0
"This is an interesting framework that uses cross attention mechanisms to improve sketch based image retrieval tasks. By doing so, this model is able to effectively capture both local features within a single image as well as their global relationships across images. Additionally, this method was shown to outperform traditional methods such as CNNs on benchmark datasets by a significant margin while also using fewer parameters. Overall, this research presents a novel approach for sketch based image retrieval that could potentially revolutionize how we think about these types of problems. While there may still be room for improvement, the results presented here show promising progress towards more accurate systems.",1
"Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new \textbf{Convolution-enhanced image Transformer (CeiT)} which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: \textbf{1)} instead of the straightforward tokenization from raw input images, we design an \textbf{Image-to-Tokens (I2T)} module that extracts patches from generated low-level features; \textbf{2)} the feed-froward network in each encoder block is replaced with a \textbf{Locally-enhanced Feed-Forward (LeFF)} layer that promotes the correlation among neighboring tokens in the spatial dimension; \textbf{3)} a \textbf{Layer-wise Class token Attention (LCA)} is attached at the top of the Transformer that utilizes the multi-level representations.   Experimental results on ImageNet and seven downstream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state-of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models also demonstrate better convergence with $3\times$ fewer training iterations, which can reduce the training cost significantly\footnote{Code and models will be released upon acceptance.}.",0
"This study presents a novel approach to incorporate convolutional designs within visual transformer architectures in order to improve their performance on image classification tasks. Recent advancements in computer vision have shown that deep learning models, such as convolutional neural networks (CNNs) and transformers, can achieve high accuracy on complex tasks like object detection and image segmentation. However, these architectures still face challenges in terms of efficiency and scalability.  In this work, we explore the use of convolutional designs to enhance visual transformers by adapting them from traditional CNN structures while preserving their strengths in capturing global dependencies across images. Our method leverages both spatial locality from convolutional layers and self attention mechanisms from transformers to produce highly effective representations. We evaluate our model on several benchmark datasets and demonstrate significant improvement over existing state-of-the-art methods, particularly in terms of computational cost and parameter count.  Our contributions in this paper include: (1) Introducing a new architecture framework called convtransformer which seamlessly integrates convolutional features within visual transformers; (2) Developing strategies to effectively fuse multi-scale feature maps from consecutive stages through a progressive design pipeline; (3) Demonstrating strong experimental results on popular image classification datasets such as ImageNet, CIFAR, and SVHN.  Overall, our findings suggest that combining convolutional designs with visual transformers can lead to more efficient and accurate solutions for computer vision problems, paving the way towards future research exploring hybrid architectures in deep learning.",1
"Recently, there has been great success in applying deep neural networks on graph structured data. Most work, however, focuses on either node- or graph-level supervised learning, such as node, link or graph classification or node-level unsupervised learning (e.g. node clustering). Despite its wide range of possible applications, graph-level unsupervised learning has not received much attention yet. This might be mainly attributed to the high representation complexity of graphs, which can be represented by n! equivalent adjacency matrices, where n is the number of nodes. In this work we address this issue by proposing a permutation-invariant variational autoencoder for graph structured data. Our proposed model indirectly learns to match the node ordering of input and output graph, without imposing a particular node ordering or performing expensive graph matching. We demonstrate the effectiveness of our proposed model on various graph reconstruction and generation tasks and evaluate the expressive power of extracted representations for downstream graph-level classification and regression.",0
"In this work, we propose a permutation-invariant variational autoencoder (VI-VAE) for learning graph-level representations that are robust to the order of nodes in a graph. Many real-world graphs have inherent symmetries due to their underlying structure, such as chemical compounds where atoms are indistinguishable except for their position relative to other atoms. Existing VAEs learn node embeddings that capture these local patterns but struggle with capturing global, permutation-invariant information which results in poor clustering performance on transposed graphs. We develop a VI-VAE by incorporating two key modifications: i) introducing group convolutions to maintain equivariance tograph symmetries during message passing through layers; ii) formulating message bottlenecks as Gaussian mixture models that allow information exchange across variables without breaking equivariance. Extensive experiments demonstrate the effectiveness of our approach achieving state-of-the-art clustering accuracy while providing interpretable graph reconstructions. Our method can serve as a general toolkit towards uncovering hidden structures within complex networks.",1
"Traditional and deep learning-based fusion methods generated the intermediate decision map to obtain the fusion image through a series of post-processing procedures. However, the fusion results generated by these methods are easy to lose some source image details or results in artifacts. Inspired by the image reconstruction techniques based on deep learning, we propose a multi-focus image fusion network framework without any post-processing to solve these problems in the end-to-end and supervised learning way. To sufficiently train the fusion model, we have generated a large-scale multi-focus image dataset with ground-truth fusion images. What's more, to obtain a more informative fusion image, we further designed a novel fusion strategy based on unity fusion attention, which is composed of a channel attention module and a spatial attention module. Specifically, the proposed fusion approach mainly comprises three key components: feature extraction, feature fusion and image reconstruction. We firstly utilize seven convolutional blocks to extract the image features from source images. Then, the extracted convolutional features are fused by the proposed fusion strategy in the feature fusion layer. Finally, the fused image features are reconstructed by four convolutional blocks. Experimental results demonstrate that the proposed approach for multi-focus image fusion achieves remarkable fusion performance compared to 19 state-of-the-art fusion methods.",0
"Image registration and fusion remain challenging tasks due to variations in illumination conditions, object appearance, camera poses, and sensor modalities across images captured by different sensors. Current approaches can either lead to loss of some important details or generate visible artifacts and fail to preserve edges, textures and other high frequency content leading to poor performance on edge preservation metrics as well as visual inspection scores. In addition, these methods often require accurate alignment maps which might not always be available. To address the above problems we present a new method called ""UFA-FUSE"" that leverages multiple streams of features from both single-image patch pairs and concatenation of images themselves along with spatial attention mechanism enabling feature extraction at every pixel location. To regularize our predictions, we propose a combination of perceptual loss based on VGG network and $L_2$ norm using the ground truth map. Our contributions are threefold: Firstly, we introduce a novel encoder architecture fusing coarse features directly obtained from input images and fine scale details extracted from corresponding regions containing those pixels. Secondly, we design a stream which processes low resolution representation capturing global context by computing correlation of local binary patterns within blocks of neighboring pixels. Thirdly, while registering source images into common reference frame we predict dense displacement field allowing for more flexible handling of large misalignments typically occurring during nighttime driving scenarios where significant changes occur over short periods of time. For qualitative evaluation we conduct experiments on public RGBT benchmark dataset and provide comparisons against state-of-the-art competitors revealing superior results for all proposed models. Moreover, we perform extensive quantita",1
"This work focuses on object goal visual navigation, aiming at finding the location of an object from a given class, where in each step the agent is provided with an egocentric RGB image of the scene. We propose to learn the agent's policy using a reinforcement learning algorithm. Our key contribution is a novel attention probability model for visual navigation tasks. This attention encodes semantic information about observed objects, as well as spatial information about their place. This combination of the ""what"" and the ""where"" allows the agent to navigate toward the sought-after object effectively. The attention model is shown to improve the agent's policy and to achieve state-of-the-art results on commonly-used datasets.",0
"This is my request, please respect it so I can use your work without having to rewrite later due to violation of these constraints. Thanks!",1
"Although recent years have witnessed the great advances in stereo image super-resolution (SR), the beneficial information provided by binocular systems has not been fully used. Since stereo images are highly symmetric under epipolar constraint, in this paper, we improve the performance of stereo image SR by exploiting symmetry cues in stereo image pairs. Specifically, we propose a symmetric bi-directional parallax attention module (biPAM) and an inline occlusion handling scheme to effectively interact cross-view information. Then, we design a Siamese network equipped with a biPAM to super-resolve both sides of views in a highly symmetric manner. Finally, we design several illuminance-robust losses to enhance stereo consistency. Experiments on four public datasets demonstrate the superior performance of our method. Source code is available at https://github.com/YingqianWang/iPASSR.",0
"This paper presents a novel approach to stereo image super-resolution using symmetric parallax attention. We propose a new network architecture that utilizes the inherent structure present in stereoscopic images to improve performance over traditional methods. Our method uses a deep neural network to learn a mapping from low resolution input images to high resolution output images, while taking into account both intra-frame and inter-frame dependencies. By integrating a cross-scale feature fusion module and a parallax attention mechanism, we can effectively leverage contextual information from neighboring frames and pixels to enhance local features. Experimental results on challenging benchmark datasets demonstrate the effectiveness of our proposed method, achieving state-of-the-art performance in terms of visual quality and quantitative evaluation metrics. Overall, our work provides a significant contribution to the field of computer vision and demonstrates the potential of stereoscopic imagery in improving image processing tasks.",1
"Recently, single-stage embedding based deep learning algorithms gain increasing attention in cell segmentation and tracking. Compared with the traditional ""segment-then-associate"" two-stage approach, a single-stage algorithm not only simultaneously achieves consistent instance cell segmentation and tracking but also gains superior performance when distinguishing ambiguous pixels on boundaries and overlaps. However, the deployment of an embedding based algorithm is restricted by slow inference speed (e.g., around 1-2 mins per frame). In this study, we propose a novel Faster Mean-shift algorithm, which tackles the computational bottleneck of embedding based cell segmentation and tracking. Different from previous GPU-accelerated fast mean-shift algorithms, a new online seed optimization policy (OSOP) is introduced to adaptively determine the minimal number of seeds, accelerate computation, and save GPU memory. With both embedding simulation and empirical validation via the four cohorts from the ISBI cell tracking challenge, the proposed Faster Mean-shift algorithm achieved 7-10 times speedup compared to the state-of-the-art embedding based cell instance segmentation and tracking algorithm. Our Faster Mean-shift algorithm also achieved the highest computational speed compared to other GPU benchmarks with optimized memory consumption. The Faster Mean-shift is a plug-and-play model, which can be employed on other pixel embedding based clustering inference for medical image analysis. (Plug-and-play model is publicly available: https://github.com/masqm/Faster-Mean-Shift)",0
"This should focus on key points from introduction, results/methods sections and conclusion. The paper proposes to apply mean shift based clustering with cosine embedding features onto Graphical Processing Units (GPU) for accelerating downstream applications such as cell segmentation and cell lineage tracking through time using wide field microscopy data sequences. We show that this implementation outperforms CPU only versions by a factor of up to two on Titan Xp GPUs and achieves competitive accuracy in comparison to other approaches. By providing these optimized implementations along with code within this publication we aim to make high performance image processing available for researchers working in quantitative imaging without requiring them to have specialized hardware knowledge while fostering collaboration within our open science community by allowing others build upon and optimize further for emerging hardware technologies. In summary, we present Faster Mean Shift, a novel method for accelerating clustering based on cosine embedding features on GPUs which provides competitive accuracy in comparison to prior art.",1
"Our objective in this work is fine-grained classification of actions in untrimmed videos, where the actions may be temporally extended or may span only a few frames of the video. We cast this into a query-response mechanism, where each query addresses a particular question, and has its own response label set. We make the following four contributions: (I) We propose a new model - a Temporal Query Network - which enables the query-response functionality, and a structural understanding of fine-grained actions. It attends to relevant segments for each query with a temporal attention mechanism, and can be trained using only the labels for each query. (ii) We propose a new way - stochastic feature bank update - to train a network on videos of various lengths with the dense sampling required to respond to fine-grained queries. (iii) We compare the TQN to other architectures and text supervision methods, and analyze their pros and cons. Finally, (iv) we evaluate the method extensively on the FineGym and Diving48 benchmarks for fine-grained action classification and surpass the state-of-the-art using only RGB features.",0
"Recent advances in deep learning have shown that Convolutional Neural Networks (CNNs) can achieve state-of-the-art performance on many challenging computer vision tasks such as object recognition, action detection, semantic segmentation, etc. However, most current methods focus on processing frame-based video representations or short temporal subsequence features directly without explicitly modeling long-range temporal dependencies across frames which could potentially benefit fine-grained video understanding like activity recognition or dynamic scene understanding. In order to address these limitations we present a novel framework called Temporal Query Networks (TQN). TQN employs several layers to learn query regions from every input frame, and then learns to pool query attentions spatially, temporally, semantically for final feature representation learning. Moreover, our proposed method outperforms prior arts by large margins on both Something-Something V1 dataset and Something-V2 dataset. Overall, our results demonstrate that careful handling of both attention mechanisms at multiple levels coupled with simple yet powerful network design choices may enable new state-of-the art solutions for high profile benchmark datasets.",1
"The advance of image editing techniques allows users to create artistic works, but the manipulated regions may be incompatible with the background. Localizing the inharmonious region is an appealing yet challenging task. Realizing that this task requires effective aggregation of multi-scale contextual information and suppression of redundant information, we design novel Bi-directional Feature Integration (BFI) block and Global-context Guided Decoder (GGD) block to fuse multi-scale features in the encoder and decoder respectively. We also employ Mask-guided Dual Attention (MDA) block between the encoder and decoder to suppress the redundant information. Experiments on the image harmonization dataset demonstrate that our method achieves competitive performance for inharmonious region localization. The source code is available at https://github.com/bcmi/DIRL.",0
"Automatic localisation from audio signals represents one of the most recent challenges in the research field related to Music Information Retrieval (MIR). Although several approaches have been proposed so far, none of them has achieved satisfying results yet. One possible reason for this gap could lie in the fact that they rely mainly on time domain features such as pitch contours which, despite their suitability to some extent, still fail at capturing the full intrinsic nature of music signal. We propose here to use time-frequency representations of audio signals, namely Chroma Features, for addressing both issues simultaneously: handling transients and modeling harmony. Since these features contain all necessary information relative to chord progression (either monodic or polyphonic) their usage enables us to overcome the shortcoming related to chromagram extraction without sacrificing computational efficiency. Our approach achieves promising results compared to state-of-the-art systems even if the used evaluation metric is strongly linked to harmony accuracy only.",1
"Graph neural networks (GNNs) have received much attention recently because of their excellent performance on graph-based tasks. However, existing research on GNNs focuses on designing more effective models without considering much about the quality of the input data. In this paper, we propose self-enhanced GNN (SEG), which improves the quality of the input data using the outputs of existing GNN models for better performance on semi-supervised node classification. As graph data consist of both topology and node labels, we improve input data quality from both perspectives. For topology, we observe that higher classification accuracy can be achieved when the ratio of inter-class edges (connecting nodes from different classes) is low and propose topology update to remove inter-class edges and add intra-class edges. For node labels, we propose training node augmentation, which enlarges the training set using the labels predicted by existing GNN models. SEG is a general framework that can be easily combined with existing GNN models. Experimental results validate that SEG consistently improves the performance of well-known GNN models such as GCN, GAT and SGC across different datasets.",0
"Graph neural networks (GNNs) have shown great promise in numerous domains such as computer vision, natural language processing, and chemistry. However, like many machine learning models, their performance can be limited by issues such as overfitting, underfitting, and poor generalization. In this paper, we propose a novel approach called self-enhanced graph neural network (SEGNN), which addresses these limitations by using model outputs as additional features during training. Our method leverages the power of GNNs while improving their accuracy and robustness. We evaluate our SEGNN on several benchmark datasets and demonstrate significant improvements compared to state-of-the-art methods. Furthermore, our ablation studies show that each component of our approach contributes positively to the final results. Overall, our work represents a step forward in enhancing the capabilities of GNNs and paves the way for future research in this area.",1
"Object-centric representations have recently enabled significant progress in tackling relational reasoning tasks. By building a strong object-centric inductive bias into neural architectures, recent efforts have improved generalization and data efficiency of machine learning algorithms for these problems. One problem class involving relational reasoning that still remains under-explored is multi-agent reinforcement learning (MARL). Here we investigate whether object-centric representations are also beneficial in the fully cooperative MARL setting. Specifically, we study two ways of incorporating an agent-centric inductive bias into our RL algorithm: 1. Introducing an agent-centric attention module with explicit connections across agents 2. Adding an agent-centric unsupervised predictive objective (i.e. not using action labels), to be used as an auxiliary loss for MARL, or as the basis of a pre-training step. We evaluate these approaches on the Google Research Football environment as well as DeepMind Lab 2D. Empirically, agent-centric representation learning leads to the emergence of more complex cooperation strategies between agents as well as enhanced sample efficiency and generalization.",0
"In multi-agent systems, effective communication and coordination among agents are crucial for achieving successful collective outcomes. Agent-centric representations (ACRs) can serve as a powerful tool for facilitating such interdependence by enabling each agent to model its own task requirements and the behaviors of other agents relevant to its goal attainment. This paper proposes using ACRs within a reinforcement learning framework for multi-agent systems, whereby individual agents learn and refine their internal models based on observed interactions and resulting rewards. Our approach allows each agent to reason about both local and global consequences of its actions while accounting for others' intentions and potential responses. We demonstrate through simulation experiments that our method leads to more efficient and collaborative decision making across diverse problem domains, compared to traditional decentralized or centralized approaches. By balancing exploration and exploitation, our agents adapt quickly to changing environments and evolving social dynamics, highlighting the effectiveness of ACRs in promoting robust and flexible cooperation among autonomous entities.",1
"Pixel binning is considered one of the most prominent solutions to tackle the hardware limitation of smartphone cameras. Despite numerous advantages, such an image sensor has to appropriate an artefact-prone non-Bayer colour filter array (CFA) to enable the binning capability. Contrarily, performing essential image signal processing (ISP) tasks like demosaicking and denoising, explicitly with such CFA patterns, makes the reconstruction process notably complicated. In this paper, we tackle the challenges of joint demosaicing and denoising (JDD) on such an image sensor by introducing a novel learning-based method. The proposed method leverages the depth and spatial attention in a deep network. The proposed network is guided by a multi-term objective function, including two novel perceptual losses to produce visually plausible images. On top of that, we stretch the proposed image processing pipeline to comprehensively reconstruct and enhance the images captured with a smartphone camera, which uses pixel binning techniques. The experimental results illustrate that the proposed method can outperform the existing methods by a noticeable margin in qualitative and quantitative comparisons. Code available: https://github.com/sharif-apu/BJDD_CVPR21.",0
"This paper presents a novel image processing pipeline that addresses two major challenges faced by pixel-bin image sensors: joint demosaicking and denoising. These sensors capture images at lower resolutions than their conventional counterparts but offer several advantages such as low power consumption, high speed, and compact form factor. However, they suffer from spatial aliasing due to binning of pixels, which leads to poor color accuracy and high noise levels. To overcome these limitations, we propose a multi-stage approach consisting of three main modules: image filtering using a customized filter bank, Wiener denoising based on patch priors, and post-processing enhancement of chrominance channels. Our results demonstrate that our method outperforms state-of-the-art techniques in terms of both visual quality and objective metrics such as peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). Additionally, we show that the proposed method can effectively handle variations in lighting conditions and scene complexity. Overall, our work represents a significant step towards realizing the full potential of pixel-bin image sensors for next-generation imaging applications.",1
"Deep neural networks (DNNs) have shown superior performances on various multimodal learning problems. However, it often requires huge efforts to adapt DNNs to individual multimodal tasks by manually engineering unimodal features and designing multimodal feature fusion strategies. This paper proposes Bilevel Multimodal Neural Architecture Search (BM-NAS) framework, which makes the architecture of multimodal fusion models fully searchable via a bilevel searching scheme. At the upper level, BM-NAS selects the inter/intra-modal feature pairs from the pretrained unimodal backbones. At the lower level, BM-NAS learns the fusion strategy for each feature pair, which is a combination of predefined primitive operations. The primitive operations are elaborately designed and they can be flexibly combined to accommodate various effective feature fusion modules such as multi-head attention (Transformer) and Attention on Attention (AoA). Experimental results on three multimodal tasks demonstrate the effectiveness and efficiency of the proposed BM-NAS framework. BM-NAS achieves competitive performances with much less search time and fewer model parameters in comparison with the existing generalized multimodal NAS methods.",0
"This bilevel multimodal neural architecture search (BM-NAS) method leverages reinforcement learning principles to discover novel deep network architectures capable of processing multiple modalities like vision and language paired data inputs. Our innovative approach optimizes at two levels; first, we optimize the weights of pretrained model components via gradient ascent to form submodules that can process each individual modality. Secondly, we find which combination of these modules performs best on downstream tasks by training a meta learner predictor over them using REINFORCE policy gradients optimization. By using these weighted ensembles of the best module combinations across all available models, our results demonstrate significantly improved performance on benchmark datasets from both computer vision and natural language understanding domains over standard single-modality NAS approaches. Overall, this work presents an effective new search space exploration technique for complex multimodal learning systems with promising applications towards more realistic artificial intelligence systems.",1
"Soccer broadcast video understanding has been drawing a lot of attention in recent years within data scientists and industrial companies. This is mainly due to the lucrative potential unlocked by effective deep learning techniques developed in the field of computer vision. In this work, we focus on the topic of camera calibration and on its current limitations for the scientific community. More precisely, we tackle the absence of a large-scale calibration dataset and of a public calibration network trained on such a dataset. Specifically, we distill a powerful commercial calibration tool in a recent neural network architecture on the large-scale SoccerNet dataset, composed of untrimmed broadcast videos of 500 soccer games. We further release our distilled network, and leverage it to provide 3 ways of representing the calibration results along with player localization. Finally, we exploit those representations within the current best architecture for the action spotting task of SoccerNet-v2, and achieve new state-of-the-art performances.",0
"This paper presents two important components of SoccerNet-v2: camera calibration and player localization. These components enable accurate action spotting by precisely identifying the players within each frame. We analyze different camera setups, from single cameras to multi-camera arrays, and evaluate their performance impact on detection accuracy. Our contributions consist of fine-grained analyses of representation learning methods used in SoccerNet-v2 for better understanding of action recognition and improved system configurations. Finally, we provide recommendations for setting up efficient and cost-effective camera systems that maximize action detection accuracy. By implementing these techniques, users can ensure successful deployment of SoccerNet-v2 and achieve reliable real-time action identification.",1
"With the advance of the multi-media and multi-modal data, multi-view clustering (MVC) has drawn increasing attentions recently. In this field, one of the most crucial challenges is that the characteristics and qualities of different views usually vary extensively. Therefore, it is essential for MVC methods to find an effective approach that handles the diversity of multiple views appropriately. To this end, a series of MVC methods focusing on how to integrate the loss from each view have been proposed in the past few years. Among these methods, the mainstream idea is assigning weights to each view and then combining them linearly. In this paper, inspired by the effectiveness of non-linear combination in instance learning and the auto-weighted approaches, we propose Non-Linear Fusion for Self-Paced Multi-View Clustering (NSMVC), which is totally different from the the conventional linear-weighting algorithms. In NSMVC, we directly assign different exponents to different views according to their qualities. By this way, the negative impact from the corrupt views can be significantly reduced. Meanwhile, to address the non-convex issue of the MVC model, we further define a novel regularizer-free modality of Self-Paced Learning (SPL), which fits the proposed non-linear model perfectly. Experimental results on various real-world data sets demonstrate the effectiveness of the proposed method.",0
"In recent years, multi-view clustering has emerged as a powerful tool for analyzing data from multiple perspectives. This technique enables researchers to identify hidden patterns and relationships that might otherwise go unnoticed using single-view approaches. However, traditional linear fusion methods have limitations when applied to self-paced multi-view clustering tasks because they assume equal contributions from all views. In reality, some views may provide more critical information than others depending on the application domain. Therefore, there is a need for non-linear fusion techniques capable of adaptively weighting different view contributions based on their relevance.  This work proposes a novel non-linear fusion method for self-paced multi-view clustering (MVC) that addresses these challenges by integrating the complementary strengths of multiple views through deep learning networks such as autoencoders and variational Bayesian inference. Our approach leverages both global and local similarities among samples across views to learn flexible and robust latent representations tailored to MVC applications. We demonstrate the superiority of our proposed method over several state-of-the-art alternatives across four real-world datasets, including image, text, and biological data, illustrating the versatility and effectiveness of our model. Additionally, we perform extensive analyses to assess the impact of key design choices, highlighting the importance of choosing appropriate regularization terms, network architectures, and optimization strategies for each specific problem setting. Overall, our findings pave the way towards more advanced self-paced MVC solutions that can effectively harness diverse information sources and tackle complex clustering problems in increasingly demanding domains.",1
"Algorithm Selection (AS) is concerned with the selection of the best-suited algorithm out of a set of candidates for a given problem. The area of AS has received a lot of attention from machine learning researchers and practitioners, as positive results along this line of research can make expertise in ML more readily accessible to experts in other domains as well as to the general public. Another quickly expanding area is that of Multi-Target Prediction (MTP). The ability to simultaneously predict multiple target variables of diverse types makes MTP of interest for a plethora of applications. MTP embraces several subfields of machine learning, such as multi-label classification, multi-target regression, multi-task learning, dyadic prediction, zero-shot learning, network inference, and matrix completion. This work combines the two above-mentioned areas by proposing AutoMTP, an automated framework that performs algorithm selection for MTP. AutoMTP is realized by adopting a rule-based system for the algorithm selection step and a flexible neural network architecture that can be used for the several subfields of MTP.",0
"This paper describes a method for selecting problems automatically from a large set of tasks generated using AutoMTP, which is a metaverse platform that uses machine learning techniques to generate synthetic data for training predictive models. Our approach leverages Bayesian optimization to search through task configurations space and identify promising sets of objectives that can be solved by a given model architecture. We demonstrate our framework on several case studies involving popular benchmark datasets as well as real world applications in fraud detection and stock price forecasting domains. Experimental results show consistent improvements over random selections of objectives across all scenarios studied herewith, thus emphasizing the utility of automated task generation and problem formulation methods like ours. Keywords: Multi-task Learning, Autodidactic Metaverse Population (AutoMP), Reinforcement Learning, Task Search Space",1
"How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76% compared to geometry-based fusion.",0
"This paper presents a novel approach to autonomous driving using multi-modal fusion transformers. We propose a deep learning architecture that can effectively process data from multiple sensors such as cameras, lidar, radar, and GPS, which is crucial for making accurate perception decisions for safe navigation on public roads. Our method leverages recent advances in attention-based sequence processing models and addresses several key challenges in end-to-end multimodal sensor fusion, including synchronization, calibration, and uncertainty estimation. Extensive experiments demonstrate significant improvements over state-of-the-art methods across various benchmarks, showcasing the effectiveness of our proposed framework for real-world autonomous driving applications.",1
"Caused by the difference of data distributions, intra-domain gap and inter-domain gap are widely present in image processing tasks. In the field of image dehazing, certain previous works have paid attention to the inter-domain gap between the synthetic domain and the real domain. However, those methods only establish the connection from the source domain to the target domain without taking into account the large distribution shift within the target domain (intra-domain gap). In this work, we propose a Two-Step Dehazing Network (TSDN) with an intra-domain adaptation and a constrained inter-domain adaptation. First, we subdivide the distributions within the synthetic domain into subsets and mine the optimal subset (easy samples) by loss-based supervision. To alleviate the intra-domain gap of the synthetic domain, we propose an intra-domain adaptation to align distributions of other subsets to the optimal subset by adversarial learning. Finally, we conduct the constrained inter-domain adaptation from the real domain to the optimal subset of the synthetic domain, alleviating the domain shift between domains as well as the distribution shift within the real domain. Extensive experimental results demonstrate that our framework performs favorably against the state-of-the-art algorithms both on the synthetic datasets and the real datasets.",0
"This paper presents a new approach called two-step image dehazing that utilizes both intra-domain and inter-domain adaptation. Our method leverages the benefits of both domains by adapting from simulated data and real hazy images. We first introduce the intra-domains and their characteristics, as well as our proposed feature extractor network trained on synthetic images. Next, we propose an inter-domain transfer module that fine-tunes the network using real hazy images. Both modules work together to improve the performance of image dehazing under different environments and illumination conditions. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods in terms of visual quality and quantitative measures.",1
"The mainstream crowd counting methods usually utilize the convolution neural network (CNN) to regress a density map, requiring point-level annotations. However, annotating each person with a point is an expensive and laborious process. During the testing phase, the point-level annotations are not considered to evaluate the counting accuracy, which means the point-level annotations are redundant. Hence, it is desirable to develop weakly-supervised counting methods that just rely on count level annotations, a more economical way of labeling. Current weakly-supervised counting methods adopt the CNN to regress a total count of the crowd by an image-to-count paradigm. However, having limited receptive fields for context modeling is an intrinsic limitation of these weakly-supervised CNN-based methods. These methods thus can not achieve satisfactory performance, limited applications in the real-word. The Transformer is a popular sequence-to-sequence prediction model in NLP, which contains a global receptive field. In this paper, we propose TransCrowd, which reformulates the weakly-supervised crowd counting problem from the perspective of sequence-to-count based on Transformer. We observe that the proposed TransCrowd can effectively extract the semantic crowd information by using the self-attention mechanism of Transformer. To the best of our knowledge, this is the first work to adopt a pure Transformer for crowd counting research. Experiments on five benchmark datasets demonstrate that the proposed TransCrowd achieves superior performance compared with all the weakly-supervised CNN-based counting methods and gains highly competitive counting performance compared with some popular fully-supervised counting methods. Code is available at https://github.com/dk-liang/TransCrowd.",0
"This study presents a novel weakly-supervised approach for crowd counting using convolutional neural networks (CNNs) and transformers. Previous approaches have relied heavily on expensive, fully-labeled data sets. However, these methods can become cumbersome due to the high costs involved in obtaining large amounts of annotated data. In contrast, our method leverages unlabeled video frames along with partial annotations to learn a feature extractor that accurately estimates dense crowd counts without requiring vast amounts of labeled training data. Our proposed model, called ""TransCrowd,"" utilizes a state-of-the-art transformer architecture to generate high quality features from both RGB images and optical flow predictions. These features are then fed into a lightweight CNN, allowing us to effectively estimate crowd densities across multiple scenes. Extensive experiments demonstrate that our method outperforms current state-of-the-art weakly supervised techniques by significant margins, while achieving comparable performance to strongly supervised approaches trained on many more labeled examples. Overall, our work represents an important step towards enabling efficient and accurate estimation of dense crowds under real-world deployment scenarios where full annotations may not always be feasible.",1
"Few-shot object detection (FSOD) aims at learning a detector that can fast adapt to previously unseen objects with scarce annotated examples, which is challenging and demanding. Existing methods solve this problem by performing subtasks of classification and localization utilizing a shared component (e.g., RoI head) in the detector, yet few of them take the distinct preferences of two subtasks towards feature embedding into consideration. In this paper, we carefully analyze the characteristics of FSOD, and present that a general few-shot detector should consider the explicit decomposition of two subtasks, as well as leveraging information from both of them to enhance feature representations. To the end, we propose a simple yet effective Adaptive Fully-Dual Network (AFD-Net). Specifically, we extend Faster R-CNN by introducing Dual Query Encoder and Dual Attention Generator for separate feature extraction, and Dual Aggregator for separate model reweighting. Spontaneously, separate state estimation is achieved by the R-CNN detector. Besides, for the acquisition of enhanced feature representations, we further introduce Adaptive Fusion Mechanism to adaptively perform feature fusion in different subtasks. Extensive experiments on PASCAL VOC and MS COCO in various settings show that, our method achieves new state-of-the-art performance by a large margin, demonstrating its effectiveness and generalization ability.",0
"This paper presents AFD-Net, a novel fully convolutional network designed for few-shot object detection. Our proposed architecture adapts the current trend of dual path networks by using multi-scale inputs and dynamic fusion strategies during training and inference. We introduce three new modules: feature pyramid pooling (FPP), adaptive branch selection module (ABS) and global context enhancement module (GCE). These modules can effectively capture features at different levels, dynamically select informative branches and combine them harmoniously. Extensive experiments on COCO benchmark prove that our approach achieves state-of-the-art performance among all existing competitors under both 1/2 shot settings, demonstrating superiority over previous approaches.",1
"Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code is publicly available at https://github.com/zhoudaquan/dvit_repo.",0
"In this work we present DeepViT, a new model architecture that combines the strengths of transformers and convolutional neural networks (CNNs) to improve the state of art on image classification tasks. Our approach integrates depthwise separable convolutions within transformer blocks to better capture spatial hierarchies while still retaining global context. We evaluate our method on several benchmark datasets and show significant performance improvements over strong baselines including ResNet, ViT, and Swin. Through ablation studies and visualization techniques we provide insights into why DeepViT works well and where there may still be room for improvement. Overall, our work takes important steps towards realizing the potential of vision transformers for computer vision problems beyond text processing.  A new model architecture called DeepViT has been developed to enhance the accuracy of image classification tasks by combining elements from both transformers and convolutional neural networks (CNNs). This combination enables the model to capture spatial hierarchies as well as maintain global context. Experimental results on multiple benchmark datasets demonstrate significant improvements compared against current standard models like ResNet, ViT, and Swin. By conducting ablation experiments, researchers hope to gain a deeper understanding of how DeepViT achieves such impressive results and identify any possible areas needing further development. Ultimately, this study represents a promising advancement toward making vision transformers equally effective for vision problems outside natural language processing.",1
"Substantial efforts have been devoted to the investigation of spatiotemporal correlations for improving traffic speed prediction accuracy. However, existing works typically model the correlations based solely on the observed traffic state (e.g. traffic speed) without due consideration that different correlation measurements of the traffic data could exhibit a diverse set of patterns under different traffic situations. In addition, the existing works assume that all road segments can employ the same sampling frequency of traffic states, which is impractical. In this paper, we propose new measurements to model the spatial correlations among traffic data and show that the resulting correlation patterns vary significantly under various traffic situations. We propose a Heterogeneous Spatial Correlation (HSC) model to capture the spatial correlation based on a specific measurement, where the traffic data of varying road segments can be heterogeneous (i.e. obtained with different sampling frequency). We propose a Multi-fold Correlation Attention Network (MCAN), which relies on the HSC model to explore multi-fold spatial correlations and leverage LSTM networks to capture multi-fold temporal correlations to provide discriminating features in order to achieve accurate traffic prediction. The learned multi-fold spatiotemporal correlations together with contextual factors are fused with attention mechanism to make the final predictions. Experiments on real-world datasets demonstrate that the proposed MCAN model outperforms the state-of-the-art baselines.",0
"This can lead you in the wrong direction! Instead, think of it as a mini summary you would tell someone after having read your entire paper. Use the following questions: What did you accomplish? Who cares? Why should others care? How did you make progress towards answering these questions? What were some limitations/challenges that arose during conducting research and how were they addressed? Try to hit all of them in under 2 minutes.",1
"While few-shot learning (FSL) aims for rapid generalization to new concepts with little supervision, self-supervised learning (SSL) constructs supervisory signals directly computed from unlabeled data. Exploiting the complementarity of these two manners, few-shot auxiliary learning has recently drawn much attention to deal with few labeled data. Previous works benefit from sharing inductive bias between the main task (FSL) and auxiliary tasks (SSL), where the shared parameters of tasks are optimized by minimizing a linear combination of task losses. However, it is challenging to select a proper weight to balance tasks and reduce task conflict. To handle the problem as a whole, we propose a novel approach named as Pareto self-supervised training (PSST) for FSL. PSST explicitly decomposes the few-shot auxiliary problem into multiple constrained multi-objective subproblems with different trade-off preferences, and here a preference region in which the main task achieves the best performance is identified. Then, an effective preferred Pareto exploration is proposed to find a set of optimal solutions in such a preference region. Extensive experiments on several public benchmark datasets validate the effectiveness of our approach by achieving state-of-the-art performance.",0
"In recent years, there has been growing interest in few-shot learning (FSL), which focuses on developing algorithms that can learn from very little labeled data. This technique allows models to generalize better than traditional supervised methods while using fewer training examples. However, one major challenge facing many FSL approaches lies in their reliance on pre-training. Pre-trained models require large amounts of annotated data, limiting the applicability of these techniques in certain domains where such annotations may not exist or cannot easily be obtained. The authors propose Pareto self-supervision, a framework for few-shot classification that requires no manual annotation during training. Their approach relies solely on automatically generated pseudo-labels to train a model without human intervention. These pseudo-labels serve as a form of implicit supervision that guides the optimization process towards meaningful minima. Unlike previous self-supervised methods, Pareto self-supervision employs a novel diversity constraint that encourages the discovery of diverse solutions within the embedding space. By optimizing both accuracy and diversity objectives simultaneously, Pareto self-supervision leads to substantial gains over prior methods across multiple benchmarks, including the popular OmniGlot and MiniImageNet datasets. Overall, Pareto self-supervision represents an important advance towards unlocking the true potential of few-shot learning.",1
"Recently, deep neural network models have achieved impressive results in various research fields. Come with it, an increasing number of attentions have been attracted by deep super-resolution (SR) approaches. Many existing methods attempt to restore high-resolution images from directly down-sampled low-resolution images or with the assumption of Gaussian degradation kernels with additive noises for their simplicities. However, in real-world scenarios, highly complex kernels and non-additive noises may be involved, even though the distorted images are visually similar to the clear ones. Existing SR models are facing difficulties to deal with real-world images under such circumstances. In this paper, we introduce a new kernel agnostic SR framework to deal with real-world image SR problem. The framework can be hanged seamlessly to multiple mainstream models. In the proposed framework, the degradation kernels and noises are adaptively modeled rather than explicitly specified. Moreover, we also propose an iterative supervision process and frequency-attended objective from orthogonal perspectives to further boost the performance. The experiments validate the effectiveness of the proposed framework on multiple real-world datasets.",0
"This paper presents a method for super resolution on real world images. We use pre trained deep learning models such as EDSR, ResizeNet and RCAN as base models for our experiments and compare their results. Our approach out performs existing approaches in terms of visual quality and running time. In conclusion we show that our method can produce high quality output at a fraction of the cost of previous methods.",1
"Contaminants such as dust, dirt and moisture adhering to the camera lens can greatly affect the quality and clarity of the resulting image or video. In this paper, we propose a video restoration method to automatically remove these contaminants and produce a clean video. Our approach first seeks to detect attention maps that indicate the regions that need to be restored. In order to leverage the corresponding clean pixels from adjacent frames, we propose a flow completion module to hallucinate the flow of the background scene to the attention regions degraded by the contaminants. Guided by the attention maps and completed flows, we propose a recurrent technique to restore the input frame by fetching clean pixels from adjacent frames. Finally, a multi-frame processing stage is used to further process the entire video sequence in order to enforce temporal consistency. The entire network is trained on a synthetic dataset that approximates the physical lighting properties of contaminant artifacts. This new dataset and our novel framework lead to our method that is able to address different contaminants and outperforms competitive restoration approaches both qualitatively and quantitatively.",0
"This paper presents new algorithms for solving the problem of artifact removal from video taken by moving cameras. There are many sources of such contamination, including motion blur, camera shake, image registration errors, lens distortions, sensor noise, and other kinds of image processing issues caused by the camera itself or the environment surrounding the camera during filming. Our work provides powerful tools to remove these unwanted effects from videos so that viewers can see more clearly.  The approach we take involves using advanced mathematical models of both the geometry and physics of imaging systems along with machine learning methods to accurately detect and remove the specific type of corruption present in each scene of a given video sequence. We show through experimentation on real data that our method delivers high quality results superior to existing state-of-the-art techniques. In particular, even though most previous approaches have focused exclusively either on geometric modeling or machine learning based solutions, we demonstrate how effectively combining both types of reasoning enables us to better learn the appearance of clean images which can then be used to improve their estimates for regions occluded by contaminants. Additionally, whereas other efforts have only considered simple synthetic problems with small motions or noise levels, we evaluate our system on challenging sequences from widely available public databases exhibiting large camera movements, significant vibrations, distorted optics, etc. Finally, we carefully study the tradeoff between several crucial design decisions involving feature detection/matching strategies (e.g., scale selection), optimization criteria, regularization terms (e.g., smoothness vs. sparsity preferences), as well as parameter initialization procedures and explore their impact over performance. Based o",1
"In this paper user modeling task is examined by processing a gallery of photos and videos on a mobile device. We propose novel engine for user preference prediction based on scene recognition, object detection and facial analysis. At first, all faces in a gallery are clustered and all private photos and videos with faces from large clusters are processed on the embedded system in offline mode. Other photos may be sent to the remote server to be analyzed by very deep models. The visual features of each photo are obtained from scene recognition and object detection models. These features are aggregated into a single user descriptor in the neural attention block. The proposed pipeline is implemented for the Android mobile platform. Experimental results with a subset of Photo Event Collection, Web Image Dataset for Event Recognition and Amazon Fashion datasets demonstrate the possibility to process images very efficiently without significant accuracy degradation. The source code of Android mobile application is publicly available at https://github.com/HSE-asavchenko/mobile-visual-preferences.",0
"This should give a clear idea of what your research entails without requiring reading the full text:  Imagine you are interacting with the following people who each have different needs and expectations from their mobile device:  * John wants a phone that can handle high performance tasks such as gaming and video editing but he has limited budget, so battery life and affordability are important factors. He values compactness over screen size. * Maria wants a phone that takes good quality photos and videos but she doesn't need all the bells and whistles like advanced zoom lenses or multiple camera modules, and she prioritizes portability and ease of use over technical specifications. She would prefer something more affordable than the latest flagship models. * David values durability above all else - his work requires him to take his phone into harsh environments and rough handling could result in damages - while still having enough processing power to run professional software required by his job, so longevity and ruggedness matter most. He may compromise on display quality if it means longer lifespan of the device.  If only there was a way to predict which type of phone would meet those requirements, saving time browsing through online stores and comparing countless specs. Our proposed method achieves just that - utilizing scene recognition techniques to analyze images taken with existing devices to identify key features, we can then suggest compatible options available in the market that cater to userâ€™s preferences, whether it be compact form factor or photography capabilities. By leveraging object detection algorithms combined with our knowledge database of past successful product pairings, users would receive personalized recommendations tailored to their individual usage patterns, leading to higher satisfaction rate and less returns. With the rapid advancements in smartphone technology, creating such solution is timely and valuable for both consumers looking f",1
"Sketch recognition algorithms are engineered and evaluated using publicly available datasets contributed by the sketch recognition community over the years. While existing datasets contain sketches of a limited set of generic objects, each new domain inevitably requires collecting new data for training domain specific recognizers. This gives rise to two fundamental concerns: First, will the data collection protocol yield ecologically valid data? Second, will the amount of collected data suffice to train sufficiently accurate classifiers? In this paper, we draw attention to these two concerns. We show that the ecological validity of the data collection protocol and the ability to accommodate small datasets are significant factors impacting recognizer accuracy in realistic scenarios. More specifically, using sketch-based gaming as a use case, we show that deep learning methods, as well as more traditional methods, suffer significantly from dataset shift. Furthermore, we demonstrate that in realistic scenarios where data is scarce and expensive, standard measures taken for adapting deep learners to small datasets fall short of comparing favorably with alternatives. Although transfer learning, and extensive data augmentation help deep learners, they still perform significantly worse compared to standard setups (e.g., SVMs and GBMs with standard feature representations). We pose learning from small datasets as a key problem for the deep sketch recognition field, one which has been ignored in the bulk of the existing literature.",0
"This paper presents a new methodology for training sketch recognizers on data from new domains. Traditionally, these models are trained on large amounts of annotated data from specific domains such as fashion, furniture, or electronics, limiting their ability to generalize to unseen domains. Our approach addresses this issue by leveraging transfer learning techniques and a novel domain adaptation algorithm that can effectively adapt a pretrained model to a new domain with only a small amount of labeled data. We evaluate our approach on multiple datasets across different domains, demonstrating improved performance compared to state-of-the-art methods. Additionally, we provide qualitative analysis of the generated results, illustrating the effectiveness of our proposed technique in enabling sketch recognizers to learn more generalized representations of objects. Overall, our work shows promise in advancing the field towards developing versatile sketch recognition systems capable of operating across diverse domains without requiring vast amounts of specialized training data.",1
"Image light source transfer (LLST), as the most challenging task in the domain of image relighting, has attracted extensive attention in recent years. In the latest research, LLST is decomposed three sub-tasks: scene reconversion, shadow estimation, and image re-rendering, which provides a new paradigm for image relighting. However, many problems for scene reconversion and shadow estimation tasks, including uncalibrated feature information and poor semantic information, are still unresolved, thereby resulting in insufficient feature representation. In this paper, we propose novel down-sampling feature self-calibrated block (DFSB) and up-sampling feature self-calibrated block (UFSB) as the basic blocks of feature encoder and decoder to calibrate feature representation iteratively because the LLST is similar to the recalibration of image light source. In addition, we fuse the multi-scale features of the decoder in scene reconversion task to further explore and exploit more semantic information, thereby providing more accurate primary scene structure for image re-rendering. Experimental results in the VIDIT dataset show that the proposed approach significantly improves the performance for LLST.",0
"This could be considered as an example of an ""executive summary"". If you want me to write an actual abstract instead, please tell me so: --- The goal here is image processing: take some images on one side, transfer their lighting conditions to other pictures on the opposite side. We present an effective network architecture that accomplishes just that. Called MSSN, it has several neat features, such as self-calibration across scales, which helps produce high quality output quickly and efficiently. We compare it against other state-of-the-art methods and our experiments confirm what we already knew -- MSSN is simply better! Not only can it process images faster than alternative systems but it delivers higher fidelity results time after time. And all of this with no need for lots of GPU memory like many competitors require. So whether your application needs realtime performance or fantastic outputs during post production work, our new MSSN system provides a compelling choice that offers superior tradeoffs compared to existing solutions. Give it a try â€“ you wonâ€™t regret it!",1
"In this paper, we present a simple yet effective method (ABSGD) for addressing the data imbalance issue in deep learning. Our method is a simple modification to momentum SGD where we leverage an attentional mechanism to assign an individual importance weight to each gradient in the mini-batch. Unlike many existing heuristic-driven methods for tackling data imbalance, our method is grounded in {\it theoretically justified distributionally robust optimization (DRO)}, which is guaranteed to converge to a stationary point of an information-regularized DRO problem. The individual-level weight of a sampled data is systematically proportional to the exponential of a scaled loss value of the data, where the scaling factor is interpreted as the regularization parameter in the framework of information-regularized DRO. Compared with existing class-level weighting schemes, our method can capture the diversity between individual examples within each class. Compared with existing individual-level weighting methods using meta-learning that require three backward propagations for computing mini-batch stochastic gradients, our method is more efficient with only one backward propagation at each iteration as in standard deep learning methods. To balance between the learning of feature extraction layers and the learning of the classifier layer, we employ a two-stage method that uses SGD for pretraining followed by ABSGD for learning a robust classifier and finetuning lower layers. Our empirical studies on several benchmark datasets demonstrate the effectiveness of the proposed method.",0
"In many real world applications such as spam detection, medical diagnosis, facial recognition etc., the target variable is often imbalanced i.e., one class dominates over the other. This leads to poor classification performance due to the skewness of data distribution. To overcome this problem, researchers have proposed different techniques which either oversample the minority classes or undersample the majority classes. However, these methods often suffer from high computational cost and yield limited improvements. Therefore, a novel methodology called Attentional Biased Stochastic Gradient (ABSG) is proposed that utilizes an attention mechanism to focus on important features and balance the impacts of both major and minority classes during training. Through experiments on numerous datasets, it was observed that ABSG outperformed existing state-of-the-art approaches by achieving higher accuracy while requiring fewer computations.",1
"Despite significant progress, we show that state of the art 3D human pose and shape estimation methods remain sensitive to partial occlusion and can produce dramatically wrong predictions although much of the body is observable. To address this, we introduce a soft attention mechanism, called the Part Attention REgressor (PARE), that learns to predict body-part-guided attention masks. We observe that state-of-the-art methods rely on global feature representations, making them sensitive to even small occlusions. In contrast, PARE's part-guided attention mechanism overcomes these issues by exploiting information about the visibility of individual body parts while leveraging information from neighboring body-parts to predict occluded parts. We show qualitatively that PARE learns sensible attention masks, and quantitative evaluation confirms that PARE achieves more accurate and robust reconstruction results than existing approaches on both occlusion-specific and standard benchmarks. Code will be available for research purposes at https://pare.is.tue.mpg.de/.",0
"Paper Abstract: This paper presents a method called ""Part Attention"" which allows deep neural networks trained on large datasets of 2D images of human bodies to create detailed three dimensional estimates of those bodies - or rather the surface positions of their body parts such as arms, legs and torso . This has applications to augmented reality, robotics, entertainment , gaming and many other fields. While some researchers have used similar methods based on machine learning for generating images from textual descriptions or audio input, this paper's focus lies primarily on regression towards precise geometric features of objects depicted in the input images. The main innovation of the proposed method consists in weighting different regions of interest (RoIs) within each image, according to how well they encode the information necessary for accurate part localization. To that end, we introduce ""part attention maps"", which represent probabilistic likelihoods of individual body parts corresponding to each RoI across all training images. By taking the most likely parts present in every patch into account during training and inference, our approach can capture the distribution over possible part locations at test time. We show through experiments that the resulting outputs correspond closely to ground truth annotations, especially if compared against approaches which simply classify each location as occupied by one specific part category without estimating depth/geometry. Furthermore, due to using only single view input data (as opposed to multi-view reconstruction techniques), the computational cost remains manageably low while still providing high precision in both positioning and shape estimation. Our results suggest that Part Attention may prove a valuable tool for realtime motion prediction, immersive user interfaces as well as photorealistic AR experiences among others; we discuss thes",1
"A variety of vision ailments are associated with geographic atrophy (GA) in the foveal region of the eye. In current clinical practice, the ophthalmologist manually detects potential presence of such GA based on fundus autofluorescence (FAF) images, and hence diagnoses the disease, when relevant. However, in view of the general scarcity of ophthalmologists relative to the large number of subjects seeking eyecare, especially in remote regions, it becomes imperative to develop methods to direct expert time and effort to medically significant cases. Further, subjects from either disadvantaged background or remote localities, who face considerable economic/physical barrier in consulting trained ophthalmologists, tend to seek medical attention only after being reasonably certain that an adverse condition exists. To serve the interest of both the ophthalmologist and the potential patient, we plan a screening step, where healthy and diseased eyes are algorithmically differentiated with limited input from only optometrists who are relatively more abundant in number. Specifically, an early treatment diabetic retinopathy study (ETDRS) grid is placed by an optometrist on each FAF image, based on which sectoral statistics are automatically collected. Using such statistics as features, healthy and diseased eyes are proposed to be classified by training an algorithm using available medical records. In this connection, we demonstrate the efficacy of support vector machines (SVM). Specifically, we consider SVM with linear as well as radial basis function (RBF) kernel, and observe satisfactory performance of both variants. Among those, we recommend the latter in view of its slight superiority in terms of classification accuracy (90.55% at a standard training-to-test ratio of 80:20), and practical class-conditional costs.",0
"Our method utilizes support vector machine (SVM) classification algorithms along with two features derived from FAF images of human eyes: Mean Fluorescence Intensity and Intermediate Ring Ratio. To evaluate our approachâ€™s effectiveness in detecting CNV, we analyze data collected from patients presenting symptoms including unexplained sudden vision loss, central visual field defects or decreased color vision as well as those without any known retinal disease which act as control groups. Experiments reveal that our proposed approach achieves higher accuracy compared to existing works which use single feature extraction techniques such as mean intensity alone. In addition, experiments show promising results in differentiating CNV cases from other diseases with high overall accuracies ranging from 89% to 97%.",1
"The ability to quickly learn from a small quantity oftraining data widens the range of machine learning applications. In this paper, we propose a data-efficient image captioning model, VisualGPT, which leverages the linguistic knowledge from a large pretrained language model(LM). A crucial challenge is to balance between the use of visual information in the image and prior linguistic knowledge acquired from pretraining. We designed a novel self-resurrecting encoder-decoder attention mechanism to quickly adapt the pretrained LM as the language decoder ona small amount of in-domain training data. The proposed self-resurrecting activation unit produces sparse activations but has reduced susceptibility to zero gradients. We train the proposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual Captions training data. Under these conditions, we outperform the best baseline model by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual Captions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray, a medical report generation dataset. To the best of our knowledge, this is the first work that improves data efficiency of image captioning by utilizing LM pretrained on unimodal data. Our code is available at: https://github.com/Vision-CAIR/VisualGPT.",0
Title: Data-Efficient Adaptation of Pre-trained Languag...,1
"Making accurate inferences about other individuals' locus of attention is essential for human social interactions and will be important for AI to effectively interact with humans. In this study, we compare how a CNN (convolutional neural network) based model of gaze and humans infer the locus of attention in images of real-world scenes with a number of individuals looking at a common location. We show that compared to the model, humans' estimates of the locus of attention are more influenced by the context of the scene, such as the presence of the attended target and the number of individuals in the image.",0
"In recent years, gaze perception has become an increasingly important topic in human cognition research. This is because understanding how humans perceive each otherâ€™s gazes can give us insights into our social interactions, communication abilities, and even decision making processes. Recently, advancements in computer vision have made it possible to develop models that can accurately predict where another person is looking, simply by analyzing their facial features. One such model is the Convolutional Neural Network (CNN)-based approach, which uses deep learning techniques to classify eye movements based on visual cues alone.  The aim of this study was to explore the effectiveness of using the CNN-based approach for detecting gaze direction from static images of faces compared to traditional methods used in psychological studies, such as point-light displays or dynamic stimuli presentations. We conducted experiments comparing the accuracy of gaze detection using the CNN-based method against these more conventional approaches, measuring the amount of error in degree of deviation from actual gaze direction.  Results showed that the CNN-based approach achieved higher levels of accuracy than traditional methods, particularly in cases where participants were facing directly towards or away from the camera. These findings suggest that using CNN-based approaches may offer significant advantages over existing methods, allowing for greater precision in gaze perception. Overall, we conclude that the ability to accurately predict gaze direction through artificial intelligence could open up new opportunities for exploring the neural mechanisms underlying complex social behaviors such as deception or empathy.",1
"Recent advancements in Artificial intelligence, especially deep learning, has changed many fields irreversibly by introducing state of the art methods for automation. Construction monitoring has not been an exception; as a part of construction monitoring systems, material classification and recognition have drawn the attention of deep learning and machine vision researchers. However, to create production-ready systems, there is still a long path to cover. Real-world problems such as varying illuminations and reaching acceptable accuracies need to be addressed in order to create robust systems. In this paper, we have addressed these issues and reached a state of the art performance, i.e., 97.35% accuracy rate for this task. Also, a new dataset containing 1231 images of 11 classes taken from several construction sites is gathered and publicly published to help other researchers in this field.",0
"Abstract: A material recognition system that leverages deep learning methods can significantly improve automated progress monitoring capabilities by accurately identifying construction materials at jobsites. This research proposes a novel methodology that integrates high-resolution aerial imagery with multi-viewpoint image processing techniques, enabling accurate labeling of objects across multiple perspectives. By combining a Convolutional Neural Network (CNN) architecture with Multi-Task Deep Learning (MTDL), we aim to achieve robust object detection and classification under real-world conditions encountered on building sites. Our approach evaluates performance metrics such as accuracy, precision, recall, F1 score, and IoU values to determine its effectiveness. We demonstrate proof-of-concept through experiments conducted using a publicly available dataset containing images captured from diverse angles and lighting scenarios. These preliminary results verify our hypothesis and provide a compelling case study for applying advanced computer vision and machine learning approaches to enhance project management practices in the construction industry.",1
"We tackle the problem of semantic image layout manipulation, which aims to manipulate an input image by editing its semantic label map. A core problem of this task is how to transfer visual details from the input images to the new semantic layout while making the resulting image visually realistic. Recent work on learning cross-domain correspondence has shown promising results for global layout transfer with dense attention-based warping. However, this method tends to lose texture details due to the resolution limitation and the lack of smoothness constraint of correspondence. To adapt this paradigm for the layout manipulation task, we propose a high-resolution sparse attention module that effectively transfers visual details to new layouts at a resolution up to 512x512. To further improve visual quality, we introduce a novel generator architecture consisting of a semantic encoder and a two-stage decoder for coarse-to-fine synthesis. Experiments on the ADE20k and Places365 datasets demonstrate that our proposed approach achieves substantial improvements over the existing inpainting and layout manipulation methods.",0
"Semantic Layout Manipulation with High-Resolution Sparse Attention Semantic layout manipulation refers to the process of altering the arrangement of elements within an image or other visual representation in order to convey different meanings or emphasize particular aspects. This can be useful in a variety of applications, such as graphic design, market research, and education. However, traditional methods of semantic layout manipulation tend to be laborious and time consuming, requiring skilled human intervention at every step. Moreover, these approaches often lack the ability to manipulate high-resolution images effectively, resulting in subpar results. In this work, we present a novel methodology that addresses both of these limitations by utilizing machine learning algorithms and computer vision techniques to automate the process of semantic layout manipulation on high resolution images while allowing users to control the level of detail they desire without loss of quality. Our approach employs sparse attention mechanisms which reduce computational cost while maximizing accuracy, making our solution scalable to large datasets. We demonstrate the effectiveness of our method using diverse case studies from real-world scenarios where precise fine tuning of object position or adding/removing objects is crucial for successful outcomes. Finally, we provide ablation experiments that validate how each component contributes positively to the overall performance of the system, making significant contributions towards automating complex tasks involving high-resolution images in general.",1
"Deep neural network (DNN) accelerators received considerable attention in recent years due to the potential to save energy compared to mainstream hardware. Low-voltage operation of DNN accelerators allows to further reduce energy consumption significantly, however, causes bit-level failures in the memory storing the quantized DNN weights. Furthermore, DNN accelerators have been shown to be vulnerable to adversarial attacks on voltage controllers or individual bits. In this paper, we show that a combination of robust fixed-point quantization, weight clipping, as well as random bit error training (RandBET) or adversarial bit error training (AdvBET) improves robustness against random or adversarial bit errors in quantized DNN weights significantly. This leads not only to high energy savings for low-voltage operation as well as low-precision quantization, but also improves security of DNN accelerators. Our approach generalizes across operating voltages and accelerators, as demonstrated on bit errors from profiled SRAM arrays, and achieves robustness against both targeted and untargeted bit-level attacks. Without losing more than 0.8%/2% in test accuracy, we can reduce energy consumption on CIFAR10 by 20%/30% for 8/4-bit quantization using RandBET. Allowing up to 320 adversarial bit errors, AdvBET reduces test error from above 90% (chance level) to 26.22% on CIFAR10.",0
"Artificial intelligence (AI) has become increasingly important as technology continues to advance. Deep neural networks (DNNs), a type of artificial intelligence model, have been widely adopted due to their remarkable performance on a range of tasks such as image recognition, speech synthesis, and natural language processing. However, DNNs can be vulnerable to bit errors that occur during inference, which can lead to incorrect results and reduced accuracy. In this paper, we explore two types of error resilience techniques for improving the robustness of DNN accelerators against random and adversarial bit errors: energy-efficient and secure methods. We demonstrate through experiments using state-of-the-art deep learning workloads and popular datasets that our proposed techniques significantly enhance the tolerance of DNN models to noise and reduce the risk of exploitation by malicious parties without sacrificing performance efficiency. Our findings provide valuable insights into the design of secure and reliable AI systems, highlighting the importance of addressing potential sources of error early on in the development process.",1
"Learning methods for relative camera pose estimation have been developed largely in isolation from classical geometric approaches. The question of how to integrate predictions from deep neural networks (DNNs) and solutions from geometric solvers, such as the 5-point algorithm, has as yet remained under-explored. In this paper, we present a novel framework that involves probabilistic fusion between the two families of predictions during network training, with a view to leveraging their complementary benefits in a learnable way. The fusion is achieved by learning the DNN uncertainty under explicit guidance by the geometric uncertainty, thereby learning to take into account the geometric solution in relation to the DNN prediction. Our network features a self-attention graph neural network, which drives the learning by enforcing strong interactions between different correspondences and potentially modeling complex relationships between points. We propose motion parmeterizations suitable for learning and show that our method achieves state-of-the-art performance on the challenging DeMoN and ScanNet datasets. While we focus on relative pose, we envision that our pipeline is broadly applicable for fusing classical geometry and deep learning.",0
"This paper presents a method for fusing two different approaches to learning camera pose from image data: geometry-guided uncertainty (GU), which provides accurate estimates but only works well on highly textured scenes; and monocular visual odometry (VO), which generalizes better to varied environments but can suffer from drift and scale ambiguity. By combining GU and VO through a shared feature space learned via self-supervised training on large datasets of panoramic images, we achieve a system that learns both high accuracy and robustness across domains. Our results demonstrate state-of-the-art performance on several benchmarks, including KITTI and ETH3D, while maintaining competitive efficiency compared to previous methods. In addition, our framework allows us to learn geometric constraints explicitly rather than relying solely on optimization tricks, enabling more generalization without sacrificing accuracy. Overall, our work shows how traditional techniques can be adapted using modern machine learning paradigms to create hybrid systems capable of outperforming pure vision-based solutions.",1
"Heavy rain removal from a single image is the task of simultaneously eliminating rain streaks and fog, which can dramatically degrade the quality of captured images. Most existing rain removal methods do not generalize well for the heavy rain case. In this work, we propose a novel network architecture consisting of three sub-networks to remove heavy rain from a single image without estimating rain streaks and fog separately. The first sub-net, a U-net-based architecture that incorporates our Spatial Channel Attention (SCA) blocks, extracts global features that provide sufficient contextual information needed to remove atmospheric distortions caused by rain and fog. The second sub-net learns the additive residues information, which is useful in removing rain streak artifacts via our proposed Residual Inception Modules (RIM). The third sub-net, the multiplicative sub-net, adopts our Channel-attentive Inception Modules (CIM) and learns the essential brighter local features which are not effectively extracted in the SCA and additive sub-nets by modulating the local pixel intensities in the derained images. Our three clean image results are then combined via an attentive blending block to generate the final clean image. Our method with SCA, RIM, and CIM significantly outperforms the previous state-of-the-art single-image deraining methods on the synthetic datasets, shows considerably cleaner and sharper derained estimates on the real image datasets. We present extensive experiments and ablation studies supporting each of our method's contributions on both synthetic and real image datasets.",0
"Our goal was to improve rain removal algorithms that could effectively mitigate heavy raindrops from single images without losing details of the underlying scene. We proposed an encoder-decoder architecture based method utilizing global and local attention mechanisms. By applying attention to both global context and local features within each block of the image, we were able to achieve state-of-the-art performance while preserving essential image details. In addition to enhancing visual quality, our algorithm reduced over-cleaning artifacts commonly observed in other methods. Our work demonstrates the potential for using multi-scale attentional processing to address challenges faced by rain removal algorithms. We hope that our approach serves as a foundation for future research in the field and leads to more advanced solutions. -----",1
"Recently, data-driven single-view reconstruction methods have shown great progress in modeling 3D dressed humans. However, such methods suffer heavily from depth ambiguities and occlusions inherent to single view inputs. In this paper, we address such issues by lifting the single-view input with additional views and investigate the best strategy to suitably exploit information from multiple views. We propose an end-to-end approach that learns an implicit 3D representation of dressed humans from sparse camera views. Specifically, we introduce two key components: first an attention-based fusion layer that learns to aggregate visual information from several viewpoints; second a mechanism that encodes local 3D patterns under the multi-view context. In the experiments, we show the proposed approach outperforms the state of the art on standard data both quantitatively and qualitatively. Additionally, we apply our method on real data acquired with a multi-camera platform and demonstrate our approach can obtain results comparable to multi-view stereo with dramatically less views.",0
"Learning implicit representations refers to learning a high level representation that can generalize well across tasks by directly predicting task relevant outputs without requiring intermediate explicit representations. Recent research has shown the effectiveness of using convolutional neural networks (CNNs) to learn implicit 3D representations of humans in both pose estimation and shape reconstruction. In this work we focus on the problem of estimating 3D human body meshes from sparse multi-view images. Current state-of-the-art approaches rely on template models to explicitly represent 3D geometry, which then need to be refined through optimization techniques such as non-rigid structure from motion. We propose a novel framework called Sparse Convolutional Network (SConvNet), that learns an implicit 3D human mesh representation solely based on raw image input data. Our approach uses a cascade of CNN layers along with view synthesis operations to efficiently encode a large receptive field while still allowing for end-to-end training. Experiments show that our method outperforms previous state-of-the-art methods for 3D human mesh estimation from multiple views, achieving more accurate results while also being faster due to the lack of explicit geometry. Further experiments demonstrate that our learned representation generalizes well across datasets and tasks and can even estimate full view representations given only partial observations.",1
"Pre-training Graph Neural Networks (GNN) via self-supervised contrastive learning has recently drawn lots of attention. However, most existing works focus on node-level contrastive learning, which cannot capture global graph structure. The key challenge to conducting subgraph-level contrastive learning is to sample informative subgraphs that are semantically meaningful. To solve it, we propose to learn graph motifs, which are frequently-occurring subgraph patterns (e.g. functional groups of molecules), for better subgraph sampling. Our framework MotIf-driven Contrastive leaRning Of Graph representations (MICRO-Graph) can: 1) use GNNs to extract motifs from large graph datasets; 2) leverage learned motifs to sample informative subgraphs for contrastive learning of GNN. We formulate motif learning as a differentiable clustering problem, and adopt EM-clustering to group similar and significant subgraphs into several motifs. Guided by these learned motifs, a sampler is trained to generate more informative subgraphs, and these subgraphs are used to train GNNs through graph-to-subgraph contrastive learning. By pre-training on the ogbg-molhiv dataset with MICRO-Graph, the pre-trained GNN achieves 2.04% ROC-AUC average performance enhancement on various downstream benchmark datasets, which is significantly higher than other state-of-the-art self-supervised learning baselines.",0
"""Motivated by the successes of contrastive learning in computer vision and natural language processing, we present a new framework for graph representation learning that leverages motifs as the central element for constructing positive pairs and negative samples during training.  In recent years, graphs have emerged as a powerful data structure for modeling complex relationships within data across diverse domains, from social networks to biological interactions. However, designing effective representations that capture key characteristics and patterns in these data remains a challenge. While there exist several approaches based on different paradigms (e.g., random walks, embedding methods), they often struggle with scalability or cannot effectively encode information about higher-order patterns or motifs found in real-world datasets.  This work addresses these shortcomings by proposing a novel end-to-end trainable architecture called MotifContrast, which learns contextually rich node representations via minimizing cross-entropy losses over constructed positive and negative samples built upon motif patterns. Our contributions include introducing motifs as discriminative elements for constructing positive/negative pairs, developing efficient algorithms for mining frequent and meaningful subgraphs using a memory footprint similar to existing methods but improved pattern coverage, and demonstrating the effectiveness and efficiency of our method through extensive experiments on four benchmark datasets across multiple application areas.  Empirical evaluation shows MotifContrast consistently outperforms prior arts across all metrics including unsupervised clustering, semi-supervised classification, link prediction, and visualization quality. Our ablation studies further verify the importance of specific components in our system such as the choice of loss function, batch construction strategies, and the strength of regularization terms. Finally, sensitivity analyses illustrate how MotifContrast can adaptively balance information captured by local connectivity vs. global topology, highlighting potential future research directions.""",1
"As an emerging and challenging problem in the computer vision community, weakly supervised object localization and detection plays an important role for developing new generation computer vision systems and has received significant attention in the past decade. As methods have been proposed, a comprehensive survey of these topics is of great importance. In this work, we review (1) classic models, (2) approaches with feature representations from off-the-shelf deep networks, (3) approaches solely based on deep learning, and (4) publicly available datasets and standard evaluation metrics that are widely used in this field. We also discuss the key challenges in this field, development history of this field, advantages/disadvantages of the methods in each category, the relationships between methods in different categories, applications of the weakly supervised object localization and detection methods, and potential future directions to further promote the development of this research field.",0
"Despite significant advances made by supervised deep learning methods in object detection tasks, weakly supervised approaches have been gaining attention due to their potential for reducing annotation cost while improving model accuracy. In this work, we conduct a comprehensive survey on state-of-the-art techniques related to weakly supervised object localization and detection (WSOD). Our analysis covers different WSOL frameworks such as bounding box regression, anchors refinement, objectness score map filtering, and proposal generation guided by pseudo boxes. We discuss recent efforts aimed at incorporating contextual knowledge into these frameworks through self-training, active learning, co-localization clustering, and multi-task learning. Furthermore, we review evaluation metrics used in WSOD literature including mean average precision, localization error rate curve, and recall-based measures, among others. Finally, we provide insights into future research directions that could further advance the development of effective WSOD models. Overall, our study serves as a guide for practitioners interested in leveraging weakly supervised learning methods for object localization and detection tasks.",1
"Graph neural networks (GNNs) have been widely used in deep learning on graphs. They can learn effective node representations that achieve superior performances in graph analysis tasks such as node classification and node clustering. However, most methods ignore the heterogeneity in real-world graphs. Methods designed for heterogeneous graphs, on the other hand, fail to learn complex semantic representations because they only use meta-paths instead of meta-graphs. Furthermore, they cannot fully capture the content-based correlations between nodes, as they either do not use the self-attention mechanism or only use it to consider the immediate neighbors of each node, ignoring the higher-order neighbors. We propose a novel Higher-order Attribute-Enhancing (HAE) framework that enhances node embedding in a layer-by-layer manner. Under the HAE framework, we propose a Higher-order Attribute-Enhancing Graph Neural Network (HAEGNN) for heterogeneous network representation learning. HAEGNN simultaneously incorporates meta-paths and meta-graphs for rich, heterogeneous semantics, and leverages the self-attention mechanism to explore content-based nodes interactions. The unique higher-order architecture of HAEGNN allows examining the first-order as well as higher-order neighborhoods. Moreover, HAEGNN shows good explainability as it learns the importances of different meta-paths and meta-graphs. HAEGNN is also memory-efficient, for it avoids per meta-path based matrix calculation. Experimental results not only show HAEGNN superior performance against the state-of-the-art methods in node classification, node clustering, and visualization, but also demonstrate its superiorities in terms of memory efficiency and explainability.",0
"Our newest work applies graph neural networks to problems involving higher-order attributes (e.g., edge labels), where we apply multiple graph convolutional layers on heterogeneous graphs using different attribute functions and achieve excellent results. We show that our approach can improve upon previous state-of-the-art methods for node classification tasks. To enable end-to-end training, we use the attribute function as another component of the message passing process like other graph convolution approaches. In this way, the model automatically learns how to make use of the additional information provided by edge annotations without any manual engineering. Extensive experiments validate the effectiveness of our proposed methodology across five benchmark datasets. Overall, these findings offer promise for future research into developing more generalizable models capable of leveraging rich edge features effectively.",1
"Few researches have been proposed specifically for real-time semantic segmentation in rainy environments. However, the demand in this area is huge and it is challenging for lightweight networks. Therefore, this paper proposes a lightweight network which is specially designed for the foreground segmentation in rainy environments, named De-raining Semantic Segmentation Network (DRSNet). By analyzing the characteristics of raindrops, the MultiScaleSE Block is targetedly designed to encode the input image, it uses multi-scale dilated convolutions to increase the receptive field, and SE attention mechanism to learn the weights of each channels. In order to combine semantic information between different encoder and decoder layers, it is proposed to use Asymmetric Skip, that is, the higher semantic layer of encoder employs bilinear interpolation and the output passes through pointwise convolution, then added element-wise to the lower semantic layer of decoder. According to the control experiments, the performances of MultiScaleSE Block and Asymmetric Skip compared with SEResNet18 and Symmetric Skip respectively are improved to a certain degree on the Foreground Accuracy index. The parameters and the floating point of operations (FLOPs) of DRSNet is only 0.54M and 0.20GFLOPs separately. The state-of-the-art results and real-time performances are achieved on both the UESTC all-day Scenery add rain (UAS-add-rain) and the Baidu People Segmentation add rain (BPS-add-rain) benchmarks with the input sizes of 192*128, 384*256 and 768*512. The speed of DRSNet exceeds all the networks within 1GFLOPs, and Foreground Accuracy index is also the best among the similar magnitude networks on both benchmarks.",0
"Semantic segmentation has been one of the most popular topics in recent years due to the development of deep learning methods like Fully Convolutional Networks (FCN) and UNet which have achieved state-of-the-art results on numerous benchmark datasets. However, current semantic segmentation models often require large amounts of computation power and time to process images, making them difficult to use for real-time applications such as video conferencing or autonomous driving where low latency is crucial. In addition, these models may struggle when dealing with rainy or foggy weather conditions that affect image clarity and make accurate object detection challenging. This paper proposes a new approach called De-raining Semantic Segmentation Network (DSN) to improve the accuracy and speed of semantic segmentation under adverse weather conditions. Our proposed method uses a rain removal module prior to the main FCN model, followed by an adversarial training scheme to enhance the networkâ€™s robustness against rain and other disturbances. Extensive experiments demonstrate that our DSN can effectively suppress rain streaks while maintaining high accuracy for semantic segmentation compared to existing methods, even under heavy rainfall conditions. Furthermore, our method significantly reduces computational cost without sacrificing performance, making it well suited for real-time applications where quick processing times are essential. Our findings contribute towards advancing the field of semantic segmentation by providing a more efficient solution capable of performing accurately under diverse environmental scenarios",1
"The main progress for action segmentation comes from densely-annotated data for fully-supervised learning. Since manual annotation for frame-level actions is time-consuming and challenging, we propose to exploit auxiliary unlabeled videos, which are much easier to obtain, by shaping this problem as a domain adaptation (DA) problem. Although various DA techniques have been proposed in recent years, most of them have been developed only for the spatial direction. Therefore, we propose Mixed Temporal Domain Adaptation (MTDA) to jointly align frame- and video-level embedded feature spaces across domains, and further integrate with the domain attention mechanism to focus on aligning the frame-level features with higher domain discrepancy, leading to more effective domain adaptation. Finally, we evaluate our proposed methods on three challenging datasets (GTEA, 50Salads, and Breakfast), and validate that MTDA outperforms the current state-of-the-art methods on all three datasets by large margins (e.g. 6.4% gain on F1@50 and 6.8% gain on the edit score for GTEA).",0
"This paper presents a method for action segmentation that utilizes mixed temporal domain adaptation (MTDA) to improve performance. Existing approaches often struggle with varying conditions across different segments, leading to suboptimal results. Our proposed approach addresses this issue by adapting only the features from each video clip rather than applying the same transformation on all clips. We achieve state-of-the-art performance on challenging benchmarks such as THUMOS'14 and YouTube-Seg.  Our method can effectively handle large changes in illumination, background appearance, motion blur, occlusion, etc., which are common issues encountered during unsupervised domain adaption. By fine-tuning models using task-specific losses, our system can better capture the variations within and across actions. Experiments show that MTDA significantly improves the accuracy of boundary detection compared to prior methods while maintaining fast inference speed. Finally, we demonstrate the versatility of our system through transfer learning on a new dataset without additional training. Overall, our work advances the field of action segmentation and shows promising applications in real-world scenarios.",1
"Heterogeneous presentation of a neurological disorder suggests potential differences in the underlying pathophysiological changes that occur in the brain. We propose to model heterogeneous patterns of functional network differences using a demographic-guided attention (DGA) mechanism for recurrent neural network models for prediction from functional magnetic resonance imaging (fMRI) time-series data. The context computed from the DGA head is used to help focus on the appropriate functional networks based on individual demographic information. We demonstrate improved classification on 3 subsets of the ABIDE I dataset used in published studies that have previously produced state-of-the-art results, evaluating performance under a leave-one-site-out cross-validation framework for better generalizeability to new data. Finally, we provide examples of interpreting functional network differences based on individual demographic variables.",0
"This paper presents Demographic-Guided Attention (DGA) as a novel approach for modeling neuropathophysiological heterogeneity in recurrent neural networks. DGA enables fine-grained control over the attention mechanism used by these models, allowing them to selectively focus on specific demographics such as age groups, genders or other categorical variables while processing sequential input data. Our approach operates without modifying the underlying architecture and can be applied seamlessly to existing RNN architectures. We demonstrate the effectiveness of our method on two challenging tasks: the prediction of Alzheimer's disease from structural MRI scans and predicting clinical outcomes for patients diagnosed with schizophrenia based on Electronic Health Records (EHR). Experimental results show that our approach significantly improves performance across both benchmark datasets compared to state-of-the art baselines. Furthermore, we perform ablation studies which confirm that our proposed method indeed benefits from exploiting demographic-specific attention mechanisms. Overall, our work provides evidence that leveraging demographic information through attention modules in RNNs can lead to improved generalization capabilities and better predictions of complex human conditions. We believe that our approach paves the way towards more personalized and effective machine learning algorithms, particularly for those operating in healthcare domains where demographic disparities play a critical role.",1
"Underwater image restoration attracts significant attention due to its importance in unveiling the underwater world. This paper elaborates on a novel method that achieves state-of-the-art results for underwater image restoration based on the unsupervised image-to-image translation framework. We design our method by leveraging from contrastive learning and generative adversarial networks to maximize mutual information between raw and restored images. Additionally, we release a large-scale real underwater image dataset to support both paired and unpaired training modules. Extensive experiments with comparisons to recent approaches further demonstrate the superiority of our proposed method.",0
"This paper presents a novel approach to single image underwater restoration using contrastive learning. With the increasing availability of high-resolution cameras on autonomous vehicles such as drones and remotely operated vehicles (ROVs), there has been a growing interest in collecting large amounts of images from underwater environments. However, these images often suffer from degradations due to light absorption, reflections and diffractions caused by water, leading to poor visibility. To address this issue, we propose a deep neural network architecture that leverages the power of contrastive learning. Our model takes into account both global structure and local details through two parallel branches - one focusing on low resolution features and the other focused on high resolution details. We conduct extensive experiments comparing our method against state-of-the art techniques for single image underwater restoration and demonstrate significant improvements in visual quality across a variety of datasets. Our work provides a new direction towards improving the interpretation of the submerged environment through enhanced image recovery algorithms.",1
"Siamese deep-network trackers have received significant attention in recent years due to their real-time speed and state-of-the-art performance. However, Siamese trackers suffer from similar looking confusers, that are prevalent in aerial imagery and create challenging conditions due to prolonged occlusions where the tracker object re-appears under different pose and illumination. Our work proposes SiamReID, a novel re-identification framework for Siamese trackers, that incorporates confuser rejection during prolonged occlusions and is well-suited for aerial tracking. The re-identification feature is trained using both triplet loss and a class balanced loss. Our approach achieves state-of-the-art performance in the UAVDT single object tracking benchmark.",0
"This paper presents a new approach to tracking and re-identifying objects in images and video frames using deep learning techniques called SiamReID. The system uses a siamese neural network architecture that can learn to associate objects across multiple frames by identifying unique features and relationships between them. In addition, we introduce a confuser aware mechanism which helps the tracker to focus on more challenging objects and ignore easy ones, leading to better accuracy and efficiency. We evaluate our method on several benchmark datasets and demonstrate significant improvements over state-of-the-art trackers in terms of precision, recall, and speed. Our work shows promising results towards developing intelligent computer vision systems capable of real-time object identification and tracking.",1
"Graph convolutional networks (GCNs) have received considerable research attention recently. Most GCNs learn the node representations in Euclidean geometry, but that could have a high distortion in the case of embedding graphs with scale-free or hierarchical structure. Recently, some GCNs are proposed to deal with this problem in non-Euclidean geometry, e.g., hyperbolic geometry. Although hyperbolic GCNs achieve promising performance, existing hyperbolic graph operations actually cannot rigorously follow the hyperbolic geometry, which may limit the ability of hyperbolic geometry and thus hurt the performance of hyperbolic GCNs. In this paper, we propose a novel hyperbolic GCN named Lorentzian graph convolutional network (LGCN), which rigorously guarantees the learned node features follow the hyperbolic geometry. Specifically, we rebuild the graph operations of hyperbolic GCNs with Lorentzian version, e.g., the feature transformation and non-linear activation. Also, an elegant neighborhood aggregation method is designed based on the centroid of Lorentzian distance. Moreover, we prove some proposed graph operations are equivalent in different types of hyperbolic geometry, which fundamentally indicates their correctness. Experiments on six datasets show that LGCN performs better than the state-of-the-art methods. LGCN has lower distortion to learn the representation of tree-likeness graphs compared with existing hyperbolic GCNs. We also find that the performance of some hyperbolic GCNs can be improved by simply replacing the graph operations with those we defined in this paper.",0
"Recently, graph convolutional networks (GCN) have gained attention due to their ability to learn features on non-Euclidean domains such as graphs. However, these models rely heavily on the assumption that time is Abelian, meaning they cannot model nonlinear temporal dependencies. To address this limitation, we propose a new approach called Lorentzian GCN (LGCN), which uses Lorentz transformations to handle nonlinear temporal dependencies while still preserving the simplicity of GCN. Our experiments show that LGCN outperforms traditional GCNs on several tasks, including node classification, link prediction, and anomaly detection on real-world datasets. Additionally, our ablation studies demonstrate the effectiveness of each component in our proposed framework. Overall, our work extends the scope of GCNs by allowing them to capture complex temporal patterns, paving the way for improved performance in many applications involving irregularly sampled data.",1
"Adversarial attacks on machine learning-based classifiers, along with defense mechanisms, have been widely studied in the context of single-label classification problems. In this paper, we shift the attention to multi-label classification, where the availability of domain knowledge on the relationships among the considered classes may offer a natural way to spot incoherent predictions, i.e., predictions associated to adversarial examples lying outside of the training data distribution. We explore this intuition in a framework in which first-order logic knowledge is converted into constraints and injected into a semi-supervised learning problem. Within this setting, the constrained classifier learns to fulfill the domain knowledge over the marginal distribution, and can naturally reject samples with incoherent predictions. Even though our method does not exploit any knowledge of attacks during training, our experimental analysis surprisingly unveils that domain-knowledge constraints can help detect adversarial examples effectively, especially if such constraints are not known to the attacker.",0
"This could be quite challenging as I need to write an abstract without knowing what the paper will actually say! However, here goes:  Domain knowledge plays an important role in improving performance and robustness in multi-label classification tasks. Incorporating domain expertise can help alleviate adversarial attacks by enabling the model to focus on relevant features that may be overlooked in feature engineering or data preprocessing steps. By utilizing prior knowledge of the problem space, models can better identify potentially malicious inputs and make more informed predictions. Additionally, domain knowledge can provide valuable insights into the design of new evaluation metrics that capture both accuracy and stability under attack.  In this study, we evaluate the effectiveness of using domain expertise in reducing the vulnerability of state-of-the-art multi-label classifiers to common adversarial attacks. We use several benchmark datasets across different domains such as image processing, natural language processing, and bioinformatics. Our results demonstrate that incorporating domain knowledge leads to significant improvements in model robustness while maintaining high levels of accuracy. Furthermore, our analysis shows that well-designed, knowledge-driven countermeasures outperform simpler heuristics and traditional defenses against adversaries. Finally, we discuss future research directions towards developing comprehensive frameworks that integrate domain knowldege seamlessly within existing machine learning pipelines.  This work highlights the importance of domain knowledge in improving robustness in multi-label classifiers and provides practical techniques for developers seeking to build resilient AI systems in real-world applications. Overall, our findings contribute to the growing body of literature exploring how to effectively mitigate the effects of adversarial perturbations while preserving desirable model behaviors like efficiency and interpretabilit",1
"In this paper, we present a process to investigate the effects of transfer learning for automatic facial expression recognition from emotions to pain. To this end, we first train a VGG16 convolutional neural network to automatically discern between eight categorical emotions. We then fine-tune successively larger parts of this network to learn suitable representations for the task of automatic pain recognition. Subsequently, we apply those fine-tuned representations again to the original task of emotion recognition to further investigate the differences in performance between the models. In the second step, we use Layer-wise Relevance Propagation to analyze predictions of the model that have been predicted correctly previously but are now wrongly classified. Based on this analysis, we rely on the visual inspection of a human observer to generate hypotheses about what has been forgotten by the model. Finally, we test those hypotheses quantitatively utilizing concept embedding analysis methods. Our results show that the network, which was fully fine-tuned for pain recognition, indeed payed less attention to two action units that are relevant for expression recognition but not for pain recognition.",0
"In recent years, deep neural networks (DNNs) have been successfully applied to facial expression recognition tasks. However, one issue that arises in DNN models is catastrophic forgetting - where new knowledge overrides previously learned knowledge. This study examines whether DNNs trained on health related facial expressions experience catastrophic forgetting. To mitigate this problem, transfer learning was used to fine-tune pre-trained face detection models on two different datasets: EmotioNet and DISFA. Results show no significant difference in performance compared to training from scratch using cross validation and fivefold stratified random sampling. These results suggest that deep neural network models may not completely forget previous learned knowledge when applied to new datasets. Further research is recommended to better understand how DNNs retain previous knowledge and perform across different applications.",1
"The current Siamese network based on region proposal network (RPN) has attracted great attention in visual tracking due to its excellent accuracy and high efficiency. However, the design of the RPN involves the selection of the number, scale, and aspect ratios of anchor boxes, which will affect the applicability and convenience of the model. Furthermore, these anchor boxes require complicated calculations, such as calculating their intersection-over-union (IoU) with ground truth bounding boxes.Due to the problems related to anchor boxes, we propose a simple yet effective anchor-free tracker (named Siamese corner networks, SiamCorners), which is end-to-end trained offline on large-scale image pairs. Specifically, we introduce a modified corner pooling layer to convert the bounding box estimate of the target into a pair of corner predictions (the bottom-right and the top-left corners). By tracking a target as a pair of corners, we avoid the need to design the anchor boxes. This will make the entire tracking algorithm more flexible and simple than anchorbased trackers. In our network design, we further introduce a layer-wise feature aggregation strategy that enables the corner pooling module to predict multiple corners for a tracking target in deep networks. We then introduce a new penalty term that is used to select an optimal tracking box in these candidate corners. Finally, SiamCorners achieves experimental results that are comparable to the state-of-art tracker while maintaining a high running speed. In particular, SiamCorners achieves a 53.7% AUC on NFS30 and a 61.4% AUC on UAV123, while still running at 42 frames per second (FPS).",0
"Here is one example of an abstract:  The development of high-performance visual tracking algorithms has been a longstanding challenge in computer vision. In recent years, deep learning approaches have shown promising results in addressing this problem by learning features that can be used for object detection and localization in video sequences. One such approach is the use of Siamese neural networks, which consist of two identical subnetworks trained on pairs of positive and negative examples. These networks are then used to predict whether or not a given patch extracted from a search image corresponds to the target object. While these methods have achieved state-of-the-art performance on many benchmark datasets, they still suffer from limitations in terms of their ability to handle occlusions, large appearance changes, and variations in scale and aspect ratio. To overcome these challenges, we propose a new framework called SiamCorners, which builds upon the idea of using a pairwise loss function but incorporates novel corner pooling operations that allow us to model spatial relationships within each feature map. This allows our method to achieve more accurate and robust predictions, even under difficult conditions where other trackers may fail. Our experiments on several popular benchmark datasets demonstrate the effectiveness of our approach, outperforming many current state-of-the-art techniques. Overall, we believe that SiamCorners represents a significant step forward in the field of visual tracking, opening up exciting possibilities for future research in this area.",1
"Large-scale trademark retrieval is an important content-based image retrieval task. A recent study shows that off-the-shelf deep features aggregated with Regional-Maximum Activation of Convolutions (R-MAC) achieve state-of-the-art results. However, R-MAC suffers in the presence of background clutter/trivial regions and scale variance, and discards important spatial information. We introduce three simple but effective modifications to R-MAC to overcome these drawbacks. First, we propose the use of both sum and max pooling to minimise the loss of spatial information. We also employ domain-specific unsupervised soft-attention to eliminate background clutter and unimportant regions. Finally, we add multi-resolution inputs to enhance the scale-invariance of R-MAC. We evaluate these three modifications on the million-scale METU dataset. Our results show that all modifications bring non-trivial improvements, and surpass previous state-of-the-art results.",0
"In today's world where intellectual property rights play a significant role in businesses, trademark recognition has become increasingly important. This research presents a novel approach to enhance the performance of deep convolutional neural networks (CNN) based feature extraction methods used for trademark retrieval. Our method learns regional attention on multi-resolution features generated by a CNN architecture. Unlike traditional approaches that rely solely on global context from dense representations, we argue that local context plays a crucial role in identifying patterns such as logos, brand names, and mascots present in images. Experimental results show that our proposed model achieves state-of-the-art accuracy, outperforming existing techniques on several benchmark datasets. Furthermore, analysis of the learned attention maps reveals meaningful insights into the trademarks and regions of interest captured by the network. These findings demonstrate the effectiveness of incorporating local context learning into the feature extraction pipeline for improved trademark retrieval.",1
"The attention mechanism has been widely adopted in acoustic scene classification. However, we find that during the process of attention exclusively emphasizing information, it tends to excessively discard information although improving the performance. We propose a mechanism referred to as the attentive max feature map which combines two effective techniques, attention and max feature map, to further elaborate the attention mechanism and mitigate the abovementioned phenomenon. Furthermore, we explore various joint learning methods that utilize additional labels originally generated for subtask B (3-classes) on top of existing labels for subtask A (10-classes) of the DCASE2020 challenge. We expect that using two kinds of labels simultaneously would be helpful because the labels of the two subtasks differ in their degree of abstraction. Applying two proposed techniques, our proposed system achieves state-of-the-art performance among single systems on subtask A. In addition, because the model has a complexity comparable to subtask B's requirement, it shows the possibility of developing a system that fulfills the requirements of both subtasks; generalization on multiple devices and low-complexity.",0
"In recent years, there has been significant interest in developing automated systems that can classify acoustic scenes based on audio recordings. To tackle this challenging task, many approaches have focused on extracting high-level features from audio signals using machine learning techniques. In particular, deep convolutional neural networks (CNNs) have proven effective in capturing complex patterns and relationships within audio data. However, traditional CNN architectures often struggle to capture global contextual information across different regions of the input signal, resulting in suboptimal performance.  To address this limitation, we propose a novel network architecture called ""Attentive Max Feature Maps"" (AMFM). Our approach adapts the popular U-Net architecture by incorporating attentional mechanisms into each max pooling layer, enabling the model to selectively focus on important spatial regions at multiple scales. This design choice allows AMFM to jointly learn local and global representations while also allowing for efficient computation.  Furthermore, we introduce an ablation study to investigate the effectiveness of our proposed method, comparing it against several baseline models including U-Nets, ResNeSt, and FPN. Experimental results on two benchmark datasets demonstrate that AMFM significantly outperforms these competitive methods, achieving new state-of-the-art accuracy rates across both tasks.  In summary, our work presents a novel approach for acoustic scene classification using joint attention maps, providing insights into how attentive feature extraction can improve the robustness of deep learning models to handle diverse audio inputs. We hope our findings will inspire further research towards creating intelligent systems capable of accurately understanding complex auditory environments.",1
"Change detection from synthetic aperture radar (SAR) imagery is a critical yet challenging task. Existing methods mainly focus on feature extraction in spatial domain, and little attention has been paid to frequency domain. Furthermore, in patch-wise feature analysis, some noisy features in the marginal region may be introduced. To tackle the above two challenges, we propose a Dual-Domain Network. Specifically, we take features from the discrete cosine transform domain into consideration and the reshaped DCT coefficients are integrated into the proposed model as the frequency domain branch. Feature representations from both frequency and spatial domain are exploited to alleviate the speckle noise. In addition, we further propose a multi-region convolution module, which emphasizes the central region of each patch. The contextual information and central region features are modeled adaptively. The experimental results on three SAR datasets demonstrate the effectiveness of the proposed model. Our codes are available at https://github.com/summitgao/SAR_CD_DDNet.",0
"This paper presents a novel approach for detecting changes in synthetic aperture radar (SAR) images using deep learning techniques. SAR imagery has become increasingly important in many applications such as environmental monitoring, disaster management, and urban planning due to its ability to capture high resolution images even under adverse weather conditions. However, automatic change detection in large volumes of SAR data remains challenging due to factors like varying acquisition parameters, complex atmospheric effects, and diverse land cover characteristics.  To address these issues, we propose a dual-domain convolutional neural network architecture that exploits both the spatial and spectral domains of SAR intensity images. Specifically, our method extracts two separate feature representations from the SAR image by applying domain-specific convolutional layers, which are then fused through multi-scale fusion blocks to enhance complementary information across different scales. Furthermore, we introduce a new adaptive attention module that enables the model to dynamically weight features according to their relevance for change detection, allowing better handling of variable acquisition configurations.  Experimental results on several public benchmark datasets demonstrate the effectiveness of our proposed approach compared to state-of-the-art methods in terms of accuracy, robustness, and efficiency. Our method can successfully identify subtle changes in urban environments, coastal areas, and rural regions, highlighting its potential utility for various real-world applications. Overall, our work contributes towards enabling reliable and scalable change detection capabilities in SAR imagery using advanced machine learning techniques.",1
"Collaborative bandit learning, i.e., bandit algorithms that utilize collaborative filtering techniques to improve sample efficiency in online interactive recommendation, has attracted much research attention as it enjoys the best of both worlds. However, all existing collaborative bandit learning solutions impose a stationary assumption about the environment, i.e., both user preferences and the dependency among users are assumed static over time. Unfortunately, this assumption hardly holds in practice due to users' ever-changing interests and dependence relations, which inevitably costs a recommender system sub-optimal performance in practice.   In this work, we develop a collaborative dynamic bandit solution to handle a changing environment for recommendation. We explicitly model the underlying changes in both user preferences and their dependency relation as a stochastic process. Individual user's preference is modeled by a mixture of globally shared contextual bandit models with a Dirichlet Process prior. Collaboration among users is thus achieved via Bayesian inference over the global bandit models. Model selection and arm selection for each user are done via Thompson sampling to balance exploitation and exploration. Our solution is proved to maintain a standard $\tilde O(\sqrt{T})$ sublinear regret even in such a challenging environment. And extensive empirical evaluations on both synthetic and real-world datasets further confirmed the necessity of modeling a changing environment and our algorithm's practical advantages against several state-of-the-art online learning solutions.",0
"In today's rapidly changing world, companies must adapt to stay competitive. One critical factor for success is collaborating with other organizations to leverage their strengths and achieve mutually beneficial outcomes. However, identifying suitable partners and determining how to allocate resources among them can be challenging, especially when the environment is constantly evolving. This study presents a novel approach to address these problems using a collaborative dynamic bandit model. We propose a solution that learns which collaboration opportunities to pursue and how to allocate resources over time, maximizing the expected total value of collaborations while accounting for uncertainty and tradeoffs. Our simulations show that our method consistently outperforms static approaches, demonstrating its effectiveness in real-world settings. This research provides valuable insights for managers seeking to make data-driven decisions on whom to collaborate with and when.",1
"People's visual experiences of the world are easy to carve up and examine along natural language boundaries, e.g., by category labels, attribute labels, etc. However, it is more difficult to elicit detailed visuospatial information about what a person attends to, e.g., the specific shape of a tree. Paying attention to the shapes of things not only feeds into well defined tasks like visual category learning, but it is also what enables us to differentiate similarly named objects and to take on creative visual pursuits, like poetically describing the shape of a thing, or finding shapes in the clouds or stars. We use a new data collection method that elicits people's prioritized attention to shapes during visual photo inspection by asking them to trace important parts of the image under varying time constraints. Using data collected via crowdsourcing over a set of 187 photographs, we examine changes in patterns of visual attention across individuals, across image types, and across time constraints.",0
"This research paper examines how time constraints affect attentional priorities during visual inspection tasks. Specifically, we investigate whether there is a re-prioritization of attention towards shape features under time pressure conditions compared to unconstrained viewing situations. Using behavioral measures such as response accuracy and RTs (reaction times), as well as physiological indices like pupil size, we tested participants' performance on photo inspection tasks while manipulating time constraints through different task instructions and presentation speeds. Our results showed that indeed, under tight time constraints, participants were more likely to attend to shapes than other categories of features, including texture and color. These findings provide evidence for the adaptive nature of visuospatial attention, suggesting that people may shift their focus towards more relevant stimuli features depending on the demands of the situation. Implications for the understanding of attention mechanisms and real-world applications are discussed.",1
"Guided depth super-resolution (GDSR) is a hot topic in multi-modal image processing. The goal is to use high-resolution (HR) RGB images to provide extra information on edges and object contours, so that low-resolution depth maps can be upsampled to HR ones. To solve the issues of RGB texture over-transferred, cross-modal feature extraction difficulty and unclear working mechanism of modules in existing methods, we propose an advanced Discrete Cosine Transform Network (DCTNet), which is composed of four components. Firstly, the paired RGB/depth images are input into the semi-coupled feature extraction module. The shared convolution kernels extract the cross-modal common features, and the private kernels extract their unique features, respectively. Then the RGB features are input into the edge attention mechanism to highlight the edges useful for upsampling. Subsequently, in the Discrete Cosine Transform (DCT) module, where DCT is employed to solve the optimization problem designed for image domain GDSR. The solution is then extended to implement the multi-channel RGB/depth features upsampling, which increases the rationality of DCTNet, and is more flexible and effective than conventional methods. The final depth prediction is output by the reconstruction module. Numerous qualitative and quantitative experiments demonstrate the effectiveness of our method, which can generate accurate and HR depth maps, surpassing state-of-the-art methods. Meanwhile, the rationality of modules is also proved by ablation experiments.",0
"This should only contain content from the paper itself. The paper ""Discrete Cosine Transform Network for Guided Depth Map Super-Resolution"" presents a new approach to super-resolving depth maps using deep learning. Existing methods often rely on explicit 3D geometry information, but these can be limited by computational complexity or require specialized hardware. To address these limitations, we introduce a novel network architecture based on the discrete cosine transform (DCT), which allows efficient training and inference without sacrificing accuracy. Our DCTNet leverages guided filters to effectively fuse high frequency details with low resolution input depth maps, producing state-of-the-art results on public benchmarks. We evaluate our method on several challenging datasets and demonstrate that it achieves superior performance compared to existing approaches. By providing both qualitative and quantitative comparisons, our work highlights the effectiveness of the proposed method for improving the quality of depth maps used in computer vision applications. Overall, this research has implications for advancing real-time processing of depth data, which holds great promise for future work in areas such as robotics and autonomous vehicles.",1
"In this paper, we present a regression-based pose recognition method using cascade Transformers. One way to categorize the existing approaches in this domain is to separate them into 1). heatmap-based and 2). regression-based. In general, heatmap-based methods achieve higher accuracy but are subject to various heuristic designs (not end-to-end mostly), whereas regression-based approaches attain relatively lower accuracy but they have less intermediate non-differentiable steps. Here we utilize the encoder-decoder structure in Transformers to perform regression-based person and keypoint detection that is general-purpose and requires less heuristic design compared with the existing approaches. We demonstrate the keypoint hypothesis (query) refinement process across different self-attention layers to reveal the recursive self-attention mechanism in Transformers. In the experiments, we report competitive results for pose recognition when compared with the competing regression-based methods.",0
"Here we present a novel method for pose recognition using cascading transformer networks. Our approach leverages the recent advances in deep learning architectures for image classification tasks. In our model, each layer performs multi-scale feature extraction followed by a self attention mechanism that allows the network to attend to relevant features across all spatial scales. We demonstrate state-of-the-art performance on three benchmark datasets: MPII Human Pose, COCO Keypoints, and DeepMind Action Recognition. Our results suggest that our proposed architecture outperforms previous methods based on convolutional neural networks (CNNs) as well as other contemporary approaches such as those utilizing recurrent models. Overall, our work highlights the efficacy of cascade transformers for human pose estimation applications. Further analysis shows that our model successfully captures global dependencies between body parts, enabling better disambiguation during inference. These findings have implications beyond computer vision, including robotics and AR/VR technologies where accurate understanding of human poses is critical.",1
"Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem -- continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The project page is located at https://universome.github.io/class-norm.",0
"This research proposes a new method called class normalization for continual generalized zero-shot learning, which addresses the problem of catastrophic forgetting by using class embedding matrices to store knowledge about past tasks. Inspired by human memory mechanisms such as rehearsal and transfer learning, our approach enables deep neural networks to incrementally learn multiple tasks without losing performance on previously learned tasks. We evaluate our method on several benchmark datasets and demonstrate that it significantly outperforms state-of-the-art methods for continual and few-shot learning while achieving competitive results on standard classification tasks. Our findings have important implications for artificial intelligence and machine learning applications where adaptability and versatility are critical requirements.",1
"The early outcome prediction of ongoing or completed processes confers competitive advantage to organizations. The performance of classic machine learning and, more recently, deep learning techniques such as Long Short-Term Memory (LSTM) on this type of classification problem has been thorougly investigated. Recently, much research focused on applying Convolutional Neural Networks (CNN) to time series problems including classification, however not yet to outcome prediction. The purpose of this paper is to close this gap and compare CNNs to LSTMs. Attention is another technique that, in combination with LSTMs, has found application in time series classification and was included in our research. Our findings show that all these neural networks achieve satisfactory to high predictive power provided sufficiently large datasets. CNNs perfom on par with LSTMs; the Attention mechanism adds no value to the latter. Since CNNs run one order of magnitude faster than both types of LSTM, their use is preferable. All models are robust with respect to their hyperparameters and achieve their maximal predictive power early on in the cases, usually after only a few events, making them highly suitable for runtime predictions. We argue that CNNs' speed, early predictive power and robustness should pave the way for their application in process outcome prediction.",0
"Deep learning has revolutionized many fields by enabling computers to automatically learn from data without explicit programming. In particular, Recurrent Neural Networks (RNNs) have been used effectively to model sequential data such as time series and natural language texts. RNN architectures like Long Short Term Memory (LSTM) networks are particularly popular due to their ability to learn long term dependencies that traditional feedforward neural nets cannot capture. However, in real world applications where datasets may contain missing values or irrelevant features, these models can suffer from overfitting since they rely on strong assumptions about temporal ordering of input variables which may not hold true in practice. This study compares two deep learning methods - Convolutional Neural Network (CNN) with attention mechanism and the more traditional LSTM network architecture in predicting process outcomes. We show how we overcome the limitations of existing state of art techniques and use benchmark datasets to demonstrate superior performance of our approach. Our results indicate that the CNN-Attn combination achieves better accuracy than LSTM alone on multiple metrics. Additionally, visualizing feature maps learned during training gives insights into specific patterns the models pick up while making predictions. Lastly, experiments with missing values provide evidence for why certain techniques should be used to preprocess data beforehand. Overall, our work offers guidance for selecting appropriate models depending on dataset characteristics and highlights benefits of incorporating attention mechanisms in certain applications.",1
"Graph Convolutional Networks (GCNs) have been extensively used to classify vertices in graphs and have been shown to outperform other vertex classification methods. GCNs have been extended to graph classification tasks (GCT). In GCT, graphs with different numbers of edges and vertices belong to different classes, and one attempts to predict the graph class. GCN based GCT have mostly used pooling and attention-based models. The accuracy of existing GCT methods is still limited. We here propose a novel solution combining GCN, methods from knowledge graphs, and a new self-regularized activation function to significantly improve the accuracy of the GCN based GCT. We present quadratic GCN (QGCN) - A GCN formalism with a quadratic layer. Such a layer produces an output with fixed dimensions, independent of the graph vertex number. We applied this method to a wide range of graph classification problems, and show that when using a self regularized activation function, QGCN outperforms the state of the art methods for all graph classification tasks tested with or without external input on each graph. The code for QGCN is available at: https://github.com/Unknown-Data/QGCN .",0
"A quadratic graph convolutional network (QGCN) is proposed as a novel approach to graph classification that achieves state-of-the-art performance on numerous benchmark datasets. This model employs polynomial kernels up to degree two, which capture significant structural features, allowing QGCNs to outperform traditional first-order methods such as graph convolutional networks (GCNs). Experimental results demonstrate that QGCN significantly boosts accuracy across all datasets evaluated, including large-scale and real-world graphs. Furthermore, this study demonstrates that high-degree polynomials can improve performance without overfitting by utilizing regularization techniques such as DropEdge. These findings contribute important insights into the design and training of more powerful models for graph representation learning and graph-related tasks.",1
"Recently, the study of graph neural network (GNN) has attracted much attention and achieved promising performance in molecular property prediction. Most GNNs for molecular property prediction are proposed based on the idea of learning the representations for the nodes by aggregating the information of their neighbor nodes (e.g. atoms). Then, the representations can be passed to subsequent layers to deal with individual downstream tasks. Therefore, the architectures of GNNs can be considered as being composed of two core parts: graph-related layers and task-specific layers. Facing real-world molecular problems, the hyperparameter optimization for those layers are vital. Hyperparameter optimization (HPO) becomes expensive in this situation because evaluating candidate solutions requires massive computational resources to train and validate models. Furthermore, a larger search space often makes the HPO problems more challenging. In this research, we focus on the impact of selecting two types of GNN hyperparameters, those belonging to graph-related layers and those of task-specific layers, on the performance of GNN for molecular property prediction. In our experiments. we employed a state-of-the-art evolutionary algorithm (i.e., CMA-ES) for HPO. The results reveal that optimizing the two types of hyperparameters separately can gain the improvements on GNNs' performance, but optimising both types of hyperparameters simultaneously will lead to predominant improvements. Meanwhile, our study also further confirms the importance of HPO for GNNs in molecular property prediction problems.",0
"The paper presents a study on using evolutionary hyperparameter optimization (EHOP) in graph neural networks (GNNs) for molecular property prediction tasks such as predicting chemical compounds' physical properties like boiling point and flashpoint predictions, drug discovery for pharmacokinetic profiling, etc...",1
"This paper introduces a conditional generative adversarial network to redesign a street-level image of urban scenes by generating 1) an urban intervention policy, 2) an attention map that localises where intervention is needed, 3) a high-resolution street-level image (1024 X 1024 or 1536 X1536) after implementing the intervention. We also introduce a new dataset that comprises aligned street-level images of before and after urban interventions from real-life scenarios that make this research possible. The introduced method has been trained on different ranges of urban interventions applied to realistic images. The trained model shows strong performance in re-modelling cities, outperforming existing methods that apply image-to-image translation in other domains that is computed in a single GPU. This research opens the door for machine intelligence to play a role in re-thinking and re-designing the different attributes of cities based on adversarial learning, going beyond the mainstream of facial landmarks manipulation or image synthesis from semantic segmentation.",0
"Abstract: This paper discusses the use of conditional adversarial networks (CANs) to re-imagine urban design practices by integrating multiple conflicting objectives under uncertainty into a single optimization model using CANs that learn and adapt from real-time feedback. By exploiting structured architectures such as convolutional neural networks, our approach can generate high resolution urban plans while considering complex constraints including energy consumption, land prices, transportation needs, social equity, natural disaster resilience, zoning regulations, and more, making the planning process more efficient and sustainable. We present a multi-objective scenario analysis framework based on a game theory formulation to identify Pareto frontier solutions. Our case study results show that our method improves decision making processes and yields innovative designs that satisfy both local government requirements and stakeholders' preferences. Overall, we aim to facilitate cooperation between humans and artificial intelligence systems to create better living environments through open collaboration frameworks.",1
"Human pose transfer has received great attention due to its wide applications, yet is still a challenging task that is not well solved. Recent works have achieved great success to transfer the person image from the source to the target pose. However, most of them cannot well capture the semantic appearance, resulting in inconsistent and less realistic textures on the reconstructed results. To address this issue, we propose a new two-stage framework to handle the pose and appearance translation. In the first stage, we predict the target semantic parsing maps to eliminate the difficulties of pose transfer and further benefit the latter translation of per-region appearance style. In the second one, with the predicted target semantic maps, we suggest a new person image generation method by incorporating the region-adaptive normalization, in which it takes the per-region styles to guide the target appearance generation. Extensive experiments show that our proposed SPGNet can generate more semantic, consistent, and photo-realistic results and perform favorably against the state of the art methods in terms of quantitative and qualitative evaluation. The source code and model are available at https://github.com/cszy98/SPGNet.git.",0
"Title: Learning Semantic Person Image Generation by Region-Adaptive Normalization  Abstract: In recent years, generative models have made significant advancements in generating realistic images of human faces. However, these models often struggle to generate high-quality results for different semantic concepts that define facial features such as hair color, glasses, or makeup. In this work, we propose a novel approach called region-adaptive normalization (RAN) which learns to adaptively normalize generated images at the regional level. By doing so, our model can better capture fine details and produce diverse variations that better align with the input data distribution. Our method uses adversarial training along with perceptual loss functions to improve visual fidelity while maintaining semantic consistency. Additionally, we introduce a context encoder network that helps the generator learn spatial relationships among regions within images. We evaluate our model on several benchmark datasets including FFHQ, LSUN Church, and CelebAMath, demonstrating state-of-the-art performance across multiple metrics. Our results show that RAN significantly improves image quality, semantic coherence, and generates more diverse outputs compared to existing methods. Overall, our work represents a major step forward in developing robust and reliable methods for learning semantically meaningful person image generation.",1
"Video inpainting aims to fill the given spatiotemporal holes with realistic appearance but is still a challenging task even with prosperous deep learning approaches. Recent works introduce the promising Transformer architecture into deep video inpainting and achieve better performance. However, it still suffers from synthesizing blurry texture as well as huge computational cost. Towards this end, we propose a novel Decoupled Spatial-Temporal Transformer (DSTT) for improving video inpainting with exceptional efficiency. Our proposed DSTT disentangles the task of learning spatial-temporal attention into 2 sub-tasks: one is for attending temporal object movements on different frames at same spatial locations, which is achieved by temporally-decoupled Transformer block, and the other is for attending similar background textures on same frame of all spatial positions, which is achieved by spatially-decoupled Transformer block. The interweaving stack of such two blocks makes our proposed model attend background textures and moving objects more precisely, and thus the attended plausible and temporally-coherent appearance can be propagated to fill the holes. In addition, a hierarchical encoder is adopted before the stack of Transformer blocks, for learning robust and hierarchical features that maintain multi-level local spatial structure, resulting in the more representative token vectors. Seamless combination of these two novel designs forms a better spatial-temporal attention scheme and our proposed model achieves better performance than state-of-the-art video inpainting approaches with significant boosted efficiency.",0
"Title: Deep Learning Approaches for Video Inpainting  Video inpainting refers to the process of filling in missing regions in video frames due to occlusions, camera movements, or other reasons. This task requires handling both spatial and temporal dependencies effectively. In recent years, deep learning approaches have shown great potential in tackling this problem. However, existing methods suffer from either limited capability in modeling spatio-temporal contexts or high computational complexity. To address these issues, we propose a novel decoupled spatial-temporal transformer architecture that disentangles the processing of spatial and temporal features separately while maintaining their interactions. Our method achieves state-of-the-art performance on popular benchmark datasets while requiring significantly less computation compared to prior arts. Furthermore, extensive ablation studies are conducted to validate the effectiveness of each component in our proposed framework. Overall, this work advances the field of deep learning-based video inpainting by providing a more efficient and effective solution.",1
"Although vanilla Convolutional Neural Network (CNN) based detectors can achieve satisfactory performance on fake face detection, we observe that the detectors tend to seek forgeries on a limited region of face, which reveals that the detectors is short of understanding of forgery. Therefore, we propose an attention-based data augmentation framework to guide detector refine and enlarge its attention. Specifically, our method tracks and occludes the Top-N sensitive facial regions, encouraging the detector to mine deeper into the regions ignored before for more representative forgery. Especially, our method is simple-to-use and can be easily integrated with various CNN models. Extensive experiments show that the detector trained with our method is capable to separately point out the representative forgery of fake faces generated by different manipulation techniques, and our method enables a vanilla CNN-based detector to achieve state-of-the-art performance without structure modification.",0
"This research proposes a new method for fake face detection using representative forgery mining. The problem of deepfakes and manipulated media has become increasingly prevalent and can have severe consequences on society. Existing approaches primarily focus on analyzing image or video qualities, but they may not generalize well across different domains or datasets. In contrast, our approach uses generative models to mine representative forgery instances from real images and trains a classifier to distinguish them from authentic ones. We evaluate our method on multiple benchmarks and demonstrate significant improvement over prior arts under various settings. Our work shows that utilizing generative models for representation learning provides an effective direction towards addressing the issue of fake faces.",1
"Simplicial complexes form an important class of topological spaces that are frequently used to in many applications areas such as computer-aided design, computer graphics, and simulation. The representation learning on graphs, which are just 1-d simplicial complexes, has witnessed a great attention and success in the past few years. Due to the additional complexity higher dimensional simplicial hold, there has not been enough effort to extend representation learning to these objects especially when it comes to learn entire-simplicial complex representation. In this work, we propose a method for simplicial complex-level representation learning that embeds a simplicial complex to a universal embedding space in a way that complex-to-complex proximity is preserved. Our method utilizes a simplex-level embedding induced by a pre-trained simplicial autoencoder to learn an entire simplicial complex representation. To the best of our knowledge, this work presents the first method for learning simplicial complex-level representation.",0
"Abstraction:  In recent years there has been an increasing interest in understanding complex representations and how they can be learned by machines. One particular approach that has gained attention is simplicial complex representation learning, which involves representing data as combinations of simplexes (elementary building blocks) within higher-dimensional spaces. This method provides several advantages over traditional approaches, including improved robustness to noise and outliers, better ability to capture high-level features and patterns in data, and more efficient storage and computation. In this paper we present a comprehensive review of the state-of-the-art in simplicial complex representation learning, covering topics such as modeling choices, optimization techniques, visualization methods, and applications across different domains. We highlight key challenges and future research directions in this rapidly evolving field. By providing a thorough understanding of the strengths and limitations of this approach, our work serves as a valuable resource for both researchers and practitioners interested in developing novel methods for handling complex data sets.",1
"In real-world video surveillance applications, person re-identification (ReID) suffers from the effects of occlusions and detection errors. Despite recent advances, occlusions continue to corrupt the features extracted by state-of-art CNN backbones, and thereby deteriorate the accuracy of ReID systems. To address this issue, methods in the literature use an additional costly process such as pose estimation, where pose maps provide supervision to exclude occluded regions. In contrast, we introduce a novel Holistic Guidance (HG) method that relies only on person identity labels, and on the distribution of pairwise matching distances of datasets to alleviate the problem of occlusion, without requiring additional supervision. Hence, our proposed student-teacher framework is trained to address the occlusion problem by matching the distributions of between- and within-class distances (DCDs) of occluded samples with that of holistic (non-occluded) samples, thereby using the latter as a soft labeled reference to learn well separated DCDs. This approach is supported by our empirical study where the distribution of between- and within-class distances between images have more overlap in occluded than holistic datasets. In particular, features extracted from both datasets are jointly learned using the student model to produce an attention map that allows separating visible regions from occluded ones. In addition to this, a joint generative-discriminative backbone is trained with a denoising autoencoder, allowing the system to self-recover from occlusions. Extensive experiments on several challenging public datasets indicate that the proposed approach can outperform state-of-the-art methods on both occluded and holistic datasets",0
"This paper presents a holistic approach towards occlusion-aware person re-identification. We argue that existing methods have limitations due to their single-modality focus on RGB or thermal images. Our proposed method utilizes both modalities along with contextual scene understanding via object detection and human pose estimation. In doing so, we aim to provide holistic guidance for occluded person re-identification by effectively handling partial occlusions common in real-world surveillance scenarios. To achieve robust performance, our framework includes a novel modality-fusion module based on attention mechanisms and feature pyramids. Experimental results demonstrate significant improvements over state-of-the-art methods across several benchmark datasets while providing more informative explanations through the use of grounding boxes generated by the object detector component. Additionally, qualitative evaluations showcase how our system generates improved occlusion reasoning compared to prior work, making it suitable for critical applications such as public safety monitoring. Overall, this research represents a notable step forward in improving security camera analytics under complex real-world conditions, paving the path for future advancements in artificial intelligence applied to video analysis tasks.",1
"Neural architecture search (NAS) has shown great promise in designing state-of-the-art (SOTA) models that are both accurate and efficient. Recently, two-stage NAS, e.g. BigNAS, decouples the model training and searching process and achieves remarkable search efficiency and accuracy. Two-stage NAS requires sampling from the search space during training, which directly impacts the accuracy of the final searched models. While uniform sampling has been widely used for its simplicity, it is agnostic of the model performance Pareto front, which is the main focus in the search process, and thus, misses opportunities to further improve the model accuracy. In this work, we propose AttentiveNAS that focuses on improving the sampling strategy to achieve better performance Pareto. We also propose algorithms to efficiently and effectively identify the networks on the Pareto during training. Without extra re-training or post-processing, we can simultaneously obtain a large number of networks across a wide range of FLOPs. Our discovered model family, AttentiveNAS models, achieves top-1 accuracy from 77.3% to 80.7% on ImageNet, and outperforms SOTA models, including BigNAS and Once-for-All networks. We also achieve ImageNet accuracy of 80.1% with only 491 MFLOPs. Our training code and pretrained models are available at https://github.com/facebookresearch/AttentiveNAS.",0
"Abstract Neural architecture search (NAS) has emerged as a promising technique for automating the design process of deep neural networks (DNNs). Despite recent advancements, challenges remain in terms of computational efficiency, diversity, stability, and interpretability. In particular, existing NAS approaches often rely on uniform sampling of architecture parameters and suffer from issues related to model instability due to randomness introduced during training. To address these limitations, we propose AttentiveNASâ€”a novel approach that adopts an attention mechanism to adaptively focus the search process on specific regions of the architecture space. Our method samples candidate architectures guided by both heuristics and learned knowledge from previous evaluations, enabling efficient exploration and improved robustness across multiple benchmark datasets and hardware platforms. Extensive experiments demonstrate that our proposed method can significantly improve performance over state-of-the-art NAS techniques while maintaining competitive computational cost. Our findings highlight the effectiveness of attentive sampling in steering the search towards more effective designs and provide insights into balancing stability, diversity, and accuracy in differentiable architecture learning frameworks. Overall, our work offers new perspectives in NAS research that emphasize attentivity and personalization for accelerating progress in discovering high-quality DNN models efficiently.",1
"Determinantal point processes (DPPs) have attracted significant attention in machine learning for their ability to model subsets drawn from a large item collection. Recent work shows that nonsymmetric DPP (NDPP) kernels have significant advantages over symmetric kernels in terms of modeling power and predictive performance. However, for an item collection of size $M$, existing NDPP learning and inference algorithms require memory quadratic in $M$ and runtime cubic (for learning) or quadratic (for inference) in $M$, making them impractical for many typical subset selection tasks. In this work, we develop a learning algorithm with space and time requirements linear in $M$ by introducing a new NDPP kernel decomposition. We also derive a linear-complexity NDPP maximum a posteriori (MAP) inference algorithm that applies not only to our new kernel but also to that of prior work. Through evaluation on real-world datasets, we show that our algorithms scale significantly better, and can match the predictive performance of prior work.",0
"This could just be the scientific basis behind your chatbot character. Here we go:  Deep learning has revolutionized many areas of computer science and engineering by enabling scalable solutions for complex tasks such as image recognition, speech synthesis, and natural language processing. One area that has seen less progress from these techniques is probabilistic modeling, where most existing work focuses on either simple models without scale benefits or approximate inference methods tailored for specific applications.  In recent years, determinantal point processes (DPPs) have emerged as powerful tools in machine learning due to their flexibility, interpretability, and strong mathematical guarantees. DPPs provide efficient algorithms for exact inference over permutations, making them suitable for numerous applications such as crowd counting, sensor placement, recommender systems, active learning, visual reasoning, graph clustering, bioinformatics, and more. Despite their wide applicability, existing work mostly considers symmetric kernels, which can lead to suboptimal results when modeling diverse and nonsymmetrical data structures.  This paper presents a scalable framework for learning and maximum aposteriori (MAP) inference under general nonsymmetric DPP kernels using deep neural networks (DNNs). Our method utilizes recent advances in representation learning and stochastic gradient descent (SGD) optimization with GPU acceleration and mini-batch discrimination. We analyze two popular choices of the base measure (background distribution), one based on Gaussian mixtures and another derived from kernel density estimation (KDE), demonstrating excellent performance in both benchmark experiments and real-world applications. Our approach outperforms several state-of-the-art alternatives across different evaluation metrics and scales linearly with respect to dataset size, while maintaining high accuracy and efficiency. Finally, extensive ablation studies reveal important insights into designing effective regularization strategies, choosing appropriate architectures, and selecting hyperparameters to optimize our system for targeted use cases.  Overall, this research contributes a novel, flexible, accurate, and scalable solution for general nonsymmetric DPPs applied throughout data science, providing essential ingredients toward achieving intelligent automation and artificial intelligence.",1
"While the importance of automatic image analysis is increasing at an enormous pace, recent meta-research revealed major flaws with respect to algorithm validation. Specifically, performance metrics are key for objective, transparent and comparative performance assessment, but relatively little attention has been given to the practical pitfalls when using specific metrics for a given image analysis task. A common mission of several international initiatives is therefore to provide researchers with guidelines and tools to choose the performance metrics in a problem-aware manner. This dynamically updated document has the purpose to illustrate important limitations of performance metrics commonly applied in the field of image analysis. The current version is based on a Delphi process on metrics conducted by an international consortium of image analysis experts.",0
"What image processing metrics measure how well images have been processed? An example could be the structural similarity index (SSIM), which measures the difference in structure between two images on a scale from 0 to 1. SSIM is used as a proxy for perceptual quality, since high SSIM scores mean that the images look more similar, although there may still be some differences due to limitations in measurement capabilities. Another metric is peak signal-to-noise ratio (PSNR) which calculates the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. This can give you a rough indication of the amount of compression applied, but does not take into account visual artifacts caused by lossy compression, which may lead to a decrease in perceived image quality despite the same PSNR value. Other examples include edge preservation and feature matching, these metrics suffer from different drawbacks such as overfitting, limited context awareness and sensitivity to changes that do not affect human perception of natural scenes. These factors contribute to common limitations across many state of the art methods, resulting in performance drops and failure cases when tested against real images and scenarios. To address the issues described above we propose a new framework using reinforcement learning that encourages exploration and exploitation of the state space by selecting actions based on their expected utility function taking into account uncertainty associated with predictions obtained by generative models of appearance flow. The proposed method is trained end-to-end in simulation, without any explicit correspondence groundtruth supervision. We present results comparing our method against SOTA on the KITTI benchmark dataset showing improvem",1
"In the past decade, model-free reinforcement learning (RL) has provided solutions to challenging domains such as robotics. Model-based RL shows the prospect of being more sample-efficient than model-free methods in terms of agent-environment interactions, because the model enables to extrapolate to unseen situations. In the more recent past, model-based methods have shown superior results compared to model-free methods in some challenging domains with non-linear state transitions. At the same time, it has become apparent that RL is not market-ready yet and that many real-world applications are going to require model-based approaches, because model-free methods are too sample-inefficient and show poor performance in early stages of training. The latter is particularly important in industry, e.g. in production systems that directly impact a company's revenue. This demonstrates the necessity for a toolbox to push the boundaries for model-based RL. While there is a plethora of toolboxes for model-free RL, model-based RL has received little attention in terms of toolbox development. Bellman aims to fill this gap and introduces the first thoroughly designed and tested model-based RL toolbox using state-of-the-art software engineering practices. Our modular approach enables to combine a wide range of environment models with generic model-based agent classes that recover state-of-the-art algorithms. We also provide an experiment harness to compare both model-free and model-based agents in a systematic fashion w.r.t. user-defined evaluation metrics (e.g. cumulative reward). This paves the way for new research directions, e.g. investigating uncertainty-aware environment models that are not necessarily neural-network-based, or developing algorithms to solve industrially-motivated benchmarks that share characteristics with real-world problems.",0
"The abstract should provide an overview of the paper. Use your own words as far as possible but if you must quote please use quotes around them. No less than four sentences required. Output plain text.  ---  This paper presents Bellman, a toolbox that provides an implementation of model-based reinforcement learning algorithms in TensorFlow. The toolbox includes algorithms such as Q-learning, SARSA, actor-critic methods, deep deterministic policy gradients (DDPG), and trust region policy optimization (TRPO). These algorithms are implemented using the TensorFlow library and can handle both continuous action spaces and discrete action spaces. In addition, the toolbox supports both tabular representations and neural network function approximators. The authors demonstrate the effectiveness of their toolbox through several experiments on common RL benchmarks, showing competitive performance compared to other state-of-the-art implementations. Overall, Bellman offers researchers and practitioners an easy-to-use and high-performance framework for model-based RL with TensorFlow.",1
"Human recognition of the actions of other humans is very efficient and is based on patterns of movements. Our theoretical starting point is that the dynamics of the joint movements is important to action categorization. On the basis of this theory, we present a novel action recognition system that employs a hierarchy of Self-Organizing Maps together with a custom supervised neural network that learns to categorize actions. The system preprocesses the input from a Kinect like 3D camera to exploit the information not only about joint positions, but also their first and second order dynamics. We evaluate our system in two experiments with publicly available data sets, and compare its performance to the performance with less sophisticated preprocessing of the input. The results show that including the dynamics of the actions improves the performance. We also apply an attention mechanism that focuses on the parts of the body that are the most involved in performing the actions.",0
"In this paper we propose and evaluate an action recognition system that uses second order dynamics (SOD) and hierarchical self organizing maps (HSOSMs). Our approach captures both first and second order dynamics using HSOSMs and is able to recognize actions from sequences of depth images produced by Microsoft Kinect cameras. This work extends previous research on action recognition which has focused primarily on either SOD or static representations, but not both. To our knowledge, this is the first work to use hierarchical SOMs for action recognition. We demonstrate through extensive experiments on three benchmark datasets that our method outperforms state-of-the-art methods in terms of accuracy and computational efficiency while providing interpretable results. The proposed framework provides new insights into understanding human motion patterns in complex environments. The implications of these findings go beyond action recognition as they provide new tools to study human behavioral patterns and their interactions with the environment. Overall, this work represents a significant contribution to computer vision and offers opportunities for future research in areas such as robotics and virtual reality where real time analysis of human motion is critical.",1
"In this paper, we propose a novel graph learning framework for phrase grounding in the image. Developing from the sequential to the dense graph model, existing works capture coarse-grained context but fail to distinguish the diversity of context among phrases and image regions. In contrast, we pay special attention to different motifs implied in the context of the scene graph and devise the disentangled graph network to integrate the motif-aware contextual information into representations. Besides, we adopt interventional strategies at the feature and the structure levels to consolidate and generalize representations. Finally, the cross-modal attention network is utilized to fuse intra-modal features, where each phrase can be computed similarity with regions to select the best-grounded one. We validate the efficiency of disentangled and interventional graph network (DIGN) through a series of ablation studies, and our model achieves state-of-the-art performance on Flickr30K Entities and ReferIt Game benchmarks.",0
"In todayâ€™s world, natural language processing (NLP) has become increasingly important as human interactions with computers have shifted towards text-based communication, such as email and social media. To improve NLP, one challenge lies in resolving ambiguity that arises from colloquial expressions like phrases without explicit definitions, known as â€œgroundingâ€. Previous work on phrase grounding relies heavily on rules-based systems, which struggle due to their reliance on pre-defined templates and static patterns. In contrast, we propose using graph learning methods to jointly identify phrases within sentences while disambiguating them via motifs. By representing the data through graphs, our method effectively captures implicit relationships among contexts by encoding sentence structure explicitly into a tensor factorization problem. This approach results in high accuracy at detecting relevant entities, with improved performance across multiple languages including low resource ones. Through rigorous evaluation compared against baselines and prior state-of-the art models, we demonstrate the effectiveness of our proposed model on several benchmark datasets. Overall, our contribution provides further evidence for the power of graph based techniques in advancing NLP tasks beyond rule-based approaches.",1
"Light field data has been demonstrated to facilitate the depth estimation task. Most learning-based methods estimate the depth infor-mation from EPI or sub-aperture images, while less methods pay attention to the focal stack. Existing learning-based depth estimation methods from the focal stack lead to suboptimal performance because of the defocus blur. In this paper, we propose a multi-modal learning method for robust light field depth estimation. We first excavate the internal spatial correlation by designing a context reasoning unit which separately extracts comprehensive contextual information from the focal stack and RGB images. Then we integrate the contextual information by exploiting a attention-guide cross-modal fusion module. Extensive experiments demonstrate that our method achieves superior performance than existing representative methods on two light field datasets. Moreover, visual results on a mobile phone dataset show that our method can be widely used in daily life.",0
"Title: ""Learning Multimodal Information for Robust Light Field Depth Estimation""  Abstract: Depth estimation from light fields has gained significant attention in recent years due to their ability to capture both angular and spatial information about scenes. However, accurately estimating depth from light fields remains challenging due to occlusions, varying illumination conditions, and other artifacts that can affect the quality of depth maps. In this work, we present a novel approach that leverages multimodal information for robust depth estimation in light fields. Our method uses deep learning techniques to fuse multiple modalities including color images, disparity maps, and normals to improve accuracy and reduce noise in the final depth map. We train our model on synthetic data and demonstrate its effectiveness through comprehensive experiments on real datasets. Results show that our proposed method outperforms state-of-the-art methods for light field depth estimation by achieving higher accuracies and reduced error rates across various metrics. This research offers new insights into how multimodal fusion can enhance depth estimation in complex environments, paving the way for improved applications in computer vision and virtual reality.",1
"Graph neural networks (GNN) have been proven to be mature enough for handling graph-structured data on node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation is critical yet still challenging. Existing pooling methods either struggle to capture the local substructure or fail to effectively utilize high-order dependency, thus diminishing the expression capability. In this paper we propose HAP, a hierarchical graph-level representation learning framework, which is adaptively sensitive to graph structures, i.e., HAP clusters local substructures incorporating with high-order dependencies. HAP utilizes a novel cross-level attention mechanism MOA to naturally focus more on close neighborhood while effectively capture higher-order dependency that may contain crucial information. It also learns a global graph content GCont that extracts the graph pattern properties to make the pre- and post-coarsening graph content maintain stable, thus providing global guidance in graph coarsening. This novel innovation also facilitates generalization across graphs with the same form of features. Extensive experiments on fourteen datasets show that HAP significantly outperforms twelve popular graph pooling methods on graph classification task with an maximum accuracy improvement of 22.79%, and exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.5% and 16.7%.",0
"This paper presents a new method called hierarchical adaptive pooling (HAP) that captures high-order dependency among nodes in graph representation learning. HAP uses multiple levels of attention mechanisms to weigh different parts of graphs, resulting in more accurate node representations. Experiments on several benchmark datasets show significant improvements over state-of-the-art methods across various tasks such as link prediction, semi-supervised classification, and node clustering.",1
"As the basic task of point cloud analysis, classification is fundamental but always challenging. To address some unsolved problems of existing methods, we propose a network that captures geometric features of point clouds for better representations. To achieve this, on the one hand, we enrich the geometric information of points in low-level 3D space explicitly. On the other hand, we apply CNN-based structures in high-level feature spaces to learn local geometric context implicitly. Specifically, we leverage an idea of error-correcting feedback structure to capture the local features of point clouds comprehensively. Furthermore, an attention module based on channel affinity assists the feature map to avoid possible redundancy by emphasizing its distinct channels. The performance on both synthetic and real-world point clouds datasets demonstrate the superiority and applicability of our network. Comparing with other state-of-the-art methods, our approach balances accuracy and efficiency.",0
"Avoid using passive voice at all costs. Use active instead. The new method we propose, geometric back projection network (GBPNet), uses the novel concept of geometric back-projection which can better handle different sizes of input point clouds by directly learning the feature space that aligns well with the shapes of objects. Our method learns a mapping from local point features onto their corresponding global coordinates through explicitly predicting object center, size and orientation, resulting in high accuracy classification even on small datasets such as ScanNet. We achieve state-of-the-art performance across several benchmarks including ShapeNet and ModelNet40 while requiring fewer parameters compared to similar methods, demonstrating its effectiveness in accurately classifying point cloud data without requiring excessively large models. In summary, our work presents a promising advancement towards real world applications for point cloud processing tasks.",1
"Underwater image enhancement has attracted much attention due to the rise of marine resource development in recent years. Benefit from the powerful representation capabilities of Convolution Neural Networks(CNNs), multiple underwater image enhancement algorithms based on CNNs have been proposed in the last few years. However, almost all of these algorithms employ RGB color space setting, which is insensitive to image properties such as luminance and saturation. To address this problem, we proposed Underwater Image Enhancement Convolution Neural Network using 2 Color Space (UICE^2-Net) that efficiently and effectively integrate both RGB Color Space and HSV Color Space in one single CNN. To our best knowledge, this method is the first to use HSV color space for underwater image enhancement based on deep learning. UIEC^2-Net is an end-to-end trainable network, consisting of three blocks as follow: a RGB pixel-level block implements fundamental operations such as denoising and removing color cast, a HSV global-adjust block for globally adjusting underwater image luminance, color and saturation by adopting a novel neural curve layer, and an attention map block for combining the advantages of RGB and HSV block output images by distributing weight to each pixel. Experimental results on synthetic and real-world underwater images show the good performance of our proposed method in both subjective comparisons and objective metrics. The code are available at https://github.com/BIGWangYuDong/UWEnhancement.",0
"This paper presents UIEC^2-Net (UnderWater Image Enhancement via Channel Convolution), which enhances underwater images using two color spaces - RGB and CIE XYZ. We propose a novel convolutional neural network architecture based on multi-channel input features extracted from both RGB and CIE XYZ color spaces, resulting in improved performance compared to traditional single channel methods. Our method utilizes spatial domain guided filtering along with pixel wise non-local processing guiding filter generation to achieve superior results. Quantitative evaluations against state-of-the-art algorithms demonstrate that our approach outperforms current techniques. Additionally, visual comparisons show noticeable improvements in enhancing image quality by recovering finer details lost due to light attenuation caused by water impurities. Overall, our algorithm achieves significant enhancements while still maintaining realism in underwater imagery.",1
"Model extraction increasingly attracts research attentions as keeping commercial AI models private can retain a competitive advantage. In some scenarios, AI models are trained proprietarily, where neither pre-trained models nor sufficient in-distribution data is publicly available. Model extraction attacks against these models are typically more devastating. Therefore, in this paper, we empirically investigate the behaviors of model extraction under such scenarios. We find the effectiveness of existing techniques significantly affected by the absence of pre-trained models. In addition, the impacts of the attacker's hyperparameters, e.g. model architecture and optimizer, as well as the utilities of information retrieved from queries, are counterintuitive. We provide some insights on explaining the possible causes of these phenomena. With these observations, we formulate model extraction attacks into an adaptive framework that captures these factors with deep reinforcement learning. Experiments show that the proposed framework can be used to improve existing techniques, and show that model extraction is still possible in such strict scenarios. Our research can help system designers to construct better defense strategies based on their scenarios.",0
"Recently, deep learning models have become increasingly popular due to their ability to achieve state-of-the-art results in many domains. As these models are becoming more widely used, there has been increased interest in understanding how they can be attacked. One particularly worrisome attack vector that has emerged involves extracting model parameters from a trained model using techniques such as model inversion attacks or side channel analysis. These extracted models can then be used to launch targeted attacks on specific individuals or groups by stealing sensitive personal data, creating synthetic data that deceives human evaluators, or even generating custom malware based on knowledge gained from the original model. To better understand this new type of attack and its potential impact, we propose a comprehensive study of the feasibility and effectiveness of model extraction attacks against real-world applications. Our work investigates both traditional machine learning models and modern neural networks, including models deployed in industry settings. We find that while some protections exist, model extraction remains a significant threat to the security of systems relying on deep learning models. By shining light on this understudied area, our goal is to raise awareness among practitioners and encourage further research into countermeasures. Ultimately, we hope to contribute towards building secure models that can be trusted to make critical decisions without fear of exposure through model extraction attacks.",1
"Forecasting graph-based time-dependent data has many practical applications. This task is challenging as models need not only to capture spatial dependency and temporal dependency within the data, but also to leverage useful auxiliary information for accurate predictions. In this paper, we analyze limitations of state-of-the-art models on dealing with temporal dependency. To address this limitation, we propose GSA-Forecaster, a new deep learning model for forecasting graph-based time-dependent data. GSA-Forecaster leverages graph sequence attention (GSA), a new attention mechanism proposed in this paper, for effectively capturing temporal dependency. GSA-Forecaster embeds the graph structure of the data into its architecture to address spatial dependency. GSA-Forecaster also accounts for auxiliary information to further improve predictions. We evaluate GSA-Forecaster with large-scale real-world graph-based time-dependent data and demonstrate its effectiveness over state-of-the-art models with 6.7% RMSE and 5.8% MAPE reduction.",0
"Increasingly popular in many applications such as bioinformatics, network analysis, and recommendation systems, graph data has become essential due to their ability to represent complex relationships among entities. Predictive analytics using time-dependent graphs, however, remains challenging due to nonlinearity caused by interactions between temporal changes within nodes and edges. To address these issues, we propose GSA-Forecaster, an end-to-end trainable model that utilizes graph sequence attention mechanisms based on the transformer architecture. Our model is designed to capture dynamic patterns and dependencies across both node attributes and interaction sequences. We evaluate our proposed method through extensive experiments on three real datasets (CORA, CITESEERX, PUBMED), demonstrating its effectiveness compared to competitive baselines including graph recurrent neural networks, gated graph sequential models, and graph convolutional methods. This research paves the way towards improved forecast accuracy and interpretability in predictive graph analytics.",1
"Depth map records distance between the viewpoint and objects in the scene, which plays a critical role in many real-world applications. However, depth map captured by consumer-grade RGB-D cameras suffers from low spatial resolution. Guided depth map super-resolution (DSR) is a popular approach to address this problem, which attempts to restore a high-resolution (HR) depth map from the input low-resolution (LR) depth and its coupled HR RGB image that serves as the guidance.   The most challenging problems for guided DSR are how to correctly select consistent structures and propagate them, and properly handle inconsistent ones. In this paper, we propose a novel attention-based hierarchical multi-modal fusion (AHMF) network for guided DSR. Specifically, to effectively extract and combine relevant information from LR depth and HR guidance, we propose a multi-modal attention based fusion (MMAF) strategy for hierarchical convolutional layers, including a feature enhance block to select valuable features and a feature recalibration block to unify the similarity metrics of modalities with different appearance characteristics. Furthermore, we propose a bi-directional hierarchical feature collaboration (BHFC) module to fully leverage low-level spatial information and high-level structure information among multi-scale features. Experimental results show that our approach outperforms state-of-the-art methods in terms of reconstruction accuracy, running speed and memory efficiency.",0
"This work presents a method for generating high-resolution depth maps using attention-based hierarchical multi-modal fusion (AMF). Depth estimation has become increasingly important in computer vision applications such as autonomous driving, robotics, and AR/VR. Conventional methods typically rely on single modalities like RGB cameras which often lead to poor results due to lack of robustness under varying environments and lighting conditions. To overcome these limitations, we propose AMF, a novel approach that fuses multiple modalities at different scales by leveraging attention mechanisms to selectively focus on relevant features. Our experiments show that our proposed method outperforms state-of-the-art algorithms on two benchmark datasets and demonstrate improved performance in terms of accuracy, precision, recall, and robustness to changing environmental conditions. Our findings provide insights into designing more efficient depth estimation systems for real-world scenarios.",1
"We study how to introduce locality mechanisms into vision transformers. The transformer network originates from machine translation and is particularly good at modelling long-range dependencies within a long sequence. Although the global interaction between the token embeddings could be well modelled by the self-attention mechanism of transformers, what is lacking a locality mechanism for information exchange within a local region. Yet, locality is essential for images since it pertains to structures like lines, edges, shapes, and even objects.   We add locality to vision transformers by introducing depth-wise convolution into the feed-forward network. This seemingly simple solution is inspired by the comparison between feed-forward networks and inverted residual blocks. The importance of locality mechanisms is validated in two ways: 1) A wide range of design choices (activation function, layer placement, expansion ratio) are available for incorporating locality mechanisms and all proper choices can lead to a performance gain over the baseline, and 2) The same locality mechanism is successfully applied to 4 vision transformers, which shows the generalization of the locality concept. In particular, for ImageNet2012 classification, the locality-enhanced transformers outperform the baselines DeiT-T and PVT-T by 2.6\% and 3.1\% with a negligible increase in the number of parameters and computational effort. Code is available at \url{https://github.com/ofsoundof/LocalViT}.",0
"This paper presents LocalViT (Local Vision Transformer), a novel variant of the popular vision transformer architecture that introduces locality into self-attention computation. Inspired by recent works on efficient tokenization for natural language processing, we propose partitioning the image plane locally according to a coarse grid, enabling parallel attention computations within each subregion. To preserve global context, inter-grid interactions are performed through channel and spatial splits using depthwise convolutions. Experimental results show significant improvements over vanilla ViT models across multiple benchmark datasets, demonstrating that incorporating locality in transformers leads to more effective visual representations for computer vision tasks. Our work suggests the importance of exploiting both local and global context for effective image understanding.",1
"Driving is a routine activity for many, but it is far from simple. Drivers deal with multiple concurrent tasks, such as keeping the vehicle in the lane, observing and anticipating the actions of other road users, reacting to hazards, and dealing with distractions inside and outside the vehicle. Failure to notice and respond to the surrounding objects and events can cause accidents.   The ongoing improvements of the road infrastructure and vehicle mechanical design have made driving safer overall. Nevertheless, the problem of driver inattention has remained one of the primary causes of accidents. Therefore, understanding where the drivers look and why they do so can help eliminate sources of distractions and identify unsafe attention patterns. Research on driver attention has implications for many practical applications such as policy-making, improving driver education, enhancing road infrastructure and in-vehicle infotainment systems, as well as designing systems for driver monitoring, driver assistance, and automated driving.   This report covers the literature on changes in drivers' visual attention distribution due to factors, internal and external to the driver. Aspects of attention during driving have been explored across multiple disciplines, including psychology, human factors, human-computer interaction, intelligent transportation, and computer vision, each offering different perspectives, goals, and explanations for the observed phenomena. We link cross-disciplinary theoretical and behavioral research on driver's attention to practical solutions. Furthermore, limitations and directions for future research are discussed. This report is based on over 175 behavioral studies, nearly 100 practical papers, 20 datasets, and over 70 surveys published since 2010. A curated list of papers used for this report is available at https://github.com/ykotseruba/attention_and_driving.",0
"""Behavioral research on driver attention has focused on understanding how drivers allocate their attention while operating vehicles. This study aimed to develop practical models of driver attention that can predict how drivers attend to different stimuli and situations. Using a combination of experimental methods, including eye tracking and driving simulators, we investigated the effects of task demands and road conditions on driver attention. Results showed that drivers tend to allocate attention based on perceived risk and expectancy, as well as changes in the environment. Furthermore, our findings highlighted the importance of considering individual differences in attentional control and visual exploration strategies when developing these models. These results have important implications for designing interventions aimed at improving driver safety by enhancing attentional awareness.""",1
"2D image-based virtual try-on has attracted increased attention from the multimedia and computer vision communities. However, most of the existing image-based virtual try-on methods directly put both person and the in-shop clothing representations together, without considering the mutual correlation between them. What is more, the long-range information, which is crucial for generating globally consistent results, is also hard to be established via the regular convolution operation. To alleviate these two problems, in this paper we propose a novel two-stage Cloth Interactive Transformer (CIT) for virtual try-on. In the first stage, we design a CIT matching block, aiming to perform a learnable thin-plate spline transformation that can capture more reasonable long-range relation. As a result, the warped in-shop clothing looks more natural. In the second stage, we propose a novel CIT reasoning block for establishing the global mutual interactive dependence. Based on this mutual dependence, the significant region within the input data can be highlighted, and consequently, the try-on results can become more realistic. Extensive experiments on a public fashion dataset demonstrate that our CIT can achieve the new state-of-the-art virtual try-on performance both qualitatively and quantitatively. The source code and trained models are available at https://github.com/Amazingren/CIT.",0
"This research presents a new approach for cloth simulation using deep learning. We introduce a novel method for virtual try-on using cloth interaction transformer (CIT) that leverages convolutional and transformer neural networks to accurately simulate fabric behavior during deformation. Our model takes as input two 2D images - one of the user wearing clothes and another image of the desired outfit on a mannequin. Our model then generates high quality predictions of how the garment would fit on the body and create realistic wrinkles.  To achieve this, our method consists of several key components: first, we generate a heatmap which highlights regions where the garment deviates from the body. Secondly, we pass these heatmaps through a multi-layered network consisting of both convolutional and transformer blocks to predict displacements required to achieve a good fit. Finally, the predicted displacement maps are applied to the original cloth shape to obtain the final output mesh of the virtual try-on results.  We demonstrate the effectiveness of our approach by comparing against baseline methods and show substantial improvement over existing techniques. Furthermore, we present qualitative results showcasing our model's ability to handle complex interactions such as collisions and folds during virtual try-on. Finally, we provide an ablation study analysis which shows how each component contributes towards overall performance improvements. \end{abstract}",1
"Predicting vulnerable road user behavior is an essential prerequisite for deploying Automated Driving Systems (ADS) in the real-world. Pedestrian crossing intention should be recognized in real-time, especially for urban driving. Recent works have shown the potential of using vision-based deep neural network models for this task. However, these models are not robust and certain issues still need to be resolved. First, the global spatio-temproal context that accounts for the interaction between the target pedestrian and the scene has not been properly utilized. Second, the optimum strategy for fusing different sensor data has not been thoroughly investigated. This work addresses the above limitations by introducing a novel neural network architecture to fuse inherently different spatio-temporal features for pedestrian crossing intention prediction. We fuse different phenomena such as sequences of RGB imagery, semantic segmentation masks, and ego-vehicle speed in an optimum way using attention mechanisms and a stack of recurrent neural networks. The optimum architecture was obtained through exhaustive ablation and comparison studies. Extensive comparative experiments on the JAAD pedestrian action prediction benchmark demonstrate the effectiveness of the proposed method, where state-of-the-art performance was achieved. Our code is open-source and publicly available.",0
"In recent years, pedestrian crossing intention prediction has gained significant attention due to the increasing number of accidents involving pedestrians and vehicles. Many studies have focused on developing algorithms that can accurately predict a pedestrian's intention to cross the road using sensor data such as cameras and LiDAR. However, these methods often struggle to handle complex scenarios and require large amounts of training data.  In this study, we propose a novel method for predicting pedestrian crossing intentions using feature fusion and spatio-temporal attention mechanisms. Our approach uses multiple sensors (camera and LiDAR) and integrates their features through a one-stream Convolutional Neural Network (CNN). We introduce a spatial attention module to focus on relevant regions in each frame and incorporate temporal attention to capture changes over time. Additionally, our model utilizes an Adaptive Non-Local Block (ANLB) to capture dependencies across space and time, improving performance by fusing complementary information from different modalities.  We evaluate our proposed method on two public datasets, KAIST and Junctionâ€™s ADAS Challenge dataset, and demonstrate improved accuracy compared to state-of-the-art methods. Furthermore, our results show that our method generalizes well across different urban environments, making it a promising solution for real-world applications in autonomous driving and advanced driver assistance systems (ADAS).  Our research presents a valuable contribution towards enhancing the safety of road users by providing accurate predictions of pedestrian behavior at intersections. By leveraging multiple sensors and advanced deep learning techniques, our approach paves the way for more robust and reliable solutions for safer autonomous transportation.",1
"The reading of arbitrarily-shaped text has received increasing research attention. However, existing text spotters are mostly built on two-stage frameworks or character-based methods, which suffer from either Non-Maximum Suppression (NMS), Region-of-Interest (RoI) operations, or character-level annotations. In this paper, to address the above problems, we propose a novel fully convolutional Point Gathering Network (PGNet) for reading arbitrarily-shaped text in real-time. The PGNet is a single-shot text spotter, where the pixel-level character classification map is learned with proposed PG-CTC loss avoiding the usage of character-level annotations. With a PG-CTC decoder, we gather high-level character classification vectors from two-dimensional space and decode them into text symbols without NMS and RoI operations involved, which guarantees high efficiency. Additionally, reasoning the relations between each character and its neighbors, a graph refinement module (GRM) is proposed to optimize the coarse recognition and improve the end-to-end performance. Experiments prove that the proposed method achieves competitive accuracy, meanwhile significantly improving the running speed. In particular, in Total-Text, it runs at 46.7 FPS, surpassing the previous spotters with a large margin.",0
"In summary, this paper presents PGNet, which can identify text within arbitrary shapes and orientations. This real time solution allows point gathering networks (PGN) to perform spotting and segmentation tasks by learning shape features through end-to-end training on synthetic data with random masks applied. Experimental results demonstrate the performance superiority over prior arts as well as robustness against object occlusions, irregularly shaped objects, distractors, and varying light conditions. As such, our work provides a new methodology for scene understanding beyond traditional rectangular detection frameworks. Please note that I am an AI language model and cannot generate a PDF from scratch but you can use any scientific journal template that fits your requirements. Let me know if there is anything else I can assist you with!",1
"Bilevel optimization problems are receiving increasing attention in machine learning as they provide a natural framework for hyperparameter optimization and meta-learning. A key step to tackle these problems is the efficient computation of the gradient of the upper-level objective (hypergradient). In this work, we study stochastic approximation schemes for the hypergradient, which are important when the lower-level problem is empirical risk minimization on a large dataset. The method that we propose is a stochastic variant of the approximate implicit differentiation approach in (Pedregosa, 2016). We provide bounds for the mean square error of the hypergradient approximation, under the assumption that the lower-level problem is accessible only through a stochastic mapping which is a contraction in expectation. In particular, our main bound is agnostic to the choice of the two stochastic solvers employed by the procedure. We provide numerical experiments to support our theoretical analysis and to show the advantage of using stochastic hypergradients in practice.",0
"This article is focused on stochastic hypergradient descent methods for training machine learning models. In particular, we examine two classes of such algorithms: minibatch gradient descent and SGD (stochastic gradient descent) with momentum and adaptive step sizes. We provide theoretical evidence that these methods converge under appropriate conditions for convex and nonconvex problems. Our results build upon recent advances in the analysis of convergence rates for SGD methods as well as convergence theory for stochastic approximation more generally. Our work provides new insights into the behavior of these algorithms under realistic assumptions and should be relevant for practitioners working in large-scale data science applications where these techniques are widely used. By establishing convergence properties, our study also contributes foundational research in the field, helping us better understand how these methods operate and their potential limitations. Overall, our findings have important implications for both theory and practice of machine learning.",1
"Unsupervised person re-identification (re-ID) attracts increasing attention due to its practical applications in industry. State-of-the-art unsupervised re-ID methods train the neural networks using a memory-based non-parametric softmax loss. They store the pre-computed instance feature vectors inside the memory, assign pseudo labels to them us-ing clustering algorithm, and compare the query instances to the cluster using a form of contrastive loss. During training, the instance feature vectors are updated. How-ever, due to the varying cluster size, the updating progress for each cluster is inconsistent. To solve this problem, we present Cluster Contrast which stores feature vectors and computes contrast loss in the cluster level. We demonstrate that the inconsistency problem for cluster feature representation can be solved by the cluster-level memory dictionary.By straightforwardly applying Cluster Contrast to a standard unsupervised re-ID pipeline, it achieves considerable improvements of 9.5%, 7.5%, 6.6% compared to state-of-the-art purely unsupervised re-ID methods and 5.1%, 4.0%,6.5% mAP compared to the state-of-the-art unsupervised domain adaptation re-ID methods on the Market, Duke, andMSMT17 datasets.Our source code is available at https://github.com/alibaba/cluster-contrast.",0
"This paper presents a new unsupervised method for person re-identification across multiple camera views. The proposed approach uses clustering algorithms to group images of the same person together based on their visual similarity. These clusters are then used as ""templates"" to identify the same person across different cameras. We evaluate our method using two benchmark datasets and demonstrate that it outperforms several state-of-the-art methods for unsupervised person re-identification. Our results show that cluster contrast can effectively capture discriminative features and improve the performance of unsupervised person re-id systems.",1
"Modern human-object interaction (HOI) detection approaches can be divided into one-stage methods and twostage ones. One-stage models are more efficient due to their straightforward architectures, but the two-stage models are still advantageous in accuracy. Existing one-stage models usually begin by detecting predefined interaction areas or points, and then attend to these areas only for interaction prediction; therefore, they lack reasoning steps that dynamically search for discriminative cues. In this paper, we propose a novel one-stage method, namely Glance and Gaze Network (GGNet), which adaptively models a set of actionaware points (ActPoints) via glance and gaze steps. The glance step quickly determines whether each pixel in the feature maps is an interaction point. The gaze step leverages feature maps produced by the glance step to adaptively infer ActPoints around each pixel in a progressive manner. Features of the refined ActPoints are aggregated for interaction prediction. Moreover, we design an actionaware approach that effectively matches each detected interaction with its associated human-object pair, along with a novel hard negative attentive loss to improve the optimization of GGNet. All the above operations are conducted simultaneously and efficiently for all pixels in the feature maps. Finally, GGNet outperforms state-of-the-art methods by significant margins on both V-COCO and HICODET benchmarks. Code of GGNet is available at https: //github.com/SherlockHolmes221/GGNet.",0
"This paper presents a method for accurately detecting human-object interactions in images using glance and gaze cues. Previous approaches have focused on hand gesture recognition, but these methods often struggle to handle occlusions, cluttered backgrounds, and varying lighting conditions. Our approach uses deep learning techniques to extract glance and gaze features from image data, which can then be used to infer action-aware points that indicate human-object interaction. We evaluate our method on several benchmark datasets and demonstrate improved accuracy compared to state-of-the-art methods. Our results suggest that our approach provides a powerful tool for understanding complex human behavior in images and has applications in fields such as robotics, computer vision, and human-computer interaction.",1
"One-shot weight sharing methods have recently drawn great attention in neural architecture search due to high efficiency and competitive performance. However, weight sharing across models has an inherent deficiency, i.e., insufficient training of subnetworks in hypernetworks. To alleviate this problem, we present a simple yet effective architecture distillation method. The central idea is that subnetworks can learn collaboratively and teach each other throughout the training process, aiming to boost the convergence of individual models. We introduce the concept of prioritized path, which refers to the architecture candidates exhibiting superior performance during training. Distilling knowledge from the prioritized paths is able to boost the training of subnetworks. Since the prioritized paths are changed on the fly depending on their performance and complexity, the final obtained paths are the cream of the crop. We directly select the most promising one from the prioritized paths as the final architecture, without using other complex search methods, such as reinforcement learning or evolution algorithms. The experiments on ImageNet verify such path distillation method can improve the convergence ratio and performance of the hypernetwork, as well as boosting the training of subnetworks. The discovered architectures achieve superior performance compared to the recent MobileNetV3 and EfficientNet families under aligned settings. Moreover, the experiments on object detection and more challenging search space show the generality and robustness of the proposed method. Code and models are available at https://github.com/microsoft/cream.git.",0
"""One-shot neural architecture search has emerged as a powerful technique to automate the design process of deep learning models, but it can often generate many solutions that may not always perform optimally. In our work, we propose a novel approach called Cream of the Crop (CoC) which distills prioritized paths from the search space, resulting in improved performance and efficiency. We evaluate CoC on several benchmark datasets using popular search spaces like MobileNetV2, ShuffleNetV2, ResNet, and DenseNet, achieving state-of-the-art results in most cases. Our proposed method outperforms existing one-shot methods by up to 8%, while requiring fewer GPU days compared to random searches. Additionally, we show that CoC improves upon human-designed architectures across different model sizes. This study demonstrates the effectiveness of leveraging prioritization techniques during architecture search and presents insights into design choices for future research.""",1
"Automatic and accurate lung nodule detection from 3D Computed Tomography scans plays a vital role in efficient lung cancer screening. Despite the state-of-the-art performance obtained by recent anchor-based detectors using Convolutional Neural Networks, they require predetermined anchor parameters such as the size, number, and aspect ratio of anchors, and have limited robustness when dealing with lung nodules with a massive variety of sizes. We propose a 3D sphere representation-based center-points matching detection network (SCPM-Net) that is anchor-free and automatically predicts the position, radius, and offset of nodules without the manual design of nodule/anchor parameters. The SCPM-Net consists of two novel pillars: sphere representation and center points matching. To mimic the nodule annotation in clinical practice, we replace the conventional bounding box with the newly proposed bounding sphere. A compatible sphere-based intersection over-union loss function is introduced to train the lung nodule detection network stably and efficiently.We empower the network anchor-free by designing a positive center-points selection and matching (CPM) process, which naturally discards pre-determined anchor boxes. An online hard example mining and re-focal loss subsequently enable the CPM process more robust, resulting in more accurate point assignment and the mitigation of class imbalance. In addition, to better capture spatial information and 3D context for the detection, we propose to fuse multi-level spatial coordinate maps with the feature extractor and combine them with 3D squeeze-and-excitation attention modules. Experimental results on the LUNA16 dataset showed that our proposed SCPM-Net framework achieves superior performance compared with existing used anchor-based and anchor-free methods for lung nodule detection.",0
"The paper presents a novel approach to anchor-free lung nodule detection using sphere representation and center points matching (SCPM). This method utilizes a convolutional neural network architecture, termed SCPM-Net, which processes images as sphere representations rather than traditional pixel grids. By employing center point matching as opposed to typical bounding box regression used by other methods, our proposed solution reduces errors caused by overlapping annotations while improving computational efficiency. In order to evaluate the effectiveness of SCPM-Net, we trained the model on a dataset of CT scans consisting of both benign and malignant nodules. Our results demonstrate that SCPM-Net outperforms state-of-the-art approaches while maintaining comparable computational requirements. Overall, our work represents a significant advancement in the field of medical image analysis and has potential applications in early cancer diagnosis.",1
"The computational prediction algorithm of neural network, or deep learning, has drawn much attention recently in statistics as well as in image recognition and natural language processing. Particularly in statistical application for censored survival data, the loss function used for optimization has been mainly based on the partial likelihood from Cox's model and its variations to utilize existing neural network library such as Keras, which was built upon the open source library of TensorFlow. This paper presents a novel application of the neural network to the quantile regression for survival data with right censoring, which is adjusted by the inverse of the estimated censoring distribution in the check function. The main purpose of this work is to show that the deep learning method could be flexible enough to predict nonlinear patterns more accurately compared to existing quantile regression methods such as traditional linear quantile regression and nonparametric quantile regression with total variation regularization, emphasizing practicality of the method for censored survival data. Simulation studies were performed to generate nonlinear censored survival data and compare the deep learning method with existing quantile regression methods in terms of prediction accuracy. The proposed method is illustrated with two publicly available breast cancer data sets with gene signatures. The method has been built into a package and is freely available at \url{https://github.com/yicjia/DeepQuantreg}.",0
"In recent years, deep learning techniques have become increasingly popular for solving complex problems in various fields such as computer vision, natural language processing, and even economics. One area where these methods have shown promising results is quantile regression, which has applications in finance, healthcare, engineering, and other domains. This paper presents a novel approach called ""DeepQuantReg"" that leverages deep neural networks to estimate quantiles of continuous response variables subject to right censoring. By integrating existing literature on censored data analysis and deep learning methods for quantile regression, we develop a computationally efficient framework that can handle both numerical stability issues caused by extreme values and high-dimensional covariates present in modern datasets. Our method produces accurate estimates of multiple quantiles simultaneously while accounting for the presence of unobserved heterogeneity or random effects. Simulation studies demonstrate the superior performance of DeepQuantReg compared to state-of-the-art competitors under different scenarios. We then apply our method to two real-world examples, including predicting income ranks after college education completion based on student characteristics and estimating conditional treatment effect for educational attainment gap given pre-treatment factors. Findings from these analyses confirm our method's promise in providing valuable insights for decision-making across diverse application areas affected by censorship or truncation issues in observations. Overall, this research contributes new theory, algorithms, and empirical evidence towards advancing deep learning approaches for addressing practical challenges in quantile regression with right censoring.",1
"Recently, label distribution learning (LDL) has drawn much attention in machine learning, where LDL model is learned from labelel instances. Different from single-label and multi-label annotations, label distributions describe the instance by multiple labels with different intensities and accommodate to more general scenes. Since most existing machine learning datasets merely provide logical labels, label distributions are unavailable in many real-world applications. To handle this problem, we propose two novel label enhancement methods, i.e., Label Enhancement with Sample Correlations (LESC) and generalized Label Enhancement with Sample Correlations (gLESC). More specifically, LESC employs a low-rank representation of samples in the feature space, and gLESC leverages a tensor multi-rank minimization to further investigate the sample correlations in both the feature space and label space. Benefitting from the sample correlations, the proposed methods can boost the performance of label enhancement. Extensive experiments on 14 benchmark datasets demonstrate the effectiveness and superiority of our methods.",0
"In natural language processing (NLP) tasks involving data sets, labels play a significant role as they provide ground truths that machines can use to learn from. However, label quality can impact model performance, especially if there are discrepancies between training and testing distributions. To address these issues, one common approach involves adding additional labeled data, but it may not always be feasible or economical. Therefore, researchers have proposed alternative methods such as active learning, which selects informative samples to ask humans to label. Meanwhile, sample selection based on mutual information maximization has been shown to improve efficiency by reducing human annotation cost while maintaining high model accuracy. This study proposes a novel method called generalized label enhancement with correlations (GLEC), which adapts mutual information criterion to handle variable imbalance problems commonly found in real-world datasets. GLEC calculates feature expectations using softmax probabilities, and then uses the cross entropy between predicted probabilities of different categories given all available features to measure how well each unlabeled example would contribute to improving label quality. By doing so, the method effectively balances between diversity and representativeness when selecting samples. Through extensive experimentation on several public benchmark datasets across diverse domains, we demonstrate that our method consistently outperforms state-of-the-art baselines and even approaches human-level accuracy under some circumstances. These results indicate that GLEC offers a powerful tool for NLP practitioners seeking more efficient ways to enhance label quality without collecting new data.",1
"Camera pose regression methods apply a single forward pass to the query image to estimate the camera pose. As such, they offer a fast and light-weight alternative to traditional localization schemes based on image retrieval. Pose regression approaches simultaneously learn two regression tasks, aiming to jointly estimate the camera position and orientation using a single embedding vector computed by a convolutional backbone. We propose an attention-based approach for pose regression, where the convolutional activation maps are used as sequential inputs. Transformers are applied to encode the sequential activation maps as latent vectors, used for camera pose regression. This allows us to pay attention to spatially-varying deep features. Using two Transformer heads, we separately focus on the features for camera position and orientation, based on how informative they are per task. Our proposed approach is shown to compare favorably to contemporary pose regressors schemes and achieves state-of-the-art accuracy across multiple outdoor and indoor benchmarks. In particular, to the best of our knowledge, our approach is the only method to attain sub-meter average accuracy across outdoor scenes. We make our code publicly available from here.",0
"This paper explores the importance of paying attention to activation maps in camera pose regression tasks. It argues that understanding the behavior of these activation maps can greatly improve our ability to accurately estimate camera poses from image data alone. To support this claim, the paper presents experimental evidence demonstrating that careful consideration of activation maps leads to significant improvements in camera pose estimation accuracy over traditional methods that do not take them into account. Further analysis suggests that incorporating activation map information allows the model to better capture contextual features present in images but may otherwise go unnoticed by standard regression techniques. Overall, the results provide new insights into how we might optimize camera pose estimation models in real-world applications where precise localization is essential.",1
"Convolution has been the core ingredient of modern neural networks, triggering the surge of deep learning in vision. In this work, we rethink the inherent principles of standard convolution for vision tasks, specifically spatial-agnostic and channel-specific. Instead, we present a novel atomic operation for deep neural networks by inverting the aforementioned design principles of convolution, coined as involution. We additionally demystify the recent popular self-attention operator and subsume it into our involution family as an over-complicated instantiation. The proposed involution operator could be leveraged as fundamental bricks to build the new generation of neural networks for visual recognition, powering different deep learning models on several prevalent benchmarks, including ImageNet classification, COCO detection and segmentation, together with Cityscapes segmentation. Our involution-based models improve the performance of convolutional baselines using ResNet-50 by up to 1.6% top-1 accuracy, 2.5% and 2.4% bounding box AP, and 4.7% mean IoU absolutely while compressing the computational cost to 66%, 65%, 72%, and 57% on the above benchmarks, respectively. Code and pre-trained models for all the tasks are available at https://github.com/d-li14/involution.",0
"Title: Involution: Inverting the Inherent Nature of Convolutions for Enhanced Visual Recognition Abstract Deep learning approaches have achieved remarkable successes in computer vision tasks such as image classification, object detection, and segmentation. At their core lie convolutional neural networks (CNN), which primarily operate on local receptive fields obtained by applying convolutions over input images. While effective, these operations can hinder generalization abilities by failing to capture relevant contextual relationships among spatially disparate features. This study introduces the concept of ""involution""â€”the inverse operation of convolutionâ€”for enhancing visual recognition performance via fine-grained control over feature interactions. By designing new architectures leveraging inverted filters that adaptively weigh neighborhood contributions, our approach improves both accuracy and interpretability without incurring significant computational overhead. Experimental evaluations across diverse datasets demonstrate consistent gains over conventional CNN baselines, further verifying the utility of this innovative paradigm shift. Keywords: Computer Vision, Convolution, Involution, Neural Networks, Image Processing. Note: In this work we propose using inverted filters in place of regular ones to enhance the quality of the feature maps produced from given inputs. Our method called â€œInvolutionâ€ works by employing filters that assign weights to neighboring pixels, making them more interpretable while allowing us to better manage global dependencies between all the patches within a feature map. Extensive experiments demonstrated improvement consistently against current state of art results.",1
"Zero-shot learning, the task of learning to recognize new classes not seen during training, has received considerable attention in the case of 2D image classification. However, despite the increasing ubiquity of 3D sensors, the corresponding 3D point cloud classification problem has not been meaningfully explored and introduces new challenges. In this paper, we identify some of the challenges and apply 2D Zero-Shot Learning (ZSL) methods in the 3D domain to analyze the performance of existing models. Then, we propose a novel approach to address the issues specific to 3D ZSL. We first present an inductive ZSL process and then extend it to the transductive ZSL and Generalized ZSL (GZSL) settings for 3D point cloud classification. To this end, a novel loss function is developed that simultaneously aligns seen semantics with point cloud features and takes advantage of unlabeled test data to address some known issues (e.g., the problems of domain adaptation, hubness, and data bias). While designed for the particularities of 3D point cloud classification, the method is shown to also be applicable to the more common use-case of 2D image classification. An extensive set of experiments is carried out, establishing state-of-the-art for ZSL and GZSL on synthetic (ModelNet40, ModelNet10, McGill) and real (ScanObjectNN) 3D point cloud datasets.",0
"This is a challenging problem that has only recently been tackled by machine learning techniques. One reason why zero-shot learning (ZSL) on point clouds remains unsolved is because existing data-driven object detection methods like PointNet++ are hard to generalize to objects they have never seen before due to their fixed architecture nature. To handle new classes without any additional training data, we propose a novel method called SelfGuided3D which predicts class prototypes given by user annotations and self-supervisedly learns how different viewpoints affect 3D shapes using a shape completion module. Our experiments show significant improvement over baselines trained on all classes as well as state-of-the art ZSL models for both small and large scale datasets on 3D object detection tasks. Besides, we extend our work from 3D points clouds to other modalities such as CAD models and textured meshes, achieving impressive results while keeping similar simplicity. By doing so, our approach demonstrates strong evidence that prototype representation can serve as a universal key for transferring knowledge across objects from diverse domains.",1
"Although recent advances in deep learning accelerated an improvement in a weakly supervised object localization (WSOL) task, there are still challenges to identify the entire body of an object, rather than only discriminative parts. In this paper, we propose a novel residual fine-grained attention (RFGA) module that autonomously excites the less activated regions of an object by utilizing information distributed over channels and locations within feature maps in combination with a residual operation. To be specific, we devise a series of mechanisms of triple-view attention representation, attention expansion, and feature calibration. Unlike other attention-based WSOL methods that learn a coarse attention map, having the same values across elements in feature maps, our proposed RFGA learns fine-grained values in an attention map by assigning different attention values for each of the elements. We validated the superiority of our proposed RFGA module by comparing it with the recent methods in the literature over three datasets. Further, we analyzed the effect of each mechanism in our RFGA and visualized attention maps to get insights.",0
"This paper presents a new approach to weakly supervised object localization using fine-grained attention mechanisms. The proposed method allows for accurate detection and segmentation of objects in images without requiring expensive pixel-level annotations. Our model uses both global context and spatial attention to effectively learn from weak labels, such as image-level tags, resulting in improved accuracy over previous methods. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach compared to state-of-the-art techniques. Overall, this work represents a significant step towards enabling real-world applications that can accurately locate objects in unlabeled images and videos.",1
"Attention-based scene text recognizers have gained huge success, which leverages a more compact intermediate representation to learn 1d- or 2d- attention by a RNN-based encoder-decoder architecture. However, such methods suffer from attention-drift problem because high similarity among encoded features leads to attention confusion under the RNN-based local attention mechanism. Moreover, RNN-based methods have low efficiency due to poor parallelization. To overcome these problems, we propose the MASTER, a self-attention based scene text recognizer that (1) not only encodes the input-output attention but also learns self-attention which encodes feature-feature and target-target relationships inside the encoder and decoder and (2) learns a more powerful and robust intermediate representation to spatial distortion, and (3) owns a great training efficiency because of high training parallelization and a high-speed inference because of an efficient memory-cache mechanism. Extensive experiments on various benchmarks demonstrate the superior performance of our MASTER on both regular and irregular scene text. Pytorch code can be found at https://github.com/wenwenyu/MASTER-pytorch, and Tensorflow code can be found at https://github.com/jiangxiluning/MASTER-TF.",0
"This abstract describes a new system called ""MASTER"" that uses a multi-aspect non-local network for scene text recognition. The system can handle variations in lighting conditions, backgrounds, and text sizes using a combination of convolutional neural networks (CNNs) and recurrent neural networks (RNNs). Experimental results show that MASTER outperforms other state-of-the-art methods on several benchmark datasets, including ICDAR2013, ICDAR2015, and MSRA-TD500. The proposed system has potential applications in areas such as automated document processing, image retrieval, and computer vision. Overall, MASTER represents a significant advancement in scene text recognition technology.",1
"Existing object detection-based text detectors mainly concentrate on detecting horizontal and multioriented text. However, they do not pay enough attention to complex-shape text (curved or other irregularly shaped text). Recently, segmentation-based text detection methods have been introduced to deal with the complex-shape text; however, the pixel level processing increases the computational cost significantly. To further improve the accuracy and efficiency, we propose a novel detection framework for arbitrary-shape text detection, termed as RayNet. RayNet uses Center Point Set (CPS) and Ray Distance (RD) to fit text, where CPS is used to determine the text general position and the RD is combined with CPS to compute Ray Points (RP) to localize the text accurate shape. Since RP are disordered, we develop the Ray Points Connection (RPC) algorithm to reorder RP, which significantly improves the detection performance of complex-shape text. RayNet achieves impressive performance on existing curved text dataset (CTW1500) and quadrangle text dataset (ICDAR2015), which demonstrate its superiority against several state-of-the-art methods.",0
"Real-time scene arbitrary shape text detection presents significant challenges due to complex backgrounds, varying lighting conditions, and diverse font styles. To address these difficulties, we propose a new method based on ray casting which effectively detects texts with various shapes and sizes. Our approach, called RayNet, utilizes multiple parallel rays originating from the image center and extends towards peripheral areas to capture rich contextual features that represent different scales and orientations of texts.  In particular, our key contributions can be summarized as follows: * We introduce RayNet, a novel real-time text detector that leverages ray casting to efficiently localize texts within images or videos. This allows our system to achieve state-of-the-art performance while maintaining low computational complexity. * Our method employs a cascading architecture composed of three stages, each equipped with a set of parallel rays that adaptively expand the search space based on previously detected texts. This design enables better coverage of possible text locations and reduces the number of unnecessary computations. * Our network architectures are carefully designed to retain sufficient spatial resolution throughout the entire processing pipeline while preserving contextual information through pyramidal feature representations. As a result, our models exhibit excellent robustness against variations in scale, orientation, and appearance. * Comprehensive experiments demonstrate that our proposed framework surpasses existing approaches under both qualitative and quantitative evaluations across several datasets. These results confirm RayNetâ€™s effectiveness at accurately detecting texts under complicated scenarios encountered in practice.  Overall, our work represents a step forward toward developing efficient real-time text detection systems capable of handling arbitrary shapes with high accuracy and versatility. By bridging the gap between efficiency and performance, RayNet establishes itself as a promising tool for numerous applications including autonomous driving, augmented reality, and content filtering.",1
"Graph Neural Networks (GNNs) have proved to be an effective representation learning framework for graph-structured data, and have achieved state-of-the-art performance on many practical predictive tasks, such as node classification, link prediction and graph classification. Among the variants of GNNs, Graph Attention Networks (GATs) learn to assign dense attention coefficients over all neighbors of a node for feature aggregation, and improve the performance of many graph learning tasks. However, real-world graphs are often very large and noisy, and GATs are prone to overfitting if not regularized properly. Even worse, the local aggregation mechanism of GATs may fail on disassortative graphs, where nodes within local neighborhood provide more noise than useful information for feature aggregation. In this paper, we propose Sparse Graph Attention Networks (SGATs) that learn sparse attention coefficients under an $L_0$-norm regularization, and the learned sparse attentions are then used for all GNN layers, resulting in an edge-sparsified graph. By doing so, we can identify noisy/task-irrelevant edges, and thus perform feature aggregation on most informative neighbors. Extensive experiments on synthetic and real-world graph learning benchmarks demonstrate the superior performance of SGATs. In particular, SGATs can remove about 50\%-80\% edges from large assortative graphs, while retaining similar classification accuracies. On disassortative graphs, SGATs prune majority of noisy edges and outperform GATs in classification accuracies by significant margins. Furthermore, the removed edges can be interpreted intuitively and quantitatively. To the best of our knowledge, this is the first graph learning algorithm that shows significant redundancies in graphs and edge-sparsified graphs can achieve similar or sometimes higher predictive performances than original graphs.",0
"Recently there has been growing interest in graph neural networks (GNN), which aim at capturing node features in large graphs by propagating information through edges iteratively. In practice, real-world graphs often contain many sparsely connected nodes without significant links among them. This fact brings two challenges: Firstly, traditional GNN architectures may suffer from oversmoothing issue due to excessive feature mixing across layers; secondly, most edges have little contributions to predicting node properties, which leads to wasted computational resources on redundant messages passing. To address these issues, we propose sparse graph attention networks (SGAN) that enable dynamic information exchange based on edge weights, allowing each message passing layer adaptively focuses on top-K important neighbours. Experiments show our methods achieve competitive performance against other state-of-the-art models on several benchmark datasets while requiring fewer parameters. Our work opens up opportunities for developing more efficient GNN architecture designs under different settings in future studies.",1
"Training and deploying graph neural networks (GNNs) remains difficult due to their high memory consumption and inference latency. In this work we present a new type of GNN architecture that achieves state-of-the-art performance with lower memory consumption and latency, along with characteristics suited to accelerator implementation. Our proposal uses memory proportional to the number of vertices in the graph, in contrast to competing methods which require memory proportional to the number of edges; we find our efficient approach actually achieves higher accuracy than competing approaches across 5 large and varied datasets against strong baselines. We achieve our results by using a novel adaptive filtering approach inspired by signal processing; it can be interpreted as enabling each vertex to have its own weight matrix, and is not related to attention. Following our focus on efficient hardware usage, we propose aggregator fusion, a technique to enable GNNs to significantly boost their representational power, with only a small increase in latency of 19% over standard sparse matrix multiplication. Code and pretrained models can be found at this URL: https://github.com/shyam196/egc.",0
"This paper presents an efficient approach to graph convolutional networks using adaptive filters and aggregators through fusion techniques. We show that fusing low-pass and high-pass versions of the graph Laplacian operator can significantly improve performance while reducing computational complexity compared to traditional graph filtering methods. Our proposed method outperforms existing state-of-the-art models on various benchmark datasets for node classification tasks. Additionally, we demonstrate how our technique effectively captures nonlinear features from graphs by analyzing activation maps produced during training. Overall, our findings suggest that adaptive filter and aggregator fusion for graph convolutions provides a promising direction for advancing machine learning algorithms on irregularly structured data.",1
"Drug-drug interaction(DDI) prediction is an important task in the medical health machine learning community. This study presents a new method, multi-view graph contrastive representation learning for drug-drug interaction prediction, MIRACLE for brevity, to capture inter-view molecule structure and intra-view interactions between molecules simultaneously. MIRACLE treats a DDI network as a multi-view graph where each node in the interaction graph itself is a drug molecular graph instance. We use GCNs and bond-aware attentive message passing networks to encode DDI relationships and drug molecular graphs in the MIRACLE learning stage, respectively. Also, we propose a novel unsupervised contrastive learning component to balance and integrate the multi-view information. Comprehensive experiments on multiple real datasets show that MIRACLE outperforms the state-of-the-art DDI prediction models consistently.",0
"This paper presents a novel approach for drug-drug interaction prediction using multi-view graph contrastive representation learning (MGCL). Traditional approaches rely on single-view representations of drugs or their molecular structures, which may limit their ability to capture important interactions across multiple views. In MGCL, we learn a shared latent space that aligns different types of graphs representing chemical structure, gene expression data, protein-protein interactions, and other relevant features associated with each drug. Our method involves pretraining two InfoMax-based models, one for predicting the presence of edges within each view and another for discriminating edge pairs from same/different drug pairs. After pretraining, these two models are integrated into a final model for drug-drug interaction prediction by computing similarity scores based on the learned embedding space. Experimental results show that our proposed method outperforms state-of-the-art baseline methods in terms of accuracy, providing evidence of the importance of integrating diverse sources of information for improving predictions. By utilizing MGCL, we aim to improve the accuracy of drug safety assessment during early stages of drug development and reduce potential risks posed by adverse drug reactions.",1
"As airborne vehicles are becoming more autonomous and ubiquitous, it has become vital to develop the capability to detect the objects in their surroundings. This paper attempts to address the problem of drones detection from other flying drones. The erratic movement of the source and target drones, small size, arbitrary shape, large intensity variations, and occlusion make this problem quite challenging. In this scenario, region-proposal based methods are not able to capture sufficient discriminative foreground-background information. Also, due to the extremely small size and complex motion of the source and target drones, feature aggregation based methods are unable to perform well. To handle this, instead of using region-proposal based methods, we propose to use a two-stage segmentation-based approach employing spatio-temporal attention cues. During the first stage, given the overlapping frame regions, detailed contextual information is captured over convolution feature maps using pyramid pooling. After that pixel and channel-wise attention is enforced on the feature maps to ensure accurate drone localization. In the second stage, first stage detections are verified and new probable drone locations are explored. To discover new drone locations, motion boundaries are used. This is followed by tracking candidate drone detections for a few frames, cuboid formation, extraction of the 3D convolution feature map, and drones detection within each cuboid. The proposed approach is evaluated on two publicly available drone detection datasets and outperforms several competitive baselines.",0
"This paper presents our approach to detect if the videos contain drone footage in it. In this day and age, there is no shortage of drones that record everything they see. From picturesque landscapes to your daily commute, you can find it all on YouTube and other popular video sharing websites. But one question remains: Can we tell which footages were taken by drones? We present our solution by using object detection techniques such as YOLOv8 and SSD along with image manipulation libraries like Pillow to enhance images before feeding them into models. Our experiments show promising results across several publicly available datasets containing drone footages and regular video data. We believe this research has applications ranging from content filtering systems to counter terrorism measures where distinguishing real footages captured through expensive cameras compared to cheap quadcopters can save lives.",1
"Deep neural network (DNN) accelerators received considerable attention in past years due to saved energy compared to mainstream hardware. Low-voltage operation of DNN accelerators allows to further reduce energy consumption significantly, however, causes bit-level failures in the memory storing the quantized DNN weights. In this paper, we show that a combination of robust fixed-point quantization, weight clipping, and random bit error training (RandBET) improves robustness against random bit errors in (quantized) DNN weights significantly. This leads to high energy savings from both low-voltage operation as well as low-precision quantization. Our approach generalizes across operating voltages and accelerators, as demonstrated on bit errors from profiled SRAM arrays. We also discuss why weight clipping alone is already a quite effective way to achieve robustness against bit errors. Moreover, we specifically discuss the involved trade-offs regarding accuracy, robustness and precision: Without losing more than 1% in accuracy compared to a normally trained 8-bit DNN, we can reduce energy consumption on CIFAR-10 by 20%. Higher energy savings of, e.g., 30%, are possible at the cost of 2.5% accuracy, even for 4-bit DNNs.",0
"Artificial neural networks (ANNs) have achieved significant improvements across many domains due to their ability to learn complex relationships within datasets that would otherwise require manual feature engineering. Deep Neural Networks (DNNs), in particular, provide state-of-the art performance on tasks ranging from image recognition to natural language processing. However, these models often exhibit poor error resilience under noisy environments owing to their reliance on high precision floating point arithmetic during inference. As such, specialized hardware accelerator architectures are required to maintain accuracy at reduced bitwidths without incurring substantial loss in energy efficiency. This paper presents several techniques aimed towards improving error robustness of reduced precision deep learning workloads while optimizing overall system performance. By utilizing data encoding methods alongside novel quantization schemes tailored specifically to DNNs, we demonstrate up to 2X improvement in biterror rates along with corresponding reductions in area footprint and dynamic power consumption compared to prior research. Our results show promise towards enabling deployment of low-power embedded devices capable of running computationally intensive deep learning algorithms while ensuring reliable operation under realworld conditions.",1
"A 360{\deg} perception of scene geometry is essential for automated driving, notably for parking and urban driving scenarios. Typically, it is achieved using surround-view fisheye cameras, focusing on the near-field area around the vehicle. The majority of current depth estimation approaches focus on employing just a single camera, which cannot be straightforwardly generalized to multiple cameras. The depth estimation model must be tested on a variety of cameras equipped to millions of cars with varying camera geometries. Even within a single car, intrinsics vary due to manufacturing tolerances. Deep learning models are sensitive to these changes, and it is practically infeasible to train and test on each camera variant. As a result, we present novel camera-geometry adaptive multi-scale convolutions which utilize the camera parameters as a conditional input, enabling the model to generalize to previously unseen fisheye cameras. Additionally, we improve the distance estimation by pairwise and patchwise vector-based self-attention encoder networks. We evaluate our approach on the Fisheye WoodScape surround-view dataset, significantly improving over previous approaches. We also show a generalization of our approach across different camera viewing angles and perform extensive experiments to support our contributions. To enable comparison with other approaches, we evaluate the front camera data on the KITTI dataset (pinhole camera images) and achieve state-of-the-art performance among self-supervised monocular methods. An overview video with qualitative results is provided at https://youtu.be/bmX0UcU9wtA. Baseline code and dataset will be made public.",0
"In this work, we present SVDistNet, a self-supervised method for estimating near-field distance using surround view fisheye cameras without relying on ground truth labels. Traditional approaches require precise camera calibration, specialized hardware, or dense depth maps, which can be impractical for many applications. We overcome these limitations by leveraging the spatial structure within the fisheye images themselves to learn robust features that capture both geometric and photometric cues from surrounding pixels. Our network uses a novel architecture inspired by successful recent advances in supervised monocular estimation techniques, adapted to operate directly on raw sensor data under real-world illumination conditions. By applying cyclical self-training with progressive distillation, we improve the accuracy and generalization performance of our model substantially over previous unsupervised methods. Experiments demonstrate excellent near-field distance estimates across challenging outdoor scenarios such as varying weather and lighting conditions, dynamic objects, and camera motion during operation. This approach paves the way towards enabling safe and reliable perception systems for autonomous vehicles operating at close range in complex urban environments where precise localization is crucial.",1
"Zeroth-order (ZO, also known as derivative-free) methods, which estimate the gradient only by two function evaluations, have attracted much attention recently because of its broad applications in machine learning community. The two function evaluations are normally generated with random perturbations from standard Gaussian distribution. To speed up ZO methods, many methods, such as variance reduced stochastic ZO gradients and learning an adaptive Gaussian distribution, have recently been proposed to reduce the variances of ZO gradients. However, it is still an open problem whether there is a space to further improve the convergence of ZO methods. To explore this problem, in this paper, we propose a new reinforcement learning based ZO algorithm (ZO-RL) with learning the sampling policy for generating the perturbations in ZO optimization instead of using random sampling. To find the optimal policy, an actor-critic RL algorithm called deep deterministic policy gradient (DDPG) with two neural network function approximators is adopted. The learned sampling policy guides the perturbed points in the parameter space to estimate a more accurate ZO gradient. To the best of our knowledge, our ZO-RL is the first algorithm to learn the sampling policy using reinforcement learning for ZO optimization which is parallel to the existing methods. Especially, our ZO-RL can be combined with existing ZO algorithms that could further accelerate the algorithms. Experimental results for different ZO optimization problems show that our ZO-RL algorithm can effectively reduce the variances of ZO gradient by learning a sampling policy, and converge faster than existing ZO algorithms in different scenarios.",0
"This research proposes a new learning sampling policy for derivative free optimization that significantly speeds up convergence compared to existing methods. Our approach uses a novel reinforcement learning algorithm to optimize hyperparameters of sampling policies. We show through extensive experimental evaluation that our method outperforms state-of-the-art techniques across a range of benchmark problems by a significant margin. In addition, we demonstrate the generality of our method by applying it to three different types of sampling policies: random search, Bayesian optimization, and gradient-free natural evolution strategy. Our results highlight the potential of using machine learning to improve the efficiency of derivative free optimization algorithms. Overall, our work makes a valuable contribution to the field of computational optimization.",1
"Deep learning architectures have an extremely high-capacity for modeling complex data in a wide variety of domains. However, these architectures have been limited in their ability to support complex prediction problems using insurance claims data, such as readmission at 30 days, mainly due to data sparsity issue. Consequently, classical machine learning methods, especially those that embed domain knowledge in handcrafted features, are often on par with, and sometimes outperform, deep learning approaches. In this paper, we illustrate how the potential of deep learning can be achieved by blending domain knowledge within deep learning architectures to predict adverse events at hospital discharge, including readmissions. More specifically, we introduce a learning architecture that fuses a representation of patient data computed by a self-attention based recurrent neural network, with clinically relevant features. We conduct extensive experiments on a large claims dataset and show that the blended method outperforms the standard machine learning approaches.",0
"In recent years, deep recurrent neural networks (DRNNs) have been widely used for predicting adverse events after hospital discharge, due to their ability to process sequential data and learn complex representations from large datasets. However, these models often suffer from poor generalization performance on unseen patient populations and lack interpretability, leading to limited clinical utility. To address these challenges, we propose a novel approach that combines knowledge blending with DRNNs for improved prediction accuracy and interpretability of adverse event predictions during hospital discharge. Our method leverages both task-specific and domain expertise to enhance DRNN learning by incorporating relevant prior knowledge into model training. This enables our system to better account for variable relationships among features and improve its robustness to different patient populations. We evaluate the proposed framework through extensive experiments using real-world patient discharge records and demonstrate substantial improvements over baseline methods in terms of adverse event prediction accuracy, feature importance analysis, and model interpretation. Our results suggest that integrating external knowledge sources into DRNN architecture can lead to more effective, explainable, and reliable predictive models for healthcare applications. Overall, this study offers valuable insights into how to design and develop interpretable machine learning algorithms for medical decision support systems.",1
"This paper presents Contrastive Reconstruction, ConRec - a self-supervised learning algorithm that obtains image representations by jointly optimizing a contrastive and a self-reconstruction loss. We showcase that state-of-the-art contrastive learning methods (e.g. SimCLR) have shortcomings to capture fine-grained visual features in their representations. ConRec extends the SimCLR framework by adding (1) a self-reconstruction task and (2) an attention mechanism within the contrastive learning task. This is accomplished by applying a simple encoder-decoder architecture with two heads. We show that both extensions contribute towards an improved vector representation for images with fine-grained visual features. Combining those concepts, ConRec outperforms SimCLR and SimCLR with Attention-Pooling on fine-grained classification datasets.",0
"This article presents a new approach towards visual representations that combines contrastive learning with image reconstruction and attention-weighted pooling. By doing so, we aim to create more fine-grained visual representations that capture subtle differences between similar images while retaining important features. Our method starts by training two convolutional neural networks (CNNs) on pairs of positive and negative samples generated using random crops from the same image. We then use the trained model as a feature extractor for image reconstruction, where each pixel value is reconstructed based on the surrounding contextual information captured by the CNN. Finally, we apply attention-weighted pooling to these features to obtain a global representation of the entire image. We evaluate our method on several benchmark datasets and show significant improvement over existing methods. Our results suggest that combining contrastive learning with image reconstruction and attention-weighted pooling can indeed lead to better fine-grained visual representations.",1
"Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.",0
"Abstract: This review provides an overview of graph neural networks (GNN), which have emerged as powerful tools for modeling complex relationships within graphs. GNNs learn representations of nodes by iteratively aggregating their neighborhood information from individual layers. They have been applied to many tasks such as node classification, link prediction, and community detection. The methods used by these models, including message passing, attention mechanisms, and pooling operations, are discussed in detail along with recent advancements that aim at improving their performance and scalability on large graphs. In addition, open challenges and future directions of research on GNNs are presented to inspire new developments in this field. Ultimately, this review serves as a reference guide to both researchers and practitioners interested in employing graph convolutional networks for their own applications.",1
"Few-shot learning aims to correctly recognize query samples from unseen classes given a limited number of support samples, often by relying on global embeddings of images. In this paper, we propose to equip the backbone network with an attention agent, which is trained by reinforcement learning. The policy gradient algorithm is employed to train the agent towards adaptively localizing the representative regions on feature maps over time. We further design a reward function based on the prediction of the held-out data, thus helping the attention mechanism to generalize better across the unseen classes. The extensive experiments show, with the help of the reinforced attention, that our embedding network has the capability to progressively generate a more discriminative representation in few-shot learning. Moreover, experiments on the task of image classification also show the effectiveness of the proposed design.",0
"Improving few-shot learning performance has been a key challenge in artificial intelligence research due to the lack of labeled data available for training machine learning models on new tasks. Traditional attention mechanisms have provided some improvements in few-shot learning performance but they still struggle to capture complex relationships within task examples, particularly as the number of shots decreases. To address these issues, we propose reinforced attention (RA), which uses policy gradient methods inspired by reinforcement learning to optimize attention weights during inference. Experimental results show that our RA approach significantly outperforms traditional attention methods and other state-of-the-art approaches on popular benchmark datasets such as Omniglot and miniImageNet. We further demonstrate that our method generalizes well across different architectures and can effectively transfer learned knowledge from one dataset to another. Finally, we provide detailed analysis and ablation studies to better understand how our proposed technique works and where it excels compared to existing alternatives. Our work represents an important step towards more effective few-shot learning techniques and opens up opportunities for future research on applying reinforcement learning principles to improve neural network behavior beyond supervised learning settings.",1
"Pose-guided person image generation usually involves using paired source-target images to supervise the training, which significantly increases the data preparation effort and limits the application of the models. To deal with this problem, we propose a novel multi-level statistics transfer model, which disentangles and transfers multi-level appearance features from person images and merges them with pose features to reconstruct the source person images themselves. So that the source images can be used as supervision for self-driven person image generation. Specifically, our model extracts multi-level features from the appearance encoder and learns the optimal appearance representation through attention mechanism and attributes statistics. Then we transfer them to a pose-guided generator for re-fusion of appearance and pose. Our approach allows for flexible manipulation of person appearance and pose properties to perform pose transfer and clothes style transfer tasks. Experimental results on the DeepFashion dataset demonstrate our method's superiority compared with state-of-the-art supervised and unsupervised methods. In addition, our approach also performs well in the wild.",0
"In recent years, Generative Adversarial Networks (GANs) have become increasingly popular due to their ability to generate high quality images that closely resemble real-world examples. One challenge faced by GANs is the difficulty in controlling the output image. To address this issue, we propose a new approach called ""Multi-level Statistics Transfer"" (MUST). Our method enables more fine-grained control over the generated images by incorporating multiple levels of statistics from the input data into the generator network. These statistics are transferred at different resolutions through convolutional neural networks to capture both local and global features from the dataset. We then employ these statistics as constraints on the generator network to enable self-supervised training without paired samples. Experimental results demonstrate that our proposed MUST-GAN outperforms previous state-of-the-art methods in terms of visual fidelity and diversity, while maintaining efficient computational performance. Additionally, our method achieves better performance across a wide range of tasks including face generation, object generation, and image translation. Overall, MUST provides a promising direction for generating high-quality images via self-supervision techniques.",1
"Matching contactless fingerprints or finger photos to contact-based fingerprint impressions has received increased attention in the wake of COVID-19 due to the superior hygiene of the contactless acquisition and the widespread availability of low cost mobile phones capable of capturing photos of fingerprints with sufficient resolution for verification purposes. This paper presents an end-to-end automated system, called C2CL, comprised of a mobile finger photo capture app, preprocessing, and matching algorithms to handle the challenges inhibiting previous cross-matching methods; namely i) low ridge-valley contrast of contactless fingerprints, ii) varying roll, pitch, yaw, and distance of the finger to the camera, iii) non-linear distortion of contact-based fingerprints, and vi) different image qualities of smartphone cameras. Our preprocessing algorithm segments, enhances, scales, and unwarps contactless fingerprints, while our matching algorithm extracts both minutiae and texture representations. A sequestered dataset of 9,888 contactless 2D fingerprints and corresponding contact-based fingerprints from 206 subjects (2 thumbs and 2 index fingers for each subject) acquired using our mobile capture app is used to evaluate the cross-database performance of our proposed algorithm. Furthermore, additional experimental results on 3 publicly available datasets demonstrate, for the first time, contact to contactless fingerprint matching accuracy that is comparable to existing contact to contact fingerprint matching systems (TAR in the range of 96.67% to 98.15% at FAR=0.01%).",0
"Incorporating both contact and contactless fingerprints into authentication systems improves user convenience by allowing for touch-free recognition while still ensuring security via traditional contact methods. However, matching these two types of prints separately has hindered efficient identity verification, since most existing solutions require users to switch between contact and contactless modes. This research proposes Contact-to-Contactless (C2CL), a novel algorithm that bridges the gap between the different modalities of fingerprint capture. By comparing minutiae points from both types of prints, C2CL enables fast identification without sacrificing accuracy. Our experimental results show promising performance on three public datasets, with accuracy surpassing state-of-the-art approaches. These findings pave the way towards seamless biometric recognition across various environments and applications, ultimately enhancing user experience and security.",1
"Self-attention learns pairwise interactions to model long-range dependencies, yielding great improvements for video action recognition. In this paper, we seek a deeper understanding of self-attention for temporal modeling in videos. We first demonstrate that the entangled modeling of spatio-temporal information by flattening all pixels is sub-optimal, failing to capture temporal relationships among frames explicitly. To this end, we introduce Global Temporal Attention (GTA), which performs global temporal attention on top of spatial attention in a decoupled manner. We apply GTA on both pixels and semantically similar regions to capture temporal relationships at different levels of spatial granularity. Unlike conventional self-attention that computes an instance-specific attention matrix, GTA directly learns a global attention matrix that is intended to encode temporal structures that generalize across different samples. We further augment GTA with a cross-channel multi-head fashion to exploit channel interactions for better temporal modeling. Extensive experiments on 2D and 3D networks demonstrate that our approach consistently enhances temporal modeling and provides state-of-the-art performance on three video action recognition datasets.",0
"Deep learning models have revolutionized the field of computer vision by enabling powerful methods for solving complex problems such as image classification, object detection, and semantic segmentation. However, video understanding remains a challenging task due to the complexity of temporal patterns in videos. This paper proposes a novel deep convolutional neural network architecture called Global Temporal Attention (GTA) for action recognition in videos.  The key challenge in modeling actions in videos lies in capturing spatio-temporal dependencies within and across frames. Most existing approaches rely on handcrafted features that capture local motion information but lack global temporal context. In contrast, we propose a fully convolutional approach that processes entire clips at once while explicitly accounting for both short-range temporal interactions through dilated convolutions and long-range dependency via attention mechanisms.  Our proposed method introduces two main components - a global spatial attention module that selects important regions from all frame pairs globally; and a temporal attention mechanism that adaptively aggregates information over time. These modules are combined into one streamlined end-to-end trainable network designed specifically for video action recognition. Our experiments show that our GTA model significantly outperforms strong baselines on multiple benchmark datasets.  In summary, this work presents a new approach to temporal attention in deep networks for video action recognition. By leveraging selective global spatial attention and dynamic temporal attention, our model can effectively capture the complex interplay of spatio-temporal cues present in natural human activities. We believe these contributions represent an important step forward towards intelligent systems capable of robustly analyzing real-world behavior data.",1
"We propose a novel transformer-based styled handwritten text image generation approach, HWT, that strives to learn both style-content entanglement as well as global and local writing style patterns. The proposed HWT captures the long and short range relationships within the style examples through a self-attention mechanism, thereby encoding both global and local style patterns. Further, the proposed transformer-based HWT comprises an encoder-decoder attention that enables style-content entanglement by gathering the style representation of each query character. To the best of our knowledge, we are the first to introduce a transformer-based generative network for styled handwritten text generation. Our proposed HWT generates realistic styled handwritten text images and significantly outperforms the state-of-the-art demonstrated through extensive qualitative, quantitative and human-based evaluations. The proposed HWT can handle arbitrary length of text and any desired writing style in a few-shot setting. Further, our HWT generalizes well to the challenging scenario where both words and writing style are unseen during training, generating realistic styled handwritten text images.",0
"In recent years, deep learning techniques have been applied to many fields such as computer vision, natural language processing, speech recognition, autonomous driving, and more recently generative tasks such as synthesizing images. Despite these successes, most generators still rely on recurrent architectures like LSTMs or GRUs. However, the ability to generate sequential data using transformer networks has become a hot research topic due to their parallelizable self attention mechanism, which allows them to model global dependencies without needing to maintain state over time. These models can now be fine-tuned on smaller datasets making them even more attractive given todayâ€™s large amounts of available training data compared to just a few years ago. Handwriting generation itself is particularly interesting given that human handwritings often contain both structured information (text) and unstructured information(emotions). This makes these two types of information difficult to disentangle since human handwriting contains complex interactions between different elements, some of which may be related, but others less so. Therefore analyzing a single character written by multiple humans offers different cues for those same characters depending on contextual factors. As a result, we believe that the development of Handwriting Transformer Models could have significant implications towards improving our understanding of how these systems work and how well they scale, while simultaneously offering improved performance relative to traditional methods of generating realistic looking text via RNNs, LSTMS, ConvNets or other approaches.",1
"Saliency prediction has made great strides over the past two decades, with current techniques modeling low-level information, such as color, intensity and size contrasts, and high-level one, such as attention and gaze direction for entire objects. Despite this, these methods fail to account for the dissimilarity between objects, which humans naturally do. In this paper, we introduce a detection-guided saliency prediction network that explicitly models the differences between multiple objects, such as their appearance and size dissimilarities. Our approach is general, allowing us to fuse our object dissimilarities with features extracted by any deep saliency prediction network. As evidenced by our experiments, this consistently boosts the accuracy of the baseline networks, enabling us to outperform the state-of-the-art models on three saliency benchmarks, namely SALICON, MIT300 and CAT2000.",0
"""Modeling object dissimilarity is important for many computer vision tasks such as saliency prediction, where the goal is to identify which parts of an image draw human attention. In our work, we propose a novel approach to modeling object dissimilarity using deep learning techniques, specifically convolutional neural networks (CNNs). Our method leverages the ability of CNNs to learn features that capture subtle differences between objects, allowing us to accurately predict their level of dissimilarity. We demonstrate the effectiveness of our approach by evaluating it on a benchmark dataset commonly used for saliency prediction, showing significantly improved performance compared to state-of-the-art methods.""",1
"Font generation is a challenging problem especially for some writing systems that consist of a large number of characters and has attracted a lot of attention in recent years. However, existing methods for font generation are often in supervised learning. They require a large number of paired data, which is labor-intensive and expensive to collect. Besides, common image-to-image translation models often define style as the set of textures and colors, which cannot be directly applied to font generation. To address these problems, we propose novel deformable generative networks for unsupervised font generation (DGFont). We introduce a feature deformation skip connection (FDSC) which predicts pairs of displacement maps and employs the predicted maps to apply deformable convolution to the low-level feature maps from the content encoder. The outputs of FDSC are fed into a mixer to generate the final results. Taking advantage of FDSC, the mixer outputs a high-quality character with a complete structure. To further improve the quality of generated images, we use three deformable convolution layers in the content encoder to learn style-invariant feature representations. Experiments demonstrate that our model generates characters in higher quality than state-of-art methods. The source code is available at https://github.com/ecnuycxie/DG-Font.",0
"In recent years, deep learning has shown great promise in generating images, videos, audio, text, and other media types through generative models such as GANs (Generative Adversarial Networks). However, despite their versatility, these methods often require large amounts of labeled data or extensive hyperparameter tuning to achieve good results. To address this issue, we propose a new approach called ""DG-Font"" which uses deformable convolutional networks (ConvNets) for unsupervised font generation. Our method leverages the unique advantages of ConvNets over traditional methods like autoencoders by directly operating on raw pixel inputs without requiring any preprocessing steps. We show that our model can generate high quality fonts under both supervised and semi-supervised settings. Furthermore, we demonstrate that DG-Font outperforms state-of-the-art font generation methods on several benchmark datasets. This work represents a step forward towards more efficient and effective generative models that are able to learn from limited data or even noisy labels.",1
"Since its inception, Visual Question Answering (VQA) is notoriously known as a task, where models are prone to exploit biases in datasets to find shortcuts instead of performing high-level reasoning. Classical methods address this by removing biases from training data, or adding branches to models to detect and remove biases. In this paper, we argue that uncertainty in vision is a dominating factor preventing the successful learning of reasoning in vision and language problems. We train a visual oracle and in a large scale study provide experimental evidence that it is much less prone to exploiting spurious dataset biases compared to standard models. We propose to study the attention mechanisms at work in the visual oracle and compare them with a SOTA Transformer-based model. We provide an in-depth analysis and visualizations of reasoning patterns obtained with an online visualization tool which we make publicly available (https://reasoningpatterns.github.io). We exploit these insights by transferring reasoning patterns from the oracle to a SOTA Transformer-based VQA model taking standard noisy visual inputs via fine-tuning. In experiments we report higher overall accuracy, as well as accuracy on infrequent answers for each question type, which provides evidence for improved generalization and a decrease of the dependency on dataset biases.",0
"This research examines whether visual reasoning patterns used on one Visual Question Answering (VQA) dataset can generalize to another VQA task. To study this phenomenon, we fine-tuned Bottom Up Top Down (BUTD) networks pretrained on the original VQA dataset on three transfer learning datasets. We found that while some tasks benefited from pretraining, others did not show significant improvements. These results suggest that the type of reasoning required by each VQA task influences how easily transferable models are across different domains. Furthermore, our findings imply that while visual reasoning abilities may transfer from training data to new VQA tasks, there remains room for improvement in developing more robust models capable of generalizing better across multiple VQA benchmarks. Overall, our work provides insights into the nature of VQA model performance, highlighting future directions towards creating versatile, high-performance models adaptive to diverse VQA settings.",1
"Perceptual organization remains one of the very few established theories on the human visual system. It underpinned many pre-deep seminal works on segmentation and detection, yet research has seen a rapid decline since the preferential shift to learning deep models. Of the limited attempts, most aimed at interpreting complex visual scenes using perceptual organizational rules. This has however been proven to be sub-optimal, since models were unable to effectively capture the visual complexity in real-world imagery. In this paper, we rejuvenate the study of perceptual organization, by advocating two positional changes: (i) we examine purposefully generated synthetic data, instead of complex real imagery, and (ii) we ask machines to synthesize novel perceptually-valid patterns, instead of explaining existing data. Our overall answer lies with the introduction of a novel visual challenge -- the challenge of perceptual question answering (PQA). Upon observing example perceptual question-answer pairs, the goal for PQA is to solve similar questions by generating answers entirely from scratch (see Figure 1). Our first contribution is therefore the first dataset of perceptual question-answer pairs, each generated specifically for a particular Gestalt principle. We then borrow insights from human psychology to design an agent that casts perceptual organization as a self-attention problem, where a proposed grid-to-grid mapping network directly generates answer patterns from scratch. Experiments show our agent to outperform a selection of naive and strong baselines. A human study however indicates that ours uses astronomically more data to learn when compared to an average human, necessitating future research (with or without our dataset).",0
"This paper presents a novel approach to question answering called ""Perceptual Question Answering"" (PQA). PQA combines perception and language understanding to provide more accurate answers to questions posed by humans. Traditional Q&A systems rely on text-based reasoning alone but fail to take into account visual context which can significantly impact the meaning of a sentence. By incorporating perception as well, our system has been shown to provide more intuitive answers to complex queries. We evaluate our approach using standard metrics such as accuracy, precision and recall and show that our method outperforms traditional methods. Our work holds great promise for future applications where computers need to interact effectively with human users.",1
"Differentiable Architecture Search (DARTS) has attracted extensive attention due to its efficiency in searching for cell structures. DARTS mainly focuses on the operation search and derives the cell topology from the operation weights. However, the operation weights can not indicate the importance of cell topology and result in poor topology rating correctness. To tackle this, we propose to Decouple the Operation and Topology Search (DOTS), which decouples the topology representation from operation weights and makes an explicit topology search. DOTS is achieved by introducing a topology search space that contains combinations of candidate edges. The proposed search space directly reflects the search objective and can be easily extended to support a flexible number of edges in the searched cell. Existing gradient-based NAS methods can be incorporated into DOTS for further improvement by the topology search. Considering that some operations (e.g., Skip-Connection) can affect the topology, we propose a group operation search scheme to preserve topology-related operations for a better topology search. The experiments on CIFAR10/100 and ImageNet demonstrate that DOTS is an effective solution for differentiable NAS.",0
"In recent years, there has been increasing interest in using differentiable programming techniques to optimize the parameters of neural network architectures. However, most current methods still require manual engineering and often lead to suboptimal results. In this work, we present DOTS (Decoupling Operation and Topology in Differentiable Architecture Search), which introduces a new methodology that decouples architecture search from specific operations and allows for more flexible optimization. Our approach first optimizes the topology of the architecture, then fixes the connections and iteratively improves them along with the operation choices through gradient descent. We show that our method achieves state-of-the-art performance on several benchmark datasets across different tasks while requiring significantly less computational resources compared to previous approaches. This research demonstrates the potential of automatically learned models to outperform human designed architectures, paving the way for future advancements in artificial intelligence.",1
"Video inpainting aims to fill spatio-temporal ""corrupted"" regions with plausible content. To achieve this goal, it is necessary to find correspondences from neighbouring frames to faithfully hallucinate the unknown content. Current methods achieve this goal through attention, flow-based warping, or 3D temporal convolution. However, flow-based warping can create artifacts when optical flow is not accurate, while temporal convolution may suffer from spatial misalignment. We propose 'Progressive Temporal Feature Alignment Network', which progressively enriches features extracted from the current frame with the feature warped from neighbouring frames using optical flow. Our approach corrects the spatial misalignment in the temporal feature propagation stage, greatly improving visual quality and temporal consistency of the inpainted videos. Using the proposed architecture, we achieve state-of-the-art performance on the DAVIS and FVI datasets compared to existing deep learning approaches. Code is available at https://github.com/MaureenZOU/TSAM.",0
"This sounds like a fascinating topic! I would love to try my hand at writing an abstract on ""Progressive Temporal Feature Alignment Network for Video Inpainting."" Here we go:  Video inpainting refers to the process of restoring missing or damaged areas within video frames using machine learning techniques. While there have been several advances in this field recently, existing approaches suffer from limitations such as slow inference speed, poor alignment accuracy, and difficulty in handling complex scenes. To overcome these challenges, we propose a novel framework called Progressive Temporal Feature Alignment Network (PTFAN). Our method introduces progressiveness into temporal feature alignment by iteratively refining both spatial features and warped depth maps through multi-scale feedback fusion networks. Unlike previous works that rely exclusively on single frame reconstruction loss, PTFAN further incorporates spatiotemporal consistency constraint for more accurate filling results, especially under occlusion scenarios. We demonstrate the effectiveness of our approach on two publicly available datasets and show that PTFAN outperforms state-of-the-art methods in terms of visual fidelity, structural similarity index (SSIM), and user study evaluation metrics. Overall, our work represents a significant step forward in pushing the boundaries of video inpainting research.",1
"The application of deep learning to 3D point clouds is challenging due to its lack of order. Inspired by the point embeddings of PointNet and the edge embeddings of DGCNNs, we propose three improvements to the task of point cloud analysis. First, we introduce a novel feature-attentive neural network layer, a FAT layer, that combines both global point-based features and local edge-based features in order to generate better embeddings. Second, we find that applying the same attention mechanism across two different forms of feature map aggregation, max pooling and average pooling, gives better performance than either alone. Third, we observe that residual feature reuse in this setting propagates information more effectively between the layers, and makes the network easier to train. Our architecture achieves state-of-the-art results on the task of point cloud classification, as demonstrated on the ModelNet40 dataset, and an extremely competitive performance on the ShapeNet part segmentation challenge.",0
"Title: FatNet: A Feature-attentive Network for 3D Point Cloud Processing Abstract: With the proliferation of modern technology such as LiDAR sensors in autonomous vehicles and high-resolution 3D scanning systems, there has been an explosion of 3D point cloud data that needs efficient processing. However, traditional convolutional neural networks (CNNs) struggle to process raw point clouds due to their irregular nature and lack of structural priors. In this work, we introduce FatNet, a feature-attentive network designed specifically for 3D point cloud processing. Unlike standard CNNs, FatNet operates directly on unstructured point clouds without requiring any preprocessing steps such as voxelization, truncation, or downsampling. Our approach utilizes multi-scale attention mechanisms to selectively focus on relevant local features in each layer, enabling accurate learning from sparse and noisy inputs. We demonstrate the effectiveness of our method by comparing against state-of-the-art techniques on challenging benchmark datasets, achieving significant improvements across several tasks including semantic segmentation, object detection, and surface reconstruction. Our results validate the importance of designing specialized architectures tailored towards point cloud data while showcasing the versatility of our framework in tackling diverse applications within the domain of computer vision and robotics. By providing efficient and reliable solutions to 3D point cloud processing, our research contributes significantly towards advancing real-world deployments in these domains. Keywords: 3D point cloud processing; feature-attentive networks; attention mechanism; LiDAR sensor; auton",1
"We consider the problem of Human-Object Interaction (HOI) Detection, which aims to locate and recognize HOI instances in the form of human, action, object in images. Most existing works treat HOIs as individual interaction categories, thus can not handle the problem of long-tail distribution and polysemy of action labels. We argue that multi-level consistencies among objects, actions and interactions are strong cues for generating semantic representations of rare or previously unseen HOIs. Leveraging the compositional and relational peculiarities of HOI labels, we propose ConsNet, a knowledge-aware framework that explicitly encodes the relations among objects, actions and interactions into an undirected graph called consistency graph, and exploits Graph Attention Networks (GATs) to propagate knowledge among HOI categories as well as their constituents. Our model takes visual features of candidate human-object pairs and word embeddings of HOI labels as inputs, maps them into visual-semantic joint embedding space and obtains detection results by measuring their similarities. We extensively evaluate our model on the challenging V-COCO and HICO-DET datasets, and results validate that our approach outperforms state-of-the-arts under both fully-supervised and zero-shot settings. Code is available at https://github.com/yeliudev/ConsNet.",0
"In recent years, zero-shot human object interaction detection has become increasingly important as computers play a greater role in our daily lives. Many algorithms have been developed to tackle this task, but they often rely on large amounts of data and expensive computational resources. This work proposes a new algorithm called ConsNet that leverages consistency graph learning techniques to accurately detect zero-shot human object interactions without requiring vast datasets or computational power. Our approach uses a novel global consistency loss function along with multiple local geometric losses to learn the relationships between objects and actions. We evaluate our method using several benchmark datasets and show that it outperforms state-of-the-art methods across all metrics while maintaining efficient computation times. Overall, our work demonstrates the potential benefits of utilizing consistency graphs for zero-shot human object interaction detection and paves the way for future research in this field.",1
"Object detection with transformers (DETR) reaches competitive performance with Faster R-CNN via a transformer encoder-decoder architecture. Inspired by the great success of pre-training transformers in natural language processing, we propose a pretext task named random query patch detection to Unsupervisedly Pre-train DETR (UP-DETR) for object detection. Specifically, we randomly crop patches from the given image and then feed them as queries to the decoder. The model is pre-trained to detect these query patches from the original image. During the pre-training, we address two critical issues: multi-task learning and multi-query localization. (1) To trade off classification and localization preferences in the pretext task, we freeze the CNN backbone and propose a patch feature reconstruction branch which is jointly optimized with patch detection. (2) To perform multi-query localization, we introduce UP-DETR from single-query patch and extend it to multi-query patches with object query shuffle and attention mask. In our experiments, UP-DETR significantly boosts the performance of DETR with faster convergence and higher average precision on object detection, one-shot detection and panoptic segmentation. Code and pre-training models: https://github.com/dddzg/up-detr.",0
"In the age of artificial intelligence, object detection has become one of the most important tasks in computer vision. With the rise of deep learning techniques like Convolutional Neural Networks (CNN), many advancements have been made in improving object detection accuracy. However, training these models requires large amounts of labeled data which can be time consuming and expensive to collect. As such, there exists a need for developing unsupervised pre-training methods that can improve object detection performance without requiring extensive labeled data. To address this challenge, we propose the use of transformer architectures. Specifically, our approach uses the Vision Transformer architecture, trained using the Self-Supervised Representation Learning methodology, for feature extraction on image patches. We then fine-tune the resulting model for object detection using standard supervised loss functions. Our experiments demonstrate that our proposed method significantly outperforms other unsupervised pre-training approaches while achieving state-of-the-art results on several benchmark datasets.  ----- In summary, my job as the AI language model is to write an academic research paper abstract based on given guidelines. I am provided with input text describing a particular paper topic and I create the abstract accordingly. This process involves paraphrasing key concepts and ideas from the original text into concise and clear sentences. I may also ask questions if needed to clarify any ambiguity or confusion.",1
"Unmanned aerial vehicle (UAV)-based tracking is attracting increasing attention and developing rapidly in applications such as agriculture, aviation, navigation, transportation and public security. Recently, discriminative correlation filters (DCF)-based trackers have stood out in UAV tracking community for their high efficiency and appealing robustness on a single CPU. However, due to limited onboard computation resources and other challenges the efficiency and accuracy of existing DCF-based approaches is still not satisfying. In this paper, we explore using segmentation by the GrabCut to improve the wildly adopted discriminative scale estimation in DCF-based trackers, which, as a mater of fact, greatly impacts the precision and accuracy of the trackers since accumulated scale error degrades the appearance model as online updating goes on. Meanwhile, inspired by residue representation, we exploit the residue nature inherent to videos and propose residue-aware correlation filters that show better convergence properties in filter learning. Extensive experiments are conducted on four UAV benchmarks, namely, UAV123@10fps, DTB70, UAVDT and Vistrone2018 (VisDrone2018-test-dev). The results show that our method achieves state-of-the-art performance.",0
"In the context of object tracking using Unmanned Aerial Vehicles (UAVs), robustness can be improved by explicitly estimating the scale of the target object. However, traditional methods based on geometric assumptions may result in suboptimal estimates when dealing with complex scenarios such as occlusions, deformations, lighting changes, or cluttered backgrounds. To address these limitations, we propose a novel framework that learns residue-aware correlation filters from synthetic data generated by simulations. Our method leverages two key components: residual generation network to create fake images corresponding to different scale configurations; and a meta learning approach adapted from deep reinforcement learning to learn optimal features that minimize the discrepancy between real and simulated examples. Through extensive experiments on standard benchmark datasets, our approach achieves state-of-the-art performance while providing accurate scale estimation maps that significantly improve tracker robustness against large scale variations. Moreover, since both filter updates and scale refinement operations only depend on convolutional layers without requiring any further backpropagation, it becomes suitable for real-time applications in resource constrained platforms like drones or smart cameras.",1
"The drone navigation requires the comprehensive understanding of both visual and geometric information in the 3D world. In this paper, we present a Visual-Geometric Fusion Network(VGF-Net), a deep network for the fusion analysis of visual/geometric data and the construction of 2.5D height maps for simultaneous drone navigation in novel environments. Given an initial rough height map and a sequence of RGB images, our VGF-Net extracts the visual information of the scene, along with a sparse set of 3D keypoints that capture the geometric relationship between objects in the scene. Driven by the data, VGF-Net adaptively fuses visual and geometric information, forming a unified Visual-Geometric Representation. This representation is fed to a new Directional Attention Model(DAM), which helps enhance the visual-geometric object relationship and propagates the informative data to dynamically refine the height map and the corresponding keypoints. An entire end-to-end information fusion and mapping system is formed, demonstrating remarkable robustness and high accuracy on the autonomous drone navigation across complex indoor and large-scale outdoor scenes. The dataset can be found in http://vcc.szu.edu.cn/research/2021/VGFNet.",0
"In recent years, there has been growing interest in developing accurate and efficient methods for aerial imaging tasks such as drone navigation and height mapping. Convolutional Neural Networks (CNN) have emerged as powerful tools in addressing these challenges due to their ability to learn complex features from large datasets of images and labels. However, traditional CNN architectures often struggle to capture high level geometric representations that can accurately predict surface depth, height and camera pose. To address these limitations, we propose a new model architecture called ""VGF-Net"" that leverages visual and geometric cues simultaneously by jointly learning visual representation and explicit geometry reasoning modules in a unified framework. Our approach builds upon recent advancements in self-supervised visual odometry and uses photometrically calibrated multi-view image sequences of real world environments captured using commercial drones equipped with dual RGB-D cameras and IMUs. Experimental results on public benchmarks demonstrate significant improvements over state-of-the-art approaches, achieving better accuracy in terms of absolute trajectory error, relative localization error, and estimated surface normal prediction metrics. Overall, our proposed method demonstrates the feasibility of learning both visual representation and explicit geometry reasoning through combined supervision and provides promising directions for future work on robust aerial imaging applications.",1
"Higher-order proximity preserved network embedding has attracted increasing attention. In particular, due to the superior scalability, random-walk-based network embedding has also been well developed, which could efficiently explore higher-order neighborhoods via multi-hop random walks. However, despite the success of current random-walk-based methods, most of them are usually not expressive enough to preserve the personalized higher-order proximity and lack a straightforward objective to theoretically articulate what and how network proximity is preserved. In this paper, to address the above issues, we present a general scalable random-walk-based network embedding framework, in which random walk is explicitly incorporated into a sound objective designed theoretically to preserve arbitrary higher-order proximity. Further, we introduce the random walk with restart process into the framework to naturally and effectively achieve personalized-weighted preservation of proximities of different orders. We conduct extensive experiments on several real-world networks and demonstrate that our proposed method consistently and substantially outperforms the state-of-the-art network embedding methods.",0
"This study proposes RWNE (Random Walk-based NErmalization), which tackles scalability and higher-order proximity preservation simultaneously by randomly walking on k-NN graphs at different scales, enabling personalized embedding generation without introducing additional parameters. Our framework achieves state-of-the-art results on benchmarks like Citeseer, Pubmed, Cora, and DBLP, illustrating effectiveness and efficiency. Empirical comparisons show that compared to popular baselines (node2vec and DeepWalk), our method demonstrates clear advantages in both quality and speed when dealing with large datasets such as Reddit and Stack Exchange. In summary, this paper presents a novel random walk based network embedding framework capable of scaling to larger networks while preserving high order proximities, providing competitive performance across multiple real world datasets.",1
"Real-world data is often unbalanced and long-tailed, but deep models struggle to recognize rare classes in the presence of frequent classes. To address unbalanced data, most studies try balancing the data, the loss, or the classifier to reduce classification bias towards head classes. Far less attention has been given to the latent representations learned with unbalanced data. We show that the feature extractor part of deep networks suffers greatly from this bias. We propose a new loss based on robustness theory, which encourages the model to learn high-quality representations for both head and tail classes. While the general form of the robustness loss may be hard to compute, we further derive an easy-to-compute upper bound that can be minimized efficiently. This procedure reduces representation bias towards head classes in the feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT, and iNaturalist long-tail benchmarks. We find that training with robustness increases recognition accuracy of tail classes while largely maintaining the accuracy of head classes. The new robustness loss can be combined with various classifier balancing techniques and can be applied to representations at several layers of the deep model.",0
"In many machine learning tasks, it has been observed that models tend to perform well on common examples but struggle with rare cases, known as the ""long tail"" problem. This results in lower overall performance and can negatively impact real-world applications. To address this issue, we propose a new loss function called Distributional Robustness Loss (DRL). DRL encourages models to make confident predictions even when they encounter uncommon inputs by penalizing large prediction errors more severely than small ones. We demonstrate through extensive experiments on multiple benchmark datasets that our proposed method outperforms existing state-of-the-art techniques for handling the long-tailed distribution. Our approach achieves superior accuracy and leads to better calibration of confidence scores. These findings suggest that incorporating DRL into training procedures could significantly improve generalization ability and enhance the practicality of machine learning models in real-world applications.",1
"Visual attention mechanisms are a key component of neural network models for computer vision. By focusing on a discrete set of objects or image regions, these mechanisms identify the most relevant features and use them to build more powerful representations. Recently, continuous-domain alternatives to discrete attention models have been proposed, which exploit the continuity of images. These approaches model attention as simple unimodal densities (e.g. a Gaussian), making them less suitable to deal with images whose region of interest has a complex shape or is composed of multiple non-contiguous patches. In this paper, we introduce a new continuous attention mechanism that produces multimodal densities, in the form of mixtures of Gaussians. We use the EM algorithm to obtain a clustering of relevant regions in the image, and a description length penalty to select the number of components in the mixture. Our densities decompose as a linear combination of unimodal attention mechanisms, enabling closed-form Jacobians for the backpropagation step. Experiments on visual question answering in the VQA-v2 dataset show competitive accuracies and a selection of regions that mimics human attention more closely in VQA-HAT. We present several examples that suggest how multimodal attention maps are naturally more interpretable than their unimodal counterparts, showing the ability of our model to automatically segregate objects from ground in complex scenes.",0
"Attempts have been made at developing a continuous attention mechanism that takes into account multimodality data inputs such as image and text; however, existing methods still lack satisfactory performance due to their reliance on fixed spatial attention maps or heuristics that may not capture all relevant features. In response, our proposed method employs adaptive local search algorithms to generate dynamic attention regions for each modality while ensuring that intermodality consistency constraints are met. With evaluation results demonstrating improved accuracy on benchmark datasets across various tasks (e.g., object detection), we believe this work represents a step forward towards more effective use of multimodal input in artificial intelligence systems.",1
"Single image deraining (SID) is an important and challenging topic in emerging vision applications, and most of emerged deraining methods are supervised relying on the ground truth (i.e., paired images) in recent years. However, in practice it is rather common to have no un-paired images in real deraining task, in such cases how to remove the rain streaks in an unsupervised way will be a very challenging task due to lack of constraints between images and hence suffering from low-quality recovery results. In this paper, we explore the unsupervised SID task using unpaired data and propose a novel net called Attention-guided Deraining by Constrained CycleGAN (or shortly, DerainCycleGAN), which can fully utilize the constrained transfer learning abilitiy and circulatory structure of CycleGAN. Specifically, we design an unsu-pervised attention guided rain streak extractor (U-ARSE) that utilizes a memory to extract the rain streak masks with two constrained cycle-consistency branches jointly by paying attention to both the rainy and rain-free image domains. As a by-product, we also contribute a new paired rain image dataset called Rain200A, which is constructed by our network automatically. Compared with existing synthesis datasets, the rainy streaks in Rain200A contains more obvious and diverse shapes and directions. As a result, existing supervised methods trained on Rain200A can perform much better for processing real rainy images. Extensive experiments on synthesis and real datasets show that our net is superior to existing unsupervised deraining networks, and is also very competitive to other related supervised networks.",0
"This paper presents the first automatic deraining algorithm that takes into account not only rain streaks but also their interaction with underlying scene content such as edges and corners. We introduce DerainCycleGAN, which combines rain attention mechanisms and cycle consistency loss. Our model can generate realistic rainy images from clean input images while preserving detailed scene structure. We show our method significantly outperforms previous state-of-the-art methods on subjective evaluations by human raters. Moreover, we find our approach performs favorably against other image generation methods including StarGANv2+, SPADE, DiscoGAN and AttnGAN in user studies. Compared to previous work, we found better tradeoffs among quality, speed, training complexity, and size. Code is available at https://github.com/facebookresearch/DerainCycleGAN. This study proposes a new technique for automated image processing called ""rain attentive cyclic generative adversarial networks,"" or ""RainCycleGAN."" The goal of this technology is to improve the accuracy of image editing tools in removing rain streaks from photos, while still maintaining important details like edges and corners within the image. The proposed method uses machine learning techniques and specialized algorithms to create more realistic looking artificial rain than current methods. Additionally, RainCycleGAN is faster and requires less computational power compared to traditional approaches. Overall, this study represents a significant advance in computer vision research and has important implications for many applications in photography and video production.",1
"Audio-visual event localization aims to localize an event that is both audible and visible in the wild, which is a widespread audio-visual scene analysis task for unconstrained videos. To address this task, we propose a Multimodal Parallel Network (MPN), which can perceive global semantics and unmixed local information parallelly. Specifically, our MPN framework consists of a classification subnetwork to predict event categories and a localization subnetwork to predict event boundaries. The classification subnetwork is constructed by the Multimodal Co-attention Module (MCM) and obtains global contexts. The localization subnetwork consists of Multimodal Bottleneck Attention Module (MBAM), which is designed to extract fine-grained segment-level contents. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance both in fully supervised and weakly supervised settings on the Audio-Visual Event (AVE) dataset.",0
"We present MPN, a novel multimodal parallel network architecture designed specifically for audio-visual event localization tasks. Our proposed approach integrates two streams - one for audio processing and another for visual processing - using feature fusion techniques and a temporal attention mechanism to improve the performance of both modalities on their own as well as jointly. The proposed MPN is trained end-to-end from raw sensory input data using a carefully crafted dataset containing diverse real world audio-video events which encompass multiple object categories and scene types. By fusing deep features extracted independently by each stream while paying extra attention to the most relevant frames and spectrograms for each task, we show that our system outperforms several existing state-of-the-art approaches in terms of accuracy and efficiency. Additionally, our ablation studies demonstrate the effectiveness of individual components of the model including different variants of attention mechanisms and late-fusion schemes. Overall, we believe that the MPN framework has the potential to form a strong foundation for further research into other multi-modal tasks involving audio-visual data such as video classification, action recognition, and speech recognition.",1
"Weakly-supervised temporal action localization aims to localize action instances temporal boundary and identify the corresponding action category with only video-level labels. Traditional methods mainly focus on foreground and background frames separation with only a single attention branch and class activation sequence. However, we argue that apart from the distinctive foreground and background frames there are plenty of semantically ambiguous action context frames. It does not make sense to group those context frames to the same background class since they are semantically related to a specific action category. Consequently, it is challenging to suppress action context frames with only a single class activation sequence. To address this issue, in this paper, we propose an action-context modeling network termed ACM-Net, which integrates a three-branch attention module to measure the likelihood of each temporal point being action instance, context, or non-action background, simultaneously. Then based on the obtained three-branch attention values, we construct three-branch class activation sequences to represent the action instances, contexts, and non-action backgrounds, individually. To evaluate the effectiveness of our ACM-Net, we conduct extensive experiments on two benchmark datasets, THUMOS-14 and ActivityNet-1.3. The experiments show that our method can outperform current state-of-the-art methods, and even achieve comparable performance with fully-supervised methods. Code can be found at https://github.com/ispc-lab/ACM-Net",0
"Here is my attempt at writing an abstract for your paper:  We present a new weakly supervised approach for temporal action localization using a novel architecture called ""Action Context Modeling Network"" (ACM-Net). Our method addresses several key challenges in this field by leveraging both actionness criteria and long-range contextual relationships among video frames. By jointly optimizing detection quality and boundary refinement objectives under weak annotations, we achieve state-of-the art performance on popular benchmark datasets such as THUMOS14 and ActivityNet Captions without using any fully annotated data during training. In summary, our proposed method demonstrates significant improvement over existing approaches and sets a new standard for weakly-supervised temporal action localization research.",1
"Graph embedding, aiming to learn low-dimensional representations (aka. embeddings) of nodes, has received significant attention recently. Recent years have witnessed a surge of efforts made on static graphs, among which Graph Convolutional Network (GCN) has emerged as an effective class of models. However, these methods mainly focus on the static graph embedding. In this work, we propose an efficient dynamic graph embedding approach, Dynamic Graph Convolutional Network (DyGCN), which is an extension of GCN-based methods. We naturally generalizes the embedding propagation scheme of GCN to dynamic setting in an efficient manner, which is to propagate the change along the graph to update node embeddings. The most affected nodes are first updated, and then their changes are propagated to the further nodes and leads to their update. Extensive experiments conducted on various dynamic graphs demonstrate that our model can update the node embeddings in a time-saving and performance-preserving way.",0
"This research proposes a new model called DyGCN (Dynamic Graph Embedding with Graph Convolutional Network) which improves existing GCN models by integrating dynamic convolution into the graph embedding process. Our proposed model captures temporal changes in node attributes while preserving structural information through a self-attention mechanism, allowing us to learn more accurate representations of nodes in dynamic graphs. We evaluate our method on several benchmark datasets including social networks, citation networks, and protein interaction networks, showing significant improvements over state-of-the-art methods across all domains. Overall, our work represents a promising step towards developing powerful graph representation learning techniques that can handle complex real-world problems involving time-varying data.",1
"This paper introduces a visual-based localization method for autonomous vehicles (AVs) that operate in the absence of any complicated hardware system but a single camera. Visual localization refers to techniques that aim to find the location of an object based on visual information of its surrounding area. The problem of localization has been of interest for many years. However, visual localization is a relatively new subject in the literature of transportation. Moreover, the inevitable application of this type of localization in the context of autonomous vehicles demands special attention from the transportation community to this problem. This study proposes a two-step localization method that requires a database of geotagged images and a camera mounted on a vehicle that can take pictures while the car is moving. The first step which is image retrieval uses SIFT local feature descriptor to find an initial location for the vehicle using image matching. The next step is to utilize the Kalman filter to estimate a more accurate location for the vehicle as it is moving. All stages of the introduced method are implemented as a complete system using different Python libraries. The proposed system is tested on the KITTI dataset and has shown an average accuracy of 2 meters in finding the final location of the vehicle.",0
"This study presents a proof of concept for the use of computer vision techniques in localizing autonomous vehicles. Accurate vehicle positioning is crucial for safe operation of self-driving cars, as well as providing enhanced navigation capabilities. We propose a method that uses visual odometry (VO) along with road markings as inputs to our camera motion estimation algorithm. Our approach is able to accurately estimate the ego-motion of the vehicle by computing the differences in pixel intensities of predefined regions of interest on successive images from our monocular camera stream. Experimental results on both real-world footage captured during test drives and synthetic data generated using Unity demonstrate the effectiveness of our method. Furthermore, we compare the performance of our proposed solution against state-of-the-art LIDAR-based approaches and showcase its potential as an affordable alternative for low-cost self-driving systems. Overall, these findings contribute towards establishing the feasibility of utilizing cameras alone in achieving high accuracy localization required for advanced driver assistance system applications and fully automated driving.",1
"While single-image super-resolution (SISR) has attracted substantial interest in recent years, the proposed approaches are limited to learning image priors in order to add high frequency details. In contrast, multi-frame super-resolution (MFSR) offers the possibility of reconstructing rich details by combining signal information from multiple shifted images. This key advantage, along with the increasing popularity of burst photography, have made MFSR an important problem for real-world applications.   We propose a novel architecture for the burst super-resolution task. Our network takes multiple noisy RAW images as input, and generates a denoised, super-resolved RGB image as output. This is achieved by explicitly aligning deep embeddings of the input frames using pixel-wise optical flow. The information from all frames are then adaptively merged using an attention-based fusion module. In order to enable training and evaluation on real-world data, we additionally introduce the BurstSR dataset, consisting of smartphone bursts and high-resolution DSLR ground-truth. We perform comprehensive experimental analysis, demonstrating the effectiveness of the proposed architecture.",0
"""Abstract:  This paper presents a new method for burst super resolution called deep burst super-resolution (DBSR). DBSR is based on deep learning techniques and uses multiple low-quality images taken from different angles and distances to reconstruct one high-quality image. The proposed method outperforms traditional methods by taking advantage of both spatial and temporal information contained within the burst image set. Experiments show that DBSR can produce more accurate results than state-of-the-art algorithms under varying lighting conditions and motion blur.""",1
"Visual salient object detection (SOD) aims at finding the salient object(s) that attract human attention, while camouflaged object detection (COD) on the contrary intends to discover the camouflaged object(s) that hidden in the surrounding. In this paper, we propose a paradigm of leveraging the contradictory information to enhance the detection ability of both salient object detection and camouflaged object detection. We start by exploiting the easy positive samples in the COD dataset to serve as hard positive samples in the SOD task to improve the robustness of the SOD model. Then, we introduce a similarity measure module to explicitly model the contradicting attributes of these two tasks. Furthermore, considering the uncertainty of labeling in both tasks' datasets, we propose an adversarial learning network to achieve both higher order similarity measure and network confidence estimation. Experimental results on benchmark datasets demonstrate that our solution leads to state-of-the-art (SOTA) performance for both tasks.",0
"Artificial intelligence has made significant progress in recent years in areas such as object detection and segmentation. However, many existing methods struggle to handle uncertainty and ambiguity in these tasks, resulting in poor performance on challenging scenarios that involve occlusions, cluttered backgrounds, or camouflage. To address these limitations, we propose a novel approach for joint salient object and camouflaged object detection that explicitly models uncertainty and handles occlusion situations effectively. Our framework leverages attention mechanisms and uncertainty maps to selectively attend to informative features while suppressing irrelevant ones. Experiments demonstrate that our method achieves state-of-the-art results across several benchmark datasets, outperforming other approaches by large margins in terms of precision and recall metrics. This study highlights the importance of incorporating uncertainty modeling into computer vision tasks, enabling more robust and reliable scene understanding in complex real-world scenarios. Overall, our work represents a step forward towards developing intelligent systems capable of handling ambiguous data and making informed decisions based on incomplete knowledge.",1
"The advent of deep learning has brought a significant improvement in the quality of generated media. However, with the increased level of photorealism, synthetic media are becoming hardly distinguishable from real ones, raising serious concerns about the spread of fake or manipulated information over the Internet. In this context, it is important to develop automated tools to reliably and timely detect synthetic media. In this work, we analyze the state-of-the-art methods for the detection of synthetic images, highlighting the key ingredients of the most successful approaches, and comparing their performance over existing generative architectures. We will devote special attention to realistic and challenging scenarios, like media uploaded on social networks or generated by new and unseen architectures, analyzing the impact of suitable augmentation and training strategies on the detectors' generalization ability.",0
"This paper presents a comprehensive review and analysis of current research on Generative Adversarial Network (GAN) generated image detection methods. With rapid advancements in deep learning techniques, generative models like GANs have achieved unprecedented performance in generating realistic images across different domains. However, these advances have made it challenging for humans to distinguish real from synthetic images, leading to concerns over their use in various applications such as digital media, security cameras, biometrics, and medical imaging. In response, there has been growing interest in developing algorithms that can accurately detect GAN generated images.  This review highlights key insights into the effectiveness of existing approaches in terms of accuracy, robustness, generalizability, scalability, interpretability, usability, and ethics/privacy considerations. We present an exhaustive discussion on popular detection strategies such as feature extraction, statistical analysis, machine learning, and visualizations. Our study provides a thorough assessment of each method's strengths and weaknesses, highlighting promising directions for future research in combating advanced GAN attacks. Furthermore, we discuss open challenges, limitations, and potential impacts of successful detection solutions. Overall, our work offers valuable contributions towards establishing new protocols and guidelines for reliable GAN image verification systems.",1
"Deep learning has recently demonstrated its promising performance for vision-based parking-slot detection. However, very few existing methods explicitly take into account learning the link information of the marking-points, resulting in complex post-processing and erroneous detection. In this paper, we propose an attentional graph neural network based parking-slot detection method, which refers the marking-points in an around-view image as graph-structured data and utilize graph neural network to aggregate the neighboring information between marking-points. Without any manually designed post-processing, the proposed method is end-to-end trainable. Extensive experiments have been conducted on public benchmark dataset, where the proposed method achieves state-of-the-art accuracy. Code is publicly available at \url{https://github.com/Jiaolong/gcn-parking-slot}.",0
"This article proposes a novel deep learning framework based on attentional graph neural networks (AGNNs) for parking slot detection from aerial imagery. Our AGNN model uses attention mechanisms that capture contextual information across multiple levels of abstraction, allowing us to extract features relevant for accurate parking slot prediction. By integrating high level semantic knowledge through edge convolutions, we can better adapt to changing image conditions while still maintaining computational efficiency. Extensive experiments demonstrate the superiority of our approach compared against state-of-the-art methods both quantitatively and qualitatively, achieving improvements up to +7% IoU, confirming the effectiveness of our proposed methodology. Finally, we conclude by discussing potential future research directions related to extending these techniques beyond binary object segmentation to more complex multi-class problems.",1
"This paper strives to predict fine-grained fashion similarity. In this similarity paradigm, one should pay more attention to the similarity in terms of a specific design/attribute between fashion items. For example, whether the collar designs of the two clothes are similar. It has potential value in many fashion related applications, such as fashion copyright protection. To this end, we propose an Attribute-Specific Embedding Network (ASEN) to jointly learn multiple attribute-specific embeddings, thus measure the fine-grained similarity in the corresponding space. The proposed ASEN is comprised of a global branch and a local branch. The global branch takes the whole image as input to extract features from a global perspective, while the local branch takes as input the zoomed-in region-of-interest (RoI) w.r.t. the specified attribute thus able to extract more fine-grained features. As the global branch and the local branch extract the features from different perspectives, they are complementary to each other. Additionally, in each branch, two attention modules, i.e., Attribute-aware Spatial Attention and Attribute-aware Channel Attention, are integrated to make ASEN be able to locate the related regions and capture the essential patterns under the guidance of the specified attribute, thus make the learned attribute-specific embeddings better reflect the fine-grained similarity. Extensive experiments on three fashion-related datasets, i.e., FashionAI, DARN, and DeepFashion, show the effectiveness of ASEN for fine-grained fashion similarity prediction and its potential for fashion reranking. Code and data are available at https://github.com/maryeon/asenpp .",0
"This paper focuses on fashion similarity prediction using attribute-specific embedding learning. We present a novel approach that predicts similarities between fine-grained attributes rather than directly predicting image similarity scores. Our method involves training attribute-specific embeddings for each product image to represent the characteristics related to each attribute category. These attribute embeddings capture distinct features from images in the corresponding domains. By integrating these domain specific embeddings into a unified space, we learn a common representation across all categories. Evaluation results demonstrate significant improvements over previous state-of-the-art methods. Additionally, our model provides high fidelity visualization of attributes and can effectively quantify their importance relative to overall similarity predictions. The proposed framework has great potential value for many computer vision applications beyond just the fashion industry, including recommendation systems, e-commerce platforms, and personalized shopping experiences. Overall, this research opens new doors towards developing better models for understanding relationships within complex data distributions.",1
"Generative models able to synthesize layouts of different kinds (e.g. documents, user interfaces or furniture arrangements) are a useful tool to aid design processes and as a first step in the generation of synthetic data, among other tasks. We exploit the properties of self-attention layers to capture high level relationships between elements in a layout, and use these as the building blocks of the well-known Variational Autoencoder (VAE) formulation. Our proposed Variational Transformer Network (VTN) is capable of learning margins, alignments and other global design rules without explicit supervision. Layouts sampled from our model have a high degree of resemblance to the training data, while demonstrating appealing diversity. In an extensive evaluation on publicly available benchmarks for different layout types VTNs achieve state-of-the-art diversity and perceptual quality. Additionally, we show the capabilities of this method as part of a document layout detection pipeline.",0
"This paper presents Variational Transformer Networks (VTN), which leverage recent advancements in Transformers to model sequential data distributions and generate layouts on both 2D and 3D grids. VTNs adopt a probabilistic framework that allows efficient inference via backpropagation through stochasticity. To achieve this, we propose a new normalizing flow layer designed specifically for high dimensional spaces. With these building blocks, VTNs can infer novel spatial arrangements from ambiguous specifications, even if only textual cues are given. Empirically, we showcase VTN capabilities on diverse domains such as web design, furniture arrangement, fashion outfits composition, molecule docking, protein structure prediction, as well as other important generative tasks where geometry plays a key role. Compared against alternative methods, our approach delivers superior quality results and often converges faster to solutions exhibiting coherent structures over vast search spaces. These contributions expand the scope of deep learning architectures beyond image generation and demonstrate their potential as viable options for solving complex realworld problems involving spatial reasoning under uncertainty.",1
"6D object pose estimation is widely applied in robotic tasks such as grasping and manipulation. Prior methods using RGB-only images are vulnerable to heavy occlusion and poor illumination, so it is important to complement them with depth information. However, existing methods using RGB-D data cannot adequately exploit consistent and complementary information between RGB and depth modalities. In this paper, we present a novel method to effectively consider the correlation within and across both modalities with attention mechanism to learn discriminative and compact multi-modal features. Then, effective fusion strategies for intra- and inter-correlation modules are explored to ensure efficient information flow between RGB and depth. To our best knowledge, this is the first work to explore effective intra- and inter-modality fusion in 6D pose estimation. The experimental results show that our method can achieve the state-of-the-art performance on LineMOD and YCB-Video dataset. We also demonstrate that the proposed method can benefit a real-world robot grasping task by providing accurate object pose estimation.",0
"Our work presents a novel approach to 6D pose estimation that leverages correlation fusion to achieve improved accuracy and robustness. We introduce a method based on deep learning architectures that learns to fuse visual features from multiple views into a single representation that can accurately estimate the orientation and position of objects in cluttered scenes. This approach builds upon recent advances in multi-view geometry and deep feature extraction, enabling the use of complementary cues to improve performance under challenging conditions. We evaluate our technique using public datasets and demonstrate significant improvements over state-of-the-art methods across several metrics. Our results showcase the effectiveness of our proposed framework in addressing complex real-world scenarios where accurate object localization and pose estimation is critical.",1
"An efficient linear self-attention fusion model is proposed in this paper for the task of hyperspectral image (HSI) and LiDAR data joint classification. The proposed method is comprised of a feature extraction module, an attention module, and a fusion module. The attention module is a plug-and-play linear self-attention module that can be extensively used in any model. The proposed model has achieved the overall accuracy of 95.40\% on the Houston dataset. The experimental results demonstrate the superiority of the proposed method over other state-of-the-art models.",0
"In summary: This research focuses on classifying hyperspectral and LiDAR data using linear self attention as the main feature extractor. Unlike traditional CNNs, linear self-attention (LSA) allows capturing dependencies across all spatial positions and channels simultaneously. We evaluated our methodology on several datasets including DFC2004, KSC2009, GaoFen-2, and ISPRS. Our results outperformed those obtained by conventional methods such as Random Forest and Convolutional Neural Networks while maintaining low computational complexity. For example, we achieved OA accuracy of 87.6% and Kappa coefficient of 0.722 on the GaoFen-2 dataset compared to 85.3% OA and 0.699 Kappa from state-of-the-art methods. Overall, LSA shows promising potential in improving remote sensing image analysis tasks, especially in processing big data with high spectral resolution. By doing so, we aimed to facilitate more accurate environmental monitoring, precision agriculture, urban planning and other relevant applications. Is there something specific you would like me to change/modify in the abstract?",1
"Generalizable person re-identification has recently got increasing attention due to its research values as well as practical values. However, the efficiency of learning from large-scale data has not yet been much studied. In this paper, we argue that the most popular random sampling method, the well-known PK sampler, is not informative and efficient for deep metric learning. Though online hard example mining improves the learning efficiency to some extent, the mining in mini batches after random sampling is still limited. Therefore, this inspires us that the hard example mining should be shifted backward to the data sampling stage. To address this, in this paper, we propose an efficient mini batch sampling method called Graph Sampling (GS) for large-scale metric learning. The basic idea is to build a nearest neighbor relationship graph for all classes at the beginning of each epoch. Then, each mini batch is composed of a randomly selected class and its nearest neighboring classes so as to provide informative and challenging examples for learning. Together with an adapted competitive baseline, we improve the previous state of the arts in generalizable person re-identification significantly, by up to 22.3% in Rank-1 and 15% in mAP. Besides, the proposed method also outperforms the competitive baseline by up to 4%, with the training time significantly reduced by up to x6.6, from 12.2 hours to 1.8 hours in training a large-scale dataset RandPerson with 8,000 IDs. Code is available at \url{https://github.com/ShengcaiLiao/QAConv}.",0
"Graph sampling based deep metric learning (GDML) provides both the hard triplets and soft pairwise constraints by adaptively selecting positive/negative pairs from large batches using graph partitioning schemes such as KNN/kd-tree/hierarchical clustering. These strategies can achieve high re-identification accuracy under controlled scenarios but generalize poorly across different camera views, scales, illuminations etc., which remains challenging due to significant appearance variations. In this work, we present GPS-Aware Dense Subspace Representation (GADSR), a novel end-to-end trainable network that explicitly models global-local contextual relations between feature maps by enforcing dense subspace preservation constraint and generating discriminative features through nonlinear representation learning. Our approach constructs localized patch-level features by dividing each pedestrian image into multiple spatial regions via sliding windows while exploring intra-patch dependencies via nearest neighbors search along the channel axis. We further employ efficient minibatch discrimination strategy that utilizes informative positive and negative samples with explicit modeling of varying batch sizes and incorporates global consistency for more robust training process. Extensive experiments on seven datasets show that our method outperforms state-of-the-art techniques by achieving favorable results on unseen cross-dataset evaluations under various settings and demonstrate improved performance even without using extra data augmentation tricks during testing. Overall, our proposed framework enables effective person re-identification across diverse surveillance conditions using compact networks and simple yet powerful design choices.",1
"Training on synthetic data can be beneficial for label or data-scarce scenarios. However, synthetically trained models often suffer from poor generalization in real domains due to domain gaps. In this work, we make a key observation that the diversity of the learned feature embeddings plays an important role in the generalization performance. To this end, we propose contrastive synthetic-to-real generalization (CSG), a novel framework that leverages the pre-trained ImageNet knowledge to prevent overfitting to the synthetic domain, while promoting the diversity of feature embeddings as an inductive bias to improve generalization. In addition, we enhance the proposed CSG framework with attentional pooling (A-pool) to let the model focus on semantically important regions and further improve its generalization. We demonstrate the effectiveness of CSG on various synthetic training tasks, exhibiting state-of-the-art performance on zero-shot domain generalization.",0
"In our recent work we have explored how artificial intelligence can help us better understand some of humanityâ€™s most complex challenges. Our findings show that while humans have made significant progress in certain fields over the last few decades, many continue to face obstacles that prevent them from realizing their full potential. Our research highlights the importance of education as a tool for social justice and equality, particularly among marginalized communities who often experience barriers to educational opportunities. We argue that through continued investment in education and technology, we can empower individuals and enable them to overcome societal limitations. Ultimately, our goal is to provide hope and inspiration for those seeking to make positive change in their lives, and promote collective action towards a more equitable world.",1
"Contextual information plays an important role in action recognition. Local operations have difficulty to model the relation between two elements with a long-distance interval. However, directly modeling the contextual information between any two points brings huge cost in computation and memory, especially for action recognition, where there is an additional temporal dimension. Inspired from 2D criss-cross attention used in segmentation task, we propose a recurrent 3D criss-cross attention (RCCA-3D) module to model the dense long-range spatiotemporal contextual information in video for action recognition. The global context is factorized into sparse relation maps. We model the relationship between points in the same line along the direction of horizon, vertical and depth at each time, which forms a 3D criss-cross structure, and duplicate the same operation with recurrent mechanism to transmit the relation between points in a line to a plane finally to the whole spatiotemporal space. Compared with the non-local method, the proposed RCCA-3D module reduces the number of parameters and FLOPs by 25% and 30% for video context modeling. We evaluate the performance of RCCA-3D with two latest action recognition networks on three datasets and make a thorough analysis of the architecture, obtaining the optimal way to factorize and fuse the relation maps. Comparisons with other state-of-the-art methods demonstrate the effectiveness and efficiency of our model.",0
"In recent years, action recognition has become a fundamental task in computer vision due to its numerous applications such as video surveillance, human-computer interaction, and robotics. However, accurately recognizing actions from videos remains challenging due to variations in appearances caused by changes in viewpoint, lighting conditions, backgrounds, etc. To address these issues, we propose an efficient spatial-temporal context modeling approach that captures both short-term and long-term dependencies among features. Our method leverages a novel architecture consisting of multiple temporal convolutional layers followed by max pooling operations and dense connections to effectively capture spatiotemporal relationships at different scales. We validate our framework on several benchmark datasets, including UCF101, HMDB51, and Kinetics-400, where it consistently outperforms state-of-the-art methods under comparable computational budgets. Furthermore, extensive ablation studies demonstrate the effectiveness of each component in our design. Our work highlights the importance of balancing efficiency and accuracy for effective real-world deployment of action recognition systems.",1
"We present a deep neural network to predict structural similarity between 2D layouts by leveraging Graph Matching Networks (GMN). Our network, coined LayoutGMN, learns the layout metric via neural graph matching, using an attention-based GMN designed under a triplet network setting. To train our network, we utilize weak labels obtained by pixel-wise Intersection-over-Union (IoUs) to define the triplet loss. Importantly, LayoutGMN is built with a structural bias which can effectively compensate for the lack of structure awareness in IoUs. We demonstrate this on two prominent forms of layouts, viz., floorplans and UI designs, via retrieval experiments on large-scale datasets. In particular, retrieval results by our network better match human judgement of structural layout similarity compared to both IoUs and other baselines including a state-of-the-art method based on graph neural networks and image convolution. In addition, LayoutGMN is the first deep model to offer both metric learning of structural layout similarity and structural matching between layout elements.",0
"This paper presents a novel approach called ""Layout GMN"" for neural graph matching which addresses the limitations of traditional approaches in terms of speed, memory usage, and effectiveness. The proposed method leverages recent advancements in graph matching networks by introducing two new components. First, we introduce a structural layout encoding that captures complex geometric relationships between objects in images such as occlusions, relative sizes and aspect ratios. Secondly, we propose a hybrid loss function which combines a distance term ensuring local accuracy with a regularization term promoting global consistency. Extensive experiments show that our model outperforms state-of-the-art methods on challenging benchmarks across different tasks including image retrieval, object detection, and instance segmentation, demonstrating both effectiveness and efficiency. Our code and models will be publicly available upon acceptance.",1
"Crowd counting aims to predict the number of people and generate the density map in the image. There are many challenges, including varying head scales, the diversity of crowd distribution across images and cluttered backgrounds. In this paper, we propose a multi-scale context aggregation network (MSCANet) based on single-column encoder-decoder architecture for crowd counting, which consists of an encoder based on a dense context-aware module (DCAM) and a hierarchical attention-guided decoder. To handle the issue of scale variation, we construct the DCAM to aggregate multi-scale contextual information by densely connecting the dilated convolution with varying receptive fields. The proposed DCAM can capture rich contextual information of crowd areas due to its long-range receptive fields and dense scale sampling. Moreover, to suppress the background noise and generate a high-quality density map, we adopt a hierarchical attention-guided mechanism in the decoder. This helps to integrate more useful spatial information from shallow feature maps of the encoder by introducing multiple supervision based on semantic attention module (SAM). Extensive experiments demonstrate that the proposed approach achieves better performance than other similar state-of-the-art methods on three challenging benchmark datasets for crowd counting. The code is available at https://github.com/KingMV/MSCANet",0
"This work presents an improved approach called MSCAN (Multi-scale context aggregation network) that can estimate crowd counts by exploiting multi-level contextual features from convolutional neural networks (CNNs). Our method builds upon previous crowd counting methods which typically rely on global feature extraction. Instead, we propose to perform dense sub-region based multi-scale representation learning through a novel attention guided encoder network followed by an adaptive regression module. Experimental results demonstrate that our model achieves state-of-the-art performance on popular benchmark datasets including UCF CC-50 and ShanghaiTech Part B dataset while outperforming other competitive baseline models. Furthermore, our proposed network has fewer parameters compared to existing approaches enabling faster inference times on resource constrained platforms. Overall, MSCAN represents a significant advancement towards accurate high-resolution crowd estimation under complex real-world scenarios.",1
"COVID-19 pandemic has an unprecedented impact all over the world since early 2020. During this public health crisis, reliable forecasting of the disease becomes critical for resource allocation and administrative planning. The results from compartmental models such as SIR and SEIR are popularly referred by CDC and news media. With more and more COVID-19 data becoming available, we examine the following question: Can a direct data-driven approach without modeling the disease spreading dynamics outperform the well referred compartmental models and their variants? In this paper, we show the possibility. It is observed that as COVID-19 spreads at different speed and scale in different geographic regions, it is highly likely that similar progression patterns are shared among these regions within different time periods. This intuition lead us to develop a new neural forecasting model, called Attention Crossing Time Series (\textbf{ACTS}), that makes forecasts via comparing patterns across time series obtained from multiple regions. The attention mechanism originally developed for natural language processing can be leveraged and generalized to materialize this idea. Among 13 out of 18 testings including forecasting newly confirmed cases, hospitalizations and deaths, \textbf{ACTS} outperforms all the leading COVID-19 forecasters highlighted by CDC.",0
"This study presents a novel model for predicting the spread of COVID-19 based on inter-series attention mechanisms. The proposed model utilizes historical time-series data from multiple sources, including cases, deaths, tests performed, social distancing measures, lockdown measures, contact tracing effectiveness, vaccine distribution rates, and other relevant factors that impact the spread of COVID-19. The inter-series attention mechanism allows the model to weight each factor according to its relative importance at different timescales, allowing the prediction model to adapt to changing conditions over time.  The model was trained on a dataset consisting of daily time-series data spanning two years, covering different regions across several countries with varying demographics, healthcare systems, public policies, climates, and economies. Extensive experiments demonstrate that our approach outperforms baseline models by achieving better accuracy in both short-term and long-term predictions. Specifically, the proposed model improves upon state-of-the-art methods for forecasting COVID-19 cases, deaths, and hospitalizations.  In conclusion, this research represents a significant advance in understanding how to leverage time-series data to make accurate predictions regarding the spread of COVID-19. By considering multiple factors interacting simultaneously, we can improve decision making for policy makers and ultimately save lives during this global pandemic.",1
"Regularization has long been utilized to learn sparsity in deep neural network pruning. However, its role is mainly explored in the small penalty strength regime. In this work, we extend its application to a new scenario where the regularization grows large gradually to tackle two central problems of pruning: pruning schedule and weight importance scoring. (1) The former topic is newly brought up in this work, which we find critical to the pruning performance while receives little research attention. Specifically, we propose an L2 regularization variant with rising penalty factors and show it can bring significant accuracy gains compared with its one-shot counterpart, even when the same weights are removed. (2) The growing penalty scheme also brings us an approach to exploit the Hessian information for more accurate pruning without knowing their specific values, thus not bothered by the common Hessian approximation problems. Empirically, the proposed algorithms are easy to implement and scalable to large datasets and networks in both structured and unstructured pruning. Their effectiveness is demonstrated with modern deep neural networks on the CIFAR and ImageNet datasets, achieving competitive results compared to many state-of-the-art algorithms. Our code and trained models are publicly available at https://github.com/mingsuntse/regularization-pruning.",0
"Effective neural network pruning techniques that take advantage of structured sparsity constraints have led to significant efficiency gains on high-dimensional models without sacrificing accuracy. Motivated by these advances, we investigate a method based on deep neural networks trained with growing regularizers. Our approach adds a new term to the loss function at each iteration, which grows according to a specified schedule during training. We prove that our algorithm provably converges if certain conditions hold. On several benchmark datasets, we show our method achieves state-of-the-art test log-loss even as the model size becomes much smaller compared to other pruning methods such as Lottery Ticket Hypothesis. Furthermore, our proposed method has lower computational cost since it only needs one pass through the data instead of multiple passes required by previous algorithms. Lastly, visualizations of the hidden features reveal interesting connections between different layers and modules, providing insights into both the inner workings of neural nets and potential benefits from using deeper architectures.",1
"Despite exciting progress in pre-training for visual-linguistic (VL) representations, very few aspire to a small VL model. In this paper, we study knowledge distillation (KD) to effectively compress a transformer-based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention distributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Student's detector while the features are from Teacher's own object detector. With aligned network inputs, the adapted Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly improves the performance of small VL models on image captioning and visual question answering tasks. It reaches 120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its non-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effectiveness of VL distillation in both pre-training and fine-tuning stages.",0
"This research proposes a novel approach for compressing visual-linguistic models using knowledge distillation. These large language models require substantial computational resources during inference, making them unsuitable for deployment on edge devices without extensive hardware modification. By distilling the knowledge contained within these models into smaller ones, we can reduce their size while retaining most of their performance on task. Our method leverages both task loss and attention regularization techniques to ensure faithful student model training while minimizing performance degradation. Through experimentation on several benchmark datasets, we demonstrate that our proposed approach achieves comparable or better accuracy than other state-of-the-art compression methods, while offering greater flexibility in choosing the appropriate tradeoff between model size and performance. Overall, our work advances the field by enabling efficient deployment of advanced language processing systems on resource-constrained devices.",1
"Detecting and segmenting salient objects from given image scenes has received great attention in recent years. A fundamental challenge in training the existing deep saliency detection models is the requirement of large amounts of annotated data. While gathering large quantities of training data becomes cheap and easy, annotating the data is an expensive process in terms of time, labor and human expertise. To address this problem, this paper proposes to learn the effective salient object detection model based on the manual annotation on a few training images only, thus dramatically alleviating human labor in training models. To this end, we name this task as the few-cost salient object detection and propose an adversarial-paced learning (APL)-based framework to facilitate the few-cost learning scenario. Essentially, APL is derived from the self-paced learning (SPL) regime but it infers the robust learning pace through the data-driven adversarial learning mechanism rather than the heuristic design of the learning regularizer. Comprehensive experiments on four widely-used benchmark datasets demonstrate that the proposed method can effectively approach to the existing supervised deep salient object detection models with only 1k human-annotated training images. The project page is available at https://github.com/hb-stone/FC-SOD.",0
"This research presents a new approach to salient object detection that combines adversarial training and curriculum learning. The proposed method uses an encoder network to extract features from input images and a decoder network to predict the bounding boxes of the salient objects. In order to enhance the discriminability of the model, we incorporate an adversarial loss that encourages the model to learn more informative representations. Additionally, we adopt a paced learning strategy that gradually increases the difficulty of the task during training, which helps improve the accuracy and robustness of the model. Experimental results on several benchmark datasets demonstrate that our method achieves state-of-the-art performance while requiring fewer computational resources compared to existing methods. Our work shows that combining adversarial training and paced learning can lead to significant improvements in few-cost salient object detection.",1
"Predicting user positive response (e.g., purchases and clicks) probability is a critical task in Web applications. To identify predictive features from raw data, the state-of-the-art extreme deep factorization machine (xDeepFM) model introduces a compressed interaction network (CIN) to leverage feature interactions at the vector-wise level explicitly. However, since each hidden layer in CIN is a collection of feature maps, it can be viewed essentially as an ensemble of different feature maps. In this case, only using a single objective to minimize the prediction loss may lead to overfitting. In this paper, an ensemble diversity enhanced extreme deep factorization machine model (DexDeepFM) is proposed, which introduces the ensemble diversity measure in CIN and considers both ensemble diversity and prediction accuracy in the objective function. In addition, the attention mechanism is introduced to discriminate the importance of ensemble diversity measures with different feature interaction orders. Extensive experiments on two public real-world datasets show the superiority of the proposed model.",0
"Title of Paper: DexDeepFM - Ensemble diversity enhanced extreme deep factorization machine model Abstract: In this study, we propose a novel approach that combines ensemble learning techniques with extreme deep neural networks (DNNs) for classification tasks. Our method enhances existing models like the deep factorization machines by introducing a new training strategy called Ensemble Diversity Enhancement (EDE). EDE increases diversity within the ensemble of submodels during training by minimizing their similarity through regularization methods such as Dropout, L2 regularization and random weight initialization. This allows us to improve generalizability and reduce overfitting while maintaining high accuracies on benchmark datasets such as MNIST and CIFAR10. We name our new model ""Extreme Deep Factorization Machine"" (EDFM), where each submodel consists of extremely large depth of up to one thousand layers. Our results show consistent improvements across different dataset settings such as corruptions, compressions, noise, scale jitterings, etc., further demonstrating the robustness of our EDE technique. Additionally, we provide detailed ablation studies and visualizations to analyze the effectiveness of our method compared to other state-of-the-art models. Overall, our contributions significantly advance current knowledge in the field by providing a highly competitive alternative that achieves impressive results without using more complex architectures or hyperparameter tuning strategies.",1
"Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods.",0
"In this study we develop new visualizations that highlight how specific token interactions change the embedding representations themselves. To ground our approach, we leverage recent advances in transformer interpretability from NLP (Natural Language Processing) and CV (Computational Vision), but extend them by focusing on interpreting individual token interactions as transformations applied directly to the original input embeddings rather than as subsequent attention-based operations. Our techniques allow us to unveil aspects such as how particular tokens can impact others globally across different layers. We evaluate these methods via comprehensive ablation studies on three publicly available datasets: WMT2020 Multi30K En-De/En-Fr/En-Ro benchmarks. Experiments show how they improve model performance by providing fine-grained insights into their internal workings while complementing previously proposed attentions-based visualizers. Implications for future research include enabling more informed designs where better human judgment could lead to improved models without extensive trial and error. Keywords: Transformer Models; Natural language processing; Computer vision; Data analysis.",1
"Weakly-supervised semantic segmentation (WSSS) using image-level labels has recently attracted much attention for reducing annotation costs. Existing WSSS methods utilize localization maps from the classification network to generate pseudo segmentation labels. However, since localization maps obtained from the classifier focus only on sparse discriminative object regions, it is difficult to generate high-quality segmentation labels. To address this issue, we introduce discriminative region suppression (DRS) module that is a simple yet effective method to expand object activation regions. DRS suppresses the attention on discriminative regions and spreads it to adjacent non-discriminative regions, generating dense localization maps. DRS requires few or no additional parameters and can be plugged into any network. Furthermore, we introduce an additional learning strategy to give a self-enhancement of localization maps, named localization map refinement learning. Benefiting from this refinement learning, localization maps are refined and enhanced by recovering some missing parts or removing noise itself. Due to its simplicity and effectiveness, our approach achieves mIoU 71.4% on the PASCAL VOC 2012 segmentation benchmark using only image-level labels. Extensive experiments demonstrate the effectiveness of our approach. The code is available at https://github.com/qjadud1994/DRS.",0
"In this paper, we present a novel approach for weakly-supervised semantic segmentation that leverages discriminative region suppression. Our method addresses the challenge of limited labeled data by focusing on identifying and suppressing regions where there is low confidence in the model's predictions. By doing so, our algorithm can better prioritize attention on more important areas while avoiding less informative ones. We evaluate our method on several benchmark datasets and demonstrate significant improvements over state-of-the-art methods under both supervised and semi-supervised settings. Overall, our work shows promise in reducing annotation effort and improving performance in real-world applications.",1
"Face representation learning solutions have recently achieved great success for various applications such as verification and identification. However, face recognition approaches that are based purely on RGB images rely solely on intensity information, and therefore are more sensitive to facial variations, notably pose, occlusions, and environmental changes such as illumination and background. A novel depth-guided attention mechanism is proposed for deep multi-modal face recognition using low-cost RGB-D sensors. Our novel attention mechanism directs the deep network ""where to look"" for visual features in the RGB image by focusing the attention of the network using depth features extracted by a Convolution Neural Network (CNN). The depth features help the network focus on regions of the face in the RGB image that contains more prominent person-specific information. Our attention mechanism then uses this correlation to generate an attention map for RGB images from the depth features extracted by CNN. We test our network on four public datasets, showing that the features obtained by our proposed solution yield better results on the Lock3DFace, CurtinFaces, IIIT-D RGB-D, and KaspAROV datasets which include challenging variations in pose, occlusion, illumination, expression, and time-lapse. Our solution achieves average (increased) accuracies of 87.3\% (+5.0\%), 99.1\% (+0.9\%), 99.7\% (+0.6\%) and 95.3\%(+0.5\%) for the four datasets respectively, thereby improving the state-of-the-art. We also perform additional experiments with thermal images, instead of depth images, showing the high generalization ability of our solution when adopting other modalities for guiding the attention mechanism instead of depth information",0
"Advances in face representation learning have led to significant improvements in tasks such as identification, verification, and clustering. However, current methods often rely on handcrafted features or pretrained models which can limit their performance on specific datasets or applications. In this work, we propose Depth as Attention (DA) that introduces attention mechanism into deep neural networks trained specifically for faces to enhance performance by identifying discriminative regions. We achieve stateof-the-art results on several benchmarks while using smaller model architectures compared to previous works. Our method outperforms existing approaches by up to 7% across all benchmarks demonstrating its effectiveness. Additionally, our proposed DA module adds minimal computational cost during training and inference making it an efficient solution applicable to real-world scenarios.",1
"Visual relationship detection aims to reason over relationships among salient objects in images, which has drawn increasing attention over the past few years. Inspired by human reasoning mechanisms, it is believed that external visual commonsense knowledge is beneficial for reasoning visual relationships of objects in images, which is however rarely considered in existing methods. In this paper, we propose a novel approach named Relational Visual-Linguistic Bidirectional Encoder Representations from Transformers (RVL-BERT), which performs relational reasoning with both visual and language commonsense knowledge learned via self-supervised pre-training with multimodal representations. RVL-BERT also uses an effective spatial module and a novel mask attention module to explicitly capture spatial information among the objects. Moreover, our model decouples object detection from visual relationship recognition by taking in object names directly, enabling it to be used on top of any object detection system. We show through quantitative and qualitative experiments that, with the transferred knowledge and novel modules, RVL-BERT achieves competitive results on two challenging visual relationship detection datasets. The source code is available at https://github.com/coldmanck/RVL-BERT.",0
"This research presents an approach for visual relationship detection that utilizes visual-linguistic knowledge extracted from multimodal representations. The proposed method leverages advancements in deep learning techniques such as computer vision and natural language processing (NLP) to jointly analyze image and text data. The algorithm first generates visual-semantic embeddings by fine-tuning pretrained models on large datasets. These embeddings capture high-level relationships among objects, scenes, and descriptions. Next, a graph neural network is applied to model interrelationships between entities, based on their spatial configuration and semantic connections. To improve interpretability and explainability, we introduce external knowledge sources including human annotations and commonsense reasoning modules into our framework. Experiments demonstrate that our approach achieves state-of-the-art results on public benchmarks while providing valuable insights into complex multi-modal associations across different domains. Our study contributes to broader efforts toward developing intelligent systems capable of understanding real-world scenarios through integrated perception and cognition mechanisms.",1
"Ground segmentation of point clouds remains challenging because of the sparse and unordered data structure. This paper proposes the GSECnet - Ground Segmentation network for Edge Computing, an efficient ground segmentation framework of point clouds specifically designed to be deployable on a low-power edge computing unit. First, raw point clouds are converted into a discretization representation by pillarization. Afterward, features of points within pillars are fed into PointNet to get the corresponding pillars feature map. Then, a depthwise-separable U-Net with the attention module learns the classification from the pillars feature map with an enormously diminished model parameter size. Our proposed framework is evaluated on SemanticKITTI against both point-based and discretization-based state-of-the-art learning approaches, and achieves an excellent balance between high accuracy and low computing complexity. Remarkably, our framework achieves the inference runtime of 135.2 Hz on a desktop platform. Moreover, experiments verify that it is deployable on a low-power edge computing unit powered 10 watts only.",0
"Title: ""Ground Segmentation of Point Clouds for Edge Computing"" Abstract: In recent years, edge computing has emerged as a promising technology that enables efficient processing and analysis of large datasets at the source itself. This paradigm shift from traditional cloud-based processing promises to significantly reduce latency and improve data privacy by reducing the need to transmit sensitive data over networks. However, efficient ground segmentation of point clouds generated using LiDAR sensors remains a challenging task due to the high complexity and sparsity of the data. In this study, we propose a novel approach called GSECnet, which leverages deep learning techniques combined with classical computer vision methods to efficiently partition the three-dimensional space into segments representing objects on the ground plane (e.g., road surface) and non-ground features such as buildings and vegetation. Our proposed method outperforms state-of-the-art approaches, achieving improved accuracy while ensuring minimal computational resources are required. We demonstrate the effectiveness of our approach through extensive experimentation on real-world datasets captured by LiDAR sensors mounted on vehicles. Overall, our work paves the way towards enabling reliable edge computing applications based on ground segmented point clouds.",1
"Traditionally, for most machine learning settings, gaining some degree of explainability that tries to give users more insights into how and why the network arrives at its predictions, restricts the underlying model and hinders performance to a certain degree. For example, decision trees are thought of as being more explainable than deep neural networks but they lack performance on visual tasks. In this work, we empirically demonstrate that applying methods and architectures from the explainability literature can, in fact, achieve state-of-the-art performance for the challenging task of domain generalization while offering a framework for more insights into the prediction and training process. For that, we develop a set of novel algorithms including DivCAM, an approach where the network receives guidance during training via gradient based class activation maps to focus on a diverse set of discriminative features, as well as ProDrop and D-Transformers which apply prototypical networks to the domain generalization task, either with self-challenging or attention alignment. Since these methods offer competitive performance on top of explainability, we argue that the proposed methods can be used as a tool to improve the robustness of deep neural network architectures.",0
"In recent years, deep learning has shown remarkable performance on many tasks, including image classification. However, due to their black box nature, these models often lack interpretability and can struggle with generalizing well to new domains. This work proposes a novel method that addresses both issues simultaneously by leveraging explainability techniques to guide domain adaptation. Our approach consists of training a model in one domain using standard supervised learning, and then fine-tuning it using unsupervised adversarial training to improve robustness to distribution shifts while preserving accuracy on clean images from the original domain. We further incorporate an interpretable module (e.g., Integrated Gradients) to explicitly capture important features and encourage alignment across domains. Experimental results show significant improvements over state-of-the-art methods across several benchmark datasets, demonstrating the effectiveness of our proposed method. Overall, we believe that our approach holds great potential for enabling more reliable, transparent, and adaptable artificial intelligence systems.",1
"It has been attracting more and more attention to understand the global optimality in deep learning (DL) recently. However, conventional DL solvers, have not been developed intentionally to seek for such global optimality. In this paper, we propose a novel approximation algorithm, {\em BPGrad}, towards optimizing deep models globally via branch and pruning. The proposed BPGrad algorithm is based on the assumption of Lipschitz continuity in DL, and as a result, it can adaptively determine the step size for the current gradient given the history of previous updates, wherein theoretically no smaller steps can achieve the global optimality. We prove that, by repeating such a branch-and-pruning procedure, we can locate the global optimality within finite iterations. Empirically an efficient adaptive solver based on BPGrad for DL is proposed as well, and it outperforms conventional DL solvers such as Adagrad, Adadelta, RMSProp, and Adam in the tasks of object recognition, detection, and segmentation. The code is available at \url{https://github.com/RyanCV/BPGrad}.",0
"In recent years, deep learning has emerged as a powerful tool for solving complex problems across numerous domains, including computer vision, natural language processing, and robotics. However, training deep neural networks (DNNs) remains challenging due to their high computational cost and sensitivity to hyperparameter settings. To address these issues, researchers have developed a range of optimization techniques aimed at improving the efficiency and accuracy of DNN training. One such approach is branch-and-pruning optimization, which involves iteratively dividing the search space into smaller subspaces and selecting optimal parameters based on performance criteria. This method has shown promising results in terms of finding global optima while reducing computation costs significantly. In this work, we present a comprehensive study of branch-and-pruning optimization applied to various types of DNN architectures, including feedforward, convolutional, and recurrent models. We evaluate different pruning strategies, such as magnitude-based and correlation-based methods, and compare their effectiveness in terms of accuracy and computational cost. Our experiments demonstrate that branch-and-pruning optimization can effectively find globally optimal solutions for DNN training tasks, making it a valuable technique for practitioners and researchers alike. By providing insights into the strengths and limitations of this approach, our work contributes to the ongoing quest for efficient and effective deep learning algorithms.",1
"Weakly supervised video anomaly detection (WS-VAD) is to distinguish anomalies from normal events based on discriminative representations. Most existing works are limited in insufficient video representations. In this work, we develop a multiple instance self-training framework (MIST)to efficiently refine task-specific discriminative representations with only video-level annotations. In particular, MIST is composed of 1) a multiple instance pseudo label generator, which adapts a sparse continuous sampling strategy to produce more reliable clip-level pseudo labels, and 2) a self-guided attention boosted feature encoder that aims to automatically focus on anomalous regions in frames while extracting task-specific representations. Moreover, we adopt a self-training scheme to optimize both components and finally obtain a task-specific feature encoder. Extensive experiments on two public datasets demonstrate the efficacy of our method, and our method performs comparably to or even better than existing supervised and weakly supervised methods, specifically obtaining a frame-level AUC 94.83% on ShanghaiTech.",0
"In video anomaly detection, accurately identifying abnormal events from vast amounts of normal footage is a challenging task due to the large number of training examples required to achieve satisfactory results. Existing methods usually rely on supervised learning techniques which require copious labeled data, making them costly and time consuming. To address these limitations, we propose a new framework called MIST (Multiple Instance Self-Training) that efficiently utilizes limited annotations to detect anomalies in videos. Our approach leverages both intra-instance relationships within unlabeled clips and inter-instance consistency across multiple instances to improve detection performance without relying heavily on abundant annotated data. We evaluate our method against state-of-the-art approaches using several benchmark datasets and demonstrate significant improvements in accuracy while reducing computational costs. Overall, the proposed MIST framework offers an effective solution to enhance video anomaly detection by exploiting self-training strategies and minimizing human annotation efforts.",1
"Semi-supervised video object segmentation (semi-VOS) is widely used in many applications. This task is tracking class-agnostic objects from a given target mask. For doing this, various approaches have been developed based on online-learning, memory networks, and optical flow. These methods show high accuracy but are hard to be utilized in real-world applications due to slow inference time and tremendous complexity. To resolve this problem, template matching methods are devised for fast processing speed but sacrificing lots of performance in previous models. We introduce a novel semi-VOS model based on a template matching method and a temporal consistency loss to reduce the performance gap from heavy models while expediting inference time a lot. Our template matching method consists of short-term and long-term matching. The short-term matching enhances target object localization, while long-term matching improves fine details and handles object shape-changing through the newly proposed adaptive template attention module. However, the long-term matching causes error-propagation due to the inflow of the past estimated results when updating the template. To mitigate this problem, we also propose a temporal consistency loss for better temporal coherence between neighboring frames by adopting the concept of a transition matrix. Our model obtains 79.5% J&F score at the speed of 73.8 FPS on the DAVIS16 benchmark. The code is available in https://github.com/HYOJINPARK/TTVOS.",0
"This paper presents a lightweight video object segmentation method called TTVOS (Temporal Tracking via Optimized Semantic Segmentation) that leverages an adaptive template attention module and temporal consistency loss. The proposed approach efficiently segments objects across frames without relying on heavy computational resources or multiple frames. Our model effectively captures spatial details by adaptively attending to the most relevant template regions. We enforce temporal consistency constraints through a novel objective function that promotes continuous and coherent object boundaries. Experimental results demonstrate significant improvements over state-of-the-art methods, showcasing our system's ability to accurately segment complex video scenes while operating at real-time speeds.",1
"It is time-consuming and expensive to take high-quality or high-resolution electron microscopy (EM) and fluorescence microscopy (FM) images. Taking these images could be even invasive to samples and may damage certain subtleties in the samples after long or intense exposures, often necessary for achieving high-quality or high resolution in the first place. Advances in deep learning enable us to perform image-to-image transformation tasks for various types of microscopy image reconstruction, computationally producing high-quality images from the physically acquired low-quality ones. When training image-to-image transformation models on pairs of experimentally acquired microscopy images, prior models suffer from performance loss due to their inability to capture inter-image dependencies and common features shared among images. Existing methods that take advantage of shared features in image classification tasks cannot be properly applied to image reconstruction tasks because they fail to preserve the equivariance property under spatial permutations, something essential in image-to-image transformation. To address these limitations, we propose the augmented equivariant attention networks (AEANets) with better capability to capture inter-image dependencies, while preserving the equivariance property. The proposed AEANets captures inter-image dependencies and shared features via two augmentations on the attention mechanism, which are the shared references and the batch-aware attention during training. We theoretically derive the equivariance property of the proposed augmented attention model and experimentally demonstrate its consistent superiority in both quantitative and visual results over the baseline methods.",0
"Artificial intelligence (AI) has emerged as one of the most powerful tools available today, revolutionizing fields like medicine, computer graphics, physics, robotics and many more. In medical image analysis, which involves processing large amounts of data, deep learning approaches have achieved state-of-the-art performance across different modalities such as MRI, CT, mammography and microscopy. In this work, we present a novel augmentation methodology that enhances equivariance properties in attention mechanisms for convolutional neural networks, resulting in improved generalization performance on microscopy images specifically. Using our approach, we demonstrate how well-designed architectures can consistently outperform strong baselines and prior works through direct comparison across multiple datasets including popular benchmarks TIMER, NCAT, EIDUR, HippoDataBank, JDS2018 and others. Our framework paves the roadmap towards new horizons in efficient deployment of AI models in real-world scenarios while ensuring robustness, accuracy and generalization ability required by experts in computational imaging.",1
"Existing approaches for unsupervised domain adaptive object detection perform feature alignment via adversarial training. While these methods achieve reasonable improvements in performance, they typically perform category-agnostic domain alignment, thereby resulting in negative transfer of features. To overcome this issue, in this work, we attempt to incorporate category information into the domain adaptation process by proposing Memory Guided Attention for Category-Aware Domain Adaptation (MeGA-CDA). The proposed method consists of employing category-wise discriminators to ensure category-aware feature alignment for learning domain-invariant discriminative features. However, since the category information is not available for the target samples, we propose to generate memory-guided category-specific attention maps which are then used to route the features appropriately to the corresponding category discriminator. The proposed method is evaluated on several benchmark datasets and is shown to outperform existing approaches.",0
"This research presents a new approach to unsupervised domain adaptive object detection using memory guided attention (MeGA). The proposed method utilizes a generative adversarial network to generate synthetic target images, which are used to guide the adaptation process. By leveraging the power of category-aware attention mechanisms and transfer learning, the model can effectively learn from source data while preserving important features that are relevant to detecting objects in the target domain. Experimental results demonstrate that our MeGA-CDA outperforms state-of-the-art methods on several benchmark datasets across different domains. Our work has significant implications for real-world applications where labeled training data may not be readily available.",1
"Images in the medical domain are fundamentally different from the general domain images. Consequently, it is infeasible to directly employ general domain Visual Question Answering (VQA) models for the medical domain. Additionally, medical images annotation is a costly and time-consuming process. To overcome these limitations, we propose a solution inspired by self-supervised pretraining of Transformer-style architectures for NLP, Vision and Language tasks. Our method involves learning richer medical image and text semantic representations using Masked Language Modeling (MLM) with image features as the pretext task on a large medical image+caption dataset. The proposed solution achieves new state-of-the-art performance on two VQA datasets for radiology images -- VQA-Med 2019 and VQA-RAD, outperforming even the ensemble models of previous best solutions. Moreover, our solution provides attention maps which help in model interpretability. The code is available at https://github.com/VirajBagal/MMBERT",0
"In this work we introduce MMBERT, a multimodal pretraining method based on BERT (Bidirectional Encoder Representations from Transformers). Our approach combines textual features extracted by BERT with visual representations obtained using deep convolutional neural networks. We demonstrate that our proposed model outperforms state-of-the-art methods in two medical Vision Question Answering datasets, significantly improving both recall and F1 score. This suggests the potential of multimodal fine-tuning as a general strategy to improve performance across different question answering tasks within the medical domain. Furthermore, we show that pretrained models can achieve comparable results without any specific tuning during inference. Finally, we provide analysis showing that MMBERT is able to generate high quality answers even under low confidence conditions, demonstrating improved robustness over other approaches. In summary, our work proposes a novel multimodal pretraining framework which leverages the strengths of both textual and visual modalities, resulting in significant improvements in performance for medical vision Q&A tasks. Additionally, our analyses reveal insights into how multimodality affects task accuracy, suggesting future directions for research in the field. These findings have important implications for natural language processing applications in healthcare, where accurate question answering is crucial for enabling efficient and effective decision making.",1
"Fetal cortical plate segmentation is essential in quantitative analysis of fetal brain maturation and cortical folding. Manual segmentation of the cortical plate, or manual refinement of automatic segmentations is tedious and time-consuming. Automatic segmentation of the cortical plate, on the other hand, is challenged by the relatively low resolution of the reconstructed fetal brain MRI scans compared to the thin structure of the cortical plate, partial voluming, and the wide range of variations in the morphology of the cortical plate as the brain matures during gestation. To reduce the burden of manual refinement of segmentations, we have developed a new and powerful deep learning segmentation method. Our method exploits new deep attentive modules with mixed kernel convolutions within a fully convolutional neural network architecture that utilizes deep supervision and residual connections. We evaluated our method quantitatively based on several performance measures and expert evaluations. Results show that our method outperforms several state-of-the-art deep models for segmentation, as well as a state-of-the-art multi-atlas segmentation technique. We achieved average Dice similarity coefficient of 0.87, average Hausdorff distance of 0.96 mm, and average symmetric surface difference of 0.28 mm on reconstructed fetal brain MRI scans of fetuses scanned in the gestational age range of 16 to 39 weeks. With a computation time of less than 1 minute per fetal brain, our method can facilitate and accelerate large-scale studies on normal and altered fetal brain cortical maturation and folding.",0
"This paper presents a deep attentional convolutional neural network (CNN) architecture for automatic cortical plate segmentation in fetal Magnetic Resonance Imaging (MRI). We utilize deep attention modules that incorporate high resolution features from both local regions around each pixel as well as global context from entire images, enabling robust and accurate plate delineations. Our model outperforms state-of-the-art methods on several benchmarks, including Dice coefficient and Hausdorff distance measures. The task of automating plate segmentation within the developing cerebral cortex of fetuses has become increasingly important due to advancements in fetal MRI and its ability to reveal early brain development abnormalities. Accurately identifying these plates facilitates further analysis for detecting neurological disorders before birth, leading to improved pregnancy management and perinatal care. However, manual identification remains highly subjective, laborious, and time consuming while requiring expertise in fetal brain imaging analysis. Therefore, there is a pressing need for computational models capable of efficiently performing this challenging task with high precision. In our approach, we develop upon existing works by introducing new techniques tailored towards addressing limitations inherently present in such problems. Specifically, we leverage attention mechanisms to selectively focus on informative regional details and incorporate them into the overall plate prediction process considering large scale image structure. The resulting model achieves superior performance in multiple evaluation metrics against prior arts. Our findings support the effectiveness of using deep learning models equipped with appropriate attention modules in solving difficult medical image segmentation tasks involving fine grained structures. Future directions in this domain would involve integrating multi-modal information sources like functional MRI data along with exploratio",1
"Features that are equivariant to a larger group of symmetries have been shown to be more discriminative and powerful in recent studies. However, higher-order equivariant features often come with an exponentially-growing computational cost. Furthermore, it remains relatively less explored how rotation-equivariant features can be leveraged to tackle 3D shape alignment tasks. While many past approaches have been based on either non-equivariant or invariant descriptors to align 3D shapes, we argue that such tasks may benefit greatly from an equivariant framework. In this paper, we propose an effective and practical SE(3) (3D translation and rotation) equivariant network for point cloud analysis that addresses both problems. First, we present SE(3) separable point convolution, a novel framework that breaks down the 6D convolution into two separable convolutional operators alternatively performed in the 3D Euclidean and SO(3) spaces. This significantly reduces the computational cost without compromising the performance. Second, we introduce an attention layer to effectively harness the expressiveness of the equivariant features. While jointly trained with the network, the attention layer implicitly derives the intrinsic local frame in the feature space and generates attention vectors that can be integrated into different alignment tasks. We evaluate our approach through extensive studies and visual interpretations. The empirical results demonstrate that our proposed model outperforms strong baselines in a variety of benchmarks",0
"In recent years, 3D point cloud data has become increasingly prevalent across various domains including computer vision, robotics, and engineering, among others. This widespread adoption has led to the development of numerous algorithms that process point clouds. However, most of these methods ignore equivariance properties which could lead to better generalization of models across different input transformations like scaling, rotation, and translation. To address this issue, we propose Equivariant Point Network (EPN), a new model architecture that exploits group theory and invariant representations in the context of point cloud processing tasks such as classification, segmentation and registration. Our EPN framework is based on building block functions that explicitly enforce equivariance constraints at each layer using spherical harmonics, steerability matrices, and symmetry groups. We show that our method leads to state-of-the-art performance compared to existing approaches without explicit equivariance requirements. Furthermore, through extensive ablation studies, we demonstrate the importance of various design choices in achieving optimal results. Overall, our work serves as a foundation for further research into exploiting more advanced representation learning techniques in the context of point clouds while maintaining their underlying geometric structure.",1
"Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical field or the autonomous driving field. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing fields. In this paper, we present an overview of existing explainable AI (XAI) methods applied on time series and illustrate the type of explanations they produce. We also provide a reflection on the impact of these explanation methods to provide confidence and trust in the AI systems.",0
"Title: Explainable Artificial Intelligence (XAI) on Time Series Data: A Survey  Artificial intelligence has been transforming industries across different sectors by enabling automation of complex tasks and providing valuable insights through analysis of large datasets. However, as AI becomes more prevalent and integrated into critical systems, there is growing concern over its lack of transparency and interpretability, which hinders trust and adoption. In response to these concerns, explainable artificial intelligence (XAI) has emerged as a promising approach that focuses on creating interpretable models and explaining their decisions and actions in human-understandable terms.  Time series data is among the most common forms of data used in many applications such as finance, healthcare, environmental monitoring, and other domains where time-based measurements are available. Traditional approaches using statistical techniques have been successful in analyzing time series data; however, deep learning methods can provide better accuracy in certain cases. XAI can enhance the performance and credibility of AI solutions based on time series data by providing explanations for the predictions made by those models and helping users gain confidence in the results produced by black box algorithms. This survey aims to provide a comprehensive review of existing works related to XAI applied to time series data, discussing challenges, opportunities, and future directions.  The first part of the survey covers relevant background material, including an introduction to time series data and classical machine learning techniques commonly employed in modeling time series data. Next, we explore recent developments in XAI, focusing on key principles, methodologies, and evaluation metrics. We then present a comprehensive literature review classifying the studies according to the underlying data structure and type of explanation sought. Our classification scheme divides existing research into five categories: feature importance-based, anomaly detection-based, causality-based, post-hoc analysi",1
"We address the task of indoor scene generation by generating a sequence of objects, along with their locations and orientations conditioned on a room layout. Large-scale indoor scene datasets allow us to extract patterns from user-designed indoor scenes, and generate new scenes based on these patterns. Existing methods rely on the 2D or 3D appearance of these scenes in addition to object positions, and make assumptions about the possible relations between objects. In contrast, we do not use any appearance information, and implicitly learn object relations using the self-attention mechanism of transformers. We show that our model design leads to faster scene generation with similar or improved levels of realism compared to previous methods. Our method is also flexible, as it can be conditioned not only on the room layout but also on text descriptions of the room, using only the cross-attention mechanism of transformers. Our user study shows that our generated scenes are preferred to the state-of-the-art FastSynth scenes 53.9% and 56.7% of the time for bedroom and living room scenes, respectively. At the same time, we generate a scene in 1.48 seconds on average, 20% faster than FastSynth.",0
"SceneFormer: Generating Indoor Scenes With Transformer Models In this research, we present SceneFormer, a novel framework for generating indoor scenes using transformer models. Previous work on scene generation has primarily focused on outdoor environments, leaving the design of indoor spaces relatively unexplored. Our approach addresses this gap by leveraging the powerful representation learning capabilities of transformer networks to synthesize plausible indoor layouts. We evaluate our method through quantitative metrics as well as user studies, demonstrating that it produces results comparable to human-designed floorplans while significantly reducing the time required for designing such plans. Overall, SceneFormatter provides an efficient and effective means for generating detailed and realistic indoor environments, opening up new possibilities for architecture and interior design applications.",1
"Recognizing human emotion/expressions automatically is quite an expected ability for intelligent robotics, as it can promote better communication and cooperation with humans. Current deep-learning-based algorithms may achieve impressive performance in some lab-controlled environments, but they always fail to recognize the expressions accurately for the uncontrolled in-the-wild situation. Fortunately, facial action units (AU) describe subtle facial behaviors, and they can help distinguish uncertain and ambiguous expressions. In this work, we explore the correlations among the action units and facial expressions, and devise an AU-Expression Knowledge Constrained Representation Learning (AUE-CRL) framework to learn the AU representations without AU annotations and adaptively use representations to facilitate facial expression recognition. Specifically, it leverages AU-expression correlations to guide the learning of the AU classifiers, and thus it can obtain AU representations without incurring any AU annotations. Then, it introduces a knowledge-guided attention mechanism that mines useful AU representations under the constraint of AU-expression correlations. In this way, the framework can capture local discriminative and complementary features to enhance facial representation for facial expression recognition. We conduct experiments on the challenging uncontrolled datasets to demonstrate the superiority of the proposed framework over current state-of-the-art methods. Codes and trained models are available at https://github.com/HCPLab-SYSU/AUE-CRL.",0
"Deep learning has revolutionized facial expression recognition by providing large improvements over traditional feature engineering approaches. Recently, knowledge constraints have been introduced as prior distributions over weights or latent variables that encode domain knowledge such as physical laws or human priors on object motion patterns, leading to improved performance on difficult tasks. However, most previous work focuses on action unit (AU) detection rather than full-face expressions, leaving the integration of knowledge into higher level representations an open question. This paper presents a novel AU-expression Knowledge Constrained Representation Learnin",1
"The task of image generation started to receive some attention from artists and designers to inspire them in new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control on them. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user choice by performing several optimization steps to recover optimal parameters from the model's latent space. We tested several exploration methods starting with classic gradient descents to gradient-free optimizers. Many gradient-free optimizers just need comparisons (better/worse than another image), so that they can even be used without numerical criterion, without inspirational image, but with only with human preference. Thus, by iterating on one's preferences we could make robust Facial Composite or Fashion Generation algorithms. High resolution of the produced design generations are obtained using progressive growing of GANs. Our results on four datasets of faces, fashion images, and textures show that satisfactory images are effectively retrieved in most cases.",0
"Advances in generative adversarial networks (GANs) have enabled the creation of high-fidelity images that closely resemble real-world examples. However, these GANs often struggle to generate images that possess both visual quality and semantic meaningfulness. This paper presents a novel approach to inspirational adversarial image generation, which combines traditional adversarial training with additional objectives aimed at improving the semantic relevance of generated images. Our method leverages multiple discriminators trained on different levels of abstraction to guide the generator towards generating more semantically meaningful outputs. We demonstrate the effectiveness of our approach through extensive quantitative evaluations and qualitative comparisons against state-of-the-art methods. Our results show that our model produces higher fidelity and more relevant images across a wide range of domains, making it a valuable tool for computer vision applications such as image synthesis, image completion, and visual question answering.",1
"We address the problem of weakly-supervised semantic segmentation (WSSS) using bounding box annotations. Although object bounding boxes are good indicators to segment corresponding objects, they do not specify object boundaries, making it hard to train convolutional neural networks (CNNs) for semantic segmentation. We find that background regions are perceptually consistent in part within an image, and this can be leveraged to discriminate foreground and background regions inside object bounding boxes. To implement this idea, we propose a novel pooling method, dubbed background-aware pooling (BAP), that focuses more on aggregating foreground features inside the bounding boxes using attention maps. This allows to extract high-quality pseudo segmentation labels to train CNNs for semantic segmentation, but the labels still contain noise especially at object boundaries. To address this problem, we also introduce a noise-aware loss (NAL) that makes the networks less susceptible to incorrect labels. Experimental results demonstrate that learning with our pseudo labels already outperforms state-of-the-art weakly- and semi-supervised methods on the PASCAL VOC 2012 dataset, and the NAL further boosts the performance.",0
"In weakly supervised semantic segmentation (SICP), one must learn a good model on only image-level labels. Recent advances have relied upon strong unsupervised pretext tasks such as spatial (MIL) discrimination or predicting the next frame in a video, which can be computationally expensive to deploy over large datasets and may require access to raw pixels. Our work focuses on two new methods that instead operate on top of classical linear convolutional neural networks trained with arbitrary losses and architectures: background-aware pooling (BAP) which encourages models to remember what belongs in each scene based solely on classification loss, and noise-aware loss (NAL) which reduces weight placed on masks most likely generated by incorrect classifications. BAP has no hyperparameters. On PASCAL VOC 2012 we improve mIOU from 49.6% using DeepLabv3+ and ImageNet transfer alone up to 51.7%. Adding NAL brings another improvement to 52.8%, very close to fully supervised performance. Both methods generalize well across multiple domains including COCO Stuff, achieving high scores without extra data or pretraining. Code will be made public.",1
"We address the problem of 3D object detection, that is, estimating 3D object bounding boxes from point clouds. 3D object detection methods exploit either voxel-based or point-based features to represent 3D objects in a scene. Voxel-based features are efficient to extract, while they fail to preserve fine-grained 3D structures of objects. Point-based features, on the other hand, represent the 3D structures more accurately, but extracting these features is computationally expensive. We introduce in this paper a novel single-stage 3D detection method having the merit of both voxel-based and point-based features. To this end, we propose a new convolutional neural network (CNN) architecture, dubbed HVPR, that integrates both features into a single 3D representation effectively and efficiently. Specifically, we augment the point-based features with a memory module to reduce the computational cost. We then aggregate the features in the memory, semantically similar to each voxel-based one, to obtain a hybrid 3D representation in a form of a pseudo image, allowing to localize 3D objects in a single stage efficiently. We also propose an Attentive Multi-scale Feature Module (AMFM) that extracts scale-aware features considering the sparse and irregular patterns of point clouds. Experimental results on the KITTI dataset demonstrate the effectiveness and efficiency of our approach, achieving a better compromise in terms of speed and accuracy.",0
"This paper presents a novel representation called Hybrid Voxel-Point (HVPR) that combines voxel features and point features for single-stage 3D object detection. Our approach first generates a set of spatially sparse and dense feature points from LiDAR data using a PointNet architecture. Then, we propose an adaptive grid sampling method to subdivide these feature points into regular grids on which voxels can be generated. We further design a two-stream network structure to fuse both voxel and point features hierarchically via shared weights across different layers. Extensive experimental results demonstrate significant improvements over state-of-the-art methods on three public benchmark datasets KITTI and nuScenes. Code is available at <https://github.com/cvgtsunlab/HVPR>.",1
"A novel 3D point cloud learning model for deep LiDAR odometry, named PWCLO-Net, using hierarchical embedding mask optimization is proposed in this paper. In this model, the Pyramid, Warping, and Cost volume (PWC) structure for the LiDAR odometry task is built to refine the estimated pose in a coarse-to-fine approach hierarchically. An attentive cost volume is built to associate two point clouds and obtain embedding motion patterns. Then, a novel trainable embedding mask is proposed to weigh the local motion patterns of all points to regress the overall pose and filter outlier points. The estimated current pose is used to warp the first point cloud to bridge the distance to the second point cloud, and then the cost volume of the residual motion is built. At the same time, the embedding mask is optimized hierarchically from coarse to fine to obtain more accurate filtering information for pose refinement. The trainable pose warp-refinement process is iteratively used to make the pose estimation more robust for outliers. The superior performance and effectiveness of our LiDAR odometry model are demonstrated on KITTI odometry dataset. Our method outperforms all recent learning-based methods and outperforms the geometry-based approach, LOAM with mapping optimization, on most sequences of KITTI odometry dataset.Our source codes will be released on https://github.com/IRMVLab/PWCLONet.",0
"In recent years, Light Detection And Ranging (LiDAR) technology has emerged as an important tool for applications such as autonomous vehicles, robotics, and computer vision. One crucial task in these fields is odometry, which involves estimating the motion of a sensor platform using measurements from onboard sensors. However, traditional LiDAR odometry methods have limitations due to issues related to data quality, accuracy, robustness, scalability, and generality. To address these challenges, we propose a novel framework called PWCLO-Net that leverages deep learning techniques and hierarchical embedding mask optimization to achieve state-of-the-art performance in LiDAR odometry tasks. Specifically, our method can accurately estimate both translation and rotation components of rigid body motions directly from point clouds while achieving real-time inference speeds. Through extensive experiments, we demonstrate the effectiveness and efficiency of our approach compared to several baseline methods across multiple datasets and scenarios. Our results showcase the feasibility and potential of employing advanced machine learning models and techniques for enabling high-fidelity LiDAR-based odometry in complex environments. Overall, this research paves the way towards more reliable and accurate perception systems that can enable safer and smarter mobility solutions.",1
"In graph neural networks (GNNs), message passing iteratively aggregates nodes' information from their direct neighbors while neglecting the sequential nature of multi-hop node connections. Such sequential node connections e.g., metapaths, capture critical insights for downstream tasks. Concretely, in recommender systems (RSs), disregarding these insights leads to inadequate distillation of collaborative signals. In this paper, we employ collaborative subgraphs (CSGs) and metapaths to form metapath-aware subgraphs, which explicitly capture sequential semantics in graph structures. We propose meta\textbf{P}ath and \textbf{E}ntity-\textbf{A}ware \textbf{G}raph \textbf{N}eural \textbf{N}etwork (PEAGNN), which trains multilayer GNNs to perform metapath-aware information aggregation on such subgraphs. This aggregated information from different metapaths is then fused using attention mechanism. Finally, PEAGNN gives us the representations for node and subgraph, which can be used to train MLP for predicting score for target user-item pairs. To leverage the local structure of CSGs, we present entity-awareness that acts as a contrastive regularizer on node embedding. Moreover, PEAGNN can be combined with prominent layers such as GAT, GCN and GraphSage. Our empirical evaluation shows that our proposed technique outperforms competitive baselines on several datasets for recommendation tasks. Further analysis demonstrates that PEAGNN also learns meaningful metapath combinations from a given set of metapaths.",0
"This abstract presents a novel graph neural network approach for recommendation that utilizes metapaths and entity awareness to improve performance. In traditional approaches, recommender systems often rely on only one type of relationship, such as user interactions or item attributes, which can limit their effectiveness. However, our proposed method leverages multiple relationships extracted from diverse paths within heterogeneous networks (metapaths) to capture complex correlations between entities. Additionally, we incorporate entity awareness by considering individual characteristics of both users and items in the model. Experimental results demonstrate significant improvements over baseline methods, validating the effectiveness of our approach. Our work provides a new perspective on how recommenders can leverage rich meta structures present in large scale graphs to enhance personalized predictions.",1
"Recently, much attention has been devoted to finding highly efficient and powerful activation functions for CNN layers. Because activation functions inject different nonlinearities between layers that affect performance, varying them is one method for building robust ensembles of CNNs. The objective of this study is to examine the performance of CNN ensembles made with different activation functions, including six new ones presented here: 2D Mexican ReLU, TanELU, MeLU+GaLU, Symmetric MeLU, Symmetric GaLU, and Flexible MeLU. The highest performing ensemble was built with CNNs having different activation layers that randomly replaced the standard ReLU. A comprehensive evaluation of the proposed approach was conducted across fifteen biomedical data sets representing various classification tasks. The proposed method was tested on two basic CNN architectures: Vgg16 and ResNet50. Results demonstrate the superiority in performance of this approach. The MATLAB source code for this study will be available at https://github.com/LorisNanni.",0
"Deep learning models using Convolutional Neural Networks (CNN) have achieved state-of-the-art results on many image classification tasks. However, designing CNN architectures requires careful consideration of both the choice of architecture components as well as their hyperparameters. One important component often neglected by the community concerns the use of appropriate activation functions in deep networks that can significantly impact performance. In this work, we aim to bridge this gap by evaluating different activation functions used within each layer and assess how they influence accuracy. We furthermore propose a new ensemble method based on Early Stopping for improving overall model performance. Experimental results show consistent improvement across multiple datasets using our proposed approach which could lead to better generalization ability on unseen data. Our findings indicate that choosing an appropriate activation function could greatly benefit from more rigorous evaluation since our proposed method leads to significant improvements over previously reported works.",1
"Deep Neural Network (DNN) based super-resolution algorithms have greatly improved the quality of the generated images. However, these algorithms often yield significant artifacts when dealing with real-world super-resolution problems due to the difficulty in learning misaligned optical zoom. In this paper, we introduce a Squared Deformable Alignment Network (SDAN) to address this issue. Our network learns squared per-point offsets for convolutional kernels, and then aligns features in corrected convolutional windows based on the offsets. So the misalignment will be minimized by the extracted aligned features. Different from the per-point offsets used in the vanilla Deformable Convolutional Network (DCN), our proposed squared offsets not only accelerate the offset learning but also improve the generation quality with fewer parameters. Besides, we further propose an efficient cross packing attention layer to boost the accuracy of the learned offsets. It leverages the packing and unpacking operations to enlarge the receptive field of the offset learning and to enhance the ability of extracting the spatial connection between the low-resolution images and the referenced images. Comprehensive experiments show the superiority of our method over other state-of-the-art methods in both computational efficiency and realistic details.",0
"Here is a possible abstract for your paper on ""SDAN: Squared Deformable Alignment Network for Learning Misaligned Optical Zoom"":  Optical zoom technology has been widely used in digital cameras and smartphones for decades. With advances in deep learning techniques, researchers have explored using artificial neural networks (ANNs) for performing zooming operations on images. However, traditional convolutional neural networks (CNNs), which form the backbone of most modern image processing algorithms, suffer from limitations such as limited capacity and inflexibility at modeling local geometric changes required by optical zoom applications.  To address these challenges, we propose SDAN - Squared Deformable Alignment Network for Learning Misaligned Optical Zoom. Our proposed architecture leverages deformable alignment modules to explicitly handle misalignments across different scale levels during training. Furthermore, we introduce square activation functions and utilize them for both feature generation and boundary prediction, allowing our network to learn more effective kernels and achieve better accuracy for higher magnification ratios. Finally, we design specialized pooling layers that ensure high quality features can pass through all stages of SDAN. Experiments show that SDAN outperforms existing state-of-the-art methods in terms of visual fidelity and PSNR performance while improving runtime efficiency through smaller model size requirements. Overall, our work presents new insights into designing highly capable ANN models suitable for advanced tasks within computational photography and beyond.",1
"Bridging distant context interactions is important for high quality image completion with large masks. Previous methods attempting this via deep or large receptive field (RF) convolutions cannot escape from the dominance of nearby interactions, which may be inferior. In this paper, we propose treating image completion as a directionless sequence-to-sequence prediction task, and deploy a transformer to directly capture long-range dependence in the encoder in a first phase. Crucially, we employ a restrictive CNN with small and non-overlapping RF for token representation, which allows the transformer to explicitly model the long-range context relations with equal importance in all layers, without implicitly confounding neighboring tokens when larger RFs are used. In a second phase, to improve appearance consistency between visible and generated regions, a novel attention-aware layer (AAL) is introduced to better exploit distantly related features and also avoid the insular effect of standard attention. Overall, extensive experiments demonstrate superior performance compared to state-of-the-art methods on several datasets.",0
"This paper presents a new approach to image completion using a transformer-based architecture. Traditional methods for image completion rely on predicting missing pixels based on neighboring information, but these approaches can often struggle with complex images that contain varying textures and structures. Our proposed method leverages recent advances in deep learning to achieve more accurate results by modeling contextual relationships within an image. We use a novel neural network architecture inspired by the popular transformer design used in natural language processing tasks. In comparison to state-of-the-art techniques, our approach achieves better performance across multiple metrics such as PSNR and SSIM. Moreover, we demonstrate through ablation studies and visualizations that our method improves over baseline models in handling diverse image features like edges, corners, and repeating patterns. Overall, our work highlights the promise of applying transformers to computer vision problems beyond traditional image classification applications.",1
"Scene flow depicts the dynamics of a 3D scene, which is critical for various applications such as autonomous driving, robot navigation, AR/VR, etc. Conventionally, scene flow is estimated from dense/regular RGB video frames. With the development of depth-sensing technologies, precise 3D measurements are available via point clouds which have sparked new research in 3D scene flow. Nevertheless, it remains challenging to extract scene flow from point clouds due to the sparsity and irregularity in typical point cloud sampling patterns. One major issue related to irregular sampling is identified as the randomness during point set abstraction/feature extraction -- an elementary process in many flow estimation scenarios. A novel Spatial Abstraction with Attention (SA^2) layer is accordingly proposed to alleviate the unstable abstraction problem. Moreover, a Temporal Abstraction with Attention (TA^2) layer is proposed to rectify attention in temporal domain, leading to benefits with motions scaled in a larger range. Extensive analysis and experiments verified the motivation and significant performance gains of our method, dubbed as Flow Estimation via Spatial-Temporal Attention (FESTA), when compared to several state-of-the-art benchmarks of scene flow estimation.",0
"This work proposes a novel framework for flow estimation between consecutive lidar point clouds using deep learning techniques. We introduce FESTA (Flow Estimation via Spatial-Temporal Attention), which processes both spatial and temporal features jointly by utilizing attention mechanisms. Our method models pixel-level correspondences across two consecutive scans and estimates dense optical flows that can be applied to multiple applications in robotics and autonomous driving. We evaluate our approach on public benchmark datasets against state-of-the-art methods and demonstrate improved accuracy under challenging conditions. Overall, our proposed technique advances the field of dynamic scene understanding using 3D point cloud data.",1
"Predictive business process monitoring focuses on predicting future characteristics of a running process using event logs. The foresight into process execution promises great potentials for efficient operations, better resource management, and effective customer services. Deep learning-based approaches have been widely adopted in process mining to address the limitations of classical algorithms for solving multiple problems, especially the next event and remaining-time prediction tasks. Nevertheless, designing a deep neural architecture that performs competitively across various tasks is challenging as existing methods fail to capture long-range dependencies in the input sequences and perform poorly for lengthy process traces. In this paper, we propose ProcessTransformer, an approach for learning high-level representations from event logs with an attention-based network. Our model incorporates long-range memory and relies on a self-attention mechanism to establish dependencies between a multitude of event sequences and corresponding outputs. We evaluate the applicability of our technique on nine real event logs. We demonstrate that the transformer-based model outperforms several baselines of prior techniques by obtaining on average above 80% accuracy for the task of predicting the next activity. Our method also perform competitively, compared to baselines, for the tasks of predicting event time and remaining time of a running case",0
"This paper presents ProcessTransformer, a novel approach to predictive business process monitoring using transformer networks. Traditional methods for business process monitoring use predefined rules and thresholds to detect anomalies, but these approaches can often miss complex relationships and patterns that may indicate issues within the system. Instead, ProcessTransformer leverages deep learning techniques to learn representations of normal behavior and identify abnormalities based on predicted future states of the process. Our method outperforms traditional rule-based systems as well as state-of-the-art machine learning models in several benchmark datasets, demonstrating the effectiveness of our approach. Overall, we believe that ProcessTransformer has significant potential to improve the efficiency and accuracy of business process monitoring tasks.",1
"We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level. In contrast to dense methods that use a cost volume to search correspondences, we use self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images. The global receptive field provided by Transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outperforms state-of-the-art methods by a large margin. LoFTR also ranks first on two public benchmarks of visual localization among the published methods.",0
"A new method has been developed for local feature matching without the use of detectors. This approach utilizes transformers to directly learn patch embeddings, allowing for efficient and accurate feature correspondence estimation. Experiments demonstrate that this detector-free method outperforms traditional approaches on several benchmark datasets while offering significant computational advantages. These results have important implications for computer vision tasks such as image alignment, object recognition, and scene understanding. Overall, our work represents an advancement in the field of local feature matching and has applications across many areas of research.",1
"In this paper, we introduce Coarse-Fine Networks, a two-stream architecture which benefits from different abstractions of temporal resolution to learn better video representations for long-term motion. Traditional Video models process inputs at one (or few) fixed temporal resolution without any dynamic frame selection. However, we argue that, processing multiple temporal resolutions of the input and doing so dynamically by learning to estimate the importance of each frame can largely improve video representations, specially in the domain of temporal activity localization. To this end, we propose (1) Grid Pool, a learned temporal downsampling layer to extract coarse features, and, (2) Multi-stage Fusion, a spatio-temporal attention mechanism to fuse a fine-grained context with the coarse features. We show that our method outperforms the state-of-the-arts for action detection in public datasets including Charades with a significantly reduced compute and memory footprint. The code is available at https://github.com/kkahatapitiya/Coarse-Fine-Networks",0
"This paper presents a new approach, coarse-fine networks (CFN), that enables accurate temporal activity detection in videos by leveraging both high-level semantic features from pre-trained models as well as low-level spatio-temporal cues extracted from video frames. CFN consists of two sub-network branches: one branch generates global video representations using a deep neural network trained on image classification tasks; another branch extracts local features at different spatial resolutions via convolutional neural networks. To effectively combine these complementary feature types, we design several components including feature ensemble pooling, cross attention modules, and adaptive temporal fusion mechanisms. Extensive experimental evaluations demonstrate the superiority of our proposed CFN method over other state-of-the-art approaches on multiple challenging datasets, achieving substantial improvements in accuracy. We expect that our study can serve as a valuable reference for advancing research related to video analysis and action recognition. Our code is available at <https://github.com/Kingfisher21749608/Coarse_Fine_Network>.",1
"Our objective in this work is video-text retrieval - in particular a joint embedding that enables efficient text-to-video retrieval. The challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as HowTo100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute. We address both these challenges in this paper. We propose an end-to-end trainable model that is designed to take advantage of both large-scale image and video captioning datasets. Our model is an adaptation and extension of the recent ViT and Timesformer architectures, and consists of attention in both space and time. The model is flexible and can be trained on both image and video text datasets, either independently or in conjunction. It is trained with a curriculum learning schedule that begins by treating images as 'frozen' snapshots of video, and then gradually learns to attend to increasing temporal context when trained on video datasets. We also provide a new video-text pretraining dataset WebVid-2M, comprised of over two million videos with weak captions scraped from the internet. Despite training on datasets that are an order of magnitude smaller, we show that this approach yields state-of-the-art results on standard downstream video-retrieval benchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC.",0
"Abstract: This work presents a novel joint video and image encoder that enables end-to-end retrieval of both videos and images from large datasets. By incorporating advanced temporal attention mechanisms into the encoding process, our model effectively captures the salient features of each frame within a video while also ensuring efficient use of memory resources. We demonstrate through extensive experiments on several benchmark datasets that our proposed approach significantly outperforms state-of-the-art methods across all metrics, achieving new heights in accuracy and efficiency. Overall, our results highlight the potential of deep learning models to revolutionize the field of multimedia retrieval by bridging the gap between low-level feature extraction and high-level semantic understanding.",1
"We present a dual-pathway approach for recognizing fine-grained interactions from videos. We build on the success of prior dual-stream approaches, but make a distinction between the static and dynamic representations of objects and their interactions explicit by introducing separate motion and object detection pathways. Then, using our new Motion-Guided Attention Fusion module, we fuse the bottom-up features in the motion pathway with features captured from object detections to learn the temporal aspects of an action. We show that our approach can generalize across appearance effectively and recognize actions where an actor interacts with previously unseen objects. We validate our approach using the compositional action recognition task from the Something-Something-v2 dataset where we outperform existing state-of-the-art methods. We also show that our method can generalize well to real world tasks by showing state-of-the-art performance on recognizing humans assembling various IKEA furniture on the IKEA-ASM dataset.",0
"In video understanding, detecting interactions between objects and entities is important since they provide crucial insights into the context, situation, and intent of each object/entity involved. We propose a novel architecture that learns to fuse attention maps generated by both static (i.e., image recognition) models trained on still images and dynamic motion feature representations like optical flow. These two different attention mechanisms capture complementary information: static attention captures local appearance features while dynamic attention captures changes over time as well. Our approach uses a transformer network to iteratively refine these spatial attention heatmaps across frames until we identify those regions most responsible for encoding interobject relations and actions. After training our fusion model on JHMDB dataset using only RGB inputs, we evaluate on two challenging human action prediction benchmarks HICO-DET and Charades; we surpass state-of-the art methods on both datasets with margins ranging from 2% to nearly 8%. Code available at https://github.com/google-research/motion_guided_attention_fusion .",1
"We propose NeRF-VAE, a 3D scene generative model that incorporates geometric structure via NeRF and differentiable volume rendering. In contrast to NeRF, our model takes into account shared structure across scenes, and is able to infer the structure of a novel scene -- without the need to re-train -- using amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts previous generative models with convolution-based rendering which lacks geometric structure. Our model is a VAE that learns a distribution over radiance fields by conditioning them on a latent scene representation. We show that, once trained, NeRF-VAE is able to infer and render geometrically-consistent scenes from previously unseen 3D environments using very few input images. We further demonstrate that NeRF-VAE generalizes well to out-of-distribution cameras, while convolutional models do not. Finally, we introduce and study an attention-based conditioning mechanism of NeRF-VAE's decoder, which improves model performance.",0
"Title: Geometry Aware 3D Scene Generation using Neural Radiance Fields and Variational Autoencoders (NeRF-VAE)  Abstract: In recent years, there has been growing interest in generating realistic 3D scenes using neural networks. While existing methods have achieved impressive results, they often struggle with accurately representing complex geometric structures and lighting conditions. To address these limitations, we propose a novel approach that combines the strengths of two popular deep learning architectures: Neural Radiance Fields (NeRF), which can capture detailed surface appearance and geometry; and Variational Autoencoders (VAEs), which enable efficient latent space representations and probabilistic modeling. Our method, called NeRF-VAE, effectively integrates these capabilities by leveraging a VAE as a prior over the scene parameters learned by NeRF. This enables more effective optimization of both shape and appearance components, resulting in significantly improved geometric fidelity. We demonstrate our approach on several challenging benchmark datasets and show that our model outperforms state-of-the-art alternatives in terms of visual quality and geometric accuracy. Overall, the proposed framework represents a significant step forward towards creating high-fidelity generative models capable of synthesizing realistic 3D environments across varying lighting conditions.",1
"Recently, adaptive graph convolutional network based traffic prediction methods, learning a latent graph structure from traffic data via various attention-based mechanisms, have achieved impressive performance. However, they are still limited to find a better description of spatial relationships between traffic conditions due to: (1) ignoring the prior of the observed topology of the road network; (2) neglecting the presence of negative spatial relationships; and (3) lacking investigation on uncertainty of the graph structure. In this paper, we propose a Bayesian Graph Convolutional Network (BGCN) framework to alleviate these issues. Under this framework, the graph structure is viewed as a random realization from a parametric generative model, and its posterior is inferred using the observed topology of the road network and traffic data. Specifically, the parametric generative model is comprised of two parts: (1) a constant adjacency matrix which discovers potential spatial relationships from the observed physical connections between roads using a Bayesian approach; (2) a learnable adjacency matrix that learns a global shared spatial correlations from traffic data in an end-to-end fashion and can model negative spatial correlations. The posterior of the graph structure is then approximated by performing Monte Carlo dropout on the parametric graph structure. We verify the effectiveness of our method on five real-world datasets, and the experimental results demonstrate that BGCN attains superior performance compared with state-of-the-art methods.",0
"This paper proposes a novel approach to traffic prediction using Bayesian graph convolutional networks (BGCNs). We introduce BGCNs as a flexible tool that can capture complex relationships among road segments by modeling spatial dependencies between roads through their corresponding graphs. Our method integrates data from both GPS probe vehicles and taxi trajectories, enabling us to accurately predict future traffic conditions without relying exclusively on one source of data.  Our proposed model outperforms existing state-of-the-art methods such as deep neural network models based on grid maps and graph neural networks designed specifically for spatio-temporal tasks like traffic forecasting. In addition, we demonstrate that our BGCN architecture achieves better generalization performance over multiple datasets compared with competitive baseline models trained under the same settings. Finally, we conduct extensive ablation studies to validate each component of the proposed framework and provide insights into its effectiveness.  Overall, our work contributes new knowledge towards improving urban mobility planning and decision making by leveraging cutting-edge machine learning techniques. By incorporating advanced probabilistic reasoning mechanisms within graph convolutional networks, we achieve reliable and accurate real-time predictions while addressing limitations faced by conventional approaches in traffic flow analysis.",1
"Multiple Object Tracking (MOT) is an important task in computer vision. MOT is still challenging due to the occlusion problem, especially in dense scenes. Following the tracking-by-detection framework, we propose the Box-Plane Matching (BPM) method to improve the MOT performacne in dense scenes. First, we design the Layer-wise Aggregation Discriminative Model (LADM) to filter the noisy detections. Then, to associate remaining detections correctly, we introduce the Global Attention Feature Model (GAFM) to extract appearance feature and use it to calculate the appearance similarity between history tracklets and current detections. Finally, we propose the Box-Plane Matching strategy to achieve data association according to the motion similarity and appearance similarity between tracklets and detections. With the effectiveness of the three modules, our team achieves the 1st place on the Track-1 leaderboard in the ACM MM Grand Challenge HiEve 2020.",0
"This paper proposes a novel approach for tracking multiple objects in dense scenes using box-plane matching. The proposed method utilizes box regions and plane segmentation to represent objects and their geometry, respectively. By leveraging these representations, we can efficiently track objects across frames while handling occlusions and partial visibility. Our method achieves state-of-the-art performance on challenging benchmark datasets, demonstrating its effectiveness in real-world scenarios. We provide detailed experimental evaluations and discuss potential applications of our work in computer vision and robotics.",1
"Modeling time-evolving knowledge graphs (KGs) has recently gained increasing interest. Here, graph representation learning has become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to interpret their predictions. This paper provides a link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the structural dependencies and the temporal dynamics. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and by attention propagation. Our approach provides human-understandable evidence explaining the forecast. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model obtains a relative improvement of up to 20% on Hits@1 compared to the previous best KG forecasting method. We also conduct a survey with 53 respondents, and the results show that the evidence extracted by the model for link forecasting is aligned with human understanding.",0
"This sounds like a technical/academic paper that presents research related to artificial intelligence (AI). Without more details, I am unable to provide a proper abstract. Could you please provide me with additional information?",1
"Modern online multiple object tracking (MOT) methods usually focus on two directions to improve tracking performance. One is to predict new positions in an incoming frame based on tracking information from previous frames, and the other is to enhance data association by generating more discriminative identity embeddings. Some works combined both directions within one framework but handled them as two individual tasks, thus gaining little mutual benefits. In this paper, we propose a novel unified model with synergy between position prediction and embedding association. The two tasks are linked by temporal-aware target attention and distractor attention, as well as identity-aware memory aggregation model. Specifically, the attention modules can make the prediction focus more on targets and less on distractors, therefore more reliable embeddings can be extracted accordingly for association. On the other hand, such reliable embeddings can boost identity-awareness through memory aggregation, hence strengthen attention modules and suppress drifts. In this way, the synergy between position prediction and embedding association is achieved, which leads to strong robustness to occlusions. Extensive experiments demonstrate the superiority of our proposed model over a wide range of existing methods on MOTChallenge benchmarks. Our code and models are publicly available at https://github.com/songguocode/TADAM.",0
"As online object tracking has become increasingly important for applications such as autonomous vehicles, surveillance cameras, and augmented reality devices, there remains a need for more accurate and efficient approaches. One recent development that has shown promise in addressing these limitations is cross-task synergy (CTS). In CTS systems, multiple objects are tracked simultaneously by using shared representations across tasks, resulting in improved performance compared to traditional single-task methods. This review discusses some of the key developments in the field of online multiple object tracking with CTS, highlighting advancements in both algorithm design and evaluation metrics. It examines how incorporating prior knowledge from related domains can improve tracking accuracy, particularly in challenging situations where occlusion, motion blur, and clutter are present. Additionally, the use of deep learning techniques, including convolutional neural networks, have enabled better feature extraction and representation learning for more robust tracking. Finally, future research directions are proposed to further exploit the potential benefits of CTS in real-world scenarios, paving the way towards smarter camera-based systems.",1
"In this paper, we address the problem of predicting the future motion of a dynamic agent (called a target agent) given its current and past states as well as the information on its environment. It is paramount to develop a prediction model that can exploit the contextual information in both static and dynamic environments surrounding the target agent and generate diverse trajectory samples that are meaningful in a traffic context. We propose a novel prediction model, referred to as the lane-aware prediction (LaPred) network, which uses the instance-level lane entities extracted from a semantic map to predict the multi-modal future trajectories. For each lane candidate found in the neighborhood of the target agent, LaPred extracts the joint features relating the lane and the trajectories of the neighboring agents. Then, the features for all lane candidates are fused with the attention weights learned through a self-supervised learning task that identifies the lane candidate likely to be followed by the target agent. Using the instance-level lane information, LaPred can produce the trajectories compliant with the surroundings better than 2D raster image-based methods and generate the diverse future trajectories given multiple lane candidates. The experiments conducted on the public nuScenes dataset and Argoverse dataset demonstrate that the proposed LaPred method significantly outperforms the existing prediction models, achieving state-of-the-art performance in the benchmarks.",0
"This work presents a novel method for predicting future trajectories of dynamic agents in multi-modal environments using lane information. Our approach, called LaPred, uses Convolutional Neural Networks (CNN) to learn spatial and temporal representations from raw sensor data such as camera images and LiDAR point clouds. These representations are then used to make predictions about future agent locations based on current observations and a model of traffic rules and regulations. In addition to traditional collision avoidance metrics like minimum distance or time-to-collision, our system also considers lane constraints and road connectivity to generate more accurate and realistic predictions. We evaluate our method through extensive simulation experiments that demonstrate its superior performance compared to state-of-the-art methods in terms of prediction accuracy and robustness to different scenarios and parameters.",1
"Weakly Supervised Object Detection (WSOD), aiming to train detectors with only image-level dataset, has arisen increasing attention for researchers. In this project, we focus on two-phase WSOD architecture which integrates a powerful detector with a pure WSOD model. We explore the effectiveness of some representative detectors utilized as the second-phase detector in two-phase WSOD and propose a two-phase WSOD architecture. In addition, we present a strategy to establish the pseudo ground truth (PGT) used to train the second-phase detector. Unlike previous works that regard top one bounding boxes as PGT, we consider more bounding boxes to establish the PGT annotations. This alleviates the insufficient learning problem caused by the low recall of PGT. We also propose some strategies to refine the PGT during the training of the second detector. Our strategies suspend the training in specific epoch, then refine the PGT by the outputs of the second-phase detector. After that, the algorithm continues the training with the same gradients and weights as those before suspending. Elaborate experiments are conduceted on the PASCAL VOC 2007 dataset to verify the effectiveness of our methods. As results demonstrate, our two-phase architecture improves the mAP from 49.17% to 53.21% compared with the single PCL model. Additionally, the best PGT generation strategy obtains a 0.7% mAP increment. Our best refinement strategy boosts the performance by 1.74% mAP. The best results adopting all of our methods achieve 55.231% mAP which is the state-of-the-art performance.",0
"In this research, we propose a novel two-phase approach to solve weakly supervised object detection using unlabeled data. Our method first generates pseudo ground truth annotations using clustering techniques on noisy detections obtained from online image search engines. These annotations serve as training data for our weakly supervised model which learns to detect objects under various conditions such as different scales, occlusions, and lighting variations. We evaluate our method against several baselines using standard benchmark datasets like PASCAL VOC and COCO and show that our proposed method significantly outperforms them across all metrics. This work paves the way towards efficient use of large amounts of unlabeled data to improve performance in computer vision tasks where obtaining labeled data is cost prohibitive.",1
"RGB-infrared person re-identification is a challenging task due to the intra-class variations and cross-modality discrepancy. Existing works mainly focus on learning modality-shared global representations by aligning image styles or feature distributions across modalities, while local feature from body part and relationships between person images are largely neglected. In this paper, we propose a Dual-level (i.e., local and global) Feature Fusion (DF^2) module by learning attention for discriminative feature from local to global manner. In particular, the attention for a local feature is determined locally, i.e., applying a learned transformation function on itself. Meanwhile, to further mining the relationships between global features from person images, we propose an Affinities Modeling (AM) module to obtain the optimal intra- and inter-modality image matching. Specifically, AM employes intra-class compactness and inter-class separability in the sample similarities as supervised information to model the affinities between intra- and inter-modality samples. Experimental results show that our proposed method outperforms state-of-the-arts by large margins on two widely used cross-modality re-ID datasets SYSU-MM01 and RegDB, respectively.",0
"In recent years, cross-modality person re-identification (ReID) has gained increasing attention due to the growing popularity of multi-modal surveillance systems, which capture individuals using both RGB images and thermal infrared (IR) imagery. Existing approaches typically use handcrafted features or deep learning models trained separately on each modality, then fuse them at the score level or feature level for matching. However, these methods often struggle to accurately account for the differences in appearance and spatial layout between RGB and IR imagery, resulting in poor performance. To address this challenge, we propose a novel dual-level feature fusion and affinity modeling framework called DF^2AM. Our approach first learns discriminative features from both modalities using separate convolutional neural networks (CNNs). We then utilize a mid-level fusion strategy that aligns feature maps across different layers to ensure high-quality inter-modal representation. Next, our method performs late-fusion by directly concatenating extracted features and feeding the combined vector into a fully connected layer for prediction. To further enhance ReID accuracy under difficult scenarios such as occlusions or truncations, we introduce two new modules namely Deformable Convolution and Region-based Channel Attention Module (RCA), which can adaptively focus on informative regions and learn robust representations even in challenging conditions. Finally, we formulate a novel rank normalized triplet loss function tailored for ReID tasks, which effectively captures the relationships among query and gallery samples and promotes clustering behavior. Experimental evaluatio",1
"Most image data available are often stored in a compressed format, from which JPEG is the most widespread. To feed this data on a convolutional neural network (CNN), a preliminary decoding process is required to obtain RGB pixels, demanding a high computational load and memory usage. For this reason, the design of CNNs for processing JPEG compressed data has gained attention in recent years. In most existing works, typical CNN architectures are adapted to facilitate the learning with the DCT coefficients rather than RGB pixels. Although they are effective, their architectural changes either raise the computational costs or neglect relevant information from DCT inputs. In this paper, we examine different ways of speeding up CNNs designed for DCT inputs, exploiting learning strategies to reduce the computational complexity by taking full advantage of DCT inputs. Our experiments were conducted on the ImageNet dataset. Results show that learning how to combine all DCT inputs in a data-driven fashion is better than discarding them by hand, and its combination with a reduction of layers has proven to be effective for reducing the computational costs while retaining accuracy.",0
"""As technology continues to advance at a rapid pace, researchers have been exploring ways to make artificial neural networks faster without compromising accuracy. One promising approach that has recently gained attention is training directly on compressed images such as JPEGs. This method eliminates the need for preprocessing steps like resizing and normalization while still achieving state-of-the-art performance. In this study, we investigate the effectiveness of training neural networks straight from JPEG images and demonstrate significant improvements over traditional methods across multiple benchmark datasets. Our results show that using less data can indeed lead to better performance by allowing models to learn more essential features rather than irrelevant details. Furthermore, our proposed method requires fewer computational resources during both training and inference stages, making it an attractive option for real-world applications where time and memory constraints exist.""",1
"We present a recurrent agent who perceives surroundings through a series of discrete fixations. At each timestep, the agent imagines a variety of plausible scenes consistent with the fixation history. The next fixation is planned using uncertainty in the content of the imagined scenes. As time progresses, the agent becomes more certain about the content of the surrounding, and the variety in the imagined scenes reduces. The agent is built using a variational autoencoder and normalizing flows, and trained in an unsupervised manner on a proxy task of scene-reconstruction. The latent representations of the imagined scenes are found to be useful for performing pixel-level and scene-level tasks by higher-order modules. The agent is tested on various 2D and 3D datasets.",0
"This abstract describes the recent developments in the field of artificial intelligence (AI) related to visual attention mechanisms in imaginative agents. The authors explore how these mechanisms can improve the performance of AI systems by allowing them to focus on relevant parts of their input data. They present several case studies that demonstrate the effectiveness of visual attention mechanisms in different applications, including image classification, object detection, and natural language processing. In conclusion, they argue that incorporating visual attention into imaginative agents holds great potential for advancing the state-of-the-art in AI research.",1
"Attention maps, a popular heatmap-based explanation method for Visual Question Answering (VQA), are supposed to help users understand the model by highlighting portions of the image/question used by the model to infer answers. However, we see that users are often misled by current attention map visualizations that point to relevant regions despite the model producing an incorrect answer. Hence, we propose Error Maps that clarify the error by highlighting image regions where the model is prone to err. Error maps can indicate when a correctly attended region may be processed incorrectly leading to an incorrect answer, and hence, improve users' understanding of those cases. To evaluate our new explanations, we further introduce a metric that simulates users' interpretation of explanations to evaluate their potential helpfulness to understand model correctness. We finally conduct user studies to see that our new explanations help users understand model correctness better than baselines by an expected 30% and that our proxy helpfulness metrics correlate strongly ($\rho$0.97) with how well users can predict model correctness.",0
"Title: ""Identifying Error-Inducing Regions to Improve Visual Question Answering""  Visual question answering (VQA) is an essential task in computer vision that requires understanding natural language questions and finding relevant answers within images. However, many current VQA systems often fail at providing accurate responses due to their limited ability to identify and explain error-inducing regions. In this work, we present a novel approach towards improving explanation helpfulness by pointing out these error-inducing regions. Our method consists of two components - a region selector model and an attention mechanism. Firstly, the region selector selects multiple candidate regions that could potentially cause errors in the predicted answer. Secondly, our proposed attention module highlights specific portions of each selected region that are most responsible for inducing mistakes. We evaluate our system on several benchmark datasets, demonstrating significant improvements over baseline models as well as state-of-the-art competitors in terms of accuracy and robustness. Furthermore, qualitative experiments show that our model can effectively guide users towards correct answers and provide more comprehensible explanations for wrong predictions. Overall, our findings contribute to advancements in VQA research and shed light on the importance of incorporating contextual information during interpretation. Future directions may explore additional mechanisms for error analysis and improvement of human-AI interaction in VQA settings.",1
"To capture spatial relationships and temporal dynamics in traffic data, spatio-temporal models for traffic forecasting have drawn significant attention in recent years. Most of the recent works employed graph neural networks(GNN) with multiple layers to capture the spatial dependency. However, road junctions with different hop-distance can carry distinct traffic information which should be exploited separately but existing multi-layer GNNs are incompetent to discriminate between their impact. Again, to capture the temporal interrelationship, recurrent neural networks are common in state-of-the-art approaches that often fail to capture long-range dependencies. Furthermore, traffic data shows repeated patterns in a daily or weekly period which should be addressed explicitly. To address these limitations, we have designed a Simplified Spatio-temporal Traffic forecasting GNN(SST-GNN) that effectively encodes the spatial dependency by separately aggregating different neighborhood representations rather than with multiple layers and capture the temporal dependency with a simple yet effective weighted spatio-temporal aggregation mechanism. We capture the periodic traffic patterns by using a novel position encoding scheme with historical and current data in two different models. With extensive experimental analysis, we have shown that our model has significantly outperformed the state-of-the-art models on three real-world traffic datasets from the Performance Measurement System (PeMS).",0
"This paper presents a novel approach to traffic prediction called SST-GNN (Simplified Spatio-Temporal Forecasting Model Using Graph Neural Networks). With increasing urbanization, predictive models have become essential tools for efficient transportation management. However, existing methods suffer from limitations such as complexity, computational overhead, and unsatisfactory accuracy. Therefore, we propose SST-GNN which integrates simplified spatial and temporal features into a graph neural network framework. Our method efficiently captures spatiotemporal dependencies through attention mechanisms, improving both accuracy and interpretability compared to state-of-the-art models. Extensive experiments on real-world datasets demonstrate that our SST-GNN approach significantly outperforms baseline methods in terms of mean absolute error (MAE), root mean squared error (RMSE) and other metrics. These results showcase the effectiveness of our proposed solution in simplifying traffic predictions while maintaining high levels of efficiency and accuracy. Thus, SST-GNN has great potential for applications in smart city planning and intelligent transportation systems.",1
"With the increase in available large clinical and experimental datasets, there has been substantial amount of work being done on addressing the challenges in the area of biomedical image analysis. Image segmentation, which is crucial for any quantitative analysis, has especially attracted attention. Recent hardware advancement has led to the success of deep learning approaches. However, although deep learning models are being trained on large datasets, existing methods do not use the information from different learning epochs effectively. In this work, we leverage the information of each training epoch to prune the prediction maps of the subsequent epochs. We propose a novel architecture called feedback attention network (FANet) that unifies the previous epoch mask with the feature map of the current training epoch. The previous epoch mask is then used to provide a hard attention to the learnt feature maps at different convolutional layers. The network also allows to rectify the predictions in an iterative fashion during the test time. We show that our proposed feedback attention model provides a substantial improvement on most segmentation metrics tested on seven publicly available biomedical imaging datasets demonstrating the effectiveness of the proposed FANet.",0
This paper proposes the use of a feedback attention network (FAN) for improving biomedical image segmentation. The proposed approach leverages a two-stage framework that first applies an encoder network to generate a semantic feature map and then utilizes the FAN module to incorporate local spatial context and improve segmentation accuracy. Experimental results on a variety of biomedical imaging datasets demonstrate significant improvements over traditional methods and state-of-the-art approaches. The authors conclude that FAN represents a promising new direction for tackling challenging problems in biomedical image analysis.,1
"Differential Neural Architecture Search (NAS) requires all layer choices to be held in memory simultaneously; this limits the size of both search space and final architecture. In contrast, Probabilistic NAS, such as PARSEC, learns a distribution over high-performing architectures, and uses only as much memory as needed to train a single model. Nevertheless, it needs to sample many architectures, making it computationally expensive for searching in an extensive space. To solve these problems, we propose a sampling method adaptive to the distribution entropy, drawing more samples to encourage explorations at the beginning, and reducing samples as learning proceeds. Furthermore, to search fast in the multi-variate space, we propose a coarse-to-fine strategy by using a factorized distribution at the beginning which can reduce the number of architecture parameters by over an order of magnitude. We call this method Fast Probabilistic NAS (FP-NAS). Compared with PARSEC, it can sample 64% fewer architectures and search 2.1x faster. Compared with FBNetV2, FP-NAS is 1.9x - 3.5x faster, and the searched models outperform FBNetV2 models on ImageNet. FP-NAS allows us to expand the giant FBNetV2 space to be wider (i.e. larger channel choices) and deeper (i.e. more blocks), while adding Split-Attention block and enabling the search over the number of splits. When searching a model of size 0.4G FLOPS, FP-NAS is 132x faster than EfficientNet, and the searched FP-NAS-L0 model outperforms EfficientNet-B0 by 0.7% accuracy. Without using any architecture surrogate or scaling tricks, we directly search large models up to 1.0G FLOPS. Our FP-NAS-L2 model with simple distillation outperforms BigNAS-XL with advanced in-place distillation by 0.7% accuracy using similar FLOPS.",0
"We present FP-NAS, a novel framework that couples fast probabilistic methods with neural architecture search (NAS). Our approach enables efficient exploration of large spaces of model architectures while maintaining high accuracy performance across multiple tasks. In contrast to traditional NAS techniques which rely on time-consuming heuristics and iterative sampling, our method uses latent variable models learned from small subsets of data to predict optimal architectures quickly and effectively. Experimental results on CIFAR-10 and ImageNet demonstrate the effectiveness of our approach, achieving state-of-the art results competitive with existing NAS methods but at significantly reduced computational cost. Overall, we provide evidence that efficient yet effective probabilistic approaches can be applied to challenging machine learning problems like NAS.",1
"The prevalent approach in domain adaptive object detection adopts a two-stage architecture (Faster R-CNN) that involves a number of hyper-parameters and hand-crafted designs such as anchors, region pooling, non-maximum suppression, etc. Such architecture makes it very complicated while adopting certain existing domain adaptation methods with different ways of feature alignment. In this work, we adopt a one-stage detector and design DA-DETR, a simple yet effective domain adaptive object detection network that performs inter-domain alignment with a single discriminator. DA-DETR introduces a hybrid attention module that explicitly pinpoints the hard-aligned features for simple yet effective alignment across domains. It greatly simplifies traditional domain adaptation pipelines by eliminating sophisticated routines that involve multiple adversarial learning frameworks with different types of features. Despite its simplicity, extensive experiments show that DA-DETR demonstrates superior accuracy as compared with highly-optimized state-of-the-art approaches.",0
"This paper presents a novel method for domain adaptive object detection using transformers with hybrid attention mechanisms. Object detection is a challenging task that requires accurate identification and localization of objects within images, and has numerous applications across fields such as computer vision, autonomous systems, and robotics. However, traditional methods often struggle with limited training data or changes in image domains, resulting in degraded performance or reduced robustness.  Our proposed approach addresses these limitations through a combination of transfer learning and model adaptation techniques. First, we train our detector on a large dataset containing diverse object classes and domains, allowing the network to learn generalizable features that can handle variations in lighting, texture, and other visual properties. Next, we introduce our DA-DETR architecture, which utilizes hybrid attention modules to dynamically balance between spatial and channel-wise attentional mechanisms based on the input image's complexity. By doing so, we enable our detector to better focus on relevant features while reducing computational overhead, improving accuracy and efficiency compared to previous state-of-the-art models.  We validate the effectiveness of our approach through extensive experiments across multiple benchmark datasets (COCO, PASCAL VOC, and KITTI), demonstrating consistent improvements over baseline methods, particularly under conditions of limited supervision or domain shift. Our results showcase the promise of DA-DETR for real-world deployments where adaptation to new environments may be required, paving the way towards more versatile object detectors that can tackle diverse scenarios and tasks beyond simple object recognition. Overall, this work represents a significant step forward in enabling artificial intelligence to robustly perceive complex scenes and interact with dynamic worlds, ultimately leading to smarter, more capable intelligent agents.",1
"Pedestrian trajectory prediction for surveillance video is one of the important research topics in the field of computer vision and a key technology of intelligent surveillance systems. Social relationship among pedestrians is a key factor influencing pedestrian walking patterns but was mostly ignored in the literature. Pedestrians with different social relationships play different roles in the motion decision of target pedestrian. Motivated by this idea, we propose a Social Relationship Attention LSTM (SRA-LSTM) model to predict future trajectories. We design a social relationship encoder to obtain the representation of their social relationship through the relative position between each pair of pedestrians. Afterwards, the social relationship feature and latent movements are adopted to acquire the social relationship attention of this pair of pedestrians. Social interaction modeling is achieved by utilizing social relationship attention to aggregate movement information from neighbor pedestrians. Experimental results on two public walking pedestrian video datasets (ETH and UCY), our model achieves superior performance compared with state-of-the-art methods. Contrast experiments with other attention methods also demonstrate the effectiveness of social relationship attention.",0
"This paper presents a novel approach to human trajectory prediction using Long Short Term Memory (LSTM) networks that incorporates attention mechanisms to model social relationships among individuals. The proposed method, called Social Relationship Attention LSTM (SRA-LSTM), utilizes convolutional neural network layers to extract features from raw sensor data, which are then fed into an encoder module consisting of multiple bidirectional LSTMs that encode both spatial and temporal information. An attention mechanism is applied across time steps to focus on the most relevant past observations and interactions between individuals, allowing for more accurate predictions of future trajectories. Experimental results demonstrate significant improvements over baseline methods on two publicly available datasets, showing the effectiveness of our approach for predicting human motion patterns in complex social scenarios.",1
"Recent deep generative inpainting methods use attention layers to allow the generator to explicitly borrow feature patches from the known region to complete a missing region. Due to the lack of supervision signals for the correspondence between missing regions and known regions, it may fail to find proper reference features, which often leads to artifacts in the results. Also, it computes pair-wise similarity across the entire feature map during inference bringing a significant computational overhead. To address this issue, we propose to teach such patch-borrowing behavior to an attention-free generator by joint training of an auxiliary contextual reconstruction task, which encourages the generated output to be plausible even when reconstructed by surrounding regions. The auxiliary branch can be seen as a learnable loss function, i.e. named as contextual reconstruction (CR) loss, where query-reference feature similarity and reference-based reconstructor are jointly optimized with the inpainting generator. The auxiliary branch (i.e. CR loss) is required only during training, and only the inpainting generator is required during the inference. Experimental results demonstrate that the proposed inpainting model compares favourably against the state-of-the-art in terms of quantitative and visual performance.",0
"This paper presents a novel approach to image inpainting using generative models with auxiliary contextual reconstruction. Traditional approaches to image inpainting involve either filling missing regions with synthesized content based on surrounding pixels or learning statistical priors from large datasets. However, these methods often struggle with generating coherent and visually pleasing images due to limited information in local neighborhoods and lack of global context. Our proposed method addresses these limitations by incorporating global context through an auxiliary task that reconstructs the entire input image as well as locally relevant details through a conditional generator network. By doing so, we can effectively utilize both local and global context to guide the inpainting process towards more realistic results. We demonstrate the effectiveness of our approach through extensive experiments on several benchmark datasets, outperforming state-of-the-art methods in terms of visual quality and quantitative measures such as peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). Overall, our work shows promising results for advancing the field of image inpainting using deep learning techniques.",1
"Time series forecasting has gained lots of attention recently; this is because many real-world phenomena can be modeled as time series. The massive volume of data and recent advancements in the processing power of the computers enable researchers to develop more sophisticated machine learning algorithms such as neural networks to forecast the time series data. In this paper, we propose various neural network architectures to forecast the time series data using the dynamic measurements; moreover, we introduce various architectures on how to combine static and dynamic measurements for forecasting. We also investigate the importance of performing techniques such as anomaly detection and clustering on forecasting accuracy. Our results indicate that clustering can improve the overall prediction time as well as improve the forecasting performance of the neural network. Furthermore, we show that feature-based clustering can outperform the distance-based clustering in terms of speed and efficiency. Finally, our results indicate that adding more predictors to forecast the target variable will not necessarily improve the forecasting accuracy.",0
"This abstract presents two popular types of clustering approaches used in neural network architectures (i.e., distance-based vs. feature-based) and applies them to perform a comparative analysis on their effectiveness for performing time series forecasting tasks. We use a variety of real world datasets with diverse characteristics and evaluate our models based on standard metrics such as MSE, MAE, RMSE, etc. Our results suggest that while both approaches can achieve promising performance gains over traditional single model frameworks, feature-based clustering outperforms distance-based clustering in most scenarios due to its ability to capture more detailed patterns in the data space. We conclude by discussing potential future directions for research in this area including evaluating other forms of hybrid architectures that could further improve model accuracy.",1
"Graph Neural Networks(GNNs) are useful deep learning models to deal with the non-Euclid data. However, recent works show that GNNs are vulnerable to adversarial attacks. Small perturbations can lead to poor performance in many GNNs, such as Graph attention networks(GATs). Therefore, enhancing the robustness of GNNs is a critical problem.   Robust GAT(RoGAT) is proposed to improve the robustness of GNNs in this paper, . Note that the original GAT uses the attention mechanism for different edges but is still sensitive to the perturbation, RoGAT adjusts the edges' weight to adjust the attention scores progressively. Firstly, RoGAT tunes the edges weight based on the assumption that the adjacent nodes should have similar nodes. Secondly, RoGAT further tunes the features to eliminate feature's noises since even for the clean graph, there exists some unreasonable data. Then, we trained the adjusted GAT model to defense the adversarial attacks. Different experiments against targeted and untargeted attacks demonstrate that RoGAT outperforms significantly than most the state-of-the-art defense methods. The implementation of RoGAT based on the DeepRobust repository for adversarial attacks.",0
"Graph neural networks (GNNs) have made significant progress in handling graph data such as social networks, molecular structures, and knowledge graphs. However, despite their successes, existing GNN methods still face several challenges, including difficulties in capturing structural features, insufficient expressiveness, and poor performance on imbalanced datasets. To address these issues, we propose RoGAT, a novel variant of graph attention network (GAT). RoGAT enhances GAT by incorporating several key improvements that boost accuracy and robustness, including structural proximity sampling, edge importance weighting, adaptive thresholds, and degree normalization. We evaluate RoGAT using seven benchmark datasets covering different domains and demonstrate that our proposed method outperforms state-of-the art models. Our experiments show that RoGAT achieves higher accuracy than other competitor approaches while maintaining efficiency and scalability. In conclusion, the effectiveness and generality of RoGAT make it a strong candidate for future research in the field of graph representation learning. Keywords: graph neural networks, GAT, Robustness",1
"Scene graphs are a compact and explicit representation successfully used in a variety of 2D scene understanding tasks. This work proposes a method to incrementally build up semantic scene graphs from a 3D environment given a sequence of RGB-D frames. To this end, we aggregate PointNet features from primitive scene components by means of a graph neural network. We also propose a novel attention mechanism well suited for partial and missing graph data present in such an incremental reconstruction scenario. Although our proposed method is designed to run on submaps of the scene, we show it also transfers to entire 3D scenes. Experiments show that our approach outperforms 3D scene graph prediction methods by a large margin and its accuracy is on par with other 3D semantic and panoptic segmentation methods while running at 35 Hz.",0
"This paper introduces SceneGraphFusion, a method for predicting incrementally updated versions of 3D scene graphs directly from raw RGB-D sensor sequences. To achieve this goal we propose several innovations, including dynamic graph updating using neural message passing, online graph completion based on voxel features, adaptive edge fusion with volumetric uncertainty guidance, multi-frame feature aggregation, and a learned visibility model for handling occlusions. Our approach yields high-quality 3D reconstructions, outperforming state-of-the-art methods by significant margins both qualitatively and quantitatively. We believe that our work represents a step towards real-time large-scale 3D understanding from commodity sensors.",1
"Anomaly detection is a task that recognizes whether an input sample is included in the distribution of a target normal class or an anomaly class. Conventional generative adversarial network (GAN)-based methods utilize an entire image including foreground and background as an input. However, in these methods, a useless region unrelated to the normal class (e.g., unrelated background) is learned as normal class distribution, thereby leading to false detection. To alleviate this problem, this paper proposes a novel two-stage network consisting of an attention network and an anomaly detection GAN (ADGAN). The attention network generates an attention map that can indicate the region representing the normal class distribution. To generate an accurate attention map, we propose the attention loss and the adversarial anomaly loss based on synthetic anomaly samples generated from hard augmentation. By applying the attention map to an image feature map, ADGAN learns the normal class distribution from which the useless region is removed, and it is possible to greatly reduce the problem difficulty of the anomaly detection task. Additionally, the estimated attention map can be used for anomaly segmentation because it can distinguish between normal and anomaly regions. As a result, the proposed method outperforms the state-of-the-art anomaly detection and anomaly segmentation methods for widely used datasets.",0
"This paper proposes a novel two-stage anomaly detection approach that leverages attention maps to guide hard augmentations. Our method builds upon recent advances in self-supervised learning, which learn representations by predicting inputs given corrupted versions of them. By computing attention maps during training, our model learns to focus on task-relevant areas while ignoring irrelevant regions. We then use these attention maps as guidance for generating more challenging data variants through hard augmentations. Extensive experiments show that our approach achieves state-of-the-art results on several benchmark datasets across different domains, demonstrating the effectiveness of incorporating attention mechanisms into unsupervised representation learning pipelines. Additionally, we provide ablation studies and visualizations to better understand the role played by each component in our system. Overall, our work highlights the promise of integrating human-like intelligence into machine learning models, paving the way towards more robust and interpretable artificial intelligence systems.",1
"Deep Neural Networks (DNNs) are getting increasing attention to deal with Land Cover Classification (LCC) relying on Satellite Image Time Series (SITS). Though high performances can be achieved, the rationale of a prediction yielded by a DNN often remains unclear. An architecture expressing predictions with respect to input channels is thus proposed in this paper. It relies on convolutional layers and an attention mechanism weighting the importance of each channel in the final classification decision. The correlation between channels is taken into account to set up shared kernels and lower model complexity. Experiments based on a Sentinel-2 SITS show promising results.",0
"This paper presents a novel approach for land cover classification using time series data from satellite imagery obtained by Sentinel-2 sensors. We propose a new method called ""channel-based attention"" which integrates multiple spectral bands as well as temporal features into a unified framework. Our model incorporates both spatial contextual relationships among channels (hyperspectral bands) and temporal dependency within each channel. To handle large spatio-temporal datasets, we introduce an efficient convolutional neural network architecture that captures global dependencies effectively. Experimental results on several benchmark datasets demonstrate that our method outperforms state-of-the-art algorithms achieving significant improvements in accuracy. Furthermore, we provide extensive ablation studies analyzing the contributions of different components in our method. Overall, this work advances the field of remote sensing image processing, opening up possibilities for accurate environmental monitoring at larger scales.",1
"Reinforcement learning (RL) is attracting attention as an effective way to solve sequential optimization problems that involve high dimensional state/action space and stochastic uncertainties. Many such problems involve constraints expressed by inequality constraints. This study focuses on using RL to solve constrained optimal control problems. Most RL application studies have dealt with inequality constraints by adding soft penalty terms for violating the constraints to the reward function. However, while training neural networks to learn the value (or Q) function, one can run into computational issues caused by the sharp change in the function value at the constraint boundary due to the large penalty imposed. This difficulty during training can lead to convergence problems and ultimately lead to poor closed-loop performance. To address this issue, this study proposes a dynamic penalty (DP) approach where the penalty factor is gradually and systematically increased during training as the iteration episodes proceed. We first examine the ability of a neural network to represent a value function when uniform, linear, or DP functions are added to prevent constraint violation. The agent trained by a Deep Q Network (DQN) algorithm with the DP function approach was compared with agents with other constant penalty functions in a simple vehicle control problem. Results show that the proposed approach can improve the neural network approximation accuracy and provide faster convergence when close to a solution.",0
"This paper presents a novel approach to constraints handling in reinforcement learning (RL) using dynamic penalty functions. In traditional RL methods, constraints such as safety requirements, resource limitations, or time bounds can often be difficult to enforce directly in the objective function. As a result, these constraints may lead to suboptimal solutions that violate desirable properties or cause system instability.  Our proposed method addresses this challenge by introducing a dynamic penalty function that adjusts during the course of policy optimization. We show how to learn the optimal weighting parameter of the penalty term without extra effort from additional search processes. By doing so, our approach provides a natural solution path towards constrained policies, while achieving better overall performance compared to prior work on penalized RL algorithms. Furthermore, we demonstrate experimentally that the proposed approach leads to more stable and safe policies across different benchmark domains. Our results suggest promising applications for complex systems where adherence to specific constraints is critical to achieve desirable behaviors. Overall, this research contributes to filling the gap between theory and practice in the field of constraint-based RL, emphasizing the importance of adaptive approaches based on state estimation feedback.",1
"Food recognition plays an important role in food choice and intake, which is essential to the health and well-being of humans. It is thus of importance to the computer vision community, and can further support many food-oriented vision and multimodal tasks. Unfortunately, we have witnessed remarkable advancements in generic visual recognition for released large-scale datasets, yet largely lags in the food domain. In this paper, we introduce Food2K, which is the largest food recognition dataset with 2,000 categories and over 1 million images.Compared with existing food recognition datasets, Food2K bypasses them in both categories and images by one order of magnitude, and thus establishes a new challenging benchmark to develop advanced models for food visual representation learning. Furthermore, we propose a deep progressive region enhancement network for food recognition, which mainly consists of two components, namely progressive local feature learning and region feature enhancement. The former adopts improved progressive training to learn diverse and complementary local features, while the latter utilizes self-attention to incorporate richer context with multiple scales into local features for further local feature enhancement. Extensive experiments on Food2K demonstrate the effectiveness of our proposed method. More importantly, we have verified better generalization ability of Food2K in various tasks, including food recognition, food image retrieval, cross-modal recipe retrieval, food detection and segmentation. Food2K can be further explored to benefit more food-relevant tasks including emerging and more complex ones (e.g., nutritional understanding of food), and the trained models on Food2K can be expected as backbones to improve the performance of more food-relevant tasks. We also hope Food2K can serve as a large scale fine-grained visual recognition benchmark.",0
"This can be challenging if you haven't written one before! Here's some tips on how: * Describe your subject matter in two sentences or less. What is visual food recognition? Why is large scale relevant? What makes it challenging? These questions should give you some clarity as to what belongs in your first sentence. * Next, add three-five sentences describing your approach and key results. These could describe data sources used, methods applied, any special techniques or software developed etc... They should provide concrete evidence that demonstrates why your work matters. Finally wrap up by summarizing your main contributions. How did you advance state of art? Are there applications enabled by your methodology? Might there be future directions opened up by what was learned here? All these types of statements might belong at the end of your abstract, but probably no more than three sentences. If you need even more guidance check out my website dedicated specifically for writing abstracts and papers! https://cameroncrick.com/writing/abstract_guidelines/",1
"In humans, Attention is a core property of all perceptual and cognitive operations. Given our limited ability to process competing sources, attention mechanisms select, modulate, and focus on the information most relevant to behavior. For decades, concepts and functions of attention have been studied in philosophy, psychology, neuroscience, and computing. For the last six years, this property has been widely explored in deep neural networks. Currently, the state-of-the-art in Deep Learning is represented by neural attention models in several application domains. This survey provides a comprehensive overview and analysis of developments in neural attention models. We systematically reviewed hundreds of architectures in the area, identifying and discussing those in which attention has shown a significant impact. We also developed and made public an automated methodology to facilitate the development of reviews in the area. By critically analyzing 650 works, we describe the primary uses of attention in convolutional, recurrent networks and generative models, identifying common subgroups of uses and applications. Furthermore, we describe the impact of attention in different application domains and their impact on neural networks' interpretability. Finally, we list possible trends and opportunities for further research, hoping that this review will provide a succinct overview of the main attentional models in the area and guide researchers in developing future approaches that will drive further improvements.",0
"In recent years, attention mechanisms have emerged as one of the most popular methods in natural language processing (NLP) due to their ability to selectively focus on specific parts of inputs, allowing them to better capture important features and improve model performance. This paper conducts a comprehensive review of neural attention models used in deep learning, discussing both theory and applications, providing insights into current limitations and future research directions. We introduce fundamental principles underlying different types of attention mechanisms and describe key architectures, such as Transformer Networks, that incorporate attention successfully. Additionally, we investigate several variants of attention mechanisms proposed recently by the community. This survey provides guidance on how to design and implement effective attention networks tailored to your needs, helping readers navigate through a rapidly evolving field of NLP.",1
"Few-shot class incremental learning (FSCIL) portrays the problem of learning new concepts gradually, where only a few examples per concept are available to the learner. Due to the limited number of examples for training, the techniques developed for standard incremental learning cannot be applied verbatim to FSCIL. In this work, we introduce a distillation algorithm to address the problem of FSCIL and propose to make use of semantic information during training. To this end, we make use of word embeddings as semantic information which is cheap to obtain and which facilitate the distillation process. Furthermore, we propose a method based on an attention mechanism on multiple parallel embeddings of visual data to align visual and semantic vectors, which reduces issues related to catastrophic forgetting. Via experiments on MiniImageNet, CUB200, and CIFAR100 dataset, we establish new state-of-the-art results by outperforming existing approaches.",0
"Few-shot class-incremental learning (FSCL) is a challenging task where models must learn from very few labeled samples per new class and incrementally adapt to new classes while retaining previously learned knowledge. In this work, we propose semantic-aware knowledge distillation (SAKD), which leverages cross-entropy loss as well as consistency regularization based on feature space distances to achieve better performance than state-of-the-art methods. We show that our approach leads to improved accuracy across various datasets and backbone networks, and maintains robustness against severe data corruption scenarios. Our results demonstrate the effectiveness of combining both entropy minimization and distance regularization strategies for more effective knowledge transfer and representation preservation. This study has significant implications for developing more robust and efficient machine learning models capable of handling real-world tasks involving limited supervision, continuous adaptation, and high reliability requirements.",1
"Generative Adversarial Networks (GANs) produce impressive results on unconditional image generation when powered with large-scale image datasets. Yet generated images are still easy to spot especially on datasets with high variance (e.g. bedroom, church). In this paper, we propose various improvements to further push the boundaries in image generation. Specifically, we propose a novel dual contrastive loss and show that, with this loss, discriminator learns more generalized and distinguishable representations to incentivize generation. In addition, we revisit attention and extensively experiment with different attention blocks in the generator. We find attention to be still an important module for successful image generation even though it was not used in the recent state-of-the-art models. Lastly, we study different attention architectures in the discriminator, and propose a reference attention mechanism. By combining the strengths of these remedies, we improve the compelling state-of-the-art Fr\'{e}chet Inception Distance (FID) by at least 17.5% on several benchmark datasets. We obtain even more significant improvements on compositional synthetic scenes (up to 47.5% in FID).",0
"Artificial intelligence (AI) has been making significant progress in recent years, particularly in computer vision tasks such as generative adversarial networks (GANs). In order to achieve high-quality results, these systems often rely on large amounts of training data and complex architectures. However, obtaining sufficient quantities of labeled data can be difficult and time consuming, especially in domains where annotation requires specialized expertise. This paper presents a method that addresses these limitations by using dual contrastive loss functions to train GANs without requiring large datasets. Our approach employs both intra-domain contrastive learning and inter-domain attention mechanisms to improve image generation quality and ensure realism. Experimental evaluation shows that our method outperforms state-of-the-art alternatives, demonstrating robustness across diverse datasets including faces, birds, and cars. Overall, we believe that our work represents an important contribution towards making GANs more accessible to researchers and practitioners alike.",1
"As we rely on machine learning (ML) models to make more consequential decisions, the issue of ML models perpetuating or even exacerbating undesirable historical biases (e.g., gender and racial biases) has come to the fore of the public's attention. In this paper, we focus on the problem of detecting violations of individual fairness in ML models. We formalize the problem as measuring the susceptibility of ML models against a form of adversarial attack and develop a suite of inference tools for the adversarial cost function. The tools allow auditors to assess the individual fairness of ML models in a statistically-principled way: form confidence intervals for the worst-case performance differential between similar individuals and test hypotheses of model fairness with (asymptotic) non-coverage/Type I error rate control. We demonstrate the utility of our tools in a real-world case study.",0
"Title: Statistical Inference for Individual Fairness  Abstract:  In recent years, there has been increasing interest in using statistical methods to evaluate and promote individual fairness in decision making processes. However, despite these efforts, there remain significant challenges in designing reliable and valid inferential frameworks that can account for individual differences while ensuring overall systemic fairness. This study proposes a novel approach to address these issues by developing flexible and adaptive methodologies that can accommodate complex data structures and nonlinear relationships between predictors and outcomes. Our framework combines advanced machine learning techniques with classical statistical models to provide more precise estimates of individual treatment effects, which enables fine-grained analysis of both heterogeneous treatment responses and causal mechanisms underlying unfair disparities. We demonstrate through extensive simulations and real-world case studies that our proposed approaches have better performance compared to existing state-of-the-art methods in terms of bias reduction and power improvement, particularly under conditions with strong heterogeneity and interaction effects. Overall, our work represents an important step forward in improving statistical inference for individual fairness, with potential applications across diverse fields including education, criminal justice, healthcare, labor markets, and social welfare programs.",1
"In this paper, we present a deep learning model that exploits the power of self-supervision to perform 3D point cloud completion, estimating the missing part and a context region around it. Local and global information are encoded in a combined embedding. A denoising pretext task provides the network with the needed local cues, decoupled from the high-level semantics and naturally shared over multiple classes. On the other hand, contrastive learning maximizes the agreement between variants of the same shape with different missing portions, thus producing a representation which captures the global appearance of the shape. The combined embedding inherits category-agnostic properties from the chosen pretext tasks. Differently from existing approaches, this allows to better generalize the completion properties to new categories unseen at training time. Moreover, while decoding the obtained joint representation, we better blend the reconstructed missing part with the partial shape by paying attention to its known surrounding region and reconstructing this frame as auxiliary objective. Our extensive experiments and detailed ablation on the ShapeNet dataset show the effectiveness of each part of the method with new state of the art results. Our quantitative and qualitative analysis confirms how our approach is able to work on novel categories without relying neither on classification and shape symmetry priors, nor on adversarial training procedures.",0
"This sounds like a fascinating research area! Iâ€™d love to read more about it once you have written up your paper. Does my response meet your expectations? If so, can you provide me some examples on how I could improve?",1
"Localized Narratives is a dataset with detailed natural language descriptions of images paired with mouse traces that provide a sparse, fine-grained visual grounding for phrases. We propose TReCS, a sequential model that exploits this grounding to generate images. TReCS uses descriptions to retrieve segmentation masks and predict object labels aligned with mouse traces. These alignments are used to select and position masks to generate a fully covered segmentation canvas; the final image is produced by a segmentation-to-image generator using this canvas. This multi-step, retrieval-based approach outperforms existing direct text-to-image generation models on both automatic metrics and human evaluations: overall, its generated images are more photo-realistic and better match descriptions.",0
"This paper presents a novel text-to-image generation approach based on fine-grained attention mechanisms that enable users to control over different attributes within images synthesized from textual descriptions. Existing methods have focused solely on generating whole image outputs conditioned on a given sentence without considering controllability at attribute levels. To overcome these limitations, we propose a two-stage framework consisting of global feature encoding followed by local refinement, each guided by user attention to specific attributes across objects, scenes, and styles. We further introduce an attentional discriminator network to align generated features with ground truth ones. Extensive experiments demonstrate our method significantly outperforms competing approaches while enabling precise manipulation of individual attributes. Our findings pave the way towards advanced applications, including personalized storyboards and virtual try-on fashion systems.",1
"Our objective is language-based search of large-scale image and video datasets. For this task, the approach that consists of independently mapping text and vision to a joint embedding space, a.k.a. dual encoders, is attractive as retrieval scales and is efficient for billions of images using approximate nearest neighbour search. An alternative approach of using vision-text transformers with cross-attention gives considerable improvements in accuracy over the joint embeddings, but is often inapplicable in practice for large-scale retrieval given the cost of the cross-attention mechanisms required for each sample at test time. This work combines the best of both worlds. We make the following three contributions. First, we equip transformer-based models with a new fine-grained cross-attention architecture, providing significant improvements in retrieval accuracy whilst preserving scalability. Second, we introduce a generic approach for combining a Fast dual encoder model with our Slow but accurate transformer-based model via distillation and re-ranking. Finally, we validate our approach on the Flickr30K image dataset where we show an increase in inference speed by several orders of magnitude while having results competitive to the state of the art. We also extend our method to the video domain, improving the state of the art on the VATEX dataset.",0
"In the age of visual content proliferation on social media platforms such as Instagram, TikTok, Facebook etc., there has been significant rise in interest among users for text-based image retrieval systems that would enable them to search images based on their natural language queries instead of relying solely on keywords associated with the multimedia files. This can significantly enhance user experience by allowing them to effortlessly retrieve visually similar images from large scale datasets. However, designing these models requires complex architectures that must balance several competing goals, including accuracy, efficiency, computational cost, scalability, etc. To address this issue, we propose an innovative approach using transformer networks that utilizes textual embeddings to efficiently perform text-to-visual retrieval tasks while providing state-of-the-art performance at low computational costs. We evaluate our method on three benchmark datasets commonly used for studying text-based image retrieval, namely Flickr8K, Flickr30K, and MSCOCO. Our results demonstrate that our proposed model outperforms all prior art across multiple evaluation metrics (e.g. Precision@n, Recall@n, Mean Average Precision, nDCG). Furthermore, we showcase how the proposed model can be fine-tuned and adapted to different use cases beyond mere textual embedding generation. By showing both qualitative and quantitative analysis over different downstream applications like generating captions for images and classifying images into predefined categories, the study provides evidence for robustness, versatility and effectiveness of the approach described herein. Overall, our work opens up possibilities for efficient text-to-image retrieval solutions tha",1
"The objective of this work is to annotate sign instances across a broad vocabulary in continuous sign language. We train a Transformer model to ingest a continuous signing stream and output a sequence of written tokens on a large-scale collection of signing footage with weakly-aligned subtitles. We show that through this training it acquires the ability to attend to a large vocabulary of sign instances in the input sequence, enabling their localisation. Our contributions are as follows: (1) we demonstrate the ability to leverage large quantities of continuous signing videos with weakly-aligned subtitles to localise signs in continuous sign language; (2) we employ the learned attention to automatically generate hundreds of thousands of annotations for a large sign vocabulary; (3) we collect a set of 37K manually verified sign instances across a vocabulary of 950 sign classes to support our study of sign language recognition; (4) by training on the newly annotated data from our method, we outperform the prior state of the art on the BSL-1K sign language recognition benchmark.",0
"In recent years, there has been a growing interest in developing automatic methods for analyzing sign language videos. One key challenge in this domain is temporal localization - the task of identifying specific moments within a video where particular signs occur. To tackle this problem, we propose a novel approach that combines both visual and audio cues. We first use state-of-the-art object detection techniques to detect hands and faces in each frame of the video. Then, we extract features from these detected regions, as well as from the surrounding audio, to create a multi-modal representation of the video sequence. Finally, we train a deep learning model on this representation to predict which frames contain the target sign gesture. Our experiments demonstrate that our method outperforms previous approaches by achieving significantly higher accuracy and robustness to variations in signing styles and lighting conditions. Overall, our work represents a step forward towards making sign language accessible to a wider audience through automated analysis tools.",1
"Recently, the Transformer module has been transplanted from natural language processing to computer vision. This paper applies the Transformer to video-based person re-identification, where the key issue is to extract the discriminative information from a tracklet. We show that, despite the strong learning ability, the vanilla Transformer suffers from an increased risk of over-fitting, arguably due to a large number of attention parameters and insufficient training data. To solve this problem, we propose a novel pipeline where the model is pre-trained on a set of synthesized video data and then transferred to the downstream domains with the perception-constrained Spatiotemporal Transformer (STT) module and Global Transformer (GT) module. The derived algorithm achieves significant accuracy gain on three popular video-based person re-identification benchmarks, MARS, DukeMTMC-VideoReID, and LS-VID, especially when the training and testing data are from different domains. More importantly, our research sheds light on the application of the Transformer on highly-structured visual data.",0
"Video-based person re-identification (ReID) has been gaining traction as a key technology for intelligent video analysis, security systems and other applications that require accurate detection of individuals across multiple cameras and varying environments. Recently, transformer architectures have emerged as powerful models for image classification tasks, but their efficiency for spatio-temporal recognition remains largely unexplored. In this paper, we propose a novel approach called ""Spatiotemporal Transformer"" for tackling the task of person ReID on videos by incorporating both spatial appearance features extracted from frames and temporal dynamics captured using optical flow. Our proposed model learns both local interactions within short temporal intervals via self attention mechanisms and global interaction between different time steps through cross attention modules. We evaluate our method on three publicly available benchmark datasets, demonstrating state-of-the-art performance on all three. Our extensive experimentation provides evidence that the integration of both spatial and temporal cues leads to significant improvements over traditional approaches in video based person ReID, establishing Spatiotemporal Transformers as a promising solution for efficient video representation learning.",1
"Current semantic segmentation methods focus only on mining ""local"" context, i.e., dependencies between pixels within individual images, by context-aggregation modules (e.g., dilated convolution, neural attention) or structure-aware optimization criteria (e.g., IoU-like loss). However, they ignore ""global"" context of the training data, i.e., rich semantic relations between pixels across different images. Inspired by the recent advance in unsupervised contrastive representation learning, we propose a pixel-wise contrastive framework for semantic segmentation in the fully supervised setting. The core idea is to enforce pixel embeddings belonging to a same semantic class to be more similar than embeddings from different classes. It raises a pixel-wise metric learning paradigm for semantic segmentation, by explicitly exploring the structures of labeled pixels, which were rarely explored before. Our method can be effortlessly incorporated into existing segmentation frameworks without extra overhead during testing. We experimentally show that, with famous segmentation models (i.e., DeepLabV3, HRNet, OCR) and backbones (i.e., ResNet, HR-Net), our method brings consistent performance improvements across diverse datasets (i.e., Cityscapes, PASCAL-Context, COCO-Stuff, CamVid). We expect this work will encourage our community to rethink the current de facto training paradigm in fully supervised semantic segmentation.",0
"In recent years, semantic segmentation has emerged as a critical task in computer vision, enabling machines to understand the contextual meaning of images at the pixel level. To achieve high accuracy in semantic segmentation, several approaches have been proposed that utilize low-level features such as intensity values and edge maps. However, these methods often suffer from poor generalization due to their reliance on handcrafted feature representations. This paper presents an exploration into alternative means of capturing spatial relationships within an image using cross-image pixel contrast. By exploiting the similarity structure inherent in large collections of images, our method enables improved generalization beyond local patch statistics used in traditional approaches. We demonstrate significant improvements over state-of-the-art techniques across multiple benchmark datasets, validating the effectiveness of our approach in representing richer contexts for scene understanding.",1
"Automatic surgical workflow recognition is a key component for developing context-aware computer-assisted systems in the operating theatre. Previous works either jointly modeled the spatial features with short fixed-range temporal information, or separately learned visual and long temporal cues. In this paper, we propose a novel end-to-end temporal memory relation network (TMRNet) for relating long-range and multi-scale temporal patterns to augment the present features. We establish a long-range memory bank to serve as a memory cell storing the rich supportive information. Through our designed temporal variation layer, the supportive cues are further enhanced by multi-scale temporal-only convolutions. To effectively incorporate the two types of cues without disturbing the joint learning of spatio-temporal features, we introduce a non-local bank operator to attentively relate the past to the present. In this regard, our TMRNet enables the current feature to view the long-range temporal dependency, as well as tolerate complex temporal extents. We have extensively validated our approach on two benchmark surgical video datasets, M2CAI challenge dataset and Cholec80 dataset. Experimental results demonstrate the outstanding performance of our method, consistently exceeding the state-of-the-art methods by a large margin (e.g., 67.0% v.s. 78.9% Jaccard on Cholec80 dataset).",0
"In recent years, there has been growing interest in using deep learning techniques to analyze surgical video data. With advances in hardware capabilities and algorithmic development, these models have shown promising results in tasks such as action recognition, activity analysis, and procedure prediction. However, many existing methods struggle with recognizing complex temporal relationships between actions that occur throughout the course of a surgical workflow. This paper presents a novel approach that addresses this issue by utilizing a Temporal Memory Relation Network (TMRT) which can capture temporal correlations between different memory slots and recognize complex relationships among multiple memories in time sequences. Experiments on two publicly available datasets demonstrate the effectiveness of our method compared to state-of-the-art baseline algorithms. Our proposed model offers new possibilities for analyzing surgical videos through more accurate workflow recognition, supporting better clinical decision making, outcomes assessment, and quality improvement initiatives.",1
"Referring image segmentation aims to segment the objects referred by a natural language expression. Previous methods usually focus on designing an implicit and recurrent feature interaction mechanism to fuse the visual-linguistic features to directly generate the final segmentation mask without explicitly modeling the localization information of the referent instances. To tackle these problems, we view this task from another perspective by decoupling it into a ""Locate-Then-Segment"" (LTS) scheme. Given a language expression, people generally first perform attention to the corresponding target image regions, then generate a fine segmentation mask about the object based on its context. The LTS first extracts and fuses both visual and textual features to get a cross-modal representation, then applies a cross-model interaction on the visual-textual features to locate the referred object with position prior, and finally generates the segmentation result with a light-weight segmentation network. Our LTS is simple but surprisingly effective. On three popular benchmark datasets, the LTS outperforms all the previous state-of-the-art methods by a large margin (e.g., +3.2% on RefCOCO+ and +3.4% on RefCOCOg). In addition, our model is more interpretable with explicitly locating the object, which is also proved by visualization experiments. We believe this framework is promising to serve as a strong baseline for referring image segmentation.",0
"Automatic image segmentation has become increasingly important due to its wide range of applications in computer vision. In this work, we present a strong pipeline that combines two popular methods, locate and segment, to effectively and accurately refer objects within images. Our approach begins by using HED (Histogram of Oriented Edge Detection) to preprocess the image, followed by detecting candidate regions based on color contrast. These candidates are further refined using edge features extracted from Canny edges. Finally, the CRF (Conditional Random Fields) model is used to perform object-specific semantic segmentation. We evaluate our method against several state-of-the-art approaches on four public datasets, demonstrating improved performance across all metrics. This new powerful pipeline holds great promise for future research in the field of image segmentation.",1
"Predicting future trajectories of surrounding obstacles is a crucial task for autonomous driving cars to achieve a high degree of road safety. There are several challenges in trajectory prediction in real-world traffic scenarios, including obeying traffic rules, dealing with social interactions, handling traffic of multi-class movement, and predicting multi-modal trajectories with probability. Inspired by people's natural habit of navigating traffic with attention to their goals and surroundings, this paper presents a unique dynamic graph attention network to solve all those challenges. The network is designed to model the dynamic social interactions among agents and conform to traffic rules with a semantic map. By extending the anchor-based method to multiple types of agents, the proposed method can predict multi-modal trajectories with probabilities for multi-class movements using a single model. We validate our approach on the proprietary autonomous driving dataset for the logistic delivery scenario and two publicly available datasets. The results show that our method outperforms state-of-the-art techniques and demonstrates the potential for trajectory prediction in real-world traffic.",0
"Title: Accurate Multimodal Trajectory Prediction for Autonomous Vehicles using Semantic Maps and Dynamic Graph Attention Networks.  Abstract: This research presents a novel approach for accurate multimodal trajectory prediction of pedestrians, cyclists, and other vehicles on the road using semantic maps and dynamic graph attention networks (DGAT). The proposed method utilizes high definition map data that captures multiple modalities such as lane restrictions, speed limits, crosswalks, and traffic signals. These attributes are used to build a weighted graph representation of the environment where each node represents an intersection or location along the road network.  The main contribution of our work lies in developing a DGAT model that dynamically selects relevant nodes from the graph based on their relevance at different time steps during the prediction process. Our algorithm outperforms state-of-the-art models by effectively leveraging historical motion patterns and environmental features present in the map data. We demonstrate significant improvements in accuracy across all three types of agents considered - pedestrians, cyclists, and motorized vehicles.  We evaluate our algorithm on two large scale public datasets, namely, nuScenes dataset (60 minutes of driving videos) and Waymo Open Dataset (more than 7,294 diverse driving scenarios), against strong baseline methods. Experimental results show that our framework achieves higher precision compared to current approaches while providing reliable predictions several seconds into the future. Additionally, we conduct ablation studies and provide qualitative analysis to confirm the effectiveness of our design choices. Overall, our approach provides a scalable solution towards realizing safe and efficient autonomous driving systems capable of handling multiagent interactions in complex urban environments.",1
"Estimating 3D human pose from a single image suffers from severe ambiguity since multiple 3D joint configurations may have the same 2D projection. The state-of-the-art methods often rely on context modeling methods such as pictorial structure model (PSM) or graph neural network (GNN) to reduce ambiguity. However, there is no study that rigorously compares them side by side. So we first present a general formula for context modeling in which both PSM and GNN are its special cases. By comparing the two methods, we found that the end-to-end training scheme in GNN and the limb length constraints in PSM are two complementary factors to improve results. To combine their advantages, we propose ContextPose based on attention mechanism that allows enforcing soft limb length constraints in a deep network. The approach effectively reduces the chance of getting absurd 3D pose estimates with incorrect limb lengths and achieves state-of-the-art results on two benchmark datasets. More importantly, the introduction of limb length constraints into deep networks enables the approach to achieve much better generalization performance.",0
"In recent years there has been significant interest in developing methods for modeling human pose in three dimensions. Traditional approaches have relied on monocular images alone which can lead to error due to depth ambiguities and occlusions. To address these issues, researchers have turned to exploiting additional contextual cues such as object affordance constraints, scene geometry, and temporal coherency across video frames. However, most existing work has focused on either using explicit geometric models or learning implicit representation from raw data without explicitly reasoning over the physical world. This paper presents a unified perspective that brings together both types of representations by framing the problem of human pose estimation as one of probabilistic inference under uncertainty. We demonstrate how our method outperforms state-of-the-art techniques while allowing for efficient inference in real time applications such as robotics and virtual reality. Our results provide insight into the tradeoffs involved in balancing computational cost against accuracy and generality in context modeling for computer vision tasks.",1
"In this paper we propose a new problem scenario in image processing, wide-range image blending, which aims to smoothly merge two different input photos into a panorama by generating novel image content for the intermediate region between them. Although such problem is closely related to the topics of image inpainting, image outpainting, and image blending, none of the approaches from these topics is able to easily address it. We introduce an effective deep-learning model to realize wide-range image blending, where a novel Bidirectional Content Transfer module is proposed to perform the conditional prediction for the feature representation of the intermediate region via recurrent neural networks. In addition to ensuring the spatial and semantic consistency during the blending, we also adopt the contextual attention mechanism as well as the adversarial learning scheme in our proposed method for improving the visual quality of the resultant panorama. We experimentally demonstrate that our proposed method is not only able to produce visually appealing results for wide-range image blending, but also able to provide superior performance with respect to several baselines built upon the state-of-the-art image inpainting and outpainting approaches.",0
"This paper presents a new approach to image blending that allows for seamless transitions across a wide range of images. Traditional methods of image blending have focused on smoothly transitioning from one image to another by aligning pixel values based on similarity metrics such as color or intensity. However, these methods often fail to capture important visual details or can result in unnatural artifacts at object boundaries. Our proposed method addresses this issue by using deep learning techniques to automatically identify and merge semantically meaningful features across multiple images. Experimental results show significant improvements over existing approaches, resulting in more realistic and natural looking composites. We believe our method has applications in fields ranging from computer graphics and vision to robotics and autonomous systems.",1
"Few-shot segmentation has been attracting a lot of attention due to its effectiveness to segment unseen object classes with a few annotated samples. Most existing approaches use masked Global Average Pooling (GAP) to encode an annotated support image to a feature vector to facilitate query image segmentation. However, this pipeline unavoidably loses some discriminative information due to the average operation. In this paper, we propose a simple but effective self-guided learning approach, where the lost critical information is mined. Specifically, through making an initial prediction for the annotated support image, the covered and uncovered foreground regions are encoded to the primary and auxiliary support vectors using masked GAP, respectively. By aggregating both primary and auxiliary support vectors, better segmentation performances are obtained on query images. Enlightened by our self-guided module for 1-shot segmentation, we propose a cross-guided module for multiple shot segmentation, where the final mask is fused using predictions from multiple annotated samples with high-quality support vectors contributing more and vice versa. This module improves the final prediction in the inference stage without re-training. Extensive experiments show that our approach achieves new state-of-the-art performances on both PASCAL-5i and COCO-20i datasets.",0
"This paper presents two novel methods for few-shot segmentation: self-guided learning and cross-guided learning. Both methods significantly improve the performance of state-of-the-art models on few-shot segmentation tasks by leveraging unlabeled data and utilizing inter-task relationships among different tasks.  The first method, self-guided learning, focuses on refining the modelâ€™s representation by enhancing semantic features using unsupervised loss minimization. By maximizing the agreement between teacher and student networks trained on a large set of labeled images, we can ensure that high-quality features are learned even without any task supervision. Experimental results show that our approach leads to significant improvements over baseline methods across several benchmark datasets.  The second method, cross-guided learning, exploits the commonalities among multiple few-shot segmentation tasks to further enhance model performance. We propose training the model jointly on multiple tasks by sharing knowledge through a multi-branch architecture and aligning feature representations using adversarial regularization. Our experimental evaluation demonstrates that the proposed method achieves superior results compared to individual single-task training and previous multi-task approaches.  In summary, both self-guided and cross-guided learning methods yield promising improvements in few-shot segmentation performance, which could enable more efficient use of annotation resources and facilitate the deployment of semantic segmentation models in real-world applications.",1
"Reading irregular scene text of arbitrary shape in natural images is still a challenging problem, despite the progress made recently. Many existing approaches incorporate sophisticated network structures to handle various shapes, use extra annotations for stronger supervision, or employ hard-to-train recurrent neural networks for sequence modeling. In this work, we propose a simple yet strong approach for scene text recognition. With no need to convert input images to sequence representations, we directly connect two-dimensional CNN features to an attention-based sequence decoder which guided by holistic representation. The holistic representation can guide the attention-based decoder focus on more accurate area. As no recurrent module is adopted, our model can be trained in parallel. It achieves 1.5x to 9.4x acceleration to backward pass and 1.3x to 7.9x acceleration to forward pass, compared with the RNN counterparts. The proposed model is trained with only word-level annotations. With this simple design, our method achieves state-of-the-art or competitive recognition performance on the evaluated regular and irregular scene text benchmark datasets.",0
"This paper presents a new approach to scene text recognition using deep learning techniques. Our proposed method uses a holistic representation guided attention network (HRET) which combines both local features such as individual characters and global features such as spatial relationships between them. We propose that these two types of features are complementary and can improve performance when used together. In addition, our HRET model includes an attentional mechanism that allows for selective focus on relevant parts of the image during inference, improving efficiency and accuracy. Experimental results show that our method outperforms state-of-the-art approaches on several benchmark datasets across multiple evaluation metrics, including character error rate, accuracy, and end-to-end text detection. Overall, we believe our work represents a significant step forward towards more accurate and efficient scene text recognition.",1
"Most of the existing single object trackers track the target in a unitary local search window, making them particularly vulnerable to challenging factors such as heavy occlusions and out-of-view movements. Despite the attempts to further incorporate global search, prevailing mechanisms that cooperate local and global search are relatively static, thus are still sub-optimal for improving tracking performance. By further studying the local and global search results, we raise a question: can we allow more dynamics for cooperating both results? In this paper, we propose to introduce more dynamics by devising a dynamic attention-guided multi-trajectory tracking strategy. In particular, we construct dynamic appearance model that contains multiple target templates, each of which provides its own attention for locating the target in the new frame. Guided by different attention, we maintain diversified tracking results for the target to build multi-trajectory tracking history, allowing more candidates to represent the true target trajectory. After spanning the whole sequence, we introduce a multi-trajectory selection network to find the best trajectory that delivers improved tracking performance. Extensive experimental results show that our proposed tracking strategy achieves compelling performance on various large-scale tracking benchmarks. The project page of this paper can be found at https://sites.google.com/view/mt-track/.",0
"In order to solve challenges faced by single object tracking algorithms such as occlusion, dynamic backgrounds and appearance variations caused by illumination changes and object deformations, we propose the novel framework named DAMETRAD (Dynamic Attention guided Multi-Trajectory Analysis for Single Object TRacking).",1
"3D object detection is an important module in autonomous driving and robotics. However, many existing methods focus on using single frames to perform 3D detection, and do not fully utilize information from multiple frames. In this paper, we present 3D-MAN: a 3D multi-frame attention network that effectively aggregates features from multiple perspectives and achieves state-of-the-art performance on Waymo Open Dataset. 3D-MAN first uses a novel fast single-frame detector to produce box proposals. The box proposals and their corresponding feature maps are then stored in a memory bank. We design a multi-view alignment and aggregation module, using attention networks, to extract and aggregate the temporal features stored in the memory bank. This effectively combines the features coming from different perspectives of the scene. We demonstrate the effectiveness of our approach on the large-scale complex Waymo Open Dataset, achieving state-of-the-art results compared to published single-frame and multi-frame methods.",0
"This paper presents a new approach to object detection using convolutional neural networks (CNNs). Our method, called 3D-MAN, leverages multi-frame attention mechanisms to capture temporal relationships among frames in videos. By doing so, we can improve the accuracy of object detection by taking advantage of motion patterns that occur over time. We evaluate our approach on challenging datasets such as KITTI and nuScenes and show significant improvements compared to state-of-the-art methods. Additionally, we demonstrate the effectiveness of our method on real-world scenarios where objects move at varying speeds and directions, highlighting the robustness of 3D-MAN. Overall, 3D-MAN represents a step forward in advancing the field of video-based object detection by exploiting spatio-temporal context effectively.",1
"Face photo-sketch synthesis and recognition has many applications in digital entertainment and law enforcement. Recently, generative adversarial networks (GANs) based methods have significantly improved the quality of image synthesis, but they have not explicitly considered the purpose of recognition. In this paper, we first propose an Identity-Aware CycleGAN (IACycleGAN) model that applies a new perceptual loss to supervise the image generation network. It improves CycleGAN on photo-sketch synthesis by paying more attention to the synthesis of key facial regions, such as eyes and nose, which are important for identity recognition. Furthermore, we develop a mutual optimization procedure between the synthesis model and the recognition model, which iteratively synthesizes better images by IACycleGAN and enhances the recognition model by the triplet loss of the generated and real samples. Extensive experiments are performed on both photo-tosketch and sketch-to-photo tasks using the widely used CUFS and CUFSF databases. The results show that the proposed method performs better than several state-of-the-art methods in terms of both synthetic image quality and photo-sketch recognition accuracy.",0
"This paper proposes a new algorithm called Identity-aware cycle consistency loss (IdentityCycleLoss) that improves the quality of generated images by forcing two generators in the unpaired setting of CycleGANs to satisfy both identity preservation and cross-domain mapping constraints. An additional discriminator is introduced named IdentityDiscriminator that enforces the uniqueness constraint on generated sketch drawings as well as face verification performance with real sketches from same identities. Experimental results demonstrate significant improvements on visual quality over existing cycle-consistent GAN based methods under different image resolution settings. Furthermore, we show the effectiveness of using our method in sketch-based person re-identification task where our approach outperforms state-of-the art ones. We believe this work shows promising direction towards photo-sketch synthesis problem and has potential application scenarios like forensic sketch artist systems.",1
"Few-shot classification studies the problem of quickly adapting a deep learner to understanding novel classes based on few support images. In this context, recent research efforts have been aimed at designing more and more complex classifiers that measure similarities between query and support images, but left the importance of feature embeddings seldom explored. We show that the reliance on sophisticated classifier is not necessary and a simple classifier applied directly to improved feature embeddings can outperform state-of-the-art methods. To this end, we present a new method named \textbf{DCAP} in which we investigate how one can improve the quality of embeddings by leveraging \textbf{D}ense \textbf{C}lassification and \textbf{A}ttentive \textbf{P}ooling. Specifically, we propose to pre-train a learner on base classes with abundant samples to solve dense classification problem first and then fine-tune the learner on a bunch of randomly sampled few-shot tasks to adapt it to few-shot scenerio or the test time scenerio. We suggest to pool feature maps by applying attentive pooling instead of the widely used global average pooling (GAP) to prepare embeddings for few-shot classification during meta-finetuning. Attentive pooling learns to reweight local descriptors, explaining what the learner is looking for as evidence for decision making. Experiments on two benchmark datasets show the proposed method to be superior in multiple few-shot settings while being simpler and more explainable. Code is available at: \url{https://github.com/Ukeyboard/dcap/}.",0
"This is where the previous message ends, but I can see you have written something more. Would you like me to delete that?",1
"Neural network deployment on low-cost embedded systems, hence on microcontrollers (MCUs), has recently been attracting more attention than ever. Since MCUs have limited memory capacity as well as limited compute-speed, it is critical that we employ model compression, which reduces both memory and compute-speed requirements. In this paper, we add model compression, specifically Deep Compression, and further optimize Unlu's earlier work on arXiv, which efficiently deploys PyTorch models on MCUs. First, we prune the weights in convolutional and fully connected layers. Secondly, the remaining weights and activations are quantized to 8-bit integers from 32-bit floating-point. Finally, forward pass functions are compressed using special data structures for sparse matrices, which store only nonzero weights (without impacting performance and accuracy). In the case of the LeNet-5 model, the memory footprint was reduced by 12.45x, and the inference speed was boosted by 2.57x.",0
"In recent years, deep learning has become increasingly popular due to its ability to achieve state-of-the-art results across many different tasks. As a result, there has been a growing interest in deploying deep learning models onto resource-constrained devices such as microcontrollers. However, existing methods for compressing neural networks struggle to balance model size and accuracy, making it challenging to deploy high-quality models onto these devices. This work presents Deep Compression, a novel technique that enables efficient deployment of PyTorch models onto microcontrollers without compromising their performance. We demonstrate the effectiveness of our approach through extensive experiments, showing that Deep Compression achieves significant savings in memory footprint while maintaining comparable inference speed and accuracy compared to other compression techniques. Our method provides researchers and practitioners with a simple yet effective tool for deploying complex deep learning models onto low-power devices, opening up new possibilities for edge computing and IoT applications.",1
"Nowadays, one practical limitation of deep neural network (DNN) is its high degree of specialization to a single task or domain (e.g., one visual domain). It motivates researchers to develop algorithms that can adapt DNN model to multiple domains sequentially, meanwhile still performing well on the past domains, which is known as multi-domain learning. Almost all conventional methods only focus on improving accuracy with minimal parameter update, while ignoring high computing and memory cost during training, which makes it impossible to deploy multi-domain learning into more and more widely used resource-limited edge devices, like mobile phone, IoT, embedded system, etc. During our study in multi-domain training, we observe that large memory used for activation storage is the bottleneck that largely limits the training time and cost on edge devices. To reduce training memory usage, while keeping the domain adaption accuracy performance, in this work, we propose Deep Additive Attention Adaption, a novel memory-efficient on-device multi-domain learning method, aiming to achieve domain adaption on memory-limited edge devices. To reduce the training memory consumption during on-device training, $DA^3$ freezes the weights of the pre-trained backbone model (i.e., no need to store activation features during backward propagation). Furthermore, to improve the adaption accuracy performance, we propose to improve the model capacity by learning a novel additive attention adaptor module, which is also designed to avoid activation memory buffering for improving memory efficiency. We validate $DA^3$ on multiple datasets against state-of-the-art methods, which shows good improvement in both accuracy and training time.",0
"This paper proposes DAÂ³, a novel deep additive attention adaption method for memory-efficient on-device multi-domain learning. The proposed framework addresses the challenges faced by existing methods in terms of computational efficiency and scalability by leveraging efficient attention mechanisms that adaptively learn the weighting distribution over input features without increasing memory usage. The method outperforms state-of-the-art models in accuracy while significantly reducing computational complexity and memory requirements, making it well suited for deployment on resource-constrained devices such as smartphones and embedded systems. In summary, our work presents a significant step towards enabling high-quality machine learning applications in low-resource environments.",1
"We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (\ie shift, scale, and distortion invariance) while maintaining the merits of Transformers (\ie dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (\eg ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely removed in our model, simplifying the design for higher resolution vision tasks. Code will be released at \url{https://github.com/leoxiaobin/CvT}.",0
"""The integration of convolutional neural networks (CNNs) and vision transformer architectures has been a topic of recent interest due to their complementary strengths and weaknesses. In this work, we introduce a new architecture that incorporates both CNNs and vision transformers called Convolutions to Vision Transformer (CvT). Our approach modifies the traditional vision transformer design by adding local interactions within small patch groups using convolution kernels. We demonstrate that our model achieves competitive performance on image classification benchmark datasets such as ImageNet21K, while maintaining computational efficiency and reducing parameter count compared to other hybrid approaches. Additionally, we perform ablation studies to further analyze the impact of each component in our proposed architecture.""",1
"Although supervised person re-identification (Re-ID) methods have shown impressive performance, they suffer from a poor generalization capability on unseen domains. Therefore, generalizable Re-ID has recently attracted growing attention. Many existing methods have employed an instance normalization technique to reduce style variations, but the loss of discriminative information could not be avoided. In this paper, we propose a novel generalizable Re-ID framework, named Meta Batch-Instance Normalization (MetaBIN). Our main idea is to generalize normalization layers by simulating unsuccessful generalization scenarios beforehand in the meta-learning pipeline. To this end, we combine learnable batch-instance normalization layers with meta-learning and investigate the challenging cases caused by both batch and instance normalization layers. Moreover, we diversify the virtual simulations via our meta-train loss accompanied by a cyclic inner-updating manner to boost generalization capability. After all, the MetaBIN framework prevents our model from overfitting to the given source styles and improves the generalization capability to unseen domains without additional data augmentation or complicated network design. Extensive experimental results show that our model outperforms the state-of-the-art methods on the large-scale domain generalization Re-ID benchmark and the cross-domain Re-ID problem. The source code is available at: https://github.com/bismex/MetaBIN.",0
"This paper presents a novel approach to person re-identification using meta batch normalization (MetaBN). Existing methods often struggle to generalize well across different datasets due to variations in lighting conditions, camera viewpoints, and other factors that affect image appearance. To address this issue, we propose using MetaBN to learn features that can adapt to these differences and improve re-identification performance. Our method consists of two main components: a feature extractor that uses MetaBN to capture contextual dependencies between samples; and a distance calculator that computes similarities between images based on their learned representations. We evaluate our model on multiple benchmarks and show that it significantly outperforms state-of-the-art algorithms while maintaining low computational overhead. Overall, our results demonstrate the effectiveness of using MetaBN for generalizable person re-identification tasks.",1
"Transformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model's input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability.",0
"This research proposes methods for explainability of deep learning models such as bi-modal and encoder-decoder transformers through attention modeling techniques. Using generic attention mechanisms, we aim to provide insights into how these complex models make predictions by highlighting important input features and explaining their reasoning processes. We evaluate our approach on several datasets and demonstrate improved performance over traditional methods, providing new opportunities for interpretability in state-of-the-art machine learning applications. Our results have implications for a wide range of fields where interpretation of deep learning models is critical, including healthcare, finance, and social sciences.",1
"Generative modeling of set-structured data, such as point clouds, requires reasoning over local and global structures at various scales. However, adopting multi-scale frameworks for ordinary sequential data to a set-structured data is nontrivial as it should be invariant to the permutation of its elements. In this paper, we propose SetVAE, a hierarchical variational autoencoder for sets. Motivated by recent progress in set encoding, we build SetVAE upon attentive modules that first partition the set and project the partition back to the original cardinality. Exploiting this module, our hierarchical VAE learns latent variables at multiple scales, capturing coarse-to-fine dependency of the set elements while achieving permutation invariance. We evaluate our model on point cloud generation task and achieve competitive performance to the prior arts with substantially smaller model capacity. We qualitatively demonstrate that our model generalizes to unseen set sizes and learns interesting subset relations without supervision. Our implementation is available at https://github.com/jw9730/setvae.",0
"Artificial intelligence has advanced tremendously over recent years thanks largely to innovations in deep learning models such as generative adversarial networks (GANs) that have enabled computers to synthesize novel data examples from scratch which rival human creativity in image processing tasks such as image completion, super resolution, single image/multi resolution modeling etc. However these advances were made on individual sample level generation problems without considering any potential relationships between those samples since they cannot adequately capture complex correlations or relationships among elements given limited amounts of labeled training sets by nature. We introduce set VAEs - variant autoencoders tailored to encode hierarchies into parameters controlling sampling and learn jointly how to complete missing elements within images at high frequency multi scale conditioned upon learned features via attention mechanism then perform upscaling while performing cycle consistency verification; we demonstrate state of art performance on CelebA benchmark dataset where even humans confuse our AI generated outputs as real photographs, significantly surpassing prior arts; finally we discuss generalizability concerns towards using VAEs as drop in replacements towards generating new datasets conditional upon user text prompts and/or incorporating other modalities like audio.",1
"Various combinations of cameras enrich computational photography, among which reference-based superresolution (RefSR) plays a critical role in multiscale imaging systems. However, existing RefSR approaches fail to accomplish high-fidelity super-resolution under a large resolution gap, e.g., 8x upscaling, due to the lower consideration of the underlying scene structure. In this paper, we aim to solve the RefSR problem in actual multiscale camera systems inspired by multiplane image (MPI) representation. Specifically, we propose Cross-MPI, an end-to-end RefSR network composed of a novel plane-aware attention-based MPI mechanism, a multiscale guided upsampling module as well as a super-resolution (SR) synthesis and fusion module. Instead of using a direct and exhaustive matching between the cross-scale stereo, the proposed plane-aware attention mechanism fully utilizes the concealed scene structure for efficient attention-based correspondence searching. Further combined with a gentle coarse-to-fine guided upsampling strategy, the proposed Cross-MPI can achieve a robust and accurate detail transmission. Experimental results on both digitally synthesized and optical zoom cross-scale data show that the Cross-MPI framework can achieve superior performance against the existing RefSR methods and is a real fit for actual multiscale camera systems even with large-scale differences.",0
"The development of image super-resolution has progressed rapidly, but most methods use single scale images and cannot handle large deformations or occlusions that occur across scales. To address these challenges, we present Cross-MPI, which uses cross-scale synthetic Lagrangian (LSyn) multiplane imaging (MPI) to perform robust deep learning based image super-resolution. Our method leverages self attention mechanisms from transformers to generate high quality super-resolved images by effectively fusing features extracted at multiple scales. Experimental results on two benchmark datasets demonstrate that our model significantly outperforms state-of-the-art methods across all evaluation metrics, including subjective assessments, demonstrating its effectiveness and generalizability. Overall, Cross-MPI opens up new opportunities for solving complex inverse problems involving nonrigid motion, occlusions, and dynamic scenes.",1
"Self-supervised visual representation learning has seen huge progress recently, but no large scale evaluation has compared the many models now available. We evaluate the transfer performance of 13 top self-supervised models on 40 downstream tasks, including many-shot and few-shot recognition, object detection, and dense prediction. We compare their performance to a supervised baseline and show that on most tasks the best self-supervised models outperform supervision, confirming the recently observed trend in the literature. We find ImageNet Top-1 accuracy to be highly correlated with transfer to many-shot recognition, but increasingly less so for few-shot, object detection and dense prediction. No single self-supervised method dominates overall, suggesting that universal pre-training is still unsolved. Our analysis of features suggests that top self-supervised learners fail to preserve colour information as well as supervised alternatives, but tend to induce better classifier calibration, and less attentive overfitting than supervised learners.",0
"Self-supervised learning has emerged as a powerful tool for training machine learning models on large amounts of unlabeled data. These models have shown great promise in achieving state-of-the-art performance on a variety of tasks, including image classification, speech recognition, and natural language processing. However, one important question remains: how well do these self-supervised models transfer to new domains or task? In other words, can they generalize beyond their original training data? This paper addresses this question by comparing several popular self-supervised methods on a range of benchmark datasets and evaluating their ability to transfer across different domains and tasks. Our results show that while some self-supervised methods perform better than others, overall there is still significant room for improvement in terms of cross-domain transferability. Additionally, we investigate potential approaches for improving transferability, such as fine-tuning the pre-trained model on target domain data or using adaptive pretraining techniques. Overall, our work provides insights into the current limitations and future directions of self-supervised learning for improving model transferability.",1
"Interpretability in Graph Convolutional Networks (GCNs) has been explored to some extent in computer vision in general, yet, in the medical domain, it requires further examination. Moreover, most of the interpretability approaches for GCNs, especially in the medical domain, focus on interpreting the model in a post hoc fashion. In this paper, we propose an interpretable graph learning-based model which 1) interprets the clinical relevance of the input features towards the task, 2) uses the explanation to improve the model performance and, 3) learns a population level latent graph that may be used to interpret the cohort's behavior. In a clinical scenario, such a model can assist the clinical experts in better decision-making for diagnosis and treatment planning. The main novelty lies in the interpretable attention module (IAM), which directly operates on multi-modal features. Our IAM learns the attention for each feature based on the unique interpretability-specific losses. We show the application on two publicly available datasets, Tadpole and UKBB, for three tasks of disease, age, and gender prediction. Our proposed model shows superior performance with respect to compared methods with an increase in an average accuracy of 3.2% for Tadpole, 1.6% for UKBB Gender, and 2% for the UKBB Age prediction task. Further, we show exhaustive validation and clinical interpretation of our results.",0
"This study presents IA-GCN, a novel approach to predicting disease using graph convolutional networks (GCNs). Unlike traditional GCN models that lack interpretability, our model uses attention mechanisms to highlight important features in the input data, making it easier to understand how predictions are made. We evaluate our method on three different datasets related to diseases such as breast cancer and diabetes, demonstrating excellent performance compared to state-of-the-art methods. Our results show that IA-GCN can effectively capture complex relationships among variables and provide meaningful insights into disease progression. Furthermore, we conduct extensive ablation studies to analyze the impact of various components in our model. Overall, IA-GCN represents an exciting new direction in disease prediction research, paving the way for more interpretable and effective healthcare applications powered by machine learning algorithms.",1
"A heterogeneous graph consists of different vertices and edges types. Learning on heterogeneous graphs typically employs meta-paths to deal with the heterogeneity by reducing the graph to a homogeneous network, guide random walks or capture semantics. These methods are however sensitive to the choice of meta-paths, with suboptimal paths leading to poor performance. In this paper, we propose an approach for learning on heterogeneous graphs without using meta-paths. Specifically, we decompose a heterogeneous graph into different homogeneous relation-type graphs, which are then combined to create higher-order relation-type representations. These representations preserve the heterogeneity of edges and retain their edge directions while capturing the interaction of different vertex types multiple hops apart. This is then complemented with attention mechanisms to distinguish the importance of the relation-type based neighbors and the relation-types themselves. Experiments demonstrate that our model generally outperforms other state-of-the-art baselines in the vertex classification task on three commonly studied heterogeneous graph datasets.",0
"Graphs have become a popular data structure for representing complex relationships among entities, such as social networks, biological interactions, and semantic knowledge bases. However, most graph learning methods only consider pairwise (binary) relationships between nodes, ignoring higher order connections that could provide valuable contextual information. In this work, we propose a framework called HERO for learning on heterogeneous graphs using high-order relations. Our method integrates both binary and ternary relationships into graph neural networks by treating each relationship type as a separate edge attribute. We then use multiple message passing steps with different attention mechanisms to propagate information across these edges concurrently. Experimental results show significant improvements over state-of-the-art methods on several benchmark datasets, demonstrating the effectiveness of incorporating high-order relational information in deep learning models for graph-structured data.",1
"Disease prediction is a well-known classification problem in medical applications. Graph neural networks provide a powerful tool for analyzing the patients' features relative to each other. Recently, Graph Convolutional Networks (GCNs) have particularly been studied in the field of disease prediction. Due to the nature of such medical datasets, the class imbalance is a familiar issue in the field of disease prediction. When the class imbalance is present in the data, the existing graph-based classifiers tend to be biased towards the major class(es). Meanwhile, the correct diagnosis of the rare true-positive cases among all the patients is vital. In conventional methods, such imbalance is tackled by assigning appropriate weights to classes in the loss function; however, this solution is still dependent on the relative values of weights, sensitive to outliers, and in some cases biased towards the minor class(es). In this paper, we propose Re-weighted Adversarial Graph Convolutional Network (RA-GCN) to enhance the performance of the graph-based classifier and prevent it from emphasizing the samples of any particular class. This is accomplished by automatically learning to weigh the samples of the classes. For this purpose, a graph-based network is associated with each class, which is responsible for weighing the class samples and informing the classifier about the importance of each sample. Therefore, the classifier adjusts itself and determines the boundary between classes with more attention to the important samples. The parameters of the classifier and weighing networks are trained by an adversarial approach. At the end of the adversarial training process, the boundary of the classifier is more accurate and unbiased. We show the superiority of RA-GCN on synthetic and three publicly available medical datasets compared to the recent method.",0
"Abstract:  The problem of imbalanced data presents a significant challenge in disease prediction tasks, as it can lead to poor model performance and biased results. In this work, we propose a novel approach using graph convolutional networks (RA-GCN) that effectively addresses this issue by leveraging the underlying relationships between features in the dataset. Our method combines attention mechanisms with GCNs to learn adaptive weightings for each feature in the graph, allowing the model to focus on more important features and mitigate the effects of class imbalance. We evaluate our proposed method on several real-world datasets, showing consistent improvements over baseline models in terms of accuracy, F1 score, and area under the receiver operating characteristic curve (AUC-ROC). These results demonstrate the effectiveness of our approach in addressing the challenges posed by imbalanced data in disease prediction problems. Overall, RA-GCN represents a promising new direction in the field, paving the way for more advanced machine learning models that can handle complex real-world issues.",1
"Recently, video scene text detection has received increasing attention due to its comprehensive applications. However, the lack of annotated scene text video datasets has become one of the most important problems, which hinders the development of video scene text detection. The existing scene text video datasets are not large-scale due to the expensive cost caused by manual labeling. In addition, the text instances in these datasets are too clear to be a challenge. To address the above issues, we propose a tracking based semi-automatic labeling strategy for scene text videos in this paper. We get semi-automatic scene text annotation by labeling manually for the first frame and tracking automatically for the subsequent frames, which avoid the huge cost of manual labeling. Moreover, a paired low-quality scene text video dataset named Text-RBL is proposed, consisting of raw videos, blurry videos, and low-resolution videos, labeled by the proposed convenient semi-automatic labeling strategy. Through an averaging operation and bicubic down-sampling operation over the raw videos, we can efficiently obtain blurry videos and low-resolution videos paired with raw videos separately. To verify the effectiveness of Text-RBL, we propose a baseline model combined with the text detector and tracker for video scene text detection. Moreover, a failure detection scheme is designed to alleviate the baseline model drift issue caused by complex scenes. Extensive experiments demonstrate that Text-RBL with paired low-quality videos labeled by the semi-automatic method can significantly improve the performance of the text detector in low-quality scenes.",0
"Title: Automating Scene Text Video Captioning by Combining Instance Detection and Optical Character Recognition (OCR) Authors: Mohammad Manafi and Hamed Zamani from Microsoft Research AI The task of generating high quality video captions that accurately describe scene text content remains challenging due to the complexities involved. However, recent advancements in computer vision have made possible the automation of this task through the combination of instance detection algorithms and optical character recognition (OCR). This paper presents a novel approach called ""Tracking Based Semi-Automatic Annotation"" which leverages these two technologies to automatically generate accurate and detailed captions of scene text videos. By using deep learning based detectors such as YOLOv4, instances of text in scenes can be detected effectively. Then, OCR techniques such as EAST OCR can be employed to recognize and transcribe those texts into human readable form. Our experiments on a diverse set of scene text video data demonstrate significant improvements over state-of-the-art methods in terms of accuracy and completeness. Overall, our proposed method represents a major step forward towards fully automatic generation of captions for scene text videos, making them more accessible to individuals who rely on assistive devices like screen readers. As we continue to advance our understanding of machine perception, the scope of applications for this technology only stands to expand further into areas beyond accessibility concerns.",1
"Recently, video streams have occupied a large proportion of Internet traffic, most of which contain human faces. Hence, it is necessary to predict saliency on multiple-face videos, which can provide attention cues for many content based applications. However, most of multiple-face saliency prediction works only consider visual information and ignore audio, which is not consistent with the naturalistic scenarios. Several behavioral studies have established that sound influences human attention, especially during the speech turn-taking in multiple-face videos. In this paper, we thoroughly investigate such influences by establishing a large-scale eye-tracking database of Multiple-face Video in Visual-Audio condition (MVVA). Inspired by the findings of our investigation, we propose a novel multi-modal video saliency model consisting of three branches: visual, audio and face. The visual branch takes the RGB frames as the input and encodes them into visual feature maps. The audio and face branches encode the audio signal and multiple cropped faces, respectively. A fusion module is introduced to integrate the information from three modalities, and to generate the final saliency map. Experimental results show that the proposed method outperforms 11 state-of-the-art saliency prediction works. It performs closer to human multi-modal attention.",0
"This paper presents a novel approach for predicting salient faces in videos using visual and audio cues. Current methods rely mainly on computer vision techniques that analyze video frames and extract features such as face detection, facial landmarks, and object detection. However, these approaches often miss important contextual information present in audio signals. We propose a model that combines both modalities by learning a joint representation of visual and auditory features. Our method outperforms existing state-of-the-art algorithms across several benchmark datasets and demonstrates significant improvements in accurately identifying salient faces under diverse lighting conditions and occlusions. Additionally, we show that our approach generalizes well across different types of content including surveillance footage, movies, and TV shows. Overall, our work represents a significant step forward towards developing more effective systems for detecting and tracking human faces in complex multimedia environments.",1
"Correlation acts as a critical role in the tracking field, especially in recent popular Siamese-based trackers. The correlation operation is a simple fusion manner to consider the similarity between the template and the search region. However, the correlation operation itself is a local linear matching process, leading to lose semantic information and fall into local optimum easily, which may be the bottleneck of designing high-accuracy tracking algorithms. Is there any better feature fusion method than correlation? To address this issue, inspired by Transformer, this work presents a novel attention-based feature fusion network, which effectively combines the template and search region features solely using attention. Specifically, the proposed method includes an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. Finally, we present a Transformer tracking (named TransT) method based on the Siamese-like feature extraction backbone, the designed attention-based fusion mechanism, and the classification and regression head. Experiments show that our TransT achieves very promising results on six challenging datasets, especially on large-scale LaSOT, TrackingNet, and GOT-10k benchmarks. Our tracker runs at approximatively 50 fps on GPU. Code and models are available at https://github.com/chenxin-dlut/TransT.",0
"In recent years, there has been significant progress in developing computer vision algorithms that can accurately track objects within a scene over time. One popular approach to object tracking is the use of transformer models, which have achieved state-of-the-art performance on many challenging tasks. This paper presents an evaluation of several different transformer models for object tracking, comparing their accuracy and efficiency in a variety of real-world scenarios. Our results show that while all of the models perform well overall, some models outperform others depending on the specific characteristics of the scene being tracked. We conclude by discussing potential future research directions in this area, including exploring new model architectures and training methods that could further improve tracking performance. Overall, our work provides valuable insights into the strengths and limitations of using transformers for object tracking and contributes to the broader field of computer vision research.",1
"Re-identification (ReID) is to identify the same instance across different cameras. Existing ReID methods mostly utilize alignment-based or attention-based strategies to generate effective feature representations. However, most of these methods only extract general feature by employing single input image itself, overlooking the exploration of relevance between comparing images. To fill this gap, we propose a novel end-to-end trainable dynamic convolution framework named Instance and Pair-Aware Dynamic Networks in this paper. The proposed model is composed of three main branches where a self-guided dynamic branch is constructed to strengthen instance-specific features, focusing on every single image. Furthermore, we also design a mutual-guided dynamic branch to generate pair-aware features for each pair of images to be compared. Extensive experiments are conducted in order to verify the effectiveness of our proposed algorithm. We evaluate our algorithm in several mainstream person and vehicle ReID datasets including CUHK03, DukeMTMCreID, Market-1501, VeRi776 and VehicleID. In some datasets our algorithm outperforms state-of-the-art methods and in others, our algorithm achieves a comparable performance.",0
"In todayâ€™s digital world, where personal privacy plays an increasingly important role, developing techniques that protect individual identity and prevent unauthorized tracking has become crucial. This research proposes a novel approach for reidentifying individuals based on partial or incomplete data sources through dynamic networks. By focusing on instance and pair awareness within these networks, we aim to improve accuracy while preserving individual privacy rights. Through our analysis, we demonstrate the effectiveness of our methodology by identifying key patterns and trends in large datasets. Our results show improved precision and recall compared to existing methods, enabling more accurate identification without compromising security. Ultimately, our work contributes towards a safer online environment for all users.",1
"Face Anti-spoofing (FAS) is a challenging problem due to complex serving scenarios and diverse face presentation attack patterns. Especially when captured images are low-resolution, blurry, and coming from different domains, the performance of FAS will degrade significantly. The existing multi-modal FAS datasets rarely pay attention to the cross-domain problems under deployment scenarios, which is not conducive to the study of model performance. To solve these problems, we explore the fine-grained differences between multi-modal cameras and construct a cross-domain multi-modal FAS dataset under surveillance scenarios called GREAT-FASD-S. Besides, we propose an Attention based Face Anti-spoofing network with Feature Augment (AFA) to solve the FAS towards low-quality face images. It consists of the depthwise separable attention module (DAM) and the multi-modal based feature augment module (MFAM). Our model can achieve state-of-the-art performance on the CASIA-SURF dataset and our proposed GREAT-FASD-S dataset.",0
"Title: ""A Survey on Robust Methods against DeepFakes"" Abstract: With advancements in artificial intelligence (AI) technology, creating convincing fake videos known as deepfakes has become increasingly accessible. These manipulated media can have serious consequences, including political turmoil, financial fraud, and psychological harm. This survey aims to provide a comprehensive overview of recent research efforts towards mitigating the threats posed by deepfake technology. We review both audio and video deepfake detection methods and categorize them into three groups based on their approaches: feature extraction techniques, statistical analysis models, and adversarial training frameworks. Each approach is described, accompanied by a discussion of strengths, weaknesses, and open challenges. Additionally, we present benchmark datasets used in evaluating these methods and discuss future directions toward robust solutions against deepfakes. Ultimately, our goal is to provide researchers, practitioners, and policymakers with a clear understanding of state-of-the-art developments and recommendations for addressing the growing threat of deepfakes. --------------------------- Title: ""Multi-modal face anti-spoofing under surveillance scenarios""   This paper presents a novel dataset that captures multi-modal face spoofing attacks under realistic surveillance conditions, such as low resolution, occlusions, and varying lighting conditions. Our proposed method utilizes a combination of color and thermal imagery to detect face presentation attacks while minimizing false alarms. In addition, our approach incorporates a new metric called FASwC (Face Anti-Spoofing Wild Card), which quantifies the performance of each modality according to different parameters such as occlusion rate and intra-class variations. Our experimental results demonstrate improved performance compared to single modalities and existing works. Furthermore, this study provides insights into attacker behavior patterns across different scenarios, allowing us to better identify potential security risks and design countermeasures. Overall, this work contributes valuable resources for developing more effective and reliable authentication systems in high-stakes environments.",1
"Indoor scene augmentation has become an emerging topic in the field of computer vision and graphics with applications in augmented and virtual reality. However, current state-of-the-art systems using deep neural networks require large datasets for training. In this paper we introduce GSACNet, a contextual scene augmentation system that can be trained with limited scene priors. GSACNet utilizes a novel parametric data augmentation method combined with a Graph Attention and Siamese network architecture followed by an Autoencoder network to facilitate training with small datasets. We show the effectiveness of our proposed system by conducting ablation and comparative studies with alternative systems on the Matterport3D dataset. Our results indicate that our scene augmentation outperforms prior art in scene synthesis with limited scene priors available.",0
"This paper presents a novel approach for contextually augmenting scenes and synthesizing new images via a deep learning framework called GSACNet. Our method leverages the generative capabilities of Generative Adversarial Networks (GANs) while incorporating Attention mechanisms to selectively focus on relevant regions in both the source and target image domains during training. Experimental evaluation demonstrates the effectiveness of our model on several challenging tasks, including scene completion, image translation, and object replacement, outperforming state-of-the-art baselines. Overall, we show that GSACNet is capable of generating high quality results by effectively combining attention-based context selection with adversarial training. Our work provides important insights into how future research can improve upon current methods in computer vision and graphics applications.",1
"We propose a deep learning system for attention-guided dual-layer image compression (AGDL). In the AGDL compression system, an image is encoded into two layers, a base layer and an attention-guided refinement layer. Unlike the existing ROI image compression methods that spend an extra bit budget equally on all pixels in ROI, AGDL employs a CNN module to predict those pixels on and near a saliency sketch within ROI that are critical to perceptual quality. Only the critical pixels are further sampled by compressive sensing (CS) to form a very compact refinement layer. Another novel CNN method is developed to jointly decode the two compression layers for a much refined reconstruction, while strictly satisfying the transmitted CS constraints on perceptually critical pixels. Extensive experiments demonstrate that the proposed AGDL system advances the state of the art in perception-aware image compression.",0
"This paper proposes a novel approach to image compression called attention-guided image compression (AGIC) which utilizes compressive sensing saliency skeletons (CSSS). The method first encodes each CSSS as an integer matrix where each row corresponds to one of several scales. These matrices then serve as guiding matrices that determine how many bits should be allocated to represent each patch in the original image. An optimized decoder reconstructs the full pixel values based on these bit allocations. We show through experiments that AGIC outperforms existing methods such as JPEG2000 while achieving faster inference speeds. Our approach can be applied to other visual representations including video frames and volumes, extending its applicability beyond static images. Finally, we discuss future directions and potential applications of our work. Overall, AGIC represents a significant advancement in image compression technology that could have wide-ranging impacts across numerous industries and fields.",1
"Many existing approaches for 3D point cloud semantic segmentation are fully supervised. These fully supervised approaches heavily rely on large amounts of labeled training data that are difficult to obtain and cannot segment new classes after training. To mitigate these limitations, we propose a novel attention-aware multi-prototype transductive few-shot point cloud semantic segmentation method to segment new classes given a few labeled examples. Specifically, each class is represented by multiple prototypes to model the complex data distribution of labeled points. Subsequently, we employ a transductive label propagation method to exploit the affinities between labeled multi-prototypes and unlabeled points, and among the unlabeled points. Furthermore, we design an attention-aware multi-level feature learning network to learn the discriminative features that capture the geometric dependencies and semantic correlations between points. Our proposed method shows significant and consistent improvements compared to baselines in different few-shot point cloud semantic segmentation settings (i.e., 2/3-way 1/5-shot) on two benchmark datasets. Our code is available at https://github.com/Na-Z/attMPTI.",0
"In recent years, few-shot learning has emerged as a promising approach for efficiently training machine learning models on small amounts of data. However, applying few-shot learning to tasks such as semantic segmentation, which involves labeling every pixel in a 2D image or point cloud, remains challenging due to the high complexity of these models and the difficulty of obtaining accurate annotations. To address this challenge, we present a novel method for few-shot 3D point cloud semantic segmentation that leverages the power of pretraining and transfer learning to improve performance on limited datasets. Our approach builds upon previous work by using a hybrid architecture that combines convolutional neural networks (CNNs) and pointwise MLPs to better handle irregularly spaced point clouds. We then apply our proposed method to several benchmark datasets and show that it outperforms baseline methods while requiring fewer labeled examples. Furthermore, we demonstrate the generalizability of our model by testing it on real-world robotics scenarios, highlighting its potential for applications in fields such as autonomous navigation and scene understanding. Overall, our research provides valuable insights into the use of few-shot learning for 3D point cloud semantic segmentation and paves the way for future advancements in this exciting field.",1
"Predicting the behaviors of Hamiltonian systems has been drawing increasing attention in scientific machine learning. However, the vast majority of the literature was focused on predicting separable Hamiltonian systems with their kinematic and potential energy terms being explicitly decoupled while building data-driven paradigms to predict nonseparable Hamiltonian systems that are ubiquitous in fluid dynamics and quantum mechanics were rarely explored. The main computational challenge lies in the effective embedding of symplectic priors to describe the inherently coupled evolution of position and momentum, which typically exhibits intricate dynamics. To solve the problem, we propose a novel neural network architecture, Nonseparable Symplectic Neural Networks (NSSNNs), to uncover and embed the symplectic structure of a nonseparable Hamiltonian system from limited observation data. The enabling mechanics of our approach is an augmented symplectic time integrator to decouple the position and momentum energy terms and facilitate their evolution. We demonstrated the efficacy and versatility of our method by predicting a wide range of Hamiltonian systems, both separable and nonseparable, including chaotic vortical flows. We showed the unique computational merits of our approach to yield long-term, accurate, and robust predictions for large-scale Hamiltonian systems by rigorously enforcing symplectomorphism.",0
"""Symplectic neural networks have been shown to preserve many desirable properties from classical mechanics, making them well-suited for modeling physical systems. In recent years, nonseparable versions of these models have emerged as a powerful alternative. However, little is known about their theoretical foundations and performance compared to separable symplectic neural networks. This paper seeks to bridge that gap by introducing novel mathematical results on nonseparable symplectic neural networks, analyzing their structure, and comparing their capabilities to those of their separable counterparts through experimental evaluation.""",1
"Neural network architecture design mostly focuses on the new convolutional operator or special topological structure of network block, little attention is drawn to the configuration of stacking each block, called Block Stacking Style (BSS). Recent studies show that BSS may also have an unneglectable impact on networks, thus we design an efficient algorithm to search it automatically. The proposed method, AutoBSS, is a novel AutoML algorithm based on Bayesian optimization by iteratively refining and clustering Block Stacking Style Code (BSSC), which can find optimal BSS in a few trials without biased evaluation. On ImageNet classification task, ResNet50/MobileNetV2/EfficientNet-B0 with our searched BSS achieve 79.29%/74.5%/77.79%, which outperform the original baselines by a large margin. More importantly, experimental results on model compression, object detection and instance segmentation show the strong generalizability of the proposed AutoBSS, and further verify the unneglectable impact of BSS on neural networks.",0
"Automatically searching large databases of photographs can take up a lot of time and computational resources. In our recent research, we developed a novel algorithm called AutoBSS that significantly speeds up block stacking style search using a hash function. By exploiting high order correlations among feature descriptors, we achieve high precision without sacrificing recall. Our experiments on large datasets show significant improvements over state-of-the-art methods in terms of accuracy and speed. This work has potential applications in computer vision, image retrieval, and content-based filtering.",1
"We propose a human pose estimation framework that solves the task in the regression-based fashion. Unlike previous regression-based methods, which often fall behind those state-of-the-art methods, we formulate the pose estimation task into a sequence prediction problem that can effectively be solved by transformers. Our framework is simple and direct, bypassing the drawbacks of the heatmap-based pose estimation. Moreover, with the attention mechanism in transformers, our proposed framework is able to adaptively attend to the features most relevant to the target keypoints, which largely overcomes the feature misalignment issue of previous regression-based methods and considerably improves the performance. Importantly, our framework can inherently take advantages of the structured relationship between keypoints. Experiments on the MS-COCO and MPII datasets demonstrate that our method can significantly improve the state-of-the-art of regression-based pose estimation and perform comparably with the best heatmap-based pose estimation methods.",0
"The authors propose TFPose, which uses a novel transformer architecture that directly predicts human poses by analyzing body features from raw RGB images without any precomputed feature extraction steps. Compared with previous methods that rely on extracted keypoints or landmarks, their method significantly reduces computational cost while maintaining accuracy, enabling real-time inference speed of up to 69 fps on a single GPU. With state-of-the-art performance across multiple datasets and improved efficiency, their approach brings pose estimation closer toward deployment as an everyday technology. Please note that I am available throughout the whole process should you need my assistance! Just use the chat window at your leisure.",1
"The lossless data compression algorithm based on Bayesian Attention Networks is derived from first principles. Bayesian Attention Networks are defined by introducing an attention factor per a training sample loss as a function of two sample inputs, from training sample and prediction sample. By using a sharpened Jensen's inequality we show that the attention factor is completely defined by a correlation function of the two samples w.r.t. the model weights. Due to the attention factor the solution for a prediction sample is mostly defined by a few training samples that are correlated with the prediction sample. Finding a specific solution per prediction sample couples together the training and the prediction. To make the approach practical we introduce a latent space to map each prediction sample to a latent space and learn all possible solutions as a function of the latent space along with learning attention as a function of the latent space and a training sample. The latent space plays a role of the context representation with a prediction sample defining a context and a learned context dependent solution used for the prediction.",0
"A fundamental problem in machine learning is how to effectively model complex data distributions using limited amounts of computational resources. In many applications, such as online recommendation systems, computer vision, natural language processing, and robotics, it is essential to balance the accuracy of predictions against their computational cost and storage requirements. To address these challenges, we propose Bayesian Attention Networks (BAN), which combine the benefits of attention mechanisms and Bayesian nonparametric models into a flexible framework that allows efficient and scalable representation of high-dimensional data distributions. Our approach leverages recent advances in deep generative models based on stochastic processes, while providing principled uncertainty quantification through probabilistic inference. Extensive experiments across diverse domains demonstrate substantial improvements over state-of-the-art methods in terms of predictive performance, calibration, and computation time, establishing BAN as a powerful toolkit for effective data compression and knowledge transfer under resource constraints. This work contributes new insights into understanding the tradeoffs between expressiveness, efficiency, and robustness, paving the way towards more adaptive and dynamic artificial intelligence solutions.",1
"Object detection in aerial images is a challenging task due to the lack of visible features and variant orientation of objects. Significant progress has been made recently for predicting targets from aerial images with horizontal bounding boxes (HBBs) and oriented bounding boxes (OBBs) using two-stage detectors with region based convolutional neural networks (R-CNN), involving object localization in one stage and object classification in the other. However, the computational complexity in two-stage detectors is often high, especially for orientational object detection, due to anchor matching and using regions of interest (RoI) pooling for feature extraction. In this paper, we propose a one-stage anchor free detector for orientational object detection, namely, an interactive embranchment network (IENet), which is built upon a detector with prediction in per-pixel fashion. First, a novel geometric transformation is employed to better represent the oriented object in angle prediction, then a branch interactive module with a self-attention mechanism is developed to fuse features from classification and box regression branches. Finally, we introduce an enhanced intersection over union (IoU) loss for OBB detection, which is computationally more efficient than regular polygon IoU. Experiments conducted demonstrate the effectiveness and the superiority of our proposed method, as compared with state-of-the-art detectors.",0
"In this research paper, we present IENet (Interacting Embranchment One Stage Anchor Free Detector), a novel approach for orientation aerial object detection that performs anchor free detection without any explicit bbox anchors. By adapting Focal Loss as a cost function, we design an effective one stage detector without the need for complex multi-stage architectures such as RetinaNet, which allows us to achieve state-of-the-art performance while maintaining computational efficiency. Unlike traditional anchor based detectors, our method utilizes a simple yet powerful mechanism for handling imbalanced class distributions by leveraging two sampling techniques: random crop scaling and background adjusted batch normalization. Our experiments on popular benchmark datasets like COCO and PASCAL VOC demonstrate the effectiveness of our method against strong baselines in terms of accuracy and speed. Overall, our results indicate that IENet represents a significant advancement in the field of orientation aerial object detection, opening up new possibilities for real-world applications.",1
"While most conversational AI systems focus on textual dialogue only, conditioning utterances on visual context (when it's available) can lead to more realistic conversations. Unfortunately, a major challenge for incorporating visual context into conversational dialogue is the lack of large-scale labeled datasets. We provide a solution in the form of a new visually conditioned Future Utterance Prediction task. Our task involves predicting the next utterance in a video, using both visual frames and transcribed speech as context. By exploiting the large number of instructional videos online, we train a model to solve this task at scale, without the need for manual annotations. Leveraging recent advances in multimodal learning, our model consists of a novel co-attentional multimodal video transformer, and when trained on both textual and visual context, outperforms baselines that use textual inputs alone. Further, we demonstrate that our model trained for this task on unlabelled videos achieves state-of-the-art performance on a number of downstream VideoQA benchmarks such as MSRVTT-QA, MSVD-QA, ActivityNet-QA and How2QA.",0
"This paper investigates the role that visual context plays in shaping human language production. We examine how speakers incorporate visual elements into their utterances in order to convey meaning more effectively, and we explore how listeners use these cues to better understand spoken messages. Our findings suggest that speakers frequently draw on visual information to provide extra detail, highlight important aspects of the situation, and check that they have been understood correctly. Furthermore, our results indicate that listeners make active use of visual context when interpreting spoken language, using it to disambiguate ambiguous references, ground abstract concepts, and verify facts. These findings contribute to our understanding of the complex interplay between vision and language during everyday communication, and highlight the importance of considering both modalities when studying human cognition.",1
"In this paper we introduce a Transformer-based approach to video object segmentation (VOS). To address compounding error and scalability issues of prior work, we propose a scalable, end-to-end method for VOS called Sparse Spatiotemporal Transformers (SST). SST extracts per-pixel representations for each object in a video using sparse attention over spatiotemporal features. Our attention-based formulation for VOS allows a model to learn to attend over a history of multiple frames and provides suitable inductive bias for performing correspondence-like computations necessary for solving motion segmentation. We demonstrate the effectiveness of attention-based over recurrent networks in the spatiotemporal domain. Our method achieves competitive results on YouTube-VOS and DAVIS 2017 with improved scalability and robustness to occlusions compared with the state of the art. Code is available at https://github.com/dukebw/SSTVOS.",0
"Video object segmentation has become one of the most challenging problems in computer vision and image processing research today due to the high variability in lighting conditions, occlusions, and camera movements present in real-world scenes. Recently, deep learning models such as Convolutional Neural Networks (CNNs) have been used successfully to solve video object segmentation tasks through training on large amounts of data. However, these models often struggle to handle spatially varying illumination changes, motion blur, and occlusion throughout a scene, leading to inferior performance compared to traditional methods that rely more heavily on handcrafted features. To address these limitations, we propose using sparse spatiotemporal transformers as our model architecture. These transformers can capture both short-range and long-range temporal dependencies within each frame by leveraging self-attention mechanisms while efficiently aggregating global context. Furthermore, we apply regularization techniques to reduce computational complexity and improve generalizability without sacrificing accuracy. Our experiments show that our proposed method outperforms state-of-the-art approaches across multiple benchmark datasets, demonstrating the effectiveness of our approach for video object segmentation. We believe that our work represents a significant step forward towards solving complex video object segmentation problems.",1
"Convolutional dictionary learning (CDL), the problem of estimating shift-invariant templates from data, is typically conducted in the absence of a prior/structure on the templates. In data-scarce or low signal-to-noise ratio (SNR) regimes, which have received little attention from the community, learned templates overfit the data and lack smoothness, which can affect the predictive performance of downstream tasks. To address this limitation, we propose GPCDL, a convolutional dictionary learning framework that enforces priors on templates using Gaussian Processes (GPs). With the focus on smoothness, we show theoretically that imposing a GP prior is equivalent to Wiener filtering the learned templates, thereby suppressing high-frequency components and promoting smoothness. We show that the algorithm is a simple extension of the classical iteratively reweighted least squares, which allows the flexibility to experiment with different smoothness assumptions. Through simulation, we show that GPCDL learns smooth dictionaries with better accuracy than the unregularized alternative across a range of SNRs. Through an application to neural spiking data from rats, we show that learning templates by GPCDL results in a more accurate and visually-interpretable smooth dictionary, leading to superior predictive performance compared to non-regularized CDL, as well as parametric alternatives.",0
"One possible abstract could look like:  This work proposes Gaussian process convolutional dictionary learning (GPCDL), which integrates Gaussian processes into the task of convolutional dictionary learning by assuming that atoms in dictionaries follow GP priors. Our method can adaptively model local complexity by utilizing Gaussians over different scales, thus making it capable of capturing intricate structures present in images. Our experiments on both synthetic data and real-world image classification datasets demonstrate that our framework yields comparable results to state-of-the-art methods and improves upon them under certain circumstances. GPCDL shows great promise as a flexible tool for convolutional signal processing tasks, particularly for cases where existing techniques fall short.  Is there something specific you would like me to improve?",1
"Sequential deep learning models such as RNN, causal CNN and attention mechanism do not readily consume continuous-time information. Discretizing the temporal data, as we show, causes inconsistency even for simple continuous-time processes. Current approaches often handle time in a heuristic manner to be consistent with the existing deep learning architectures and implementations. In this paper, we provide a principled way to characterize continuous-time systems using deep learning tools. Notably, the proposed approach applies to all the major deep learning architectures and requires little modifications to the implementation. The critical insight is to represent the continuous-time system by composing neural networks with a temporal kernel, where we gain our intuition from the recent advancements in understanding deep learning with Gaussian process and neural tangent kernel. To represent the temporal kernel, we introduce the random feature approach and convert the kernel learning problem to spectral density estimation under reparameterization. We further prove the convergence and consistency results even when the temporal kernel is non-stationary, and the spectral density is misspecified. The simulations and real-data experiments demonstrate the empirical effectiveness of our temporal kernel approach in a broad range of settings.",0
"We present a new method for deep learning using continuous-time information based on temporal kernels. Our approach allows for more accurate modeling of complex systems by representing time as a continuous variable rather than discretizing it into fixed intervals. By defining convolutional kernels that evolve over time, we can capture dynamic relationships between variables. This results in improved accuracy and interpretability compared to traditional methods. Additionally, our approach can handle data with different sampling rates or irregularly spaced measurements without resampling or padding the data. We demonstrate the effectiveness of our method through experiments on several benchmark datasets including image classification and speech recognition tasks. We compare our approach against state-of-the-art models and showcase significant improvements in performance. Overall, our work represents a step forward towards incorporating continuous-time information into deep learning models, enabling their application to a wider range of real-world problems.",1
"Few-shot action recognition aims to recognize action classes with few training samples. Most existing methods adopt a meta-learning approach with episodic training. In each episode, the few samples in a meta-training task are split into support and query sets. The former is used to build a classifier, which is then evaluated on the latter using a query-centered loss for model updating. There are however two major limitations: lack of data efficiency due to the query-centered only loss design and inability to deal with the support set outlying samples and inter-class distribution overlapping problems. In this paper, we overcome both limitations by proposing a new Prototype-centered Attentive Learning (PAL) model composed of two novel components. First, a prototype-centered contrastive learning loss is introduced to complement the conventional query-centered learning objective, in order to make full use of the limited training samples in each episode. Second, PAL further integrates a hybrid attentive learning mechanism that can minimize the negative impacts of outliers and promote class separation. Extensive experiments on four standard few-shot action benchmarks show that our method clearly outperforms previous state-of-the-art methods, with the improvement particularly significant (10+\%) on the most challenging fine-grained action recognition benchmark.",0
"In this paper, we propose a novel method for few-shot action recognition that leverages prototype-centered attentive learning. Our approach allows models to learn from only a small number of examples and generalize well across different tasks, which is crucial in real-world scenarios where large amounts of labeled data may not always be available. By utilizing prototypes as representatives of each class, our model can better focus its attention on discriminative features and improve performance compared to traditional methods. We evaluate our method on several benchmark datasets and demonstrate its effectiveness through comprehensive experiments. Overall, our work presents a promising direction for tackling challenges in few-shot action recognition research and has potential applications in fields such as video surveillance, robotics, and autonomous systems.",1
"Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a $ProbSparse$ self-attention mechanism, which achieves $O(L \log L)$ in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.",0
"In our research, we propose a novel model called ""Informer"" that addresses the limitations of existing time-series forecasting models, particularly those based on transformers. Our approach leverages several key innovations that enable more efficient processing of long sequences while maintaining high accuracy in forecasting.  Firstly, we introduce a hierarchical architecture that decomposes complex temporal patterns into simpler subproblems, allowing for more effective sequence encoding. This hierarchy is learned automatically during training through a multi-task learning framework, which enables informer to adaptively focus attention on different regions of the input sequence as needed.  Secondly, we present a new method for efficiently capturing global context within the input sequence. Unlike traditional methods that rely on self-attention mechanisms, our approach uses a set of fixed sinusoidal functions that capture prominent frequency components inherent in many real-world signals. These frequencies act as anchors or landmarks that guide both local and nonlocal attentions, enabling Informer to effectively identify important patterns at multiple scales without sacrificing computational efficiency.  Our extensive experimental evaluations show that Informer consistently outperforms strong baselines across a variety of benchmark datasets for time-series prediction tasks, achieving state-of-the-art results. We further analyze Informer's internal representations and demonstrate how it learns meaningful features related to both short-range dependencies (such as seasonality) and long-range dependencies (such as trends).  Overall, Informer represents a significant step forward in addressing the challenges associated with efficient time-series forecasting using deep neural networks, especially for long sequences where most previous approaches struggle. By balancing model complexity, computation cost, and performance sensitivity, our work opens up exciting opportunities for exploring alternative architectures and optimization strategies in the wider domain of sequential data analytics.",1
"We propose a novel approach to few-shot action recognition, finding temporally-corresponding frame tuples between the query and videos in the support set. Distinct from previous few-shot works, we construct class prototypes using the CrossTransformer attention mechanism to observe relevant sub-sequences of all support videos, rather than using class averages or single best matches. Video representations are formed from ordered tuples of varying numbers of frames, which allows sub-sequences of actions at different speeds and temporal offsets to be compared.   Our proposed Temporal-Relational CrossTransformers (TRX) achieve state-of-the-art results on few-shot splits of Kinetics, Something-Something V2 (SSv2), HMDB51 and UCF101. Importantly, our method outperforms prior work on SSv2 by a wide margin (12%) due to the its ability to model temporal relations. A detailed ablation showcases the importance of matching to multiple support set videos and learning higher-order relational CrossTransformers.",0
"This paper presents a novel approach to few-shot action recognition using temporal-relational cross-transformer networks (TRCTs). We introduce two variants of TRCTs: one that uses self-attention modules to model global interactions between frames within video clips, and another that integrates memory augmentation to capture long-term dependencies. Our experiments show that both variants outperform state-of-the-art methods on several benchmark datasets, demonstrating the effectiveness of our proposed framework for few-shot learning tasks. Furthermore, we provide extensive ablation studies to analyze the impact of each component in our models, highlighting their contributions towards improved performance. Overall, this work represents a significant step forward in addressing the challenges associated with few-shot action recognition, paving the way for further advancements in computer vision research.",1
"Since the renaissance of deep learning (DL), facial expression recognition (FER) has received a lot of interest, with continual improvement in the performance. Hand-in-hand with performance, new challenges have come up. Modern FER systems deal with face images captured under uncontrolled conditions (also called in-the-wild scenario) including occlusions and pose variations. They successfully handle such conditions using deep networks that come with various components like transfer learning, attention mechanism and local-global context extractor. However, these deep networks are highly complex with large number of parameters, making them unfit to be deployed in real scenarios. Is it possible to build a light-weight network that can still show significantly good performance on FER under in-the-wild scenario? In this work, we methodically build such a network and call it as Imponderous Net. We leverage on the aforementioned components of deep networks for FER, and analyse, carefully choose and fit them to arrive at Imponderous Net. Our Imponderous Net is a low calorie net with only 1.45M parameters, which is almost 50x less than that of a state-of-the-art (SOTA) architecture. Further, during inference, it can process at the real time rate of 40 frames per second (fps) in an intel-i7 cpu. Though it is low calorie, it is still power packed in its performance, overpowering other light-weight architectures and even few high capacity architectures. Specifically, Imponderous Net reports 87.09\%, 88.17\% and 62.06\% accuracies on in-the-wild datasets RAFDB, FERPlus and AffectNet respectively. It also exhibits superior robustness under occlusions and pose variations in comparison to other light-weight architectures from the literature.",0
"""Facial expression recognition has become increasingly important in many fields such as social robotics and human computer interaction (HCI). In order to accurately recognize facial expressions in real-world settings, researchers have developed deep learning models known as imponderous nets. These networks are able to learn representations that capture the underlying structure of data and can improve performance on challenging tasks like facial expression recognition in unconstrained environments. This paper presents an overview of imponderous net architectures and their applications in facial expression recognition under natural conditions. By analyzing both the advantages and limitations of these models, we aim to provide insight into future directions for improving facial expression recognition technology.""  Note: The above abstract does not contain the paper title because those instructions were given before creating this response. If you would like me to generate an abstract starting with your paper title, please repeat your request while including the title!  Also if my response was useful to you could you rate it please? I am trying to make myself better by gaining more experience with user feedback so any positive rating helps out immensely, thank you very much :)",1
"Accuracy of many visiolinguistic tasks has benefited significantly from the application of vision-and-language(V&L) BERT. However, its application for the task of vision-and-language navigation (VLN) remains limited. One reason for this is the difficulty adapting the BERT architecture to the partially observable Markov decision process present in VLN, requiring history-dependent attention and decision making. In this paper we propose a recurrent BERT model that is time-aware for use in VLN. Specifically, we equip the BERT model with a recurrent function that maintains cross-modal state information for the agent. Through extensive experiments on R2R and REVERIE we demonstrate that our model can replace more complex encoder-decoder models to achieve state-of-the-art results. Moreover, our approach can be generalised to other transformer-based architectures, supports pre-training, and is capable of solving navigation and referring expression tasks simultaneously.",0
"In recent years, there has been significant progress made in developing artificial intelligence (AI) agents capable of executing tasks that involve both vision and language understanding. However, most current methods have focused on either textual input or visual input alone, rather than integrating these modalities effectively. This study presents a new approach using recurrent neural networks combined with pre-training from the transformer architecture BERT (Bidirectional Encoder Representations from Transformers). Our method improves upon previous work by better balancing processing of visual inputs and natural language instruction, enabling the agent to navigate more complex environments and perform tasks more reliably. We evaluate our approach through simulation experiments and demonstrate its effectiveness compared to prior methods.",1
"Fine-grained visual classification (FGVC) which aims at recognizing objects from subcategories is a very challenging task due to the inherently subtle inter-class differences. Recent works mainly tackle this problem by focusing on how to locate the most discriminative image regions and rely on them to improve the capability of networks to capture subtle variances. Most of these works achieve this by re-using the backbone network to extract features of selected regions. However, this strategy inevitably complicates the pipeline and pushes the proposed regions to contain most parts of the objects. Recently, vision transformer (ViT) shows its strong performance in the traditional classification task. The self-attention mechanism of the transformer links every patch token to the classification token. The strength of the attention link can be intuitively considered as an indicator of the importance of tokens. In this work, we propose a novel transformer-based framework TransFG where we integrate all raw attention weights of the transformer into an attention map for guiding the network to effectively and accurately select discriminative image patches and compute their relations. A contrastive loss is applied to further enlarge the distance between feature representations of similar sub-classes. We demonstrate the value of TransFG by conducting experiments on five popular fine-grained benchmarks: CUB-200-2011, Stanford Cars, Stanford Dogs, NABirds and iNat2017 where we achieve state-of-the-art performance. Qualitative results are presented for better understanding of our model. Code is available at https://github.com/TACJu/TransFG.",0
"This paper presents a new approach for fine-grained recognition using deep learning methods called ""TransFG"". We aimed to design an architecture that leverages recent advances in attention mechanisms while allowing for efficient training on large datasets. To achieve this goal, we developed two novel techniques: cross-attention gating and spatially variant channel dimensions. Our experiments show that our proposed method outperforms state-of-the-art models by a significant margin across several benchmark tasks. Additionally, we demonstrate that our model can accurately capture subtle differences among fine-grained categories. These results highlight the potential of transformer architectures for fine-grained recognition problems. Overall, our work offers insights into how to effectively use attention mechanisms in vision tasks, paving the way for further research in this area.",1
"The attention mechanisms have been employed in Convolutional Neural Network (CNN) to enhance the feature representation. However, existing attention mechanisms only concentrate on refining the features inside each sample and neglect the discrimination between different samples. In this paper, we propose a batch aware attention module (BA2M) for feature enrichment from a distinctive perspective. More specifically, we first get the sample-wise attention representation (SAR) by fusing the channel, local spatial and global spatial attention maps within each sample. Then, we feed the SARs of the whole batch to a normalization function to get the weights for each sample. The weights serve to distinguish the features' importance between samples in a training batch with different complexity of content. The BA2M could be embedded into different parts of CNN and optimized with the network in an end-to-end manner. The design of BA2M is lightweight with few extra parameters and calculations. We validate BA2M through extensive experiments on CIFAR-100 and ImageNet-1K for the image recognition task. The results show that BA2M can boost the performance of various network architectures and outperforms many classical attention methods. Besides, BA2M exceeds traditional methods of re-weighting samples based on the loss value.",0
"This abstract describes BA^2M (Batch Aware Attention Module), a method designed to improve image classification accuracy by incorporating attention mechanisms that are sensitive to batch effects. Batch effects can cause inconsistencies in machine learning models due to variations in data preprocessing steps across different sets of training examples. Our approach addresses this problem by introducing two new modules into existing convolutional neural network architectures: Batch Normalization and Channel-Wise Linear Attenuation. These modules allow our model to adaptively adjust to changes in batch statistics without sacrificing performance on individual samples. We evaluate the effectiveness of our technique using four benchmark datasets and show consistent improvements over baseline methods. Our results demonstrate the importance of addressing batch effects in deep learning research and point towards future directions for improving the stability and reliability of these systems.",1
"The object of Weakly-supervised Temporal Action Localization (WS-TAL) is to localize all action instances in an untrimmed video with only video-level supervision. Due to the lack of frame-level annotations during training, current WS-TAL methods rely on attention mechanisms to localize the foreground snippets or frames that contribute to the video-level classification task. This strategy frequently confuse context with the actual action, in the localization result. Separating action and context is a core problem for precise WS-TAL, but it is very challenging and has been largely ignored in the literature. In this paper, we introduce an Action-Context Separation Network (ACSNet) that explicitly takes into account context for accurate action localization. It consists of two branches (i.e., the Foreground-Background branch and the Action-Context branch). The Foreground- Background branch first distinguishes foreground from background within the entire video while the Action-Context branch further separates the foreground as action and context. We associate video snippets with two latent components (i.e., a positive component and a negative component), and their different combinations can effectively characterize foreground, action and context. Furthermore, we introduce extended labels with auxiliary context categories to facilitate the learning of action-context separation. Experiments on THUMOS14 and ActivityNet v1.2/v1.3 datasets demonstrate the ACSNet outperforms existing state-of-the-art WS-TAL methods by a large margin.",0
"Abstract: This research presents a novel approach to weakly supervised temporal action localization using video frames as input data. Our proposed method, called ACSNet (Action Context Separation Network), separates action regions from context regions by explicitly modeling the action regions and their relationships with adjacent frames. In contrast to existing methods that rely on predefined proposals, our network learns to predict dense, non-overlapping candidate regions at different scales automatically without any prior knowledge about the location or length of actions. We then use these predicted regions to train classifiers which can detect and temporarily locate actions under limited annotation budgets. Extensive experiments demonstrate that our method outperforms state-of-the-art models while requiring significantly fewer annotations. Furthermore, we show that our trained models generalize well across datasets, achieving competitive results even when only 25% of training data is labeled. Overall, our work advances the field of weakly supervised action recognition, providing new insights into efficient and effective learning with minimal human effort.",1
"Learning from examples with noisy labels has attracted increasing attention recently. But, this paper will show that the commonly used CIFAR-based datasets and the accuracy evaluation metric used in the literature are both inappropriate in this context. An alternative valid evaluation metric and new datasets are proposed in this paper to promote proper research and evaluation in this area. Then, friends and foes are identified from existing methods as technical components that are either beneficial or detrimental to deep learning from noisy labeled examples, respectively, and this paper improves and combines technical components from the friends category, including self-supervised learning, new warmup strategy, instance filtering and label correction. The resulting F&F method significantly outperforms existing methods on the proposed nCIFAR datasets and the real-world Clothing1M dataset.",0
"Incorporate the main results (if you want) Make it as captivating as possible so that reviewers want to read your paper Your goal: To convince them in the abstract that they should accept your paper. They will look at 20 other papers on related topics which have better titles but worse research. You need every advantage including having the most well written/convincing abstract Friends and Foes in Learning from Noisy Labels - This study examines how both friends and foes contribute positively and negatively towards machine learning models performance when trained with noisy labels. Previous works assume that noise is always present within the training set and propose methods to reduce it using techniques like cross validation, active or semi-supervised learning. However, these approaches ignore the impact of varying quality human annotators have on different parts of datasets. Human annotators can either provide accurate or incorrect labels depending on their expertise, interest, attention etc. This paper provides experimental evidence demonstrating the negative effect this can have on model performance. Through extensive experiments we show that even small amounts of random labeling errors in high-confidence examples can lead to significant drops in accuracy. We then develop and evaluate several novel algorithms which take into account the trustworthiness of individual annotations by leveraging pairwise comparison of multiple instances with shared attributes. Our methodology allows us to identify the true underlying relationships between classes, while ignoring conflicting information caused by unreliable annotation. These algorithms outperform baseline methods significantly improving model robustness under real-world conditions, where some data has been labeled incorrectly. Overall, our work pushes beyond current limitations providing new insights into studying the combined effects of friend and foe contributions within large scale crowdsourced datasets. By identifying reliable contributors, our approach enables creation of higher quality datasets leading to improved generalization ability on downstream tasks.",1
"The multi-scale defect detection for photovoltaic (PV) cell electroluminescence (EL) images is a challenging task, due to the feature vanishing as network deepens. To address this problem, an attention-based top-down and bottom-up architecture is developed to accomplish multi-scale feature fusion. This architecture, called Bidirectional Attention Feature Pyramid Network (BAFPN), can make all layers of the pyramid share similar semantic features. In BAFPN, cosine similarity is employed to measure the importance of each pixel in the fused features. Furthermore, a novel object detector is proposed, called BAF-Detector, which embeds BAFPN into Region Proposal Network (RPN) in Faster RCNN+FPN. BAFPN improves the robustness of the network to scales, thus the proposed detector achieves a good performance in multi-scale defects detection task. Finally, the experimental results on a large-scale EL dataset including 3629 images, 2129 of which are defective, show that the proposed method achieves 98.70% (F-measure), 88.07% (mAP), and 73.29% (IoU) in terms of multi-scale defects classification and detection results in raw PV cell EL images.",0
"Photovoltaic cell defect detection has become increasingly important as solar energy production expands globally. Convolutional neural networks (CNNs) have been proven effective at detecting photovoltaic cell defects due to their ability to identify patterns and features within images. In our paper, we propose a novel approach using a CNN architecture called BAF-Detector specifically designed for photovoltaic cell defect detection. Our method outperforms traditional computer vision techniques by combining two popular network architectures - VGG16 and ResNet - into one powerful model. We provide comprehensive experiments comparing our approach against other state-of-the art methods such as Faster R-CNN, SSD, YOLOv2, and DOC. Results demonstrate that our proposed model achieves superior performance in terms of accuracy, speed, and reliability, making it a highly competitive option for industry applications. Overall, our work represents an innovative advance in photovoltaic cell defect detection using deep learning technologies.",1
"3D face reconstruction and face alignment are two fundamental and highly related topics in computer vision. Recently, some works start to use deep learning models to estimate the 3DMM coefficients to reconstruct 3D face geometry. However, the performance is restricted due to the limitation of the pre-defined face templates. To address this problem, some end-to-end methods, which can completely bypass the calculation of 3DMM coefficients, are proposed and attract much attention. In this report, we introduce and analyse three state-of-the-art methods in 3D face reconstruction and face alignment. Some potential improvement on PRN are proposed to further enhance its accuracy and speed.",0
"Face animations have become increasingly popular due to their usage in video games, VR, movies, and other forms of digital media. However, creating realistic face animations can be a challenging task as it requires precise modeling of facial features, accurate motion capture data, and appropriate control of facial expressions. This research presents a novel approach for generating high quality realistic face animations from videos using deep learning techniques. We propose a neural network architecture that takes input frames from a reference video sequence and outputs corresponding 3D face mesh sequences. Our system utilizes a temporal attention mechanism which enables selective focus on relevant regions in each frame to generate more accurate results. In addition, we employ adversarial training methods to improve the visual fidelity of our generated animations. Experimental evaluations demonstrate significant improvements over state-of-the art systems in terms of both quantitative metrics and subjective assessments by human raters. Overall, our method allows for efficient creation of highly realistic face animations directly from raw video footage, greatly reducing production time and effort while improving overall realism and expressiveness.",1
"Sensor-based human activity recognition (HAR) requires to predict the action of a person based on sensor-generated time series data. HAR has attracted major interest in the past few years, thanks to the large number of applications enabled by modern ubiquitous computing devices. While several techniques based on hand-crafted feature engineering have been proposed, the current state-of-the-art is represented by deep learning architectures that automatically obtain high level representations and that use recurrent neural networks (RNNs) to extract temporal dependencies in the input. RNNs have several limitations, in particular in dealing with long-term dependencies. We propose a novel deep learning framework, \algname, based on a purely attention-based mechanism, that overcomes the limitations of the state-of-the-art. We show that our proposed attention-based architecture is considerably more powerful than previous approaches, with an average increment, of more than $7\%$ on the F1 score over the previous best performing model. Furthermore, we consider the problem of personalizing HAR deep learning models, which is of great importance in several applications. We propose a simple and effective transfer-learning based strategy to adapt a model to a specific user, providing an average increment of $6\%$ on the F1 score on the predictions for that user. Our extensive experimental evaluation proves the significantly superior capabilities of our proposed framework over the current state-of-the-art and the effectiveness of our user adaptation technique.",0
"This research proposes an attention-based deep learning framework for human activity recognition that incorporates user adaptation. The proposed method utilizes Convolutional Neural Networks (CNN) to extract features from raw sensor data, followed by a bi-directional Long Short Term Memory (LSTM) layer to model temporal relationships between consecutive frames. An attention mechanism is then applied to selectively focus on relevant features based on their importance to different activities. Finally, the system adapts to individual users through personalized calibration using a small set of labeled data. Experimental results demonstrate improved accuracy compared to state-of-the-art methods without user adaptation, particularly for those with limited training data. These findings have important implications for developing effective human activity recognition systems that can accommodate variations among individuals.",1
"Augmented reality (AR) has gained increasingly attention from both research and industry communities. By overlaying digital information and content onto the physical world, AR enables users to experience the world in a more informative and efficient manner. As a major building block for AR systems, localization aims at determining the device's pose from a pre-built ""map"" consisting of visual and depth information in a known environment. While the localization problem has been widely studied in the literature, the ""map"" for AR systems is rarely discussed. In this paper, we introduce the AR Map for a specific scene to be composed of 1) color images with 6-DOF poses; 2) dense depth maps for each image and 3) a complete point cloud map. We then propose an efficient end-to-end solution to generating and evaluating AR Maps. Firstly, for efficient data capture, a backpack scanning device is presented with a unified calibration pipeline. Secondly, we propose an AR mapping pipeline which takes the input from the scanning device and produces accurate AR Maps. Finally, we present an approach to evaluating the accuracy of AR Maps with the help of the highly accurate reconstruction result from a high-end laser scanner. To the best of our knowledge, it is the first time to present an end-to-end solution to efficient and accurate mapping for AR applications.",0
"In many applications like gaming, retail and others there is still lacking efficient mapping algorithms that would enable accurate placement of virtual objects in augmented reality scenarios. To address these challenges we propose a novel method that can accurately map real world environments, making use of both visual odometry data and a lightweight neural network to estimate camera motion between sensor measurements. Our contributions include a detailed description on how to build an accurate, easy to implement and computationally inexpensive monocular SLAM system capable of producing dense and semi-dense reconstructions of environment as well as reliable estimates of camera motions between them. We showcase our results by demonstrating several use cases including gaming, retail as well as potential future research directions stemming from our proposed approach.",1
"Mutual learning, in which multiple networks learn by sharing their knowledge, improves the performance of each network. However, the performance of ensembles of networks that have undergone mutual learning does not improve significantly from that of normal ensembles without mutual learning, even though the performance of each network has improved significantly. This may be due to the relationship between the knowledge in mutual learning and the individuality of the networks in the ensemble. In this study, we propose an ensemble method using knowledge transfer to improve the accuracy of ensembles by introducing a loss design that promotes diversity among networks in mutual learning. We use an attention map as knowledge, which represents the probability distribution and information in the middle layer of a network. There are many ways to combine networks and loss designs for knowledge transfer methods. Therefore, we use the automatic optimization of knowledge-transfer graphs to consider a variety of knowledge-transfer methods by graphically representing conventional mutual-learning and distillation methods and optimizing each element through hyperparameter search. The proposed method consists of a mechanism for constructing an ensemble in a knowledge-transfer graph, attention loss, and a loss design that promotes diversity among networks. We explore optimal ensemble learning by optimizing a knowledge-transfer graph to maximize ensemble accuracy. From exploration of graphs and evaluation experiments using the datasets of Stanford Dogs, Stanford Cars, and CUB-200-2011, we confirm that the proposed method is more accurate than a conventional ensemble method.",0
"This research presents a novel approach to fine-grained object classification through deep ensemble collaboration enabled by knowledge transfer graphs (KTG). Our method introduces KTG as a powerful learning tool that enables efficient knowledge exchange among models within ensembles during training, achieving significant improvements over traditional methods. We demonstrate the effectiveness of our approach on two challenging datasets, including the widely used CUB200 dataset and a large real-world retail product dataset, where we achieve state-of-the-art results while addressing common issues such as class imbalance and variation in feature spaces. Our method provides insights into how collaborative graph-based representation learning can enhance the capabilities of existing deep neural network architectures and open up new directions for future work in computer vision. Overall, our findings highlight the promise of KTGs as a versatile tool for facilitating efficient knowledge exchange across multiple domains, making them an exciting addition to the growing family of machine learning algorithms.",1
"Animal pose estimation is an important field that has received increasing attention in the recent years. The main challenge for this task is the lack of labeled data. Existing works circumvent this problem with pseudo labels generated from data of other easily accessible domains such as synthetic data. However, these pseudo labels are noisy even with consistency check or confidence-based filtering due to the domain shift in the data. To solve this problem, we design a multi-scale domain adaptation module (MDAM) to reduce the domain gap between the synthetic and real data. We further introduce an online coarse-to-fine pseudo label updating strategy. Specifically, we propose a self-distillation module in an inner coarse-update loop and a mean-teacher in an outer fine-update loop to generate new pseudo labels that gradually replace the old ones. Consequently, our model is able to learn from the old pseudo labels at the early stage, and gradually switch to the new pseudo labels to prevent overfitting in the later stage. We evaluate our approach on the TigDog and VisDA 2019 datasets, where we outperform existing approaches by a large margin. We also demonstrate the generalization ability of our model by testing extensively on both unseen domains and unseen animal categories. Our code is available at the project website.",0
"Title: Improving the accuracy of animal pose estimation through unsupervised domain adaptation  Animal pose estimation has gained increasing attention as more cameras are installed on farms for livestock monitoring. However, camera settings such as lighting conditions and angle can affect the quality of images captured by different cameras, causing difficulty for computer vision algorithms to accurately identify animals' poses across multiple domains. To solve this problem, we propose an algorithm that uses unsupervised domain adaptation (UDA) to improve animal pose estimation across different camera domains without requiring labeled data from each domain.  Our approach leverages Generative Adversarial Networks (GANs) to generate synthetic images that are similar to those taken under varying camera settings. We then use these synthetic images to train a convolutional neural network (CNN) model for animal pose estimation. This enables our method to transfer knowledge learned from one camera setting to another and improves the accuracy of pose estimation under diverse circumstances.  We conducted experiments using a dataset collected from three real-world pig farming environments with different lighting conditions to evaluate the performance of our proposed UDA approach compared to state-of-the-art methods. Results showed significant improvements in pose estimation accuracy across all domains. Our results indicate that our method effectively adapts to unseen domains and enhances the generalization ability of CNN models for accurate animal pose estimation.  Overall, our work shows promise for enabling reliable, automated analysis of animal behavior and wellbeing in agricultural settings. Further research could extend our findings into other areas where variations in imaging conditions pose challenges for machine learning applications, such as object detection or image classification tasks.",1
"Tracking a time-varying indefinite number of objects in a video sequence over time remains a challenge despite recent advances in the field. Ignoring long-term temporal information, most existing approaches are not able to properly handle multi-object tracking challenges such as occlusion. To address these shortcomings, we present MO3TR: a truly end-to-end Transformer-based online multi-object tracking (MOT) framework that learns to handle occlusions, track initiation and termination without the need for an explicit data association module or any heuristics/post-processing. MO3TR encodes object interactions into long-term temporal embeddings using a combination of spatial and temporal Transformers, and recursively uses the information jointly with the input data to estimate the states of all tracked objects over time. The spatial attention mechanism enables our framework to learn implicit representations between all the objects and the objects to the measurements, while the temporal attention mechanism focuses on specific parts of past information, allowing our approach to resolve occlusions over multiple frames. Our experiments demonstrate the potential of this new approach, reaching new state-of-the-art results on multiple MOT metrics for two popular multi-object tracking benchmarks. Our code will be made publicly available.",0
"This paper presents a new approach to multi-object tracking using end-to-end deep learning techniques that combines both spatial and temporal information. We introduce a novel architecture based on the transformer network that models interactions among multiple objects over time, allowing us to track them more accurately. Our method outperforms previous state-of-the-art methods by utilizing both visual features and temporal information effectively, enabling robust object tracking even under challenging scenarios such as occlusions, motion blur, and fast motions. Furthermore, we evaluate our method on several benchmark datasets and show consistent improvements across different metrics. Overall, our work demonstrates the potential of integrating complementary information from spatial and temporal domains into single frameworks for improved multi-object tracking performance.",1
"Recurrent neural networks are effective models to process sequences. However, they are unable to learn long-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced the Transformer, a model solely based on the attention mechanism that is able to relate any two positions of the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the state-of-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of a quadratic computational and memory complexity with respect to the sequence length, hindering its adoption. Fortunately, the deep learning community has always been interested in improving the models' efficiency, leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation. Recently, researchers have directly addressed the Transformer's limitation by designing lower-complexity alternatives such as the Longformer, Reformer, Linformer, and Performer. However, due to the wide range of solutions, it has become challenging for the deep learning community to determine which methods to apply in practice to meet the desired trade-off between capacity, computation, and memory. This survey addresses this issue by investigating popular approaches to make the Transformer faster and lighter and by providing a comprehensive explanation of the methods' strengths, limitations, and underlying assumptions.",0
"In recent years, transformer models have emerged as powerful tools for natural language processing tasks such as machine translation, summarization, and question answering. However, these models can often require large amounts of computational resources and memory, making them difficult to use in practice. This paper presents a survey of approaches that aim to reduce the computational cost and memory usage of transformer models while maintaining their performance. We cover methods such as pruning, quantization, knowledge distillation, and model compression, among others. We discuss the tradeoffs involved in each approach and highlight some open challenges in the field. Our goal is to provide researchers and practitioners with an overview of state-of-the-art techniques for faster and lighter transformers, helping them make informed decisions when choosing a method for their specific needs.",1
"Deep learning (DL) has drawn tremendous attention in object localization and recognition for both natural and medical images. U-Net segmentation models have demonstrated superior performance compared to conventional handcrafted feature-based methods. Medical image modality-specific DL models are better at transferring domain knowledge to a relevant target task than those that are pretrained on stock photography images. This helps improve model adaptation, generalization, and class-specific region of interest (ROI) localization. In this study, we train chest X-ray (CXR) modality-specific U-Nets and other state-of-the-art U-Net models for semantic segmentation of tuberculosis (TB)-consistent findings. Automated segmentation of such manifestations could help radiologists reduce errors and supplement decision-making while improving patient care and productivity. Our approach uses the publicly available TBX11K CXR dataset with weak TB annotations, typically provided as bounding boxes, to train a set of U-Net models. Next, we improve the results by augmenting the training data with weak localizations, post-processed into an ROI mask, from a DL classifier that is trained to classify CXRs as showing normal lungs or suspected TB manifestations. Test data are individually derived from the TBX11K CXR training distribution and other cross-institutional collections including the Shenzhen TB and Montgomery TB CXR datasets. We observe that our augmented training strategy helped the CXR modality-specific U-Net models achieve superior performance with test data derived from the TBX11K CXR training distribution as well as from cross-institutional collections (p  0.05).",0
"In this study, we aim to improve the accuracy of semantic segmentation of tuberculosis (TB) on chest x-ray images by enhancing training methods through augmenting modality-specific U-Net models with weak localization supervision. Existing approaches have produced inconsistent results due to several limitations such as small data sizes and suboptimal use of annotations. Our approach overcomes these challenges by leveraging cross-entropy loss functions that incorporate both image-level labels and pixel-wise masks of region of interest (ROIs), ensuring more comprehensive model optimization. Furthermore, our method benefits from utilizing diverse sources of labeled medical images that simulate real-world clinical scenarios, enabling robust performance across varying patient populations and image acquisition conditions. Finally, extensive evaluations demonstrate the effectiveness of our framework compared against current state-of-the-art techniques. Our proposed method presents new opportunities for advancing automated diagnostic radiology and improving public health outcomes. This research has important implications for assisting radiologists in disease diagnoses where accurate identification can lead to faster treatment initiation and better survival rates. Overall, our work contributes novel technical insights into high-stakes applications of deep learning and computer vision within global health contexts.",1
"We propose a new approach to determine correspondences between image pairs in the wild under large changes in illumination, viewpoint, context, and material. While other approaches find correspondences between pairs of images by treating the images independently, we instead condition on both images to implicitly take account of the differences between them. To achieve this, we introduce (i) a spatial attention mechanism (a co-attention module, CoAM) for conditioning the learned features on both images, and (ii) a distinctiveness score used to choose the best matches at test time. CoAM can be added to standard architectures and trained using self-supervision or supervised data, and achieves a significant performance improvement under hard conditions, e.g. large viewpoint changes. We demonstrate that models using CoAM achieve state of the art or competitive results on a wide range of tasks: local matching, camera localization, 3D reconstruction, and image stylization.",0
"In recent years, the field of image matching has seen significant advancements through the use of deep learning methods such as convolutional neural networks (CNNs). One popular approach is based on Faster R-CNN framework which performs region proposal generation followed by object detection using Region Of Interest (ROI) pooling technique. Despite their effectiveness, these approaches suffer from issues related to computational complexity, slow inference speeds and limited accuracy due to the lack of attention mechanisms that can focus selectively on discriminative features for better representation. To address these limitations, we propose a novel co-attention mechanism for conditioned image matching that integrates feature interactions across different levels of abstraction learned via local and global context aggregation in parallel. Our model achieves state-of-the-art performance on challenging benchmark datasets while demonstrating improved efficiency and interpretability. Experimental results demonstrate the superiority of our method over existing techniques under comparable settings. This work provides valuable insights into the design of efficient deep learning models for computer vision applications, especially in the domain of object recognition and instance segmentation.",1
"Many techniques have been proposed for image reconstruction in medical imaging that aim to recover high-quality images especially from limited or corrupted measurements. Model-based reconstruction methods have been particularly popular (e.g., in magnetic resonance imaging and tomographic modalities) and exploit models of the imaging system's physics together with statistical models of measurements, noise and often relatively simple object priors or regularizers. For example, sparsity or low-rankness based regularizers have been widely used for image reconstruction from limited data such as in compressed sensing. Learning-based approaches for image reconstruction have garnered much attention in recent years and have shown promise across biomedical imaging applications. These methods include synthesis dictionary learning, sparsifying transform learning, and different forms of deep learning involving complex neural networks. We briefly discuss classical model-based reconstruction methods and then review reconstruction methods at the intersection of model-based and learning-based paradigms in detail. This review includes many recent methods based on unsupervised learning, and supervised learning, as well as a framework to combine multiple types of learned models together.",0
"In recent years, model based reconstruction (MBR) has emerged as a powerful tool for computer vision tasks such as denoising, deblurring, superresolution, and image generation. However, most MBR methods rely on predefined handcrafted models that can limit their performance and generalization ability. On the other hand, deep learning approaches have demonstrated impressive results in these tasks by leveraging large amounts of data and model capacity. To bridge the gap between the two approaches, we propose a new framework called model-based reconstruction with learning (MRL). Our approach incorporates a learned prior into traditional MBR formulations, enabling more flexible and adaptive model estimation while still allowing for efficient optimization via gradient descent. We show how our method outperforms both pure MBR and standard deep learning baselines across several benchmark datasets and demonstrate its effectiveness on challenging real world images. Furthermore, MRL can be easily adapted to semi-supervised and self-supervised settings, achieving competitive performance without relying on fully annotated training data. This work presents a new direction for exploiting the synergy between domain knowledge, learned priors, and optimized inference for improved computer vision applications.",1
"Attention-based end-to-end text-to-speech synthesis (TTS) is superior to conventional statistical methods in many ways. Transformer-based TTS is one of such successful implementations. While Transformer TTS models the speech frame sequence well with a self-attention mechanism, it does not associate input text with output utterances from a syntactic point of view at sentence level. We propose a novel neural TTS model, denoted as GraphSpeech, that is formulated under graph neural network framework. GraphSpeech encodes explicitly the syntactic relation of input lexical tokens in a sentence, and incorporates such information to derive syntactically motivated character embeddings for TTS attention mechanism. Experiments show that GraphSpeech consistently outperforms the Transformer TTS baseline in terms of spectrum and prosody rendering of utterances.",0
"Hereâ€™s an example for you, Iâ€™ll make another one just for fun. ðŸ˜‰ This paper proposes a novel architecture for neural speech synthesis based on graph attention networks (GATs). Our approach extends GATs from their traditional use in natural language processing tasks such as sentiment analysis and machine translation, to modeling raw audio waveforms directly. We achieve state-of-the-art results by training our models on large datasets of text transcriptions paired with corresponding audio recordings. In order to further improve quality, we develop a new objective function that takes into account both the acoustic properties of the predicted signals and their similarity to ground truth spectrograms. Empirical evaluations show that our system significantly outperforms previous methods on a variety of metrics including mean opinion score (MOS) and spectral subband scores (SSIM), demonstrating the effectiveness of our approach.",1
"Spiking Neural Networks (SNNs) compute and communicate with asynchronous binary temporal events that can lead to significant energy savings with neuromorphic hardware. Recent algorithmic efforts on training SNNs have shown competitive performance on a variety of classification tasks. However, a visualization tool for analysing and explaining the internal spike behavior of such temporal deep SNNs has not been explored. In this paper, we propose a new concept of bio-plausible visualization for SNNs, called Spike Activation Map (SAM). The proposed SAM circumvents the non-differentiable characteristic of spiking neurons by eliminating the need for calculating gradients to obtain visual explanations. Instead, SAM calculates a temporal visualization map by forward propagating input spikes over different time-steps. SAM yields an attention map corresponding to each time-step of input data by highlighting neurons with short inter-spike interval activity. Interestingly, without both the backpropagation process and the class label, SAM highlights the discriminative region of the image while capturing fine-grained details. With SAM, for the first time, we provide a comprehensive analysis on how internal spikes work in various SNN training configurations depending on optimization types, leak behavior, as well as when faced with adversarial examples.",0
"This study presents a novel approach for generating visual explanations from spiking neural networks (SNNs) by leveraging interspike intervals (ISIs). ISIs represent the temporal patterns of activity across neurons, providing insight into how SNNs encode information. Our method extracts salient features from the ISIs using statistical analysis and visualization techniques. These features can then be used to generate intuitive explanations that highlight key aspects of network behavior. Experiments on benchmark datasets demonstrate the effectiveness of our approach in identifying meaningful patterns and producing informative visualizations. Overall, our work provides new opportunities for exploring and understanding complex systems through the use of spiking neural networks and the rich data they produce.",1
"Deep learning model (primarily convolutional networks and LSTM) for time series classification has been studied broadly by the community with the wide applications in different domains like healthcare, finance, industrial engineering and IoT. Meanwhile, Transformer Networks recently achieved frontier performance on various natural language processing and computer vision tasks. In this work, we explored a simple extension of the current Transformer Networks with gating, named Gated Transformer Networks (GTN) for the multivariate time series classification problem. With the gating that merges two towers of Transformer which model the channel-wise and step-wise correlations respectively, we show how GTN is naturally and effectively suitable for the multivariate time series classification task. We conduct comprehensive experiments on thirteen dataset with full ablation study. Our results show that GTN is able to achieve competing results with current state-of-the-art deep learning models. We also explored the attention map for the natural interpretability of GTN on time series modeling. Our preliminary results provide a strong baseline for the Transformer Networks on multivariate time series classification task and grounds the foundation for future research.",0
"Recent advances in deep learning have led to significant improvements in multivariate time series classification tasks, including those encountered in fields such as finance, healthcare, and environmental monitoring. One particularly successful architecture is the gated transformer network (GTN), which combines the strengths of both convolutional neural networks (CNNs) and attention mechanisms from transformers into a single model. In this paper, we present our analysis on how GTNs can effectively capture temporal dependencies across multiple variables while preserving spatial relationships within individual variables. We evaluate our proposed method on four real-world datasets that represent diverse application domains and demonstrate promising results compared to state-of-the-art approaches. Our findings indicate that by leveraging parallelized self-attention blocks within each variable stream, the GTN can perform efficient and accurate inference even when dealing with high-dimensional data. This work provides important insights into designing effective models for multivariate time series classification problems and has implications for broader applications in machine learning and data science.",1
"Recently proposed DNN-based stereo matching methods that learn priors directly from data are known to suffer a drastic drop in accuracy in new environments. Although supervised approaches with ground truth disparity maps often work well, collecting them in each deployment environment is cumbersome and costly. For this reason, many unsupervised domain adaptation methods based on image-to-image translation have been proposed, but these methods do not preserve the geometric structure of a stereo image pair because the image-to-image translation is applied to each view separately. To address this problem, in this paper, we propose an attention mechanism that aggregates features in the left and right views, called Stereoscopic Cross Attention (SCA). Incorporating SCA to an image-to-image translation network makes it possible to preserve the geometric structure of a stereo image pair in the process of the image-to-image translation. We empirically demonstrate the effectiveness of the proposed unsupervised domain adaptation based on the image-to-image translation with SCA.",0
"Title: Geometry-Aware Unsupervised Domain Adaptation for Stereo Matching  Abstract: This paper proposes a novel unsupervised domain adaptation method that addresses the problem of stereo matching across different domains by leveraging geometry-awareness. We develop an adversarial network architecture that enforces consistency on both the disparity and geometry features extracted from images in the target and source domains. Our approach effectively transfers knowledge from labeled data in the source domain to improve accuracy in unlabeled data in the target domain, while reducing the impact of domain shift. Experimental results show significant improvements over state-of-the-art methods for unsupervised domain adaption in stereo matching. Our framework has important applications in autonomous driving and robotics where accurate depth estimation is critical for safe operation.",1
"This paper addresses semi-supervised semantic segmentation by exploiting a small set of images with pixel-level annotations (strong supervisions) and a large set of images with only image-level annotations (weak supervisions). Most existing approaches aim to generate accurate pixel-level labels from weak supervisions. However, we observe that those generated labels still inevitably contain noisy labels. Motivated by this observation, we present a novel perspective and formulate this task as a problem of learning with pixel-level label noise. Existing noisy label methods, nevertheless, mainly aim at image-level tasks, which can not capture the relationship between neighboring labels in one image. Therefore, we propose a graph based label noise detection and correction framework to deal with pixel-level noisy labels. In particular, for the generated pixel-level noisy labels from weak supervisions by Class Activation Map (CAM), we train a clean segmentation model with strong supervisions to detect the clean labels from these noisy labels according to the cross-entropy loss. Then, we adopt a superpixel-based graph to represent the relations of spatial adjacency and semantic similarity between pixels in one image. Finally we correct the noisy labels using a Graph Attention Network (GAT) supervised by detected clean labels. We comprehensively conduct experiments on PASCAL VOC 2012, PASCAL-Context and MS-COCO datasets. The experimental results show that our proposed semi supervised method achieves the state-of-the-art performances and even outperforms the fully-supervised models on PASCAL VOC 2012 and MS-COCO datasets in some cases.",0
"This project aims to present a new perspective on semi-supervised semantic segmentation through learning from pixel-level label noise. Semantic segmentation is a challenging task that involves predicting the class labels of each pixel in an image. Traditional supervised methods require large amounts of labeled data which can be difficult and time consuming to collect. In recent years, semi-supervised techniques have emerged as a promising alternative, but they often struggle with noisy annotations which can significantly impact performance. This work addresses this issue by proposing a novel approach based on adversarial training and explicit modeling of label uncertainty. Experimental results demonstrate significant improvement over state-of-the-art methods across several benchmark datasets, showing the potential of our method for real world applications such as autonomous driving and medical imaging.",1
"In this work, we show the generative capability of an image classifier network by synthesizing high-resolution, photo-realistic, and diverse images at scale. The overall methodology, called Synthesize-It-Classifier (STIC), does not require an explicit generator network to estimate the density of the data distribution and sample images from that, but instead uses the classifier's knowledge of the boundary to perform gradient ascent w.r.t. class logits and then synthesizes images using Gram Matrix Metropolis Adjusted Langevin Algorithm (GRMALA) by drawing on a blank canvas. During training, the classifier iteratively uses these synthesized images as fake samples and re-estimates the class boundary in a recurrent fashion to improve both the classification accuracy and quality of synthetic images. The STIC shows the mixing of the hard fake samples (i.e. those synthesized by the one hot class conditioning), and the soft fake samples (which are synthesized as a convex combination of classes, i.e. a mixup of classes) improves class interpolation. We demonstrate an Attentive-STIC network that shows an iterative drawing of synthesized images on the ImageNet dataset that has thousands of classes. In addition, we introduce the synthesis using a class conditional score classifier (Score-STIC) instead of a normal image classifier and show improved results on several real-world datasets, i.e. ImageNet, LSUN, and CIFAR 10.",0
"Our paper presents an approach to training generative classifiers using recurring self-analysis. This method involves the classifier analyzing itself during each iteration of the learning process, allowing for continuous optimization and improvement. We demonstrate that by synthesizing new datasets from existing ones, the classifier can achieve better performance than traditional methods. Additionally, we show that our algorithm can generate high quality images by optimizing feature maps that maximize visual similarity while minimizing discriminator loss. With further refinement, this technique has potential applications in areas such as computer vision, natural language processing, and robotics.",1
"This paper pays close attention to the cross-modality visible-infrared person re-identification (VI Re-ID) task, which aims to match human samples between visible and infrared modes. In order to reduce the discrepancy between features of different modalities, most existing works usually use constraints based on Euclidean metric. Since the Euclidean based distance metric cannot effectively measure the internal angles between the embedded vectors, the above methods cannot learn the angularly discriminative feature embedding. Because the most important factor affecting the classification task based on embedding vector is whether there is an angularly discriminativ feature space, in this paper, we propose a new loss function called Enumerate Angular Triplet (EAT) loss. Also, motivated by the knowledge distillation, to narrow down the features between different modalities before feature embedding, we further present a new Cross-Modality Knowledge Distillation (CMKD) loss. The experimental results on RegDB and SYSU-MM01 datasets have shown that the proposed method is superior to the other most advanced methods in terms of impressive performance.",0
"""In recent years, there has been significant interest in developing algorithms for cross-modality person re-identification, which involves matching images of individuals across different types of sensors such as cameras and thermal imagers. However, traditional feature extraction methods used in this field often struggle to effectively capture relevant details from each modality, resulting in poor performance. In order to address this issue, we propose a novel method called Learning Compact and Representative (LeCoR) features that leverages both local and global discriminant information to enhance feature learning for accurate cross-modal person re-identification. Our approach uses a triplet loss framework to learn highly representative features by optimizing inter-class differences while minimizing intra-class variations. Additionally, we incorporate a regularization term to encourage compactness of learned features, ensuring they retain salient information while reducing redundancy. Experimental evaluations on multiple benchmark datasets demonstrate the effectiveness of our proposed method, achieving state-of-the-art results across different modalities.""",1
"We develop a new physical model for the rain effect and show that the well-known atmosphere scattering model (ASM) for the haze effect naturally emerges as its homogeneous continuous limit. Via depth-aware fusion of multi-layer rain streaks according to the camera imaging mechanism, the new model can better capture the sophisticated non-deterministic degradation patterns commonly seen in real rainy images. We also propose a Densely Scale-Connected Attentive Network (DSCAN) that is suitable for both deraining and dehazing tasks. Our design alleviates the bottleneck issue existent in conventional multi-scale networks and enables more effective information exchange and aggregation. Extensive experimental results demonstrate that the proposed DSCAN is able to deliver superior derained/dehazed results on both synthetic and real images as compared to the state-of-the-art. Moreover, it is shown that for our DSCAN, the synthetic dataset built using the new physical model yields better generalization performance on real images in comparison with the existing datasets based on over-simplified models.",0
"This paper presents a new approach to single image deraining and dehazing that leverages advances in deep learning to achieve state-of-the-art performance on both tasks. We propose a unified framework that can handle rain streaks and fog simultaneously, which addresses the fact that these two weather conditions often occur together in real world images. Our method builds upon existing deraining techniques by incorporating a multi-scale network architecture that captures features at different scales, as well as a novel attention mechanism that focuses on areas of interest. Through extensive experiments on benchmark datasets, we demonstrate that our method outperforms current state-of-the-art methods across multiple metrics, including visual quality and quantitative measures such as PSNR and SSIM. Overall, this work represents a significant step forward towards solving the challenging problem of single image deraining and dehazing in adverse weather conditions.",1
"The wide adoption of Electronic Health Records (EHR) has resulted in large amounts of clinical data becoming available, which promises to support service delivery and advance clinical and informatics research. Deep learning techniques have demonstrated performance in predictive analytic tasks using EHRs yet they typically lack model result transparency or explainability functionalities and require cumbersome pre-processing tasks. Moreover, EHRs contain heterogeneous and multi-modal data points such as text, numbers and time series which further hinder visualisation and interpretability. This paper proposes a deep learning framework to: 1) encode patient pathways from EHRs into images, 2) highlight important events within pathway images, and 3) enable more complex predictions with additional intelligibility. The proposed method relies on a deep attention mechanism for visualisation of the predictions and allows predicting multiple sequential outcomes.",0
"This paper presents a framework and mechanism for highlighting events in electronic health records (EHRs) to support explainable predictions made by machine learning models. Many ML-based clinical decision support systems lack transparency regarding how predictions were derived from patient data. This issue could hinder trustworthiness, especially if critical decisions rely on such tools. To address these concerns, we propose a deep event hierarchical representation (Deep EHR Spotlight) that generates evidence chains linking specific patient data elements leading to predicted outcomes. Our approach excels at explaining complex interrelationships among diagnoses, procedures, medications, laboratory tests, and social determinants of health relevant to various medical conditions. Results from case studies demonstrate that our method effectively explains prediction reasoning, improves model interpretability, enables targeted audits, enhances human comprehension through visualization, provides knowledge discovery opportunities, supports shared decision making, facilitates continuous improvement of Clinical Practice Guidelines, and engenders greater physician acceptance of CDS recommendations. Overall, we aim to promote responsible innovation in digital medicine, ensuring safer deployments while empowering patients and enhancing public well-being.",1
"In this paper, we propose a Bidirectional Attention Network (BANet), an end-to-end framework for monocular depth estimation (MDE) that addresses the limitation of effectively integrating local and global information in convolutional neural networks. The structure of this mechanism derives from a strong conceptual foundation of neural machine translation, and presents a light-weight mechanism for adaptive control of computation similar to the dynamic nature of recurrent neural networks. We introduce bidirectional attention modules that utilize the feed-forward feature maps and incorporate the global context to filter out ambiguity. Extensive experiments reveal the high degree of capability of this bidirectional attention model over feed-forward baselines and other state-of-the-art methods for monocular depth estimation on two challenging datasets -- KITTI and DIODE. We show that our proposed approach either outperforms or performs at least on a par with the state-of-the-art monocular depth estimation methods with less memory and computational complexity.",0
"Recent years have witnessed significant advancements in computer vision tasks such as object detection, segmentation, pose estimation, etc., which have been fueled by the advent of deep neural networks (DNNs) and large datasets. However, depth estimation from monocular images remains one of the most challenging problems due to its inherently ambiguous nature caused by the 2D projection of 3D world onto image planes. In this work, we present a novel framework that estimates depth maps directly from monocular RGB images using a bidirectional attention network architecture. Our approach is designed to effectively aggregate local features in both horizontal and vertical dimensions while capturing nonlinear relationships between the input image and its corresponding depth map. We demonstrate the effectiveness of our method on several benchmark datasets including KITTI Eigen Split and NYUv2, outperforming state-of-the-art approaches under different evaluation metrics, thus validating our modelâ€™s ability to accurately estimate scene depth from a single RGB image.",1
"We propose an enhanced multi-scale network, dubbed GridDehazeNet+, for single image dehazing. It consists of three modules: pre-processing, backbone, and post-processing. The trainable pre-processing module can generate learned inputs with better diversity and more pertinent features as compared to those derived inputs produced by hand-selected pre-processing methods. The backbone module implements multi-scale estimation with two major enhancements: 1) a novel grid structure that effectively alleviates the bottleneck issue via dense connections across different scales; 2) a spatial-channel attention block that can facilitate adaptive fusion by consolidating dehazing-relevant features. The post-processing module helps to reduce the artifacts in the final output. To alleviate domain shift between network training and testing, we convert synthetic data to so-called translated data with the distribution shaped to match that of real data. Moreover, to further improve the dehazing performance in real-world scenarios, we propose a novel intra-task knowledge transfer mechanism that leverages the distilled knowledge from synthetic data to assist the learning process on translated data. Experimental results indicate that the proposed GridDehazeNet+ outperforms the state-of-the-art methods on several dehazing benchmarks. The proposed dehazing method does not rely on the atmosphere scattering model, and we provide a possible explanation as to why it is not necessarily beneficial to take advantage of the dimension reduction offered by this model, even if only the dehazing results on synthetic images are concerned.",0
"This paper presents an enhanced multi-scale network called GridDehazeNet+ which utilizes intra-task knowledge transfer (KT) to enhance dehazing performance on single images with varying haze densities. The proposed method leverages the interdependencies among different scales by designing an efficient yet effective network architecture that takes advantage of both low-pass filtering and high frequency details. With two parallel branches, our model integrates intra-scale and cross-scale contextual information effectively and adaptively focuses on difficult regions during dehazing without requiring extra post-processing steps. Our experiments show improved visual quality and quantitative evaluations compared to state-of-the-art methods under diverse scenarios including heavy rain, foggy weather, and dark environments. We hope that our study can provide valuable insights into understanding how KT mechanisms benefit image restoration tasks while offering promising directions for future research.",1
"In state-of-the-art deep neural networks, both feature normalization and feature attention have become ubiquitous. % with significant performance improvement shown in a vast amount of tasks. They are usually studied as separate modules, however. In this paper, we propose a light-weight integration between the two schema and present Attentive Normalization (AN). Instead of learning a single affine transformation, AN learns a mixture of affine transformations and utilizes their weighted-sum as the final affine transformation applied to re-calibrate features in an instance-specific way. The weights are learned by leveraging channel-wise feature attention. In experiments, we test the proposed AN using four representative neural architectures in the ImageNet-1000 classification benchmark and the MS-COCO 2017 object detection and instance segmentation benchmark. AN obtains consistent performance improvement for different neural architectures in both benchmarks with absolute increase of top-1 accuracy in ImageNet-1000 between 0.5\% and 2.7\%, and absolute increase up to 1.8\% and 2.2\% for bounding box and mask AP in MS-COCO respectively. We observe that the proposed AN provides a strong alternative to the widely used Squeeze-and-Excitation (SE) module. The source codes are publicly available at https://github.com/iVMCL/AOGNet-v2 (the ImageNet Classification Repo) and https://github.com/iVMCL/AttentiveNorm\_Detection (the MS-COCO Detection and Segmentation Repo).",0
"In recent years, normalizing flows have become a popular choice for density estimation tasks due to their ability to model complex probability distributions while preserving tractability. However, traditional flow architectures can suffer from low attention to input features during inference, resulting in less accurate predictions on high-dimensional data sets such as images. To address this issue, we propose attentive normalizing flows (ANF), which introduce an attention mechanism into the architecture that allows for more focused representation learning. ANFs utilize self-attention mechanisms to selectively focus on specific regions of the input data, enabling better representations of high-resolution inputs like images. Our experiments demonstrate that our proposed method outperforms standard normalizing flows on multiple image classification benchmark datasets, validating its effectiveness in improving performance through improved attention to input features.",1
"Large, fine-grained image segmentation datasets, annotated at pixel-level, are difficult to obtain, particularly in medical imaging, where annotations also require expert knowledge. Weakly-supervised learning can train models by relying on weaker forms of annotation, such as scribbles. Here, we learn to segment using scribble annotations in an adversarial game. With unpaired segmentation masks, we train a multi-scale GAN to generate realistic segmentation masks at multiple resolutions, while we use scribbles to learn their correct position in the image. Central to the model's success is a novel attention gating mechanism, which we condition with adversarial signals to act as a shape prior, resulting in better object localization at multiple scales. Subject to adversarial conditioning, the segmentor learns attention maps that are semantic, suppress the noisy activations outside the objects, and reduce the vanishing gradient problem in the deeper layers of the segmentor. We evaluated our model on several medical (ACDC, LVSC, CHAOS) and non-medical (PPSS) datasets, and we report performance levels matching those achieved by models trained with fully annotated segmentation masks. We also demonstrate extensions in a variety of settings: semi-supervised learning; combining multiple scribble sources (a crowdsourcing scenario) and multi-task learning (combining scribble and mask supervision). We release expert-made scribble annotations for the ACDC dataset, and the code used for the experiments, at https://vios-s.github.io/multiscale-adversarial-attention-gates",0
"Hereâ€™s an attempt: In recent years, scribbled input has become increasingly common as more applications require loose annotations. However, existing segmentation approaches still rely on accurate annotations that need hours of human labor, which can be very expensive. This study presents multi-scale adversarial attention gates (MAAG), a novel method for unsupervised segmentation that uses scribbled inputs instead of detailed ones. MAAG adopts the idea of generative adversarial networks by applying them to images rather than pixels. As a result, both global context and local details are preserved, making our approach robust against different types of scribbled annotations. Extensive experiments demonstrate that our proposed framework can achieve results comparable to fully supervised methods while only relying on scribbled inputs at training time. With MAAG, we take another step toward reducing the cost of high-quality annotation.",1
"Meta-Learning has gained increasing attention in the machine learning and artificial intelligence communities. In this paper, we introduce and study an adaptive submodular meta-learning problem. The input of our problem is a set of items, where each item has a random state which is initially unknown. The only way to observe an item's state is to select that item. Our objective is to adaptively select a group of items that achieve the best performance over a set of tasks, where each task is represented as an adaptive submodular function that maps sets of items and their states to a real number. To reduce the computational cost while maintaining a personalized solution for each future task, we first select an initial solution set based on previously observed tasks, then adaptively add the remaining items to the initial solution set when a new task arrives. As compared to the solution where a brand new solution is computed for each new task, our meta-learning based approach leads to lower computational overhead at test time since the initial solution set is pre-computed in the training stage. To solve this problem, we propose a two-phase greedy policy and show that it achieves a $1/2$ approximation ratio for the monotone case. For the non-monotone case, we develop a two-phase randomized greedy policy that achieves a $1/32$ approximation ratio.",0
"This paper presents a new method called Adaptive Submodular Meta-Learning (ASML), which learns from prior experience by storing a library of learned knowledge that can guide future learning decisions. ASML adapts over time using submodular function optimization techniques, allowing for efficient reuse of existing knowledge while still exploring unseen domains effectively. We demonstrate through experiments on several benchmark datasets that ASML achieves state-of-the-art results across multiple meta-learning algorithms compared to standard fixed baselines as well as other meta-learned models. In addition, we show qualitatively that ASML exhibits desirable properties such as knowledge retention and transferability across different tasks within the same domain and even across disjoint domains, which makes it more scalable and adaptive than previously proposed methods. Our work paves the way towards realizing intelligent systems capable of autonomous life-long learning. Adaptive Submodular Meta-Learning: Scaling up Lifelong Learning Capabilities in Artificial Intelligence  Artificial intelligence (AI) has made significant strides in recent years due to advancements in deep learning techniques that enable machines to learn complex patterns from large amounts of data. However, most current AI systems suffer from catastrophic forgetting when learning novel tasks because they discard previously acquired knowledge during retraining. To address this issue, researchers have turned to meta-learning approaches designed to efficiently acquire generalized knowledge that can facilitate fast adaptation to new tasks without sacrificing performance on already mastered ones. Despite their promising results, existing meta-learning frameworks remain limited in terms of efficiency, scalability, and applicability to diverse problem sets.  In this paper, we propose a novel methodology termed Adaptive Submo",1
"Domain adaptation (DA) aims to transfer knowledge from a label-rich but heterogeneous domain to a label-scare domain, which alleviates the labeling efforts and attracts considerable attention. Different from previous methods focusing on learning domain-invariant feature representations, some recent methods present generic semi-supervised learning (SSL) techniques and directly apply them to DA tasks, even achieving competitive performance. One of the most popular SSL techniques is pseudo-labeling that assigns pseudo labels for each unlabeled data via the classifier trained by labeled data. However, it ignores the distribution shift in DA problems and is inevitably biased to source data. To address this issue, we propose a new pseudo-labeling framework called Auxiliary Target Domain-Oriented Classifier (ATDOC). ATDOC alleviates the classifier bias by introducing an auxiliary classifier for target data only, to improve the quality of pseudo labels. Specifically, we employ the memory mechanism and develop two types of non-parametric classifiers, i.e. the nearest centroid classifier and neighborhood aggregation, without introducing any additional network parameters. Despite its simplicity in a pseudo classification objective, ATDOC with neighborhood aggregation significantly outperforms domain alignment techniques and prior SSL techniques on a large variety of DA benchmarks and even scare-labeled SSL tasks.",0
"In this work, we present a new method for domain adaptation that utilizes auxiliary target domain classifiers. Our approach leverages these classifiers to improve the performance of a model when applied to a new, unseen domain by adapting it to better fit the data distribution. We evaluate our proposed method on multiple benchmark datasets and demonstrate improved accuracy over state-of-the-art methods. Furthermore, our approach can effectively handle large shifts in data distributions while maintaining high levels of robustness. Overall, our findings show promising results for using auxiliary target domain classifiers as a means of improving domain adaptation and enhancing the ability of models to generalize to unseen domains.",1
"We present a novel approach for unsupervised road segmentation in adverse weather conditions such as rain or fog. This includes a new algorithm for source-free domain adaptation (SFDA) using self-supervised learning. Moreover, our approach uses several techniques to address various challenges in SFDA and improve performance, including online generation of pseudo-labels and self-attention as well as use of curriculum learning, entropy minimization and model distillation. We have evaluated the performance on $6$ datasets corresponding to real and synthetic adverse weather conditions. Our method outperforms all prior works on unsupervised road segmentation and SFDA by at least 10.26%, and improves the training time by 18-180x. Moreover, our self-supervised algorithm exhibits similar accuracy performance in terms of mIOU score as compared to prior supervised methods.",0
"Title: Self-Supervised Source-Free Domain Adaptation for Road Segmentation in Hazardous Environments  Abstract: Automatic road segmentation from remote sensing images is challenged by significant variations due to different acquisition conditions, sensor types, atmospheric effects, and illumination changes. These difficulties are even more pronounced in hazardous environments such as disaster areas where accessibility is limited and data collection is difficult. This paper introduces a novel self-supervised source-free domain adaptation (SS-SFDA) method that addresses these issues by leveraging unlabeled target data only, without relying on any labeled source data. Our approach uses adversarial training based on cycle consistency to adapt the model from one domain to another while preserving spatial information. We employ a customized generator network combined with discriminator networks to learn features across multiple domains. Our experiments demonstrate substantial improvements over traditional unsupervised methods, achieving state-of-the-art performance in both quantitative and qualitative evaluations using several benchmark datasets under diverse scenarios. Our results showcase the effectiveness of our proposed method for road segmentation tasks under extreme environments, paving the way towards reliable automated solutions in emergency response situations.",1
"With the growing importance of preventing the COVID-19 virus, face images obtained in most video surveillance scenarios are low resolution with mask simultaneously. However, most of the previous face super-resolution solutions can not handle both tasks in one model. In this work, we treat the mask occlusion as image noise and construct a joint and collaborative learning network, called JDSR-GAN, for the masked face super-resolution task. Given a low-quality face image with the mask as input, the role of the generator composed of a denoising module and super-resolution module is to acquire a high-quality high-resolution face image. The discriminator utilizes some carefully designed loss functions to ensure the quality of the recovered face images. Moreover, we incorporate the identity information and attention mechanism into our network for feasible correlated feature expression and informative feature learning. By jointly performing denoising and face super-resolution, the two tasks can complement each other and attain promising performance. Extensive qualitative and quantitative results show the superiority of our proposed JDSR-GAN over some comparable methods which perform the previous two tasks separately.",0
"Recent advances in generative adversarial networks (GANs) have enabled the creation of high quality images and realistic videos using artificial intelligence techniques. However, masked face super-resolution, which involves enhancing low resolution facial features for better recognition purposes, remains challenging due to inherent difficulties such as incomplete data, illumination variations, and complex expressions. In this work, we propose JDSR-GAN, a joint and collaborative learning network that effectively tackles these issues by leveraging both global and local attention mechanisms to enhance feature representation at multiple levels. By combining two state-of-the-art GAN variants and adaptively focusing on discriminator guidance and generator refinement, our method achieves superior performance compared to existing methods. Our comprehensive experiments on five publicly available datasets demonstrate the effectiveness of our approach in improving facial attribute detection, face verification, and person re-identification tasks. Overall, JDSR-GAN represents a significant step towards creating intelligent systems capable of handling real-world image processing applications.",1
"For most of the anchor-based detectors, Intersection over Union(IoU) is widely utilized to assign targets for the anchors during training. However, IoU pays insufficient attention to the closeness of the anchor's center to the truth box's center. This results in two problems: (1) only one anchor is assigned to most of the slender objects which leads to insufficient supervision information for the slender objects during training and the performance on the slender objects is hurt; (2) IoU can not accurately represent the alignment degree between the receptive field of the feature at the anchor's center and the object. Thus during training, some features whose receptive field aligns better with objects are missing while some features whose receptive field aligns worse with objects are adopted. This hurts the localization accuracy of models. To solve these problems, we firstly design Gaussian Guided IoU(GGIoU) which focuses more attention on the closeness of the anchor's center to the truth box's center. Then we propose GGIoU-balanced learning method including GGIoU-guided assignment strategy and GGIoU-balanced localization loss. The method can assign multiple anchors for each slender object and bias the training process to the features well-aligned with objects. Extensive experiments on the popular benchmarks such as PASCAL VOC and MS COCO demonstrate GGIoU-balanced learning can solve the above problems and substantially improve the performance of the object detection model, especially in the localization accuracy.",0
"Title: ""Object detection metrics""  Abstract: This paper introduces Gaussian guided intersection over union (Gaussian IoU), a new metric designed to overcome limitations and biases present in existing object detection metrics such as Intersection Over Union (IoU). The main motivation behind the development of Gaussian guided IoU lies in our observation that traditional methods based solely on binary classifiers often result in low recall rates due to their tendency to reject legitimate detections. Gaussian guided IoU addresses these shortcomings by incorporating both location and scale awareness into its design while still maintaining computational efficiency. We demonstrate through experiments using five state-of-the art detectors that Gaussian guided IoU provides more balanced performance compared to existing metrics, resulting in improved average precision values across all tested datasets. Furthermore, we compare our proposed method against other popular evaluation schemes, which confirms its superiority in terms of overall accuracy and robustness. This work serves as a step towards realizing better models and benchmarks for object detection and beyond.",1
"Prediction tasks about students have practical significance for both student and college. Making multiple predictions about students is an important part of a smart campus. For instance, predicting whether a student will fail to graduate can alert the student affairs office to take predictive measures to help the student improve his/her academic performance. With the development of information technology in colleges, we can collect digital footprints which encode heterogeneous behaviors continuously. In this paper, we focus on modeling heterogeneous behaviors and making multiple predictions together, since some prediction tasks are related and learning the model for a specific task may have the data sparsity problem. To this end, we propose a variant of LSTM and a soft-attention mechanism. The proposed LSTM is able to learn the student profile-aware representation from heterogeneous behavior sequences. The proposed soft-attention mechanism can dynamically learn different importance degrees of different days for every student. In this way, heterogeneous behaviors can be well modeled. In order to model interactions among multiple prediction tasks, we propose a co-attention mechanism based unit. With the help of the stacked units, we can explicitly control the knowledge transfer among multiple tasks. We design three motivating behavior prediction tasks based on a real-world dataset collected from a college. Qualitative and quantitative experiments on the three prediction tasks have demonstrated the effectiveness of our model.",0
"This work proposes a joint model that models both student behaviors (e.g., attendance) and interactions among multiple prediction tasks within online learning platforms, including clickstream data analysis, recommender systems, and predictive analytics on student success. Our approach uses Bayesian inference and deep learning techniques to improve the accuracy of behavioral predictions by taking into account hidden relationships across different tasks. We evaluate our method using large datasets from real online learning environments. Results show significant improvement over state-of-the-art approaches and demonstrate the effectiveness of capturing underlying cross-task relationships for accurate predictions. Overall, our framework provides insights into how students interact with online education resources and helps inform decision making related to personalized educational interventions.",1
"Towards better unsupervised domain adaptation (UDA). Recently, researchers propose various domain-conditioned attention modules and make promising progresses. However, considering that the configuration of attention, i.e., the type and the position of attention module, affects the performance significantly, it is more generalized to optimize the attention configuration automatically to be specialized for arbitrary UDA scenario. For the first time, this paper proposes EvoADA: a novel framework to evolve the attention configuration for a given UDA task without human intervention. In particular, we propose a novel search space containing diverse attention configurations. Then, to evaluate the attention configurations and make search procedure UDA-oriented (transferability + discrimination), we apply a simple and effective evaluation strategy: 1) training the network weights on two domains with off-the-shelf domain adaptation methods; 2) evolving the attention configurations under the guide of the discriminative ability on the target domain. Experiments on various kinds of cross-domain benchmarks, i.e., Office-31, Office-Home, CUB-Paintings, and Duke-Market-1510, reveal that the proposed EvoADA consistently boosts multiple state-of-the-art domain adaptation approaches, and the optimal attention configurations help them achieve better performance.",0
"In recent years, deep learning has shown great successes across many domains, but often requires large amounts of data that are scarce or expensive to acquire. To overcome these limitations, domain adaptation techniques aim to learn models that can generalize well on target distributions different from their training distribution. One popular approach is feature denoising autoencoders (AEs), which have achieved state-of-the art results by assuming that the training data is corrupted by some noise process. However, finding the optimal hyperparameters and architecture of an AE remains challenging. This work proposes evolutionary computation methods as a systematic search strategy for automatically selecting the best hyperparameters of AEs in domain adaptation tasks. Our experiments show consistent improvements over hand-designed architectures on several benchmark datasets, demonstrating the effectiveness of our proposed method.",1
"Weakly supervised temporal action localization is a challenging vision task due to the absence of ground-truth temporal locations of actions in the training videos. With only video-level supervision during training, most existing methods rely on a Multiple Instance Learning (MIL) framework to predict the start and end frame of each action category in a video. However, the existing MIL-based approach has a major limitation of only capturing the most discriminative frames of an action, ignoring the full extent of an activity. Moreover, these methods cannot model background activity effectively, which plays an important role in localizing foreground activities. In this paper, we present a novel framework named HAM-Net with a hybrid attention mechanism which includes temporal soft, semi-soft and hard attentions to address these issues. Our temporal soft attention module, guided by an auxiliary background class in the classification module, models the background activity by introducing an ""action-ness"" score for each video snippet. Moreover, our temporal semi-soft and hard attention modules, calculating two attention scores for each video snippet, help to focus on the less discriminative frames of an action to capture the full action boundary. Our proposed approach outperforms recent state-of-the-art methods by at least 2.2% mAP at IoU threshold 0.5 on the THUMOS14 dataset, and by at least 1.3% mAP at IoU threshold 0.75 on the ActivityNet1.2 dataset. Code can be found at: https://github.com/asrafulashiq/hamnet.",0
"In recent years, weakly-supervised temporal action localization has become an increasingly important task in video understanding. However, most existing methods rely on predefined handcrafted features or heuristics that may not capture all relevant information from the videos. To address this limitation, we propose a novel hybrid attention mechanism that combines both spatial and temporal attentions to selectively focus on informative frames and moments within the video clips. Our approach uses pretrained visual representation models to learn discriminative attention maps without any additional annotations. Experimental results show that our method significantly improves state-of-the-art performance on several benchmark datasets while reducing annotation efforts. This research demonstrates the effectiveness of using hybrid attention mechanisms in weakly-supervised temporal action localization tasks and paves the way for future developments in video understanding with limited supervision.",1
"This paper considers the problem of multi-modal future trajectory forecast with ranking. Here, multi-modality and ranking refer to the multiple plausible path predictions and the confidence in those predictions, respectively. We propose Social-STAGE, Social interaction-aware Spatio-Temporal multi-Attention Graph convolution network with novel Evaluation for multi-modality. Our main contributions include analysis and formulation of multi-modality with ranking using interaction and multi-attention, and introduction of new metrics to evaluate the diversity and associated confidence of multi-modal predictions. We evaluate our approach on existing public datasets ETH and UCY and show that the proposed algorithm outperforms the state of the arts on these datasets.",0
"This paper presents a novel framework called Social-STAGE (Spatio-Temporal Adaptive Graph Embedding) for forecasting future trajectories of multiple objects in dynamic scenes. Our approach combines spatio-temporal graph embeddings with adaptive sampling techniques to model the complex interactions among moving agents and capture their evolving patterns over time. We demonstrate the effectiveness of our method on three challenging datasets: pedestrian tracking in public spaces, vehicle tracking on roads, and drone racing tracks. Results show that Social-STAGE outperforms state-of-the-art methods by accurately predicting multi-modal futures across various scenarios while effectively handling occlusions, object disappearance, and motion irregularities. This work paves the way for next generation forecasting systems capable of understanding social behaviors, anticipating potential conflicts, and enabling safer autonomous navigation in densely populated environments.",1
"Temporal context is key to the recognition of expressions of emotion. Existing methods, that rely on recurrent or self-attention models to enforce temporal consistency, work on the feature level, ignoring the task-specific temporal dependencies, and fail to model context uncertainty. To alleviate these issues, we build upon the framework of Neural Processes to propose a method for apparent emotion recognition with three key novel components: (a) probabilistic contextual representation with a global latent variable model; (b) temporal context modelling using task-specific predictions in addition to features; and (c) smart temporal context selection. We validate our approach on four databases, two for Valence and Arousal estimation (SEWA and AffWild2), and two for Action Unit intensity estimation (DISFA and BP4D). Results show a consistent improvement over a series of strong baselines as well as over state-of-the-art methods.",0
"This paper presents a new approach to modeling affective processes, specifically focused on understanding how emotions and facial expressions change over time. We propose using stochastic models to capture the variability and uncertainty inherent in these types of signals, which can provide more accurate predictions than traditional methods that rely solely on fixed rules or heuristics. Our method leverages recent advances in deep learning techniques such as recurrent neural networks (RNNs) and generative adversarial networks (GANs), allowing us to accurately model both short-term changes (e.g., microexpressions) and long-term trends in emotional states. Additionally, we incorporate rich visual features from facial expressions to enhance the accuracy and robustness of our approach. Finally, we demonstrate the effectiveness of our method through experiments on several benchmark datasets, achieving state-of-the-art results compared to existing approaches.",1
"A video-grounded dialogue system referred to as the Structured Co-reference Graph Attention (SCGA) is presented for decoding the answer sequence to a question regarding a given video while keeping track of the dialogue context. Although recent efforts have made great strides in improving the quality of the response, performance is still far from satisfactory. The two main challenging issues are as follows: (1) how to deduce co-reference among multiple modalities and (2) how to reason on the rich underlying semantic structure of video with complex spatial and temporal dynamics. To this end, SCGA is based on (1) Structured Co-reference Resolver that performs dereferencing via building a structured graph over multiple modalities, (2) Spatio-temporal Video Reasoner that captures local-to-global dynamics of video via gradually neighboring graph attention. SCGA makes use of pointer network to dynamically replicate parts of the question for decoding the answer sequence. The validity of the proposed SCGA is demonstrated on AVSD@DSTC7 and AVSD@DSTC8 datasets, a challenging video-grounded dialogue benchmarks, and TVQA dataset, a large-scale videoQA benchmark. Our empirical results show that SCGA outperforms other state-of-the-art dialogue systems on both benchmarks, while extensive ablation study and qualitative analysis reveal performance gain and improved interpretability.",0
"In this paper we describe a method for processing language from video using co-referencing graphs and attention mechanisms. We use these tools to build structured representations that capture relationships between entities mentioned across multiple sentences or turns of speech, which helps improve performance on natural language understanding tasks such as question answering, text summarization, and sentiment analysis. Our approach has several advantages over existing methods: it uses a graph representation that captures both local and global dependencies, our attention mechanism focuses explicitly on referential links rather than just token sequences, and we integrate visual evidence into the model. Additionally, we demonstrate that our models can transfer well to new domains without requiring significant retraining. Overall, our results indicate that combining vision with richly structured linguistic reasoning yields promising advances towards intelligent systems that understand multimodal input and generate coherent responses.",1
"Object localization has a vital role in any object detector, and therefore, has been the focus of attention by many researchers. In this article, a special training approach is proposed for a light convolutional neural network (CNN) to determine the region of interest (ROI) in an image while effectively reducing the number of probable anchor boxes. Almost all CNN-based detectors utilize a fixed input size image, which may yield poor performance when dealing with various object sizes. In this paper, a different CNN structure is proposed taking three different input sizes, to enhance the performance. In order to demonstrate the effectiveness of the proposed method, two common data set are used for training while tracking by localization application is considered to demonstrate its final performance. The promising results indicate the applicability of the presented structure and the training method in practice.",0
"We propose a new approach for object localization through the use of a single multiple-model convolutional neural network (CNN) with specific training techniques. Our method leverages recent advances in CNN architectures and loss functions that focus on localizing objects within images by optimizing a weighted sum of detection scores at different locations. By using multi-scale feature maps generated from our single CNN model, we can effectively perform object detection at multiple scales without having to train separate models for each scale level. We then train our model using two stages: first, we train our primary model with selective search region proposals; secondly, we fine-tune the learned weights using region proposal refinement which leads to improved localization accuracy compared to previous methods. To evaluate our proposed technique, we test it against state-of-the-art object detection methods on two popular datasets â€“ PASCAL VOC and MS COCO - and show competitive results. Overall, our method provides a simpler alternative architecture while maintaining high localization accuracy, making it more efficient for real-time applications where processing speed is crucial.",1
"Low-bit quantization of network weights and activations can drastically reduce the memory footprint, complexity, energy consumption and latency of Deep Neural Networks (DNNs). However, low-bit quantization can also cause a considerable drop in accuracy, in particular when we apply it to complex learning tasks or lightweight DNN architectures. In this paper, we propose a training procedure that relaxes the low-bit quantization. We call this procedure \textit{DNN Quantization with Attention} (DQA). The relaxation is achieved by using a learnable linear combination of high, medium and low-bit quantizations. Our learning procedure converges step by step to a low-bit quantization using an attention mechanism with temperature scheduling. In experiments, our approach outperforms other low-bit quantization techniques on various object recognition benchmarks such as CIFAR10, CIFAR100 and ImageNet ILSVRC 2012, achieves almost the same accuracy as a full precision DNN, and considerably reduces the accuracy drop when quantizing lightweight DNN architectures.",0
"Quantization is becoming increasingly important as researchers look for ways to deploy neural networks onto resource-constrained devices such as smartphones and embedded systems. In our work we focus on quantizing dense neural network architectures that make use of self-attention mechanisms. Self-attention has been shown to provide significant improvements over traditional recurrent and convolutional models but it comes at a high computational cost. Our approach involves training a full precision model first and then applying a mixed precision quantization scheme where some operations are performed using integer math while others retain their floating point precision. We compare three different methods for choosing which layers to quantize: random sampling; manual selection based on preliminary experiments; and an automated method that selects layers based on how well they can be compressed without sacrificing accuracy. Our results show that all three methods perform similarly and lead to consistent gains in both inference speed and memory usage compared to using float arithmetic throughout. Surprisingly, randomly selecting layers to quantize actually outperforms manually selected layers and leads to better compression ratios in most cases. Overall, our findings demonstrate the feasibility of using attention mechanisms in quantized neural networks and open up new possibilities for deploying state-of-the art language models on consumer hardware.",1
"Pervasive computing applications commonly involve user's personal smartphones collecting data to influence application behavior. Applications are often backed by models that learn from the user's experiences to provide personalized and responsive behavior. While models are often pre-trained on massive datasets, federated learning has gained attention for its ability to train globally shared models on users' private data without requiring the users to share their data directly. However, federated learning requires devices to collaborate via a central server, under the assumption that all users desire to learn the same model. We define a new approach, opportunistic federated learning, in which individual devices belonging to different users seek to learn robust models that are personalized to their user's own experiences. However, instead of learning in isolation, these models opportunistically incorporate the learned experiences of other devices they encounter opportunistically. In this paper, we explore the feasibility and limits of such an approach, culminating in a framework that supports encounter-based pairwise collaborative learning. The use of our opportunistic encounter-based learning amplifies the performance of personalized learning while resisting overfitting to encountered data.",0
"In this exploratory study we investigate how egocentric collaboration can enhance pervasive computing applications through opportunistic federated learning (OFL). Our approach leverages usersâ€™ local data silos, ensuring that learning occurs without exposing sensitive information beyond users devices. To evaluate OFL, we developed a mobile application to collect sensor readings from Android smartphones. We explore two different use cases: predicting user presence at home using smart meter electricity consumption, and user activity recognition based on camera image processing. With realworld evaluation across multiple participants over extended periods, our results demonstrate the effectiveness of OFL when compared against traditional methods of offline training followed by online inference. Additionally, we show how OFL allows us to build more accurate models in terms of precision/recall for the two target tasks. Finally, we provide insights into the challenges faced during deployment in real-world scenarios such as connectivity issues and limitations imposed by platforms that restrict access to specific device functionalities. Overall, our findings suggest that OFL has significant potential to improve future pervasive computing systems by enabling collaborative learning within ego-centric networks.",1
"Current dynamic networks and dynamic pruning methods have shown their promising capability in reducing theoretical computation complexity. However, dynamic sparse patterns on convolutional filters fail to achieve actual acceleration in real-world implementation, due to the extra burden of indexing, weight-copying, or zero-masking. Here, we explore a dynamic network slimming regime, named Dynamic Slimmable Network (DS-Net), which aims to achieve good hardware-efficiency via dynamically adjusting filter numbers of networks at test time with respect to different inputs, while keeping filters stored statically and contiguously in hardware to prevent the extra burden. Our DS-Net is empowered with the ability of dynamic inference by the proposed double-headed dynamic gate that comprises an attention head and a slimming head to predictively adjust network width with negligible extra computation cost. To ensure generality of each candidate architecture and the fairness of gate, we propose a disentangled two-stage training scheme inspired by one-shot NAS. In the first stage, a novel training technique for weight-sharing networks named In-place Ensemble Bootstrapping is proposed to improve the supernet training efficacy. In the second stage, Sandwich Gate Sparsification is proposed to assist the gate training by identifying easy and hard samples in an online way. Extensive experiments demonstrate our DS-Net consistently outperforms its static counterparts as well as state-of-the-art static and dynamic model compression methods by a large margin (up to 5.9%). Typically, DS-Net achieves 2-4x computation reduction and 1.62x real-world acceleration over ResNet-50 and MobileNet with minimal accuracy drops on ImageNet. Code release: https://github.com/changlin31/DS-Net .",0
"This paper presents a new approach to deep neural networks called dynamic slimmable networks (DSNs). DSNs enable efficient inference by automatically adjusting their architecture during runtime based on the available computational resources and input characteristics. Our key insight is that we can modify existing architectures to be more adaptive without fundamentally changing how they operate. We demonstrate the effectiveness of our method through comprehensive experiments across several benchmark datasets, showing improved accuracy and efficiency over state-of-the-art methods. In addition, we provide a thorough analysis of the tradeoffs involved in using different model sizes and resource constraints, shedding light on the design space of efficient learning systems. Overall, our work represents an important step towards making machine learning models more flexible and accessible to users who may have limited access to computing resources.",1
"In this paper, we propose a Monocular 3D Single Stage object Detector (M3DSSD) with feature alignment and asymmetric non-local attention. Current anchor-based monocular 3D object detection methods suffer from feature mismatching. To overcome this, we propose a two-step feature alignment approach. In the first step, the shape alignment is performed to enable the receptive field of the feature map to focus on the pre-defined anchors with high confidence scores. In the second step, the center alignment is used to align the features at 2D/3D centers. Further, it is often difficult to learn global information and capture long-range relationships, which are important for the depth prediction of objects. Therefore, we propose a novel asymmetric non-local attention block with multi-scale sampling to extract depth-wise features. The proposed M3DSSD achieves significantly better performance than the monocular 3D object detection methods on the KITTI dataset, in both 3D object detection and bird's eye view tasks.",0
"This paper presents a novel algorithm for single stage monocular object detection on mobile devices using deep learning techniques. The proposed method achieves state of the art results while running at real-time speed on low end hardware, making it suitable for deployment on consumer grade smartphones and tablets. Our approach uses convolutional neural networks (CNNs) to extract features from the input image and predict bounding boxes around objects in the scene. We demonstrate the effectiveness of our model through extensive experiments and compare its performance against other popular approaches in the literature. By combining accuracy and efficiency, we believe that M3DSSD has significant potential applications in many areas including autonomous driving, robotics, computer vision, and augmented reality.",1
"Common horizontal bounding box (HBB)-based methods are not capable of accurately locating slender ship targets with arbitrary orientations in synthetic aperture radar (SAR) images. Therefore, in recent years, methods based on oriented bounding box (OBB) have gradually received attention from researchers. However, most of the recently proposed deep learning-based methods for OBB detection encounter the boundary discontinuity problem in angle or key point regression. In order to alleviate this problem, researchers propose to introduce some manually set parameters or extra network branches for distinguishing the boundary cases, which make training more diffcult and lead to performance degradation. In this paper, in order to solve the boundary discontinuity problem in OBB regression, we propose to detect SAR ships by learning polar encodings. The encoding scheme uses a group of vectors pointing from the center of the ship target to the boundary points to represent an OBB. The boundary discontinuity problem is avoided by training and inference directly according to the polar encodings. In addition, we propose an Intersect over Union (IOU) -weighted regression loss, which further guides the training of polar encodings through the IOU metric and improves the detection performance. Experiments on the Rotating SAR Ship Detection Dataset (RSSDD) show that the proposed method can achieve better detection performance over other comparison algorithms and other OBB encoding schemes, demonstrating the effectiveness of our method.",0
"This could either be a real paper that has already been published or one you wish existed but hasn't yet been written. Let me know if there is any additional context I should provide such as citations or keywords related to the research topic. What is the main conclusion(s) drawn from the paper? If possible please write the first 150 characters including spaces after the word ""Abstract"".",1
"Temporal action localization is an important yet challenging task in video understanding. Typically, such a task aims at inferring both the action category and localization of the start and end frame for each action instance in a long, untrimmed video.While most current models achieve good results by using pre-defined anchors and numerous actionness, such methods could be bothered with both large number of outputs and heavy tuning of locations and sizes corresponding to different anchors. Instead, anchor-free methods is lighter, getting rid of redundant hyper-parameters, but gains few attention. In this paper, we propose the first purely anchor-free temporal localization method, which is both efficient and effective. Our model includes (i) an end-to-end trainable basic predictor, (ii) a saliency-based refinement module to gather more valuable boundary features for each proposal with a novel boundary pooling, and (iii) several consistency constraints to make sure our model can find the accurate boundary given arbitrary proposals. Extensive experiments show that our method beats all anchor-based and actionness-guided methods with a remarkable margin on THUMOS14, achieving state-of-the-art results, and comparable ones on ActivityNet v1.3. Code is available at https://github.com/TencentYoutuResearch/ActionDetection-AFSD.",0
"In summary, this paper presents a novel method that can successfully identify salient boundary features within a video, which can then be used to improve temporal action localization. The authors use state-of-the-art techniques such as Faster R-CNN detection to extract key frames from the video and then apply their new algorithm to find important boundaries where actions take place. Experimental results demonstrate that this approach significantly outperforms other anchor-free methods and achieves competitive performance compared to fully supervised approaches. Overall, this work represents a major advancement in the field of computational vision and has numerous potential applications, including object detection, activity recognition, and surveillance systems.",1
"Appearance-based gaze estimation has achieved significant improvement by using deep learning. However, many deep learning-based methods suffer from the vulnerability property, i.e., perturbing the raw image using noise confuses the gaze estimation models. Although the perturbed image visually looks similar to the original image, the gaze estimation models output the wrong gaze direction. In this paper, we investigate the vulnerability of appearance-based gaze estimation. To our knowledge, this is the first time that the vulnerability of gaze estimation to be found. We systematically characterized the vulnerability property from multiple aspects, the pixel-based adversarial attack, the patch-based adversarial attack and the defense strategy. Our experimental results demonstrate that the CA-Net shows superior performance against attack among the four popular appearance-based gaze estimation networks, Full-Face, Gaze-Net, CA-Net and RT-GENE. This study draws the attention of researchers in the appearance-based gaze estimation community to defense from adversarial attacks.",0
"This research focuses on investigating vulnerabilities that exist in appearance-based gaze estimation algorithms. While these algorithms have been successfully used in many applications such as human-computer interaction, they are prone to errors due to their reliance on unreliable visual cues from the environment. Our study reveals several shortcomings of current state-of-the-art techniques including overfitting problems during training and poor generalization performance across different scenarios. In addition, we provide insights into how adversarial examples can be generated to deceive gaze estimation systems. As our findings demonstrate potential security risks posed by malicious attacks on gaze estimation systems, our work contributes to better understanding of the limitations and challenges facing this field. By raising awareness of these weaknesses, we hope to inspire future development of more robust and reliable gaze tracking methods.",1
"Synthesizing high-quality realistic images from text descriptions is a challenging task. Almost all existing text-to-image Generative Adversarial Networks employ stacked architecture as the backbone. They utilize cross-modal attention mechanisms to fuse text and image features, and introduce extra networks to ensure text-image semantic consistency. In this work, we propose a much simpler, but more effective text-to-image model than previous works. Corresponding to the above three limitations, we propose: 1) a novel one-stage text-to-image backbone which is able to synthesize high-quality images directly by one pair of generator and discriminator, 2) a novel fusion module called deep text-image fusion block which deepens the text-image fusion process in generator, 3) a novel target-aware discriminator composed of matching-aware gradient penalty and one-way output which promotes the generator to synthesize more realistic and text-image semantic consistent images without introducing extra networks. Compared with existing text-to-image models, our proposed method (i.e., DF-GAN) is simpler but more efficient to synthesize realistic and text-matching images and achieves better performance. Extensive experiments on both Caltech-UCSD Birds 200 and COCO datasets demonstrate the superiority of the proposed model in comparison to state-of-the-art models.",0
"This paper presents a novel approach to text-to-image synthesis using deep fusion generative adversarial networks (DF-GAN). By integrating convolutional neural network (CNN) features into GAN training, our method can generate higher quality images that more closely match desired descriptions. In addition, we introduce a new technique called feature augmentation, which improves both image fidelity and diversity by injecting noise into image generator outputs during training. Our experiments show that DF-GAN outperforms existing approaches on standard benchmark datasets while maintaining competitive run times. We believe that this work represents a significant advance in the field of text-to-image synthesis and opens up exciting possibilities for future research directions.",1
"Attention is an effective mechanism to improve the deep model capability. Squeeze-and-Excite (SE) introduces a light-weight attention branch to enhance the network's representational power. The attention branch is gated using the Sigmoid function and multiplied by the feature map's trunk branch. It is too sensitive to coordinate and balance the trunk and attention branches' contributions. To control the attention branch's influence, we propose a new attention method, called Shift-and-Balance (SB). Different from Squeeze-and-Excite, the attention branch is regulated by the learned control factor to control the balance, then added into the feature map's trunk branch. Experiments show that Shift-and-Balance attention significantly improves the accuracy compared to Squeeze-and-Excite when applied in more layers, increasing more size and capacity of a network. Moreover, Shift-and-Balance attention achieves better or close accuracy compared to the state-of-art Dynamic Convolution.",0
"This paper presents a novel method for attention mechanisms in deep learning models called ""Shift-and-Balance."" We address the problem that traditional attention methods suffer from unevenly distributing attentional weights among input elements. Our approach enhances self-attention by performing multiple shifts along different dimensions before computing weighted interactions. Furthermore, we introduce balancing regularization terms that encourage the model to attend to all parts of the input and prevent overfitting to specific patterns. Extensive experiments on five benchmark datasets show significant improvements over strong baselines like BERT and demonstrate consistent performance gains across diverse tasks. By providing superior calibration and interpretability, our approach opens up new possibilities for fine-grained analysis and control in natural language processing pipelines.",1
"To accurately predict future positions of different agents in traffic scenarios is crucial for safely deploying intelligent autonomous systems in the real-world environment. However, it remains a challenge due to the behavior of a target agent being affected by other agents dynamically and there being more than one socially possible paths the agent could take. In this paper, we propose a novel framework, named Dynamic Context Encoder Network (DCENet). In our framework, first, the spatial context between agents is explored by using self-attention architectures. Then, the two-stream encoders are trained to learn temporal context between steps by taking the respective observed trajectories and the extracted dynamic spatial context as input. The spatial-temporal context is encoded into a latent space using a Conditional Variational Auto-Encoder (CVAE) module. Finally, a set of future trajectories for each agent is predicted conditioned on the learned spatial-temporal context by sampling from the latent space, repeatedly. DCENet is evaluated on one of the most popular challenging benchmarks for trajectory forecasting Trajnet and reports a new state-of-the-art performance. It also demonstrates superior performance evaluated on the benchmark inD for mixed traffic at intersections. A series of ablation studies is conducted to validate the effectiveness of each proposed module. Our code is available at https://github.com/wtliao/DCENet.",0
"This paper presents a novel approach for multi-modal trajectory prediction that accounts for dynamic context by fusing both visual perceptions from cameras onboard vehicles with other sources such as LIDAR point clouds, GPS maps, and live traffic feeds. We explore different aspects of this methodology including data collection, feature extraction, model training, evaluation metrics, and results analysis. Our method outperforms state-of-the-art approaches for pedestrian motion forecasting, which is one of the key challenges in predictive driving systems that require accurate and reliable predictions for autonomous cars operating alongside human drivers and pedestrians. Overall, our work demonstrates the potential benefits of integrating diverse sources of data into the planning process for enhanced safety and improved performance of self-driving vehicles in complex urban environments.",1
"Cross-modal recipe retrieval has recently gained substantial attention due to the importance of food in people's lives, as well as the availability of vast amounts of digital cooking recipes and food images to train machine learning models. In this work, we revisit existing approaches for cross-modal recipe retrieval and propose a simplified end-to-end model based on well established and high performing encoders for text and images. We introduce a hierarchical recipe Transformer which attentively encodes individual recipe components (titles, ingredients and instructions). Further, we propose a self-supervised loss function computed on top of pairs of individual recipe components, which is able to leverage semantic relationships within recipes, and enables training using both image-recipe and recipe-only samples. We conduct a thorough analysis and ablation studies to validate our design choices. As a result, our proposed method achieves state-of-the-art performance in the cross-modal recipe retrieval task on the Recipe1M dataset. We make code and models publicly available.",0
"This paper presents an approach using hierarchical transformers for cross-modal recipe retrieval, tackling two key challenges: textual ambiguity in queries and variability among modalities (images vs texts). By leveraging self-supervised pretraining on a large corpus of images, our method learns generic features that capture subtle patterns across both domains for more effective multi-modal fusion. We demonstrate significant improvement over strong baselines on standard benchmark datasets such as Recipe1M and FlavorNet using multiple evaluation metrics. Our approach can be used in real world applications like food search engines, personalized meal recommendations, etc. Our future work includes experimentation with other modalities like audio/video and transfer learning using different tasks.",1
"In video object tracking, there exist rich temporal contexts among successive frames, which have been largely overlooked in existing trackers. In this work, we bridge the individual video frames and explore the temporal contexts across them via a transformer architecture for robust object tracking. Different from classic usage of the transformer in natural language processing tasks, we separate its encoder and decoder into two parallel branches and carefully design them within the Siamese-like tracking pipelines. The transformer encoder promotes the target templates via attention-based feature reinforcement, which benefits the high-quality tracking model generation. The transformer decoder propagates the tracking cues from previous templates to the current frame, which facilitates the object searching process. Our transformer-assisted tracking framework is neat and trained in an end-to-end manner. With the proposed transformer, a simple Siamese matching approach is able to outperform the current top-performing trackers. By combining our transformer with the recent discriminative tracking pipeline, our method sets several new state-of-the-art records on prevalent tracking benchmarks.",0
"This paper proposes using transformers which capture global dependencies and trackers which exploit temporal context to achieve more robust tracking results. Existing approaches often have difficulty tracking objects through occlusions or changes in appearance due to background clutter and illumination variations, but by combining these two types of models we can improve performance on challenging sequences. Our model achieves competitive results on popular benchmarks while running at real time speeds. We believe our method sets the stage for future advancements in online object tracking research.",1
"Recently, convolutional neural networks (CNNs) have achieved great improvements in single image dehazing and attained much attention in research. Most existing learning-based dehazing methods are not fully end-to-end, which still follow the traditional dehazing procedure: first estimate the medium transmission and the atmospheric light, then recover the haze-free image based on the atmospheric scattering model. However, in practice, due to lack of priors and constraints, it is hard to precisely estimate these intermediate parameters. Inaccurate estimation further degrades the performance of dehazing, resulting in artifacts, color distortion and insufficient haze removal. To address this, we propose a fully end-to-end Generative Adversarial Networks with Fusion-discriminator (FD-GAN) for image dehazing. With the proposed Fusion-discriminator which takes frequency information as additional priors, our model can generator more natural and realistic dehazed images with less color distortion and fewer artifacts. Moreover, we synthesize a large-scale training dataset including various indoor and outdoor hazy images to boost the performance and we reveal that for learning-based dehazing methods, the performance is strictly influenced by the training data. Experiments have shown that our method reaches state-of-the-art performance on both public synthetic datasets and real-world images with more visually pleasing dehazed results.",0
"Title: Fusion Discriminators: Enabling Robustness and Improved Performance in GAN Based Single Image Deblurring Models  This work proposes a novel architecture for Generative Adversarial Networks (GAN), termed as ""FD-GAN"", which incorporates a fusion discriminator designed specifically for single image deblurring tasks. The utilization of this fusion discriminator within the GAN framework allows for enhanced robustness against noise, artifacts, and other undesirable effects commonly encountered in low quality images while also improving the overall performance of the model on benchmark datasets.  The proposed approach leverages multiple discriminators at different scales and resolution levels to jointly optimize both local details and global structure aspects of image generation from blurry input images. This design leads to more accurate feature extraction by combining high frequency features that capture details along with low frequency components responsible for capturing underlying structures and context. The training process further ensures that all discriminators converge to a common objective, effectively enhancing the reliability and consistency of gradients flowing into generator network during backpropagation.  Experiments conducted on several popular benchmark datasets demonstrate the significant improvement provided by our FD-GAN approach over traditional GAN architectures for single image deblurring. Both quantitative measurements and visual comparisons validate the efficacy of our methodology in restoring clear and natural looking output images while maintaining important structural cues. These improvements suggest the effectiveness of employing multi-scale fusions of discriminators within GAN models towards advancing computational imaging applications like image deconvolution and denoising.  In summary, this research introduces an innovative strategy for enhancing the capability of GAN models to perform image debulli",1
"It is encouraged to see that progress has been made to bridge videos and natural language. However, mainstream video captioning methods suffer from slow inference speed due to the sequential manner of autoregressive decoding, and prefer generating generic descriptions due to the insufficient training of visual words (e.g., nouns and verbs) and inadequate decoding paradigm. In this paper, we propose a non-autoregressive decoding based model with a coarse-to-fine captioning procedure to alleviate these defects. In implementations, we employ a bi-directional self-attention based network as our language model for achieving inference speedup, based on which we decompose the captioning procedure into two stages, where the model has different focuses. Specifically, given that visual words determine the semantic correctness of captions, we design a mechanism of generating visual words to not only promote the training of scene-related words but also capture relevant details from videos to construct a coarse-grained sentence ""template"". Thereafter, we devise dedicated decoding algorithms that fill in the ""template"" with suitable words and modify inappropriate phrasing via iterative refinement to obtain a fine-grained description. Extensive experiments on two mainstream video captioning benchmarks, i.e., MSVD and MSR-VTT, demonstrate that our approach achieves state-of-the-art performance, generates diverse descriptions, and obtains high inference efficiency. Our code is available at https://github.com/yangbang18/Non-Autoregressive-Video-Captioning.",0
"Title: Non-Autoregressive Coarse-to-Fine Video Captioning Abstract This paper presents a novel approach for video captioning that combines coarse-to-fine processing and non-autoregressive generation. Traditional approaches to video captioning generate descriptions one word at a time, making them susceptible to errors propagating throughout the sequence. In contrast, our method first generates high-level concepts which can capture important semantic information without relying on fine-grained temporal dependencies, followed by autoregressively generating detailed descriptions that refine these concepts. We achieve state-of-the-art performance across multiple metrics and datasets, demonstrating the effectiveness of our approach. Our code will be made publicly available upon acceptance of the manuscript. Keywords: video captioning, deep learning, computer vision, natural language processing, coarse-to-fine processing, non-autoregressive generation",1
"Siamese-based trackers have achieved excellent performance on visual object tracking. However, the target template is not updated online, and the features of the target template and search image are computed independently in a Siamese architecture. In this paper, we propose Deformable Siamese Attention Networks, referred to as SiamAttn, by introducing a new Siamese attention mechanism that computes deformable self-attention and cross-attention. The self attention learns strong context information via spatial attention, and selectively emphasizes interdependent channel-wise features with channel attention. The cross-attention is capable of aggregating rich contextual inter-dependencies between the target template and the search image, providing an implicit manner to adaptively update the target template. In addition, we design a region refinement module that computes depth-wise cross correlations between the attentional features for more accurate tracking. We conduct experiments on six benchmarks, where our method achieves new state of-the-art results, outperforming the strong baseline, SiamRPN++ [24], by 0.464-0.537 and 0.415-0.470 EAO on VOT 2016 and 2018. Our code is available at: https://github.com/msight-tech/research-siamattn.",0
"This paper presents Deformable Siamese Attention Networks (DSAN), a novel approach for visual object tracking that utilizes attention mechanisms to adaptively select relevant image features and model deformation to improve robustness against occlusions and similar appearance changes in objects. DSAN consists of two subnetworks: a siamese network for feature extraction and a dynamic branch for predicting offsets that account for scale variation and partial occlusion through temporal convolution. We evaluate our method on several benchmark datasets and demonstrate state-of-the-art performance while requiring less computational overhead compared to other leading methods. Our results showcase the effectiveness of integrating attention mechanisms into visual tracking frameworks.",1
"Deep CNN-based methods have so far achieved the state of the art results in multi-view 3D object reconstruction. Despite the considerable progress, the two core modules of these methods - multi-view feature extraction and fusion, are usually investigated separately, and the object relations in different views are rarely explored. In this paper, inspired by the recent great success in self-attention-based Transformer models, we reformulate the multi-view 3D reconstruction as a sequence-to-sequence prediction problem and propose a new framework named 3D Volume Transformer (VolT) for such a task. Unlike previous CNN-based methods using a separate design, we unify the feature extraction and view fusion in a single Transformer network. A natural advantage of our design lies in the exploration of view-to-view relationships using self-attention among multiple unordered inputs. On ShapeNet - a large-scale 3D reconstruction benchmark dataset, our method achieves a new state-of-the-art accuracy in multi-view reconstruction with fewer parameters ($70\%$ less) than other CNN-based methods. Experimental results also suggest the strong scaling capability of our method. Our code will be made publicly available.",0
"Infer the title from the contents of the abstract. The goal of 3D reconstruction from images has been studied extensively for decades, but has recently attracted renewed attention due to advances in deep learning techniques and the availability of large datasets. In this work, we present a new framework called Multi-View 3D Reconstruction with Transformers (MVT), which combines features learned by convolutional neural networks (CNNs) with representations obtained through transformer architectures. Our method uses multiple views to estimate depth maps, camera poses, and surface normals, followed by fusing these estimates into a single multi-scale representation. We then use Minkowski operators to enforce smoothness across levels, leading to improved accuracy. Experiments on four benchmark datasets demonstrate that our approach achieves state-of-the-art results while using less computational resources compared to previous methods.",1
"Humans learn from life events to form intuitions towards the understanding of visual environments and languages. Envision that you are instructed by a high-level instruction, ""Go to the bathroom in the master bedroom and replace the blue towel on the left wall"", what would you possibly do to carry out the task? Intuitively, we comprehend the semantics of the instruction to form an overview of where a bathroom is and what a blue towel is in mind; then, we navigate to the target location by consistently matching the bathroom appearance in mind with the current scene. In this paper, we present an agent that mimics such human behaviors. Specifically, we focus on the Remote Embodied Visual Referring Expression in Real Indoor Environments task, called REVERIE, where an agent is asked to correctly localize a remote target object specified by a concise high-level natural language instruction, and propose a two-stage training pipeline. In the first stage, we pretrain the agent with two cross-modal alignment sub-tasks, namely the Scene Grounding task and the Object Grounding task. The agent learns where to stop in the Scene Grounding task and what to attend to in the Object Grounding task respectively. Then, to generate action sequences, we propose a memory-augmented attentive action decoder to smoothly fuse the pre-trained vision and language representations with the agent's past memory experiences. Without bells and whistles, experimental results show that our method outperforms previous state-of-the-art(SOTA) significantly, demonstrating the effectiveness of our method.",0
"This paper presents a novel approach to embodied visual grounding called scene-intuitive agent (SIA), which uses deep learning techniques to enable agents to rapidly learn and perform tasks in complex environments without the need for extensive human supervision. SIA employs several advanced methods such as attention mechanisms, graph convolutions, and meta learning to improve performance on specific tasks while maintaining generalization ability across different scenes and domains. We evaluate our method using benchmark datasets for embodied visual grounding and demonstrate that our model achieves state-of-the-art results on challenging tasks requiring fine-grained understanding of visual details and interactions with objects. Furthermore, we analyze the factors contributing to the success of our method, including computational efficiency, scalability, robustness, and interpretability, making it a promising solution for real-world applications requiring remote embodied vision intelligence.",1
"Human-Object Interaction (HOI) detection is important to human-centric scene understanding tasks. Existing works tend to assume that the same verb has similar visual characteristics in different HOI categories, an approach that ignores the diverse semantic meanings of the verb. To address this issue, in this paper, we propose a novel Polysemy Deciphering Network (PD-Net) that decodes the visual polysemy of verbs for HOI detection in three distinct ways. First, we refine features for HOI detection to be polysemyaware through the use of two novel modules: namely, Language Prior-guided Channel Attention (LPCA) and Language Prior-based Feature Augmentation (LPFA). LPCA highlights important elements in human and object appearance features for each HOI category to be identified; moreover, LPFA augments human pose and spatial features for HOI detection using language priors, enabling the verb classifiers to receive language hints that reduce intra-class variation for the same verb. Second, we introduce a novel Polysemy-Aware Modal Fusion module (PAMF), which guides PD-Net to make decisions based on feature types deemed more important according to the language priors. Third, we propose to relieve the verb polysemy problem through sharing verb classifiers for semantically similar HOI categories. Furthermore, to expedite research on the verb polysemy problem, we build a new benchmark dataset named HOI-VerbPolysemy (HOIVP), which includes common verbs (predicates) that have diverse semantic meanings in the real world. Finally, through deciphering the visual polysemy of verbs, our approach is demonstrated to outperform state-of-the-art methods by significant margins on the HICO-DET, V-COCO, and HOI-VP databases. Code and data in this paper are available at https://github.com/MuchHair/PD-Net.",0
"This paper proposes a novel method for robust human object interaction detection using polysemy deciphering networks (PDNs). In many applications such as video surveillance systems or robots assisting humans, understanding human-object interactions is crucial. However, these interactions are diverse and can be ambiguous due to occlusions or similar objects leading to poor recognition rates. To overcome these challenges, we propose PDNs that take into account multiple sensory inputs from different sources in order to make predictions. By considering contextual information such as audio and text, PDNs improve performance compared to single modality approaches. Our experimental results show promising accuracy on two benchmark datasets demonstrating the effectiveness of our proposed approach. Overall, our work provides a valuable contribution towards building intelligent systems capable of recognizing complex human-object interactions in real world scenarios.",1
"Safe navigation of autonomous agents in human centric environments requires the ability to understand and predict motion of neighboring pedestrians. However, predicting pedestrian intent is a complex problem. Pedestrian motion is governed by complex social navigation norms, is dependent on neighbors' trajectories, and is multimodal in nature. In this work, we propose SCAN, a Spatial Context Attentive Network that can jointly predict socially-acceptable multiple future trajectories for all pedestrians in a scene. SCAN encodes the influence of spatially close neighbors using a novel spatial attention mechanism in a manner that relies on fewer assumptions, is parameter efficient, and is more interpretable compared to state-of-the-art spatial attention approaches. Through experiments on several datasets we demonstrate that our approach can also quantitatively outperform state of the art trajectory prediction methods in terms of accuracy of predicted intent.",0
"""This paper presents a novel approach for joint multi-agent intent prediction using a spatial context attentive network (SCAN). In many real-world scenarios such as traffic analysis and social robotics, multiple agents interact within a shared environment, requiring accurate modeling of their individual goals and plans. However, existing methods primarily focus on single agent intent prediction without considering other agents in the scene, leading to poor generalization and performance. To address these limitations, we introduce SCAN which utilizes attention mechanisms to capture spatial dependencies among agents, allowing it to reason about their interactions and predict actions accordingly. Experimental results demonstrate that our method outperforms state-of-the-art approaches by significant margins while providing more interpretable predictions.""",1
"While innovations in scientific instrumentation have pushed the boundaries of Mars rover mission capabilities, the increase in data complexity has pressured Mars Science Laboratory (MSL) and future Mars rover operations staff to quickly analyze complex data sets to meet progressively shorter tactical and strategic planning timelines. MSLWEB is an internal data tracking tool used by operations staff to perform first pass analysis on MSL image sequences, a series of products taken by the Mast camera, Mastcam. Mastcam's multiband multispectral image sequences require more complex analysis compared to standard 3-band RGB images. Typically, these are analyzed using traditional methods to identify unique features within the sequence. Given the short time frame of tactical planning in which downlinked images might need to be analyzed (within 5-10 hours before the next uplink), there exists a need to triage analysis time to focus on the most important sequences and parts of a sequence. We address this need by creating products for MSLWEB that use novelty detection to help operations staff identify unusual data that might be diagnostic of new or atypical compositions or mineralogies detected within an imaging scene. This was achieved in two ways: 1) by creating products for each sequence to identify novel regions in the image, and 2) by assigning multispectral sequences a sortable novelty score. These new products provide colorized heat maps of inferred novelty that operations staff can use to rapidly review downlinked data and focus their efforts on analyzing potentially new kinds of diagnostic multispectral signatures. This approach has the potential to guide scientists to new discoveries by quickly drawing their attention to often subtle variations not detectable with simple color composites.",0
"This paper presents research on integrating novelty detection capabilities into Mars Science Laboratory (MSL) imaging operations using the Mast Camera (Mastcam). Our work addresses an important challenge faced by mission planners â€“ identifying new discoveries among large volumes of data from planetary missions. By automatically detecting unexpected features such as geological structures that differ significantly from existing ones, our approach enhances science return while reducing human workload through automation. We evaluate the performance of several state-of-the-art algorithms under realistic settings on simulated image datasets. Results indicate significant improvement over traditional feature engineering approaches alone, particularly when combined in ensemble methods. Further analysis shows these improvements can have major implications in terms of discovery speed, efficiency, and quality of exploration decisions. In summary, this study offers promising advancements towards enabling effective use of robotic explorersâ€™ limited resources for making groundbreaking scientific findings possible.",1
"Generative modelling has been a topic at the forefront of machine learning research for a substantial amount of time. With the recent success in the field of machine learning, especially in deep learning, there has been an increased interest in explainable and interpretable machine learning. The ability to model distributions and provide insight in the density estimation and exact data likelihood is an example of such a feature. Normalizing Flows (NFs), a relatively new research field of generative modelling, has received substantial attention since it is able to do exactly this at a relatively low cost whilst enabling competitive generative results. While the generative abilities of NFs are typically explored, we focus on exploring the data distribution modelling for Out-of-Distribution (OOD) detection. Using one of the state-of-the-art NF models, GLOW, we attempt to detect OOD examples in the ISIC dataset. We notice that this model under performs in conform related research. To improve the OOD detection, we explore the masking methods to inhibit co-adaptation of the coupling layers however find no substantial improvement. Furthermore, we utilize Wavelet Flow which uses wavelets that can filter particular frequency components, thus simplifying the modeling process to data-driven conditional wavelet coefficients instead of complete images. This enables us to efficiently model larger resolution images in the hopes that it would capture more relevant features for OOD. The paper that introduced Wavelet Flow mainly focuses on its ability of sampling high resolution images and did not treat OOD detection. We present the results and propose several ideas for improvement such as controlling frequency components, using different wavelets and using other state-of-the-art NF architectures.",0
"In order to train effective deep learning models, labeled data from diverse domains and distributions must first be collected, cleaned, normalized, preprocessed, etc. However, despite all these efforts, the distribution of inputs fed into real world applications remains shifted relative to that seen during trainingâ€”the outputs of such systems degrade significantly under these OOD (Out-Of-Distribution) conditions. This phenomenon limits their deployment and makes reliable use cases challenging; as deployments often involve operating on previously unseen input sets and generating decisions accordingly. To address this issue, we explore applying generative flows conditioned on class labels to detect if inputs given at test time originate from the same distribution as those encountered during training. We find strong results indicating our proposed methodology can detect significant shifts away from training set inputs, enabling us to filter out low quality images, provide informed warnings regarding model output reliability, and enable more targeted human review where desired. Our work furthers progress towards making machine generated predictions robust enough for real world use cases.",1
"Deep neural networks (DNNs) are known to produce incorrect predictions with very high confidence on out-of-distribution (OOD) inputs. This limitation is one of the key challenges in the adoption of deep learning models in high-assurance systems such as autonomous driving, air traffic management, and medical diagnosis. This challenge has received significant attention recently, and several techniques have been developed to detect inputs where the model's prediction cannot be trusted. These techniques use different statistical, geometric, or topological signatures. This paper presents a taxonomy of OOD outlier inputs based on their source and nature of uncertainty. We demonstrate how different existing detection approaches fail to detect certain types of outliers. We utilize these insights to develop a novel integrated detection approach that uses multiple attributes corresponding to different types of outliers. Our results include experiments on CIFAR10, SVHN and MNIST as in-distribution data and Imagenet, LSUN, SVHN (for CIFAR10), CIFAR10 (for SVHN), KMNIST, and F-MNIST as OOD data across different DNN architectures such as ResNet34, WideResNet, DenseNet, and LeNet5.",0
"This research examines the diversity of outliers, particularly in the context of detecting out-of-distribution (OOD) samples. While traditional approaches treat all outliers as anomalies that can indicate data contamination or errors, our study shows that there may be different types of outliers with varying characteristics and behaviors. By analyzing large datasets from diverse domains, we identify distinct patterns in how these outliers behave under certain conditions, suggesting that they could potentially have specific causes or sources. Furthermore, by understanding their unique properties, we propose novel methods that can more effectively distinguish between benign outliers and truly anomalous OOD samples, which is crucial for ensuring robust and reliable performance of machine learning models. Our findings provide valuable insights into the nature of outliers, offering new perspectives on their role in anomaly detection and inspiring further research into developing tailored solutions for handling them in various applications. Overall, our work has important implications for both academia and industry, where accurate identification and management of OOD samples remains a critical challenge.",1
"This paper describes an approach to solving the next destination city recommendation problem for a travel reservation system. We propose a two stages approach: a heuristic approach for candidates selection and an attention neural network model for candidates re-ranking. Our method was inspired by listwise learning-to-rank methods and recent developments in natural language processing and the transformer architecture in particular. We used this approach to solve the Booking.com recommendations challenge Our team achieved 5th place on the challenge using this method, with 0.555 accuracy@4 value on the closed part of the dataset.",0
"Next city selection is a crucial step in travel planning. A suitable choice can enhance overall travel experience while saving time and budget for other enjoyments. In this work, we propose an attention based neural model that learns from user preferences and recommends appropriate cities as next destinations. Our method considers multiple aspects of preference including popularity, uniqueness, culture, climate, economy, distance and tourist attractions. We evaluate our approach on large real world data sets collected via social media platforms, online forums, hotel bookings, weather forecasts, encyclopedia articles and maps. Results indicate that our model significantly outperforms existing methods achieving higher accuracy than both content-only baselines and collaborative filtering approaches. Furthermore, qualitative analysis shows that recommendation lists generated by our model provide diverse choices satisfying different taste groups compared to competing systems. Overall, our proposed framework presents a flexible solution which could potentially enable users to customize their trips according to their preferences at scale.",1
"Health management is getting increasing attention all over the world. However, existing health management mainly relies on hospital examination and treatment, which are complicated and untimely. The emerging of mobile devices provides the possibility to manage people's health status in a convenient and instant way. Estimation of health status can be achieved with various kinds of data streams continuously collected from wearable sensors. However, these data streams are multi-source and heterogeneous, containing complex temporal structures with local contextual and global temporal aspects, which makes the feature learning and data joint utilization challenging. We propose to model the behavior-related multi-source data streams with a local-global graph, which contains multiple local context sub-graphs to learn short term local context information with heterogeneous graph neural networks and a global temporal sub-graph to learn long term dependency with self-attention networks. Then health status is predicted based on the structure-aware representation learned from the local-global behavior graph. We take experiments on StudentLife dataset, and extensive results demonstrate the effectiveness of our proposed model.",0
"In order to predict health status using local-global heterogeneous behavior graphs, we present a novel framework that utilizes both global network patterns and local node interactions. By integrating multiple sources of data and modeling patient behaviors across different domains, our approach can accurately capture complex relationships among risk factors, symptoms, and outcomes. Our experiments on real clinical datasets demonstrate the effectiveness of our methodology in predicting health outcomes, including readmission rates and hospital length of stay. Furthermore, we discuss potential applications of our model in supporting personalized medicine and public health interventions.",1
"Current face forgery detection methods achieve high accuracy under the within-database scenario where training and testing forgeries are synthesized by the same algorithm. However, few of them gain satisfying performance under the cross-database scenario where training and testing forgeries are synthesized by different algorithms. In this paper, we find that current CNN-based detectors tend to overfit to method-specific color textures and thus fail to generalize. Observing that image noises remove color textures and expose discrepancies between authentic and tampered regions, we propose to utilize the high-frequency noises for face forgery detection. We carefully devise three functional modules to take full advantage of the high-frequency features. The first is the multi-scale high-frequency feature extraction module that extracts high-frequency noises at multiple scales and composes a novel modality. The second is the residual-guided spatial attention module that guides the low-level RGB feature extractor to concentrate more on forgery traces from a new perspective. The last is the cross-modality attention module that leverages the correlation between the two complementary modalities to promote feature learning for each other. Comprehensive evaluations on several benchmark databases corroborate the superior generalization performance of our proposed method.",0
"Face forgery detection has become increasingly important as digital media becomes more prevalent and manipulation tools become more advanced. While many state-of-the-art methods rely on large datasets and complex network architectures, these approaches can often be limited by their specific training data and may struggle generalize well across different domains. To address this limitation, we propose a novel method that uses high-frequency features to improve the robustness of face forgery detection models. Our approach utilizes a lightweight CNN architecture that extracts spatial frequency components from facial images, which are then passed through a feature distillation module to generate compressed representations of each image. These distilled features are used to train classifiers that predict whether an input image contains a forgery. We evaluate our method using several benchmark databases and demonstrate significant improvements over previous methods in both within-domain and cross-domain settings, while requiring fewer parameters and less computational resources. Our results suggest that incorporating high-frequency information into face forgery detection models can greatly enhance their robustness and generalization capabilities.",1
"Change detection, i.e. identification per pixel of changes for some classes of interest from a set of bi-temporal co-registered images, is a fundamental task in the field of remote sensing. It remains challenging due to unrelated forms of change that appear at different times in input images. Here, we propose a reliable deep learning framework for the task of semantic change detection in very high-resolution aerial images. Our framework consists of a new loss function, new attention modules, new feature extraction building blocks, and a new backbone architecture that is tailored for the task of semantic change detection. Specifically, we define a new form of set similarity, that is based on an iterative evaluation of a variant of the Dice coefficient. We use this similarity metric to define a new loss function as well as a new spatial and channel convolution Attention layer (the FracTAL). The new attention layer, designed specifically for vision tasks, is memory efficient, thus suitable for use in all levels of deep convolutional networks. Based on these, we introduce two new efficient self-contained feature extraction convolution units. We validate the performance of these feature extraction building blocks on the CIFAR10 reference data and compare the results with standard ResNet modules. Further, we introduce a new encoder/decoder scheme, a network macro-topology, that is tailored for the task of change detection. Our network moves away from any notion of subtraction of feature layers for identifying change. We validate our approach by showing excellent performance and achieving state of the art score (F1 and Intersection over Union-hereafter IoU) on two building change detection datasets, namely, the LEVIRCD (F1: 0.918, IoU: 0.848) and the WHU (F1: 0.938, IoU: 0.882) datasets.",0
"This paper examines the impact that randomness can have on driving social change through the use of novel communication strategies. Drawing from examples such as flash mobs, Internet memes, and street art, we explore how these tactics can effectively capture public attention by exploiting uncertainty and surprise. By analyzing case studies and conducting experiments, we demonstrate how nonconformist approaches can challenge dominant narratives and disrupt established power dynamics, ultimately leading to meaningful societal transformations. Our findings suggest that embracing randomness may provide a powerful means for individuals and organizations seeking to effect real change amidst an increasingly complex world.",1
"In this paper, we address the problem of referring expression comprehension in videos, which is challenging due to complex expression and scene dynamics. Unlike previous methods which solve the problem in multiple stages (i.e., tracking, proposal-based matching), we tackle the problem from a novel perspective, \textbf{co-grounding}, with an elegant one-stage framework. We enhance the single-frame grounding accuracy by semantic attention learning and improve the cross-frame grounding consistency with co-grounding feature learning. Semantic attention learning explicitly parses referring cues in different attributes to reduce the ambiguity in the complex expression. Co-grounding feature learning boosts visual feature representations by integrating temporal correlation to reduce the ambiguity caused by scene dynamics. Experiment results demonstrate the superiority of our framework on the video grounding datasets VID and LiOTB in generating accurate and stable results across frames. Our model is also applicable to referring expression comprehension in images, illustrated by the improved performance on the RefCOCO dataset. Our project is available at https://sijiesong.github.io/co-grounding.",0
"Understanding referring expressions in videos, which describe entities visible on screen but may lack precise grounding, remains challenging due to variations in phrasing, occlusions, motion blur, etc. We introduce co-grounding networks (CGNet), which perform dense temporal reasoning by incorporating external knowledge via semantic attention modules that focus on informative parts of the input video stream. Our model achieves new state-of-the-art results on two challenging benchmark datasets (RefCOCO+GP and RefVQGPT) in both coreference resolution tasks: box localization accuracy and span-level recall/precision metrics. By contrast with prior work, our approach shows improved performance on complex queries involving negation, uncertainty, multi-hop relationships, comparisons, collectives, or zero antecedents. Thus, CGNets provide richer insights into query interpretation and visual understanding mechanisms underlying referring expression comprehension in videos, advancing applications such as video search or robotics.",1
"Domain Adaptation (DA) attempts to transfer knowledge learned in the labeled source domain to the unlabeled but related target domain without requiring large amounts of target supervision. Recent advances in DA mainly proceed by aligning the source and target distributions. Despite the significant success, the adaptation performance still degrades accordingly when the source and target domains encounter a large distribution discrepancy. We consider this limitation may attribute to the insufficient exploration of domain-specialized features because most studies merely concentrate on domain-general feature learning in task-specific layers and integrate totally-shared convolutional networks (convnets) to generate common features for both domains. In this paper, we relax the completely-shared convnets assumption adopted by previous DA methods and propose Domain Conditioned Adaptation Network (DCAN), which introduces domain conditioned channel attention module with a multi-path structure to separately excite channel activation for each domain. Such a partially-shared convnets module allows domain-specialized features in low-level to be explored appropriately. Further, given the knowledge transferability varying along with convolutional layers, we develop Generalized Domain Conditioned Adaptation Network (GDCAN) to automatically determine whether domain channel activations should be separately modeled in each attention module. Afterward, the critical domain-specialized knowledge could be adaptively extracted according to the domain statistic gaps. As far as we know, this is the first work to explore the domain-wise convolutional channel activations separately for deep DA networks. Additionally, to effectively match high-level feature distributions across domains, we consider deploying feature adaptation blocks after task-specific layers, which can explicitly mitigate the domain discrepancy.",0
"In recent years, domain adaptation has become an increasingly important field as more data becomes available online. Traditional machine learning models often struggle when applied to new domains due to changes in distribution, which leads to poor performance on unseen examples. To address this issue, we propose a novel method called Generalized Domain Conditioned Adaptation Network (GDCAN). GDCAN leverages both global context from all source samples and local context from each task-specific sample by introducing two separate branches in our architecture: Global Branch and Local Task Branch. Our approach improves over previous methods that rely solely on one branch to capture knowledge transfer, allowing us to achieve better performance across diverse datasets and tasks. We evaluate our model using several benchmark datasets and compare against state-of-the-art methods, demonstrating substantial improvements in accuracy. Overall, GDCAN offers a promising solution for improved generalization under varying conditions and serves as a valuable tool for future research.",1
"Online influence maximization has attracted much attention as a way to maximize influence spread through a social network while learning the values of unknown network parameters. Most previous works focus on single-item diffusion. In this paper, we introduce a new Online Competitive Influence Maximization (OCIM) problem, where two competing items (e.g., products, news stories) propagate in the same network and influence probabilities on edges are unknown. We adapt a combinatorial multi-armed bandit (CMAB) framework for the OCIM problem, but unlike the non-competitive setting, the important monotonicity property (influence spread increases when influence probabilities on edges increase) no longer holds due to the competitive nature of propagation, which brings a significant new challenge to the problem. We provide a nontrivial proof showing that the Triggering Probability Modulated (TPM) condition for CMAB still holds in OCIM, which is instrumental for our proposed algorithms OCIM-TS and OCIM-OFU to achieve logarithmic Bayesian regret and frequentist regret, respectively. We also design an OCIM-ETC algorithm that requires less feedback and easier offline computation, at the expense of a worse frequentist regret bound. Our experimental evaluations demonstrate the effectiveness of our algorithms.",0
"This paper introduces a novel approach to maximizing influence online through competitive means. We present a framework that takes into account both user engagement and competition among different entities vying for attention on social media platforms. Our method utilizes advanced machine learning techniques to analyze vast amounts of data, enabling us to identify patterns and strategies that lead to increased engagement and visibility. Through simulations and case studies, we demonstrate the effectiveness of our approach in driving online influence and achieving desired outcomes. Our work contributes new insights into how influencers can optimize their efforts, paving the way for future research in online competition and digital marketing.  Title: Online Competitive Influence Maximization Authors: John Smith and Jane Jones  Abstract: In todayâ€™s digitally connected world, online influence has become crucial in shaping public opinion and driving business success. With countless voices competing for attention on social media platforms, it is essential for individuals and organizations to find effective ways to stand out from the crowd and increase their impact. In response to this challenge, this paper presents a novel approach to online influence maximization through competitive means. Our proposed framework incorporates two key factors â€“ user engagement and inter-entity competition â€“ that play vital roles in determining successful online campaigns. By leveraging advanced machine learning algorithms and analyzing large datasets, our method allows users to identify winning strategies and achieve better results than traditional methods alone. Using simulation experiments and real-world examples, we validate our approach by demonstrating significant improvements in engagement metrics such as likes, shares, comments, clicks, retweets, etc. Our study provides valuable insights for marketers, politicians, activists, entertainers and other influencer groups seeking to enhance th",1
"Autonomous navigation in crowded, complex urban environments requires interacting with other agents on the road. A common solution to this problem is to use a prediction model to guess the likely future actions of other agents. While this is reasonable, it leads to overly conservative plans because it does not explicitly model the mutual influence of the actions of interacting agents. This paper builds a reinforcement learning-based method named MIDAS where an ego-agent learns to affect the control actions of other cars in urban driving scenarios. MIDAS uses an attention-mechanism to handle an arbitrary number of other agents and includes a ""driver-type"" parameter to learn a single policy that works across different planning objectives. We build a simulation environment that enables diverse interaction experiments with a large number of agents and methods for quantitatively studying the safety, efficiency, and interaction among vehicles. MIDAS is validated using extensive experiments and we show that it (i) can work across different road geometries, (ii) results in an adaptive ego policy that can be tuned easily to satisfy performance criteria such as aggressive or cautious driving, (iii) is robust to changes in the driving policies of external agents, and (iv) is more efficient and safer than existing approaches to interaction-aware decision-making.",0
"This research presents MIDAS, a multi-agent interaction-aware decision-making approach for urban autonomous navigation that utilizes adaptive strategies. The proposed method addresses the challenges presented by dynamic environments where multiple agents interact and influence one another. Our approach considers both short-term objectives (e.g., collision avoidance) as well as long-term goals (e.g., minimizing travel time). We designed two key components for MIDAS: interaction prediction and cooperation/competition switching. These mechanisms allow our agent to anticipate interactions with other agents and adjust accordingly, enabling effective collaboration or competition when necessary. Experiments conducted on various scenarios demonstrate significant improvement compared to prior approaches, showcasing MIDAS's effectiveness in realistic settings with diverse traffic conditions and agent behaviors. Our work contributes to the development of intelligent autonomous vehicles capable of safe and efficient operation in complex urban environments.",1
"Advancements in machine learning algorithms have had a beneficial impact on representation learning, classification, and prediction models built using electronic health record (EHR) data. Effort has been put both on increasing models' overall performance as well as improving their interpretability, particularly regarding the decision-making process. In this study, we present a temporal deep learning model to perform bidirectional representation learning on EHR sequences with a transformer architecture to predict future diagnosis of depression. This model is able to aggregate five heterogenous and high-dimensional data sources from the EHR and process them in a temporal manner for chronic disease prediction at various prediction windows. We applied the current trend of pretraining and fine-tuning on EHR data to outperform the current state-of-the-art in chronic disease prediction, and to demonstrate the underlying relation between EHR codes in the sequence. The model generated the highest increases of precision-recall area under the curve (PRAUC) from 0.70 to 0.76 in depression prediction compared to the best baseline model. Furthermore, the self-attention weights in each sequence quantitatively demonstrated the inner relationship between various codes, which improved the model's interpretability. These results demonstrate the model's ability to utilize heterogeneous EHR data to predict depression while achieving high accuracy and interpretability, which may facilitate constructing clinical decision support systems in the future for chronic disease screening and early detection.",0
"In this study, we aimed to investigate the use of bidirectional representation learning from transformers (BERT) on multimodal electronic health record data for predicting depression. We trained a BERT model on both textual and visual features extracted from patient records and assessed its performance in predicting depression diagnosis. Our results showed that the BERT model achieved high accuracy and outperformed traditional machine learning models on this task. This suggests that incorporating both textual and visual features through bidirectional representation learning can improve the prediction of mental health conditions such as depression using EHR data. Additionally, our findings highlight the potential utility of pretrained language models like BERT for medical applications, especially for tasks involving natural language processing and understanding. Further research is warranted to validate these promising initial findings across diverse populations and clinical settings.",1
"Micro-Expression Recognition (MER) is a challenging task as the subtle changes occur over different action regions of a face. Changes in facial action regions are formed as Action Units (AUs), and AUs in micro-expressions can be seen as the actors in cooperative group activities. In this paper, we propose a novel deep neural network model for objective class-based MER, which simultaneously detects AUs and aggregates AU-level features into micro-expression-level representation through Graph Convolutional Networks (GCN). Specifically, we propose two new strategies in our AU detection module for more effective AU feature learning: the attention mechanism and the balanced detection loss function. With those two strategies, features are learned for all the AUs in a unified model, eliminating the error-prune landmark detection process and tedious separate training for each AU. Moreover, our model incorporates a tailored objective class-based AU knowledge-graph, which facilitates the GCN to aggregate the AU-level features into a micro-expression-level feature representation. Extensive experiments on two tasks in MEGC 2018 show that our approach significantly outperforms the current state-of-the-arts in MER. Additionally, we also report our single model-based micro-expression AU detection results.",0
"This paper presents a novel approach for objective class-based micro-expression recognition that combines simultaneous action unit detection and feature aggregation. The proposed method involves detecting facial actions at multiple scales using a convolutional neural network (CNN) and then fusing these outputs into a global representation. The features extracted from this representation are used to train a support vector machine (SVM) classifier for discriminating between different classes of micro-expressions. Experimental results on two publicly available datasets demonstrate the effectiveness of our method, achieving state-of-the-art performance in several cases. Our approach shows promise for applications such as behavior analysis, surveillance, and lie detection.",1
"Graph Neural Networks (GNNs) have attracted increasing attention due to its successful applications on various graph-structure data. However, recent studies have shown that adversarial attacks are threatening the functionality of GNNs. Although numerous works have been proposed to defend adversarial attacks from various perspectives, most of them can be robust against the attacks only on specific scenarios. To address this shortage of robust generalization, we propose to defend the adversarial attacks on GNN through applying the Spatio-Temporal sparsification (called ST-Sparse) on the GNN hidden node representation. ST-Sparse is similar to the Dropout regularization in spirit. Through intensive experiment evaluation with GCN as the target GNN model, we identify the benefits of ST-Sparse as follows: (1) ST-Sparse shows the defense performance improvement in most cases, as it can effectively increase the robust accuracy by up to 6\% improvement; (2) ST-Sparse illustrates its robust generalization capability by integrating with the existing defense methods, similar to the integration of Dropout into various deep learning models as a standard regularization technique; (3) ST-Sparse also shows its ordinary generalization capability on clean datasets, in that ST-SparseGCN (the integration of ST-Sparse and the original GCN) even outperform the original GCN, while the other three representative defense methods are inferior to the original GCN.",0
"Title: Spatio-Temporal Sparsification for General Robust Graph Convolution Networks Abstract: This paper proposes a novel method called spatio-temporal sparsification for general robust graph convolution networks. The proposed approach takes advantage of the temporal dimension by jointly learning spatial and temporal features. We show that using a sparse representation in both dimensions leads to improved performance on benchmark datasets while reducing computational costs. Our method employs recent advancements in graph neural network training such as attention mechanisms and edge conditioning, which improve robustness against overfitting and noise respectively. We demonstrate through extensive experiments on a variety of tasks including node classification, link prediction, and anomaly detection, that our model outperforms current state-of-the-art models across all three categories. These results highlight the potential of spatio-temporal sparsification as a powerful tool for constructing highly efficient deep learning models for graph data analysis. Keywords: graph neural networks, time series analysis, feature selection, data mining, machine learning.",1
"Controllable Image Captioning (CIC) -- generating image descriptions following designated control signals -- has received unprecedented attention over the last few years. To emulate the human ability in controlling caption generation, current CIC studies focus exclusively on control signals concerning objective properties, such as contents of interest or descriptive patterns. However, we argue that almost all existing objective control signals have overlooked two indispensable characteristics of an ideal control signal: 1) Event-compatible: all visual contents referred to in a single sentence should be compatible with the described activity. 2) Sample-suitable: the control signals should be suitable for a specific image sample. To this end, we propose a new control signal for CIC: Verb-specific Semantic Roles (VSR). VSR consists of a verb and some semantic roles, which represents a targeted activity and the roles of entities involved in this activity. Given a designated VSR, we first train a grounded semantic role labeling (GSRL) model to identify and ground all entities for each role. Then, we propose a semantic structure planner (SSP) to learn human-like descriptive semantic structures. Lastly, we use a role-shift captioning model to generate the captions. Extensive experiments and ablations demonstrate that our framework can achieve better controllability than several strong baselines on two challenging CIC benchmarks. Besides, we can generate multi-level diverse captions easily. The code is available at: https://github.com/mad-red/VSR-guided-CIC.",0
"This research presents a novel approach to human-like controllable image caption generation using verb-specific semantic roles. We propose a model that generates captions by predicting the semantic role of each verb present in the sentence. Our method utilizes pre-trained language models as well as custom domain specific embeddings to better capture the nuances of each individual verb. Experimental results show that our method outperforms state-of-the-art baseline models in terms of both automatic metrics such as BLEU score and manual evaluation measures like content selection, accuracy and informativeness of generated captions. Furthermore, we demonstrate through user studies that our method can generate more human-like captions that users prefer over other methods. Overall, our work advances the field of computer vision and natural language processing by providing a more intuitive and flexible framework for generating image captions based on semantic roles.",1
"Colon and Lung cancer is one of the most perilous and dangerous ailments that individuals are enduring worldwide and has become a general medical problem. To lessen the risk of death, a legitimate and early finding is particularly required. In any case, it is a truly troublesome task that depends on the experience of histopathologists. If a histologist is under-prepared it may even hazard the life of a patient. As of late, deep learning has picked up energy, and it is being valued in the analysis of Medical Imaging. This paper intends to utilize and alter the current pre-trained CNN-based model to identify lung and colon cancer utilizing histopathological images with better augmentation techniques. In this paper, eight distinctive Pre-trained CNN models, VGG16, NASNetMobile, InceptionV3, InceptionResNetV2, ResNet50, Xception, MobileNet, and DenseNet169 are trained on LC25000 dataset. The model performances are assessed on precision, recall, f1score, accuracy, and auroc score. The results exhibit that all eight models accomplished noteworthy results ranging from 96% to 100% accuracy. Subsequently, GradCAM and SmoothGrad are also used to picture the attention images of Pre-trained CNN models classifying malignant and benign images.",0
"This paper presents a methodology that utilizes pre-trained convolutional neural network (CNN) models to predict the presence of lung and colon cancer in histopathological images through the use of class activation and saliency maps. These maps allow for visual inspection of regions within the image which contributed most significantly to the predicted classification outcome. Experimental results on two separate datasets demonstrate improved accuracy over traditional methods as well as superior interpretability via the generated visualizations. Future works aim at extending the approach towards automation of diagnostic processes, thereby supporting pathologists in their daily tasks and improving patient outcomes.",1
"Research into the task of re-identification (ReID) is picking up momentum in computer vision for its many use cases and zero-shot learning nature. This paper proposes a computationally efficient fine-grained ReID model, FGReID, which is among the first models to unify image and video ReID while keeping the number of training parameters minimal. FGReID takes advantage of video-based pre-training and spatial feature attention to improve performance on both video and image ReID tasks. FGReID achieves state-of-the-art (SOTA) on MARS, iLIDS-VID, and PRID-2011 video person ReID benchmarks. Eliminating temporal pooling yields an image ReID model that surpasses SOTA on CUHK01 and Market1501 image person ReID benchmarks. The FGReID achieves near SOTA performance on the vehicle ReID dataset VeRi as well, demonstrating its ability to generalize. Additionally we do an ablation study analyzing the key features influencing model performance on ReID tasks. Finally, we discuss the moral dilemmas related to ReID tasks, including the potential for misuse. Code for this work is publicly available at https: //github.com/ppriyank/Fine-grained-ReIdentification.",0
"This research work presents a novel approach for fine-grained re-identification (FGR), which addresses one of the most challenging tasks in computer vision: person identification across multiple cameras. FGR involves identifying individuals at a more detailed level than traditional re-identification, such as distinguishing among different instances of the same individual wearing similar clothing or different poses. Our proposed method leverages state-of-the-art deep learning techniques, including feature extraction using convolutional neural networks (CNNs) and metric learning using triplet loss. We demonstrate through extensive experiments on several benchmark datasets that our approach significantly outperforms existing methods, achieving new levels of accuracy in FGR. Our results highlight the potential applications of FGR in real-world scenarios such as surveillance, retail analytics, and automated attendance tracking.",1
"Current state-of-the-art approaches to cross-modal retrieval process text and visual input jointly, relying on Transformer-based architectures with cross-attention mechanisms that attend over all words and objects in an image. While offering unmatched retrieval performance, such models: 1) are typically pretrained from scratch and thus less scalable, 2) suffer from huge retrieval latency and inefficiency issues, which makes them impractical in realistic applications. To address these crucial gaps towards both improved and efficient cross-modal retrieval, we propose a novel fine-tuning framework which turns any pretrained text-image multi-modal model into an efficient retrieval model. The framework is based on a cooperative retrieve-and-rerank approach which combines: 1) twin networks to separately encode all items of a corpus, enabling efficient initial retrieval, and 2) a cross-encoder component for a more nuanced (i.e., smarter) ranking of the retrieved small set of items. We also propose to jointly fine-tune the two components with shared weights, yielding a more parameter-efficient model. Our experiments on a series of standard cross-modal retrieval benchmarks in monolingual, multilingual, and zero-shot setups, demonstrate improved accuracy and huge efficiency benefits over the state-of-the-art cross-encoders.",0
"This paper describes two novel approaches that can improve cross-modal retrieval tasks. The first approach uses a cooperative method where multiple models work together on different aspects of the task. For example, one model may focus on image analysis while another model focuses on natural language processing. By combining the results from these models, more accurate results can be obtained. Additionally, the second approach involves reranking retrieved items based on their relevance. This can further refine the search process by reducing irrelevant items in the top ranked results. We evaluate both methods using three datasets including image-to-text, text-to-image, and video-to-text retrieval tasks. Our experiments show significant improvements over baseline models across all datasets. These findings have important implications for cross-modal retrieval applications such as multimedia search engines and recommendation systems.",1
"Among image classification, skip and densely-connection-based networks have dominated most leaderboards. Recently, from the successful development of multi-head attention in natural language processing, it is sure that now is a time of either using a Transformer-like model or hybrid CNNs with attention. However, the former need a tremendous resource to train, and the latter is in the perfect balance in this direction. In this work, to make CNNs handle global and local information, we proposed UPANets, which equips channel-wise attention with a hybrid skip-densely-connection structure. Also, the extreme-connection structure makes UPANets robust with a smoother loss landscape. In experiments, UPANets surpassed most well-known and widely-used SOTAs with an accuracy of 96.47% in Cifar-10, 80.29% in Cifar-100, and 67.67% in Tiny Imagenet. Most importantly, these performances have high parameters efficiency and only trained in one customer-based GPU. We share implementing code of UPANets in https://github.com/hanktseng131415go/UPANets.",0
"Title of Abstract: ""Learning from Universal Pixel Attention"" This work presents UPANets (Universal Pixel Attention Networks), a new architecture that effectively utilizes universal attention mechanisms at the pixel level for image classification tasks. Conventional convolutional neural networks rely on sparse spatial connections between neurons, which limits their ability to capture fine details in images. In contrast, UPANets use self-attention modules to selectively aggregate features based on global relationships across all pixels, allowing them to learn more complex representations. Our approach improves upon previous methods by using a novel attention mechanism tailored specifically for pixel-level features, resulting in state-of-the-art performance on several benchmark datasets such as CIFAR-10, ImageNet, and SVHN. Furthermore, our model achieves comparable results to recent advances while requiring fewer parameters, making it suitable for deployment on devices with limited resources. We believe this research has significant implications for computer vision applications where high accuracy and efficiency are crucial factors.",1
"RGB-D salient object detection (SOD) is usually formulated as a problem of classification or regression over two modalities, i.e., RGB and depth. Hence, effective RGBD feature modeling and multi-modal feature fusion both play a vital role in RGB-D SOD. In this paper, we propose a depth-sensitive RGB feature modeling scheme using the depth-wise geometric prior of salient objects. In principle, the feature modeling scheme is carried out in a depth-sensitive attention module, which leads to the RGB feature enhancement as well as the background distraction reduction by capturing the depth geometry prior. Moreover, to perform effective multi-modal feature fusion, we further present an automatic architecture search approach for RGB-D SOD, which does well in finding out a feasible architecture from our specially designed multi-modal multi-scale search space. Extensive experiments on seven standard benchmarks demonstrate the effectiveness of the proposed approach against the state-of-the-art.",0
"This paper presents a novel approach to deep saliency detection using RGB-D images. Our method utilizes depth-sensitive attention mechanisms to capture important features from the depth channel, which has been previously underutilized in state-of-the-art methods. In addition, we propose automatic multi-modal fusion, where both visual and depth information are combined seamlessly without any hand engineering of feature weights. Experiments on two benchmark datasets demonstrate that our proposed method outperforms previous state-of-the-art approaches by significant margins. We also provide ablation studies to validate each contribution made by depth-sensitive attention and automulti-modal fusion. Overall, this work represents a major step forward in understanding how depth information can improve saliency prediction tasks, opening up new directions for future research.",1
"Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, \eg, the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-ViT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0\% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3\% top1 accuracy in image resolution 384$\times$384 on ImageNet. (Code: https://github.com/yitu-opensource/T2T-ViT)",0
"In recent years, Vision Transformers (ViTs) have achieved significant progress in computer vision tasks. However, training these models from scratch remains challenging due to their large size and high computational requirements. One potential solution is to use pre-training techniques that leverage data augmentation methods to improve robustness, reduce overfitting, and speed up convergence. In our research, we propose using a combination of tokenization and contrastive learning to train ViTs from scratch on ImageNet. Our approach is based on several key insights: first, breaking down images into tokens allows us to utilize strong supervision signals at finer scales; second, contrastive self-supervised learning can help learn meaningful representations without relying solely on labels; third, combining these strategies enables efficient training even without batch renormalization or other advanced techniques commonly used by prior arts. To evaluate the effectiveness of our method, we conducted experiments on standard benchmarks such as Imagenet and transferred settings like COCO object detection and segmentation. We found that our model outperforms competitive baselines while requiring fewer parameters and FLOPs during inference time. Overall, our work demonstrates that it is possible to achieve state-of-the-art results using simple yet effective pre-training principles applied directly to transformers trained on real world dataset like ImageNET.",1
"We consider a setting where multiple entities inter-act with each other over time and the time-varying statuses of the entities are represented as multiple correlated time series. For example, speed sensors are deployed in different locations in a road network, where the speed of a specific location across time is captured by the corresponding sensor as a time series, resulting in multiple speed time series from different locations, which are often correlated. To enable accurate forecasting on correlated time series, we proposes graph attention recurrent neural networks.First, we build a graph among different entities by taking into account spatial proximity and employ a multi-head attention mechanism to derive adaptive weight matrices for the graph to capture the correlations among vertices (e.g., speeds at different locations) at different timestamps. Second, we employ recurrent neural networks to take into account temporal dependency while taking into account the adaptive weight matrices learned from the first step to consider the correlations among time series.Experiments on a large real-world speed time series data set suggest that the proposed method is effective and outperforms the state-of-the-art in most settings. This manuscript provides a full version of a workshop paper [1].",0
"In recent years, time series forecasting has become increasingly important as businesses seek to make informed decisions based on future projections. One approach that shows promise for time series forecasting is graph attention recurrent neural networks (GARN). GARN models capture complex relationships between input variables by leveraging graph structures that explicitly model correlations among them. This can lead to improved accuracy compared to traditional methods that rely solely on sequential data inputs. We present a full version of our study comparing GARN to other state-of-the-art approaches on multiple benchmark datasets. Our results demonstrate the effectiveness of using GARN models for time series forecasting tasks, particularly in scenarios where the underlying system exhibits strong dependencies between components. Overall, we believe our work provides valuable insights into the utility of incorporating explicit correlation structures through graphs within deep learning architectures designed specifically for sequential data prediction tasks.",1
"Automated segmentation of brain glioma plays an active role in diagnosis decision, progression monitoring and surgery planning. Based on deep neural networks, previous studies have shown promising technologies for brain glioma segmentation. However, these approaches lack powerful strategies to incorporate contextual information of tumor cells and their surrounding, which has been proven as a fundamental cue to deal with local ambiguity. In this work, we propose a novel approach named Context-Aware Network (CANet) for brain glioma segmentation. CANet captures high dimensional and discriminative features with contexts from both the convolutional space and feature interaction graphs. We further propose context guided attentive conditional random fields which can selectively aggregate features. We evaluate our method using publicly accessible brain glioma segmentation datasets BRATS2017, BRATS2018 and BRATS2019. The experimental results show that the proposed algorithm has better or competitive performance against several State-of-The-Art approaches under different segmentation metrics on the training and validation sets.",0
"In recent years, 3D brain glioma segmentation has become increasingly important due to advancements in medical imaging technologies such as magnetic resonance (MR) and computed tomography (CT). Automatic segmentation methods have been developed that can accurately delineate tumor boundaries from these images, but many of them struggle with variability across different scans and regions within the same scan. To address this issue, we propose Context Aware Networks (CANets), which utilize contextual features extracted from both local and global image surroundings to enhance segmentation accuracy. Our experiments demonstrate improved segmentation results compared to several state-of-the-art methods on three public datasets of varying complexities. We believe our proposed method can provide clinicians with more accurate delineations of tumors, leading to better treatment planning and outcomes for patients with brain gliomas.",1
"Image inpainting task requires filling the corrupted image with contents coherent with the context. This research field has achieved promising progress by using neural image inpainting methods. Nevertheless, there is still a critical challenge in guessing the missed content with only the context pixels. The goal of this paper is to fill the semantic information in corrupted images according to the provided descriptive text. Unique from existing text-guided image generation works, the inpainting models are required to compare the semantic content of the given text and the remaining part of the image, then find out the semantic content that should be filled for missing part. To fulfill such a task, we propose a novel inpainting model named Text-Guided Dual Attention Inpainting Network (TDANet). Firstly, a dual multimodal attention mechanism is designed to extract the explicit semantic information about the corrupted regions, which is done by comparing the descriptive text and complementary image areas through reciprocal attention. Secondly, an image-text matching loss is applied to maximize the semantic similarity of the generated image and the text. Experiments are conducted on two open datasets. Results show that the proposed TDANet model reaches new state-of-the-art on both quantitative and qualitative measures. Result analysis suggests that the generated images are consistent with the guidance text, enabling the generation of various results by providing different descriptions. Codes are available at https://github.com/idealwhite/TDANet",0
"This is the abstract for the research paper ""Text-guided neural image inpainting"" which presents a novel approach for generating visually realistic images by using textual guidance: The main contributions of this work are: * A new method for guiding generative models with text prompts, which allows fine-grained control over generated outputs and enables the creation of high-quality imagery that closely adheres to descriptive language. * Quantitative evaluation on challenging benchmark datasets shows consistent improvement over prior art across both quantitative (PSNR) and qualitative metrics. This study demonstrates how text can serve as a powerful interface for controlling complex systems like modern generative networks while also outperforming previous methods. Overall, we believe that these findings will advance the field towards creating more intelligent artificial agents that are better equipped to fulfill human needs.",1
"The neuromorphic event cameras, which capture the optical changes of a scene, have drawn increasing attention due to their high speed and low power consumption. However, the event data are noisy, sparse, and nonuniform in the spatial-temporal domain with an extremely high temporal resolution, making it challenging to design backend algorithms for event-based vision. Existing methods encode events into point-cloud-based or voxel-based representations, but suffer from noise and/or information loss. Additionally, there is little research that systematically studies how to handle static and dynamic scenes with one universal design for event-based vision. This work proposes the Aligned Event Tensor (AET) as a novel event data representation, and a neat framework called Event Frame Net (EFN), which enables our model for event-based vision under static and dynamic scenes. The proposed AET and EFN are evaluated on various datasets, and proved to surpass existing state-of-the-art methods by large margins. Our method is also efficient and achieves the fastest inference speed among others.",0
"""Event-based vision"" refers to a computational approach that seeks to mimic how biological visual systems process sensory input by using event-driven processing rather than relying on continuous sensor data streams. This type of system has several potential advantages over traditional frame-based approaches, such as reduced power consumption and improved adaptability to changing lighting conditions. In recent years, there have been many advances in developing event-based vision hardware and algorithms, but these designs often lack versatility or suffer from tradeoffs between static and dynamic performance. To address these limitations, we present AET-EFN, a versatile design for static and dynamic event-based vision that combines two complementary methods of image formation: asynchronous time-domain (ATD) processing and synchronous frequency-domain (SFD) processing. ATD processing allows for high-speed detection of moving edges in real-time, while SFD processing enables high-resolution reconstruction of images based on temporal frequency components. By leveraging both ATD and SFD processing, AET-EFN achieves excellent performance in a wide range of tasks including motion detection, object tracking, and edge extraction. We evaluate the effectiveness of our design through simulations and experiments using custom event-based vision chips, demonstrating its ability to outperform previous state-of-the-art solutions under varying lighting conditions and motion speeds. Overall, AET-EFN represents a significant step forward in the development of robust and flexible event-based vision systems, opening up new possibilities for applications in areas such as robotics, automotive, and environmental monitoring.""",1
"Arbitrary-oriented objects exist widely in natural scenes, and thus the oriented object detection has received extensive attention in recent years. The mainstream rotation detectors use oriented bounding boxes (OBB) or quadrilateral bounding boxes (QBB) to represent the rotating objects. However, these methods suffer from the representation ambiguity for oriented object definition, which leads to suboptimal regression optimization and the inconsistency between the loss metric and the localization accuracy of the predictions. In this paper, we propose a Representation Invariance Loss (RIL) to optimize the bounding box regression for the rotating objects. Specifically, RIL treats multiple representations of an oriented object as multiple equivalent local minima, and hence transforms bounding box regression into an adaptive matching process with these local minima. Then, the Hungarian matching algorithm is adopted to obtain the optimal regression strategy. We also propose a normalized rotation loss to alleviate the weak correlation between different variables and their unbalanced loss contribution in OBB representation. Extensive experiments on remote sensing datasets and scene text datasets show that our method achieves consistent and substantial improvement. The source code and trained models are available at https://github.com/ming71/RIDet.",0
"In object detection tasks, designing effective representations plays a crucial role in achieving high performance. Recently, representation learning has become increasingly important due to its ability to capture complex patterns from raw input data. However, obtaining accurate detections still remains challenging, especially in cases where objects may appear at arbitrary orientations. To address these issues, we propose optimization techniques that can improve object detection models by ensuring their robustness against orientation variations. Our approach focuses on leveraging the power of invariant feature representations which allows our model to learn more efficient and discriminative features. Extensive experiments conducted on public benchmarks demonstrate significant improvements in accuracy compared to existing methods.",1
"This paper proposes a new generative adversarial network for pose transfer, i.e., transferring the pose of a given person to a target pose. We design a progressive generator which comprises a sequence of transfer blocks. Each block performs an intermediate transfer step by modeling the relationship between the condition and the target poses with attention mechanism. Two types of blocks are introduced, namely Pose-Attentional Transfer Block (PATB) and Aligned Pose-Attentional Transfer Bloc ~(APATB). Compared with previous works, our model generates more photorealistic person images that retain better appearance consistency and shape consistency compared with input images. We verify the efficacy of the model on the Market-1501 and DeepFashion datasets, using quantitative and qualitative measures. Furthermore, we show that our method can be used for data augmentation for the person re-identification task, alleviating the issue of data insufficiency. Code and pretrained models are available at https://github.com/tengteng95/Pose-Transfer.git.",0
"This paper presents a novel method for generating images of persons that uses pose attention transfer (PAT) in combination with progressive learning rate scaling (PLRS). Our approach builds upon prior work on person image generation using diffusion models, but differs significantly from these methods by introducing key advances in architecture and training regimes that enable higher quality outputs at both high resolutions and wide range of poses. In addition, we introduce a new evaluation metric for measuring pose accuracy in generated images which improves correlation between human judgments and automatic metrics. Experimental results show that our model outperforms state-of-the-art methods in terms of both visual fidelity and pose accuracy. We believe our contributions have important implications for computer vision research as well as applications such as virtual reality avatars, animation, and fashion design where controllability over personal appearance is crucial.",1
"Human activities are hugely restricted by COVID-19, recently. Robots that can conduct inter-floor navigation attract much public attention, since they can substitute human workers to conduct the service work. However, current robots either depend on human assistance or elevator retrofitting, and fully autonomous inter-floor navigation is still not available. As the very first step of inter-floor navigation, elevator button segmentation and recognition hold an important position. Therefore, we release the first large-scale publicly available elevator panel dataset in this work, containing 3,718 panel images with 35,100 button labels, to facilitate more powerful algorithms on autonomous elevator operation. Together with the dataset, a number of deep learning based implementations for button segmentation and recognition are also released to benchmark future methods in the community. The dataset will be available at \url{https://github.com/zhudelong/elevator_button_recognition",0
"In this research, we present a large-scale dataset designed specifically for benchmarking elevator button segmentation and character recognition tasks. Our dataset consists of high-resolution images captured from real-world elevators and includes annotations for both buttons and characters. We carefully selected and annotated a diverse set of images, representing different countries and regions worldwide, as well as variations in lighting conditions, camera angles, and button styles. Our dataset contains over 28000 images, making it one of the largest publicly available datasets for such task. Additionally, our dataset provides valuable resources for future researchers, enabling them to develop more accurate models that can generalize better across different settings. We believe that this dataset will play a crucial role in advancing the state-of-the-art in the field of computer vision and machine learning by providing challenges beyond existing benchmarks. Furthermore, our work contributes towards addressing some of the societal needs, since reliable automated systems in monitoring and maintenance of lifts could contribute significantly towards safety of citizens.",1
"Automatic image aesthetics assessment is a computer vision problem that deals with the categorization of images into different aesthetic levels. The categorization is usually done by analyzing an input image and computing some measure of the degree to which the image adhere to the key principles of photography (balance, rhythm, harmony, contrast, unity, look, feel, tone and texture). Owing to its diverse applications in many areas, automatic image aesthetic assessment has gained significant research attention in recent years. This paper presents a literature review of the recent techniques of automatic image aesthetics assessment. A large number of traditional hand crafted and deep learning based approaches are reviewed. Key problem aspects are discussed such as why some features or models perform better than others and what are the limitations. A comparison of the quantitative results of different methods is also provided at the end.",0
"This abstract surveys two approaches to image aesthetic assessment: hand crafted methods and deep learning based models. Hand crafted feature engineering requires domain specific knowledge to design features that can explain the relationship between low level visual features and human judgment of images, whereas end-to-end trainable models learn directly from raw pixels or preprocessed data without any explicit interpretation of their meaning. We review both types of techniques along different dimensions such as dataset size, subjectivity analysis, multi-modal fusion etc. Furthermore we evaluate these metrics on real world applications such as photo album organization, artwork classification and web search query refinement. Finally, conclusions drawn discusses merits/demerits of each approach depending upon certain scenarios as well as future directions of research.",1
"In this paper, we tackle the problem of human de-occlusion which reasons about occluded segmentation masks and invisible appearance content of humans. In particular, a two-stage framework is proposed to estimate the invisible portions and recover the content inside. For the stage of mask completion, a stacked network structure is devised to refine inaccurate masks from a general instance segmentation model and predict integrated masks simultaneously. Additionally, the guidance from human parsing and typical pose masks are leveraged to bring prior information. For the stage of content recovery, a novel parsing guided attention module is applied to isolate body parts and capture context information across multiple scales. Besides, an Amodal Human Perception dataset (AHP) is collected to settle the task of human de-occlusion. AHP has advantages of providing annotations from real-world scenes and the number of humans is comparatively larger than other amodal perception datasets. Based on this dataset, experiments demonstrate that our method performs over the state-of-the-art techniques in both tasks of mask completion and content recovery. Our AHP dataset is available at \url{https://sydney0zq.github.io/ahp/}.",0
"This paper presents a novel approach to de-occluding humans by leveraging depth estimation and image warping techniques to produce visible images from occluded input data. Our method utilizes deep learning models trained on large datasets of human poses and silhouettes to estimate depth maps that accurately capture the three-dimensional structure of human bodies. These depth maps are then used as input to an image warping system, which generates new views of the occluded subjects based on their inferred shapes. We evaluate our approach using challenging benchmarks and show that our method outperforms state-of-the-art methods in terms of accuracy, robustness, and speed. Furthermore, we demonstrate the versatility of our technique through applications such as virtual try-on, video retargeting, and 3D reconstruction. Overall, our work represents an important step towards realizing invisible perception technologies for humans and opens up exciting possibilities for future research.",1
"This paper addresses the problem of temporal sentence grounding (TSG), which aims to identify the temporal boundary of a specific segment from an untrimmed video by a sentence query. Previous works either compare pre-defined candidate segments with the query and select the best one by ranking, or directly regress the boundary timestamps of the target segment. In this paper, we propose a novel localization framework that scores all pairs of start and end indices within the video simultaneously with a biaffine mechanism. In particular, we present a Context-aware Biaffine Localizing Network (CBLN) which incorporates both local and global contexts into features of each start/end position for biaffine-based localization. The local contexts from the adjacent frames help distinguish the visually similar appearance, and the global contexts from the entire video contribute to reasoning the temporal relation. Besides, we also develop a multi-modal self-attention module to provide fine-grained query-guided video representation for this biaffine strategy. Extensive experiments show that our CBLN significantly outperforms state-of-the-arts on three public datasets (ActivityNet Captions, TACoS, and Charades-STA), demonstrating the effectiveness of the proposed localization framework.",0
"This abstract provides a brief overview of the research paper ""Context-Aware Biaffine Localizing Network for Temporal Sentence Grounding."" In recent years, there has been significant progress in developing methods for temporal sentence grounding, which refers to the process of linking natural language expressions referring to specific time intervals (e.g., ""yesterday"", ""last year"") to their corresponding spans in unstructured video data.  One approach that has proven effective in improving the accuracy of temporal sentence grounding systems is using convolutional neural networks (CNNs) with attention mechanisms to selectively focus on relevant visual features at different levels of abstraction. However, most existing approaches still suffer from limitations such as lack of robustness to changes in context across videos, sensitivity to hyperparameter settings, and high computational cost due to expensive postprocessing steps.  This research proposes a novel framework called biaffine localizer network that addresses these challenges by incorporating several innovations: Firstly, instead of relying solely on global representations, our model uses temporal pyramid pooling to capture spatial variation within each frame and aggregate information from multiple scales. Secondly, we introduce two lightweight affinity modules (AMs), each specialized for capturing either short or long-range dependencies along the temporal dimension, while minimizing computational overhead. Finally, we combine both AMs via a novel bi-affine fusion mechanism, taking advantage of complementary strengths offered by both subnetworks without increasing parameter count. Our experiments demonstrate that our proposed method outperforms state-of-the-art methods in terms of precision, recall, F1 score and speed on popular benchmark datasets.  In summary",1
"To improve the system performance towards the Shannon limit, advanced radio resource management mechanisms play a fundamental role. In particular, scheduling should receive much attention, because it allocates radio resources among different users in terms of their channel conditions and QoS requirements. The difficulties of scheduling algorithms are the tradeoffs need to be made among multiple objectives, such as throughput, fairness and packet drop rate. We propose a smart scheduling scheme based on deep reinforcement learning (DRL). We not only verify the performance gain achieved, but also provide implementation-friend designs, i.e., a scalable neural network design for the agent and a virtual environment training framework. With the scalable neural network design, the DRL agent can easily handle the cases when the number of active users is time-varying without the need to redesign and retrain the DRL agent. Training the DRL agent in a virtual environment offline first and using it as the initial version in the practical usage helps to prevent the system from suffering from performance and robustness degradation due to the time-consuming training. Through both simulations and field tests, we show that the DRL-based smart scheduling outperforms the conventional scheduling method and can be adopted in practical systems.",0
"Abstract: Cellular networks provide connectivity services across wide areas that support mission critical applications such as emergency communications systems. However, these networks can become congested due to high data traffic load, which leads to poor Quality of Service (QoS) and unsatisfied subscribers. To address the issue of network congestion and improve QoS, a smart scheduling algorithm based on deep reinforcement learning is proposed. This work demonstrates that by using deep neural networks combined with the Q-learning algorithm, the scheduler can learn from previous decisions and make better choices to maximize reward while minimizing interference among users. Furthermore, simulation results show significant improvements in system throughput and user satisfaction compared to traditional round robin scheduling algorithms. Overall, this research shows promising results towards enhancing cellular network performance.",1
"The recent advancements in machine learning (ML) have demonstrated the potential for providing a powerful solution to build complex prediction systems in a short time. However, in highly regulated industries, such as the financial technology (Fintech), people have raised concerns about the risk of ML systems discriminating against specific protected groups or individuals. To address these concerns, researchers have introduced various mathematical fairness metrics and bias mitigation algorithms. This paper discusses hidden technical debts and challenges of building fair ML systems in a production environment for Fintech. We explore various stages that require attention for fairness in the ML system development and deployment life cycle. To identify hidden technical debts that exist in building fair ML system for Fintech, we focus on key pipeline stages including data preparation, model development, system monitoring and integration in production. Our analysis shows that enforcing fairness for production-ready ML systems in Fintech requires specific engineering commitments at different stages of ML system life cycle. We also propose several initial starting points to mitigate these technical debts for deploying fair ML systems in production.",0
"In recent years there has been increasing pressure on financial institutions to adopt machine learning models to improve their decision making process. However, these models can often introduce hidden technical debts that may go unnoticed until they cause significant problems down the line. This paper examines some of the common sources of technical debts in machine learning systems used in financial services, including data quality issues, model selection biases, software engineering practices, infrastructure limitations, and regulatory compliance challenges. Drawing from real-world examples and case studies, we provide insights into how such debts accumulate over time and discuss strategies for mitigating them before they become major roadblocks in maintaining and enhancing ML applications. Our work highlights the importance of taking a holistic view of fairness considerations during all stages of the ML lifecycle and emphasizes the need for continuous monitoring and evaluation of ML models deployed in high stakes settings. Ultimately, our findings have important implications for both researchers and practitioners working at the intersection of machine learning and finance.",1
"Representation learning methods for heterogeneous networks produce a low-dimensional vector embedding for each node that is typically fixed for all tasks involving the node. Many of the existing methods focus on obtaining a static vector representation for a node in a way that is agnostic to the downstream application where it is being used. In practice, however, downstream tasks such as link prediction require specific contextual information that can be extracted from the subgraphs related to the nodes provided as input to the task. To tackle this challenge, we develop SLiCE, a framework bridging static representation learning methods using global information from the entire graph with localized attention driven mechanisms to learn contextual node representations. We first pre-train our model in a self-supervised manner by introducing higher-order semantic associations and masking nodes, and then fine-tune our model for a specific link prediction task. Instead of training node representations by aggregating information from all semantic neighbors connected via metapaths, we automatically learn the composition of different metapaths that characterize the context for a specific task without the need for any pre-defined metapaths. SLiCE significantly outperforms both static and contextual embedding learning methods on several publicly available benchmark network datasets. We also interpret the semantic association matrix and provide its utility and relevance in making successful link predictions between heterogeneous nodes in the network.",0
"In recent years, researchers have proposed many methods for training neural networks on large amounts of data to predict links in heterogenous graphs. However, most of these methods require vast quantities of labeled examples to achieve good results. This work addresses link prediction in heterogenous networks using self supervised learning of contextual embeddings, which can learn from unlabeled data alone. We evaluate our approach on several real world datasets across different domains including social, informational and biological networks, demonstrating improved performance over traditional supervised baselines while requiring far fewer labels during training time. Our findings suggest that pretraining models using self-supervised embedding techniques can lead to better generalization for link prediction tasks, even outperforming fully supervised alternatives at times. Overall, we believe this work could have broad impact within machine learning research by providing more cost effective solutions that perform well in real world scenarios, while reducing reliance on annotated data sources.",1
"Motion behaviour is driven by several factors -- goals, presence and actions of neighbouring agents, social relations, physical and social norms, the environment with its variable characteristics, and further. Most factors are not directly observable and must be modelled from context. Trajectory prediction, is thus a hard problem, and has seen increasing attention from researchers in the recent years. Prediction of motion, in application, must be realistic, diverse and controllable. In spite of increasing focus on multimodal trajectory generation, most methods still lack means for explicitly controlling different modes of the data generation. Further, most endeavours invest heavily in designing special mechanisms to learn the interactions in latent space. We present Conditional Speed GAN (CSG), that allows controlled generation of diverse and socially acceptable trajectories, based on user controlled speed. During prediction, CSG forecasts future speed from latent space and conditions its generation based on it. CSG is comparable to state-of-the-art GAN methods in terms of the benchmark distance metrics, while being simple and useful for simulation and data augmentation for different contexts such as fast or slow paced environments. Additionally, we compare the effect of different aggregation mechanisms and show that a naive approach of concatenation works comparable to its attention and pooling alternatives.",0
"This paper introduces conditional generative adversarial networks (CGAN) as a tool for speed control in trajectory simulation. In simulations involving human agents, providing realistic time delays in actions can be challenging due to computational limitations. Our method uses CGANs to estimate how fast an action should occur given certain agent states, enabling more accurate timing in simulated scenarios. Results demonstrate significant improvements over current methods in both performance metrics and human evaluation. Potential applications range from training autonomous systems to behavior analysis in social sciences. Overall, our work presents a novel approach that addresses a critical gap in trajectory simulation.",1
"The rapid spread of the new pandemic, i.e., COVID-19, has severely threatened global health. Deep-learning-based computer-aided screening, e.g., COVID-19 infected CT area segmentation, has attracted much attention. However, the publicly available COVID-19 training data are limited, easily causing overfitting for traditional deep learning methods that are usually data-hungry with millions of parameters. On the other hand, fast training/testing and low computational cost are also necessary for quick deployment and development of COVID-19 screening systems, but traditional deep learning methods are usually computationally intensive. To address the above problems, we propose MiniSeg, a lightweight deep learning model for efficient COVID-19 segmentation. Compared with traditional segmentation methods, MiniSeg has several significant strengths: i) it only has 83K parameters and is thus not easy to overfit; ii) it has high computational efficiency and is thus convenient for practical deployment; iii) it can be fast retrained by other users using their private COVID-19 data for further improving performance. In addition, we build a comprehensive COVID-19 segmentation benchmark for comparing MiniSeg to traditional methods.",0
"Title: Abstract Author(s): Your Name(s) (change as necessary!) Institution(s): Add institution(s), if desired! Contact Information (e.g., email addresses, personal/professional website URLs): Optionally provide contact details! Citation: Replace `{}` with appropriate citation info & style guide! Keywords: Include keywords relevant to your content! --- COVID-19 has affected every corner of the world. As vaccination efforts ramp up globally, there remains an urgent need for efficient and accurate diagnostic tools that can support rapid decision making at hospitals, clinics, and other medical facilities. Among the essential components of any comprehensive diagnostic solution is image analysis software that can extract valuable data from medical images such as X-rays and CT scans. In particular, automatic segmentation algorithms have emerged as crucial tools for isolating regions of interest within these images, allowing physicians to focus their attention on areas most likely to contain telltale signs of the disease. In recent years, convolutional neural networks (CNNs) have become popular choices for performing this task due to their impressive accuracy across a wide range of imaging modalities and use cases. However, despite their effectiveness, CNNs remain computationally intensive, requiring significant computational resources and time to process even moderately-sized datasets. This presents a challenge in lowresource settings where fast decisions must still be made using limited computing infrastructure, placing patients at risk. To address this issue, we present the design and evaluation of MiniSeg - a lightweight variant of UNet++, which uses dilated causal convolutions and dynamic filters to perform highly competitive abdominal COVID-19 pneumonia segmentation, while consuming fewer computational resources than traditional methods. We demonstrate our method's potential by comparing its performance against two state-of-the-art models. Our results showcased a significant reduction in FLOPs and parameters alongside improved inference speeds wit",1
"Recently, the study on object detection in aerial images has made tremendous progress in the community of computer vision. However, most state-of-the-art methods tend to develop elaborate attention mechanisms for the space-time feature calibrations with high computational complexity, while surprisingly ignoring the importance of feature calibrations in channels. In this work, we propose a simple yet effective Calibrated-Guidance (CG) scheme to enhance channel communications in a feature transformer fashion, which can adaptively determine the calibration weights for each channel based on the global feature affinity-pairs. Specifically, given a set of feature maps, CG first computes the feature similarity between each channel and the remaining channels as the intermediary calibration guidance. Then, re-representing each channel by aggregating all the channels weighted together via the guidance. Our CG can be plugged into any deep neural network, which is named as CG-Net. To demonstrate its effectiveness and efficiency, extensive experiments are carried out on both oriented and horizontal object detection tasks of aerial images. Results on two challenging benchmarks (i.e., DOTA and HRSC2016) demonstrate that our CG-Net can achieve state-of-the-art performance in accuracy with a fair computational overhead. https://github.com/WeiZongqi/CG-Net",0
"In recent years, object detection has become increasingly important for applications such as autonomous vehicles, surveillance systems, and remote sensing. Despite significant advances in computer vision algorithms, detecting objects in aerial images remains challenging due to factors such as changes in lighting conditions, occlusions, and complex backgrounds. To address these issues, we propose a novel framework that leverages calibration guidance for more accurate object detection in aerial images. Our approach involves pre-calibrating the detector by fine-tuning on a small set of labeled data before applying it to large unlabeled datasets. We demonstrate through experiments that our method outperforms state-of-the-art approaches while requiring significantly less training data. Furthermore, we show how our framework can adaptively adjust its level of guidance based on confidence scores generated from previously detected objects. These results highlight the effectiveness of our proposed method for improving object detection accuracy in aerial imagery under real-world conditions.",1
"Visual navigation for autonomous agents is a core task in the fields of computer vision and robotics. Learning-based methods, such as deep reinforcement learning, have the potential to outperform the classical solutions developed for this task; however, they come at a significantly increased computational load. Through this work, we design a novel approach that focuses on performing better or comparable to the existing learning-based solutions but under a clear time/computational budget. To this end, we propose a method to encode vital scene semantics such as traversable paths, unexplored areas, and observed scene objects -- alongside raw visual streams such as RGB, depth, and semantic segmentation masks -- into a semantically informed, top-down egocentric map representation. Further, to enable the effective use of this information, we introduce a novel 2-D map attention mechanism, based on the successful multi-layer Transformer networks. We conduct experiments on 3-D reconstructed indoor PointGoal visual navigation and demonstrate the effectiveness of our approach. We show that by using our novel attention schema and auxiliary rewards to better utilize scene semantics, we outperform multiple baselines trained with only raw inputs or implicit semantic information while operating with an 80% decrease in the agent's experience.",0
"In recent years, visual navigation has emerged as a crucial task in robotics and computer vision, enabling autonomous agents to navigate through complex environments. While many approaches have been proposed for visual navigation, they often require large amounts of computation and data resources. This paper presents a new approach called ""Map Attention with Semantic Transformers (MaAST)"" which addresses these limitations by combining map attention mechanisms with semantic transformer networks. Our approach builds upon previous work that showed that integrating image features with high level semantics can improve performance on navigation tasks. However, our method differs from prior works by proposing a novel spatial attention mechanism specifically designed for map inputs. We further improve over existing methods by incorporating self-supervised pretraining objectives based on scene completion tasks, allowing our model to generalize better across different scenes. Through extensive experiments on three challenging benchmark datasets, we demonstrate the effectiveness and efficiency of our method compared to state-of-the-art approaches. Additionally, we provide detailed ablation studies to analyze the impact of each component in our framework. Overall, our work shows that MaAST achieves both efficient and accurate performance for visual navigation, making it well suited for real world applications such as service robots navigating in human environments.",1
"Recently, video-based person re-identification (re-ID) has drawn increasing attention in compute vision community because of its practical application prospects. Due to the inaccurate person detections and pose changes, pedestrian misalignment significantly increases the difficulty of feature extraction and matching. To address this problem, in this paper, we propose a \textbf{R}eference-\textbf{A}ided \textbf{P}art-\textbf{A}ligned (\textbf{RAPA}) framework to disentangle robust features of different parts. Firstly, in order to obtain better references between different videos, a pose-based reference feature learning module is introduced. Secondly, an effective relation-based part feature disentangling module is explored to align frames within each video. By means of using both modules, the informative parts of pedestrian in videos are well aligned and more discriminative feature representation is generated. Comprehensive experiments on three widely-used benchmarks, i.e. iLIDS-VID, PRID-2011 and MARS datasets verify the effectiveness of the proposed framework. Our code will be made publicly available.",0
"Abstract: This paper proposes a new approach called reference-aided part-aligned feature disentangling (RAPID) method for video person re-identification tasks, which can effectively separate the discriminative features from background noise based on both global spatial configuration and local semantic parts alignment. By utilizing human annotations as references, our model learns to locate meaningful body regions that contain identity-preserved information, leading to more accurate and robust representations compared with existing methods. Our extensive experiments on multiple benchmarks demonstrate the superiority of RAPID over state-of-the-art algorithms, further validating the effectiveness of reference-guided part alignment for unsupervised learning under real-world scenarios. Keywords: video person re-identification; reference annotation; part alignment; unsupervised feature disentangling. Abbreviations: VPR (video person re-identification), RELU (rectified linear unit).",1
"Anomaly detection in videos has been attracting an increasing amount of attention. Despite the competitive performance of recent methods on benchmark datasets, they typically lack desirable features such as modularity, cross-domain adaptivity, interpretability, and real-time anomalous event detection. Furthermore, current state-of-the-art approaches are evaluated using the standard instance-based detection metric by considering video frames as independent instances, which is not ideal for video anomaly detection. Motivated by these research gaps, we propose a modular and unified approach to the online video anomaly detection and localization problem, called MOVAD, which consists of a novel transfer learning based plug-and-play architecture, a sequential anomaly detector, a mathematical framework for selecting the detection threshold, and a suitable performance metric for real-time anomalous event detection in videos. Extensive performance evaluations on benchmark datasets show that the proposed framework significantly outperforms the current state-of-the-art approaches.",0
"This work presents a novel framework for detecting and localizing anomalies in videos. Our approach uses convolutional neural networks (CNNs) to learn features from the video data that capture both spatial and temporal patterns. These learned features are then used to identify and locate regions of interest where unusual behavior may occur.  Our framework consists of two main components: detection and localization. In the detection phase, we use a CNN model pre-trained on a large dataset of normal videos to extract features from a given input video. We then apply a simple thresholding method to determine whether any part of the video is likely to contain anomalous activity. If so, our system passes these detected frames to the next stage of the pipeline.  In the localization step, we employ another CNN model fine-tuned specifically for bounding box regression. This model takes as input the feature maps generated by our detection network, along with additional contextual information such as object category probabilities, camera pose estimates, and optical flow. Using all of this information, the localizer produces a set of heatmaps corresponding to each potential region of interest. Each map indicates the likelihood of the presence of an anomaly at every pixel within the surrounding image frame. Finally, using non-maximum suppression (NMS), we combine the outputs from multiple runs of the detector and localizer into a single set of tightly focused boxes enclosing the actual objects of interest.  We demonstrate through extensive experiments on public benchmark datasets that our proposed framework outperforms state-of-the-art methods across several evaluation metrics. Overall, our approach provides a powerful toolkit for real-world applications such as surveillance monitoring, event detection, and autonomous driving systems.",1
"Features play an important role in various visual tasks, especially in visual place recognition applied in perceptual changing environments. In this paper, we address the challenges of place recognition due to dynamics and confusable patterns by proposing a discriminative and semantic feature selection network, dubbed as DSFeat. Supervised by both semantic information and attention mechanism, we can estimate pixel-wise stability of features, indicating the probability of a static and stable region from which features are extracted, and then select features that are insensitive to dynamic interference and distinguishable to be correctly matched. The designed feature selection model is evaluated in place recognition and SLAM system in several public datasets with varying appearances and viewpoints. Experimental results conclude that the effectiveness of the proposed method. It should be noticed that our proposal can be readily pluggable into any feature-based SLAM system.",0
"Effective place recognition is crucial for many applications such as robotics, autonomous driving, and augmented reality. In dynamic environments, traditional feature selection methods often struggle to handle changes in appearance caused by factors like time of day, seasonal variations, illumination changes, weather conditions, etc., resulting in low recognition accuracy. This paper proposes a novel approach that integrates discriminative and semantic feature selection for improved performance in highly dynamic environments. We first generate high-level visual features using a convolutional neural network (CNN) trained on diverse image datasets. Next, we use both global and local descriptors to extract spatial and structural information from these features. To achieve better generalization across varying environmental conditions, our method employs a two-stage feature pruning process. In the initial stage, a clustering algorithm groups similar visual features together based on their geometry and intensity statistics. Then, to further optimize the tradeoff between recognition accuracy and computational efficiency, we use random sampling to select a subset of features followed by linear regression to evaluate their relative importance. Our experimental evaluation demonstrates that our proposed method outperforms state-of-the-art approaches across four widely used benchmarks, showing robustness under challenging dynamic scenarios while maintaining reasonable computation times. These findings have important implications for developing efficient and reliable algorithms for place recognition in real-world dynamic settings.",1
"Images captured in snowy days suffer from noticeable degradation of scene visibility, which degenerates the performance of current vision-based intelligent systems. Removing snow from images thus is an important topic in computer vision. In this paper, we propose a Deep Dense Multi-Scale Network (\textbf{DDMSNet}) for snow removal by exploiting semantic and geometric priors. As images captured in outdoor often share similar scenes and their visibility varies with depth from camera, such semantic and geometric information provides a strong prior for snowy image restoration. We incorporate the semantic and geometric maps as input and learn the semantic-aware and geometry-aware representation to remove snow. In particular, we first create a coarse network to remove snow from the input images. Then, the coarsely desnowed images are fed into another network to obtain the semantic and geometric labels. Finally, we design a DDMSNet to learn semantic-aware and geometry-aware representation via a self-attention mechanism to produce the final clean images. Experiments evaluated on public synthetic and real-world snowy images verify the superiority of the proposed method, offering better results both quantitatively and qualitatively.",0
"This paper presents a novel deep dense multi-scale network (DDAE) architecture that effectively combines semantic segmentation and geometric reconstruction priors for high-quality snow removal from aerial images. Our method addresses two common issues faced by existing approaches: low accuracy due to limited exploitation of contextual dependencies, and poor generalization on unseen datasets/domains. By integrating powerful dense representations at multiple scales, our model captures richer spatial and semantic features while preserving geometric details essential for precise pixel-level restoration. We achieve state-of-the-art results on three publicly available benchmarks, demonstrating both quantitative improvement over competing methods and qualitatively better visual outcomes across diverse scenes with varying conditions and complexities. Our framework offers versatility as well, allowing flexible integration with popular backbones and easily adaptability to different domain requirements. This work opens up new possibilities for researchers to explore effective fusion strategies, leveraging complementary knowledge sources towards real-world applications where high-fidelity image synthesis is critical.",1
"We propose an attention-based approach for multimodal image patch matching using a Transformer encoder attending to the feature maps of a multiscale Siamese CNN. Our encoder is shown to efficiently aggregate multiscale image embeddings while emphasizing task-specific appearance-invariant image cues. We also introduce an attention-residual architecture, using a residual connection bypassing the encoder. This additional learning signal facilitates end-to-end training from scratch. Our approach is experimentally shown to achieve new state-of-the-art accuracy on both multimodal and single modality benchmarks, illustrating its general applicability. To the best of our knowledge, this is the first successful implementation of the Transformer encoder architecture to the multimodal image patch matching task.",0
"In the field of computer vision, image matching has been an active area of research for many years. With advancements in technology, images have become more complex, leading to challenges in accurately identifying objects and features across different modalities. To address these challenges, researchers have proposed using feature maps that capture multiple scales of features within an image. However, existing methods tend to focus on single modalities, overlooking the importance of multiscale feature maps in multimodal image matching tasks. This study investigates the impact of paying attention to multiscale feature maps in improving the accuracy of multimodal image matching tasks. Our experiments show significant improvements in performance compared to state-of-the-art approaches by leveraging both local and global contextual information captured through multi-scaled feature maps. We believe our findings contribute towards achieving better results in multimodal image matching tasks while paving the way for future research directions.",1
"In Zero-shot learning (ZSL), we classify unseen categories using textual descriptions about their expected appearance when observed (class embeddings) and a disjoint pool of seen classes, for which annotated visual data are accessible. We tackle ZSL by casting a ""vanilla"" convolutional neural network (e.g. AlexNet, ResNet-101, DenseNet-201 or DarkNet-53) into a zero-shot learner. We do so by crafting the softmax classifier: we freeze its weights using fixed seen classification rules, either semantic (seen class embeddings) or visual (seen class prototypes). Then, we learn a data-driven and ZSL-tailored feature representation on seen classes only to match these fixed classification rules. Given that the latter seamlessly generalize towards unseen classes, while requiring not actual unseen data to be computed, we can perform ZSL inference by augmenting the pool of classification rules at test time while keeping the very same representation we learnt: nowhere re-training or fine-tuning on unseen data is performed. The combination of semantic and visual crafting (by simply averaging softmax scores) improves prior state-of-the-art methods in benchmark datasets for standard, inductive ZSL. After rebalancing predictions to better handle the joint inference over seen and unseen classes, we outperform prior generalized, inductive ZSL methods as well. Also, we gain interpretability at no additional cost, by using neural attention methods (e.g., grad-CAM) as they are. Code will be made publicly available.",0
"In recent years, there has been growing interest in developing machine learning models that can generalize well across domains and tasks without requiring large amounts of labeled data for each specific problem. One approach towards achieving this goal is through zero-shot learning (ZSL), which involves training models on one set of classes and then testing their ability to recognize instances from completely new classes that were never seen during training. In this work, we focus specifically on convolutional neural networks (ConvNets) as our classifiers and explore ways of modifying these models to improve their ZSL performance. Our main contributions are threefold: firstly, we propose a method for transforming pretrained ConvNets into ZSL-ready classifiers by fine-tuning them using attribute annotations; secondly, we introduce a novel regularization technique called ""attribute dropout"" which further improves accuracy by reducing overfitting to spurious patterns in the attribute vectors; thirdly, we provide empirical evidence demonstrating significant improvements in both closed-set and open-set classification settings over state-of-the-art methods on two popular benchmark datasets, Caltech-UCSD Birds 200-2011 and Attribute Pascal. These results suggest that incorporating attributes provides valuable inductive biases for promoting more effective zero-shot learning, even with limited annotation resources compared to other approaches relying exclusively on fully supervised techniques. Overall, this study advances the field by introducing strategies for crafting high-performing zero-shot learners from off-the-shelf ConvNets with minimal additional annotation effort.",1
"Improving the energy efficiency of mobile applications is a topic that has gained a lot of attention recently. It has been addressed in a number of ways such as identifying energy bugs and developing a catalog of energy patterns. Previous work shows that users discuss the battery-related issues (energy inefficiency or energy consumption) of the apps in their reviews. However, there is no work that addresses the automatic extraction of battery-related issues from users' feedback. In this paper, we report on a visualization tool that is developed to empirically study machine learning algorithms and text features to automatically identify the energy consumption specific reviews with the highest accuracy. Other than the common machine learning algorithms, we utilize deep learning models with different word embeddings to compare the results. Furthermore, to help the developers extract the main topics that are discussed in the reviews, two states of the art topic modeling algorithms are applied. The visualizations of the topics represent the keywords that are extracted for each topic along with a comparison with the results of string matching. The developed web-browser based interactive visualization tool is a novel framework developed with the intention of giving the app developers insights about running time and accuracy of machine learning and deep learning models as well as extracted topics. The tool makes it easier for the developers to traverse through the extensive result set generated by the text classification and topic modeling algorithms. The dynamic-data structure used for the tool stores the baseline-results of the discussed approaches and is updated when applied on new datasets. The tool is open-sourced to replicate the research results.",0
"This paper presents ""ReviewViz"", which is a web-based tool designed to assist developers in performing empirical studies related to energy consumption reviews for mobile applications. The current practice of using manual methods can be time consuming and prone to errors, making it difficult for researchers to gain valuable insights from large datasets. ReviewViz addresses these challenges by providing advanced features such as data preprocessing tools, visualizations of review text, and sentiment analysis techniques.  The tool was evaluated through multiple case studies involving real-world data sets, where participants were able to identify patterns and trends quickly and accurately. Furthermore, interviews with domain experts suggest that ReviewViz could have significant impacts on future work, particularly in helping reduce energy consumption of mobile apps and improving user experience.  Overall, ReviewViz represents an important step towards streamlining the process of conducting empirical studies and provides a more efficient means of gathering relevant findings. With its ease of use and powerful analytics capabilities, ReviewViz holds great potential for advancing knowledge within the field of energy efficiency in mobile computing.",1
"Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-point optimization have received growing attention due to their favorable last-iterate convergence. However, their behaviors for simple bilinear games over the probability simplex are still not fully understood - previous analysis lacks explicit convergence rates, only applies to an exponentially small learning rate, or requires additional assumptions such as the uniqueness of the optimal solution. In this work, we significantly expand the understanding of last-iterate convergence for OGDA and OMWU in the constrained setting. Specifically, for OMWU in bilinear games over the simplex, we show that when the equilibrium is unique, linear last-iterate convergence is achieved with a learning rate whose value is set to a universal constant, improving the result of (Daskalakis & Panageas, 2019b) under the same assumption. We then significantly extend the results to more general objectives and feasible sets for the projected OGDA algorithm, by introducing a sufficient condition under which OGDA exhibits concrete last-iterate convergence rates with a constant learning rate whose value only depends on the smoothness of the objective function. We show that bilinear games over any polytope satisfy this condition and OGDA converges exponentially fast even without the unique equilibrium assumption. Our condition also holds for strongly-convex-strongly-concave functions, recovering the result of (Hsieh et al., 2019). Finally, we provide experimental results to further support our theory.",0
"In recent years, constrained saddle-point optimization problems have gained significant attention due to their importance in solving real-world applications such as linear complementarity problems (LCPs), mathematical programming, image processing, machine learning, and many others. Several algorithms have been proposed to solve these problems efficiently and effectively, but one challenge that still remains is how to guarantee convergence in finite iterations. To address this issue, we propose a new algorithm based on the concept of ""linear last-iterate"" which guarantees convergence to Karush-Kuhn-Tucker (KKT) points in at most twice the number of constraints. Our method significantly reduces the computational cost and time required compared to existing methods by iteratively projecting onto the intersection of the hyperplanes defined by the active constraints at each iteration. Numerical experiments demonstrate the effectiveness of our approach in solving LCPs and convex quadratic programs with box constraints. Overall, our work presents a promising direction towards efficient solution techniques for constrained saddle-point optimization problems. --",1
"Deep operator networks (DeepONets) are receiving increased attention thanks to their demonstrated capability to approximate nonlinear operators between infinite-dimensional Banach spaces. However, despite their remarkable early promise, they typically require large training data-sets consisting of paired input-output observations which may be expensive to obtain, while their predictions may not be consistent with the underlying physical principles that generated the observed data. In this work, we propose a novel model class coined as physics-informed DeepONets, which introduces an effective regularization mechanism for biasing the outputs of DeepOnet models towards ensuring physical consistency. This is accomplished by leveraging automatic differentiation to impose the underlying physical laws via soft penalty constraints during model training. We demonstrate that this simple, yet remarkably effective extension can not only yield a significant improvement in the predictive accuracy of DeepOnets, but also greatly reduce the need for large training data-sets. To this end, a remarkable observation is that physics-informed DeepONets are capable of solving parametric partial differential equations (PDEs) without any paired input-output observations, except for a set of given initial or boundary conditions. We illustrate the effectiveness of the proposed framework through a series of comprehensive numerical studies across various types of PDEs. Strikingly, a trained physics informed DeepOnet model can predict the solution of $\mathcal{O}(10^3)$ time-dependent PDEs in a fraction of a second -- up to three orders of magnitude faster compared a conventional PDE solver. The data and code accompanying this manuscript are publicly available at \url{https://github.com/PredictiveIntelligenceLab/Physics-informed-DeepONets}.",0
"Physics-Informed Neural Networks (PINN) have recently gained significant attention as powerful tools that can solve complex mathematical problems using deep learning techniques. One such problem is solving Partial Differential Equations (PDE), which describes physical phenomena like waves, heat flow, fluid motion, etc. An important aspect of PDE solutions is the Solution Operator, which maps boundary conditions to desired outcomes. In particular, Parametrized PDEs pose challenges due to their varying nature, but they often appear in real-world applications and need accurate predictions. This work introduces Physics Informed DeepONets (PhyDNETs), a new architecture capable of capturing discretization errors while solving Parametric PDEs with high accuracy. Our approach extends the traditional PINN framework by leveraging weight sharing and adaptive local refinement techniques from Deep ONE (Deep Output Network Error). We provide extensive numerical results on benchmark tests and showcase PhyDNETs effectiveness compared against classic methods like Finite Element Analysis (FEA) and spectral methods, particularly on noisy data scenarios where other approaches fail. This research opens up opportunities for exploring more robust PDE solvers via DeepONets architectures and enables scientists/engineers to tackle complex engineering design problems efficiently.",1
"Learning curves provide insight into the dependence of a learner's generalization performance on the training set size. This important tool can be used for model selection, to predict the effect of more training data, and to reduce the computational complexity of model training and hyperparameter tuning. This review recounts the origins of the term, provides a formal definition of the learning curve, and briefly covers basics such as its estimation. Our main contribution is a comprehensive overview of the literature regarding the shape of learning curves. We discuss empirical and theoretical evidence that supports well-behaved curves that often have the shape of a power law or an exponential. We consider the learning curves of Gaussian processes, the complex shapes they can display, and the factors influencing them. We draw specific attention to examples of learning curves that are ill-behaved, showing worse learning performance with more training data. To wrap up, we point out various open problems that warrant deeper empirical and theoretical investigation. All in all, our review underscores that learning curves are surprisingly diverse and no universal model can be identified.",0
"This review focuses on the shape of learning curves, exploring their fundamental characteristics and the underlying factors that influence them. We examine both theoretical and empirical studies, discussing their implications and limitations. Our analysis reveals that while the traditional view of linearity dominates the field, nonlinear relationships have been reported across different domains, tasks, and populations. Nonetheless, there remains substantial debate regarding how these relations arise and whether they can fully account for complex human learning processes. Furthermore, we identify several methodological issues and assumptions surrounding curve fitting techniques, which may challenge the validity of findings and hinder comparisons between studies. Overall, our synthesis emphasizes that while the idea of a generic learning curve has been found wanting, careful consideration of specific hypotheses related to change over time may provide valuable insights into learning mechanisms. Future research directions should aim at integrating different perspectives to gain a more comprehensive understanding of how learners adapt and improve performance under varied conditions. By bridging conceptual gaps and addressing lingering uncertainties, a reinvigorated examination of learning curves holds great potential for advancing educational theory and practice.",1
"Cross-modal person re-identification (Re-ID) is critical for modern video surveillance systems. The key challenge is to align inter-modality representations according to semantic information present for a person and ignore background information. In this work, we present AXM-Net, a novel CNN based architecture designed for learning semantically aligned visual and textual representations. The underlying building block consists of multiple streams of feature maps coming from visual and textual modalities and a novel learnable context sharing semantic alignment network. We also propose complementary intra modal attention learning mechanisms to focus on more fine-grained local details in the features along with a cross-modal affinity loss for robust feature matching. Our design is unique in its ability to implicitly learn feature alignments from data. The entire AXM-Net can be trained in an end-to-end manner. We report results on both person search and cross-modal Re-ID tasks. Extensive experimentation validates the proposed framework and demonstrates its superiority by outperforming the current state-of-the-art methods by a significant margin.",0
"This paper presents AXM-Net (Attention based eXtreme Mixing deep Neural network for Multi-modal learning), a novel approach for person re-identification across different modalities such as images and videos. We propose a cross-modal context sharing attention mechanism that allows efficient knowledge transfer between different modalities by aligning their representations through attention modules. Experimental results on several benchmark datasets demonstrate the effectiveness of our proposed method compared to state-of-the-art approaches. Furthermore, we analyze the importance of each component of AXM-Net and discuss future directions for research in multi-modal person re-identification.",1
"Deep reinforcement learning (RL) has recently led to many breakthroughs on a range of complex control tasks. However, the agent's decision-making process is generally not transparent. The lack of interpretability hinders the applicability of RL in safety-critical scenarios. While several methods have attempted to interpret vision-based RL, most come without detailed explanation for the agent's behavior. In this paper, we propose a self-supervised interpretable framework, which can discover interpretable features to enable easy understanding of RL agents even for non-experts. Specifically, a self-supervised interpretable network (SSINet) is employed to produce fine-grained attention masks for highlighting task-relevant information, which constitutes most evidence for the agent's decisions. We verify and evaluate our method on several Atari 2600 games as well as Duckietown, which is a challenging self-driving car simulator environment. The results show that our method renders empirical evidences about how the agent makes decisions and why the agent performs well or badly, especially when transferred to novel scenes. Overall, our method provides valuable insight into the internal decision-making process of vision-based RL. In addition, our method does not use any external labelled data, and thus demonstrates the possibility to learn high-quality mask through a self-supervised manner, which may shed light on new paradigms for label-free vision learning such as self-supervised segmentation and detection.",0
"In recent years, deep reinforcement learning has shown promising results on several challenges such as playing games, controlling robots and driving cars. However, training high quality agents that can solve complex tasks using limited data still remains a difficult challenge due to two main factors: sample inefficiency and the need for hand engineered features which hampers generalization. To address these issues we present a new framework which unifies deep RL and self supervision by formulating the problem as predictive modeling with latent variables whose posterior distribution encodes task relevant knowledge. This approach allows us to train agents efficiently requiring only tens of thousands of environment interactions and no hand designed features but still achieve state of art performance across multiple domains such as continuous control tasks (Acrobot), discrete action Atari domain(Pong) , and 7D UAV racing. The discovered representations exhibit clear semantic structure and provide insight into how these agent learn and represent states, actions and rewards. Finally, our method achieves competitive if not better results compared to other methods in terms of both sample efficiency and overall return while simultaneously reducing computational cost and running time.",1
"Feature pyramid network (FPN) has been an effective framework to extract multi-scale features in object detection. However, current FPN-based methods mostly suffer from the intrinsic flaw of channel reduction, which brings about the loss of semantical information. And the miscellaneous fused feature maps may cause serious aliasing effects. In this paper, we present a novel channel enhancement feature pyramid network (CE-FPN) with three simple yet effective modules to alleviate these problems. Specifically, inspired by sub-pixel convolution, we propose a sub-pixel skip fusion method to perform both channel enhancement and upsampling. Instead of the original 1x1 convolution and linear upsampling, it mitigates the information loss due to channel reduction. Then we propose a sub-pixel context enhancement module for extracting more feature representations, which is superior to other context methods due to the utilization of rich channel information by sub-pixel convolution. Furthermore, a channel attention guided module is introduced to optimize the final integrated features on each level, which alleviates the aliasing effect only with a few computational burdens. Our experiments show that CE-FPN achieves competitive performance compared to state-of-the-art FPN-based detectors on MS COCO benchmark.",0
"This paper presents a new method for enhancing channel information for object detection using Convolutional Neural Networks (CNNs). We introduce a Feature Pyramid Network (FPN) module that uses multi-scale feature maps to integrate high-level contextual information from different layers of the network. Our approach extends Faster R-CNN by using FPN to improve the accuracy and speed of object detection tasks. Experimental results on popular benchmark datasets demonstrate significant improvements over state-of-the-art methods across multiple metrics, including precision, recall, and intersection over union. Furthermore, our method achieves these gains while maintaining low computational overhead compared to previous techniques. Our work provides a promising direction towards efficient and accurate object detection in computer vision applications.",1
"The attention mechanism has been widely used in deep neural networks as a model component. By now, it has become a critical building block in many state-of-the-art natural language models. Despite its great success established empirically, the working mechanism of attention has not been investigated at a sufficient theoretical depth to date. In this paper, we set up a simple text classification task and study the dynamics of training a simple attention-based classification model using gradient descent. In this setting, we show that, for the discriminative words that the model should attend to, a persisting identity exists relating its embedding and the inner product of its key and the query. This allows us to prove that training must converge to attending to the discriminative words when the attention output is classified by a linear classifier. Experiments are performed, which validate our theoretical analysis and provide further insights.",0
"Machine learning has gained significant attention in recent years as a powerful tool for solving complex problems across multiple domains such as computer vision, natural language processing, speech recognition, etc. Among various machine learning models used today, deep neural networks (DNNs) have emerged as one of the most successful architectures due to their ability to model complex relationships among input variables. However, training these DNNs on large datasets can often lead to computationally intensive tasks, requiring days or even weeks to converge. In order to address these challenges, researchers have developed different techniques such as mini-batch gradient descent that significantly reduce training times by breaking down the optimization problem into smaller batches. In this work, we focus specifically on another important component of DNN training: the attention mechanism. Given the massive amount of data processed through modern DNNs, extracting meaningful features from raw inputs requires efficient algorithms to selectively process relevant parts of each input data point. This task is typically carried out using self-attention mechanisms that aim to identify salient regions in images or key phrases/words in text documents. Our contribution lies in developing novel theoretical insights and practical improvements to current state-of-the art attention mechanisms. We investigate several popular variants of self-attention under varying parameter regimes, allowing us to better understand how these systems operate at different scales. Our study provides detailed analysis of both time complexity and memory usage, providing practitioners with guidelines for choosing appropriate attention configurations based on desired scaling dimensions. Overall, our findings provide fresh perspectives on understanding attentional processes as well as improving t",1
"Discriminative correlation filters (DCF) and siamese networks have achieved promising performance on visual tracking tasks thanks to their superior computational efficiency and reliable similarity metric learning, respectively. However, how to effectively take advantages of powerful deep networks, while maintaining the real-time response of DCF, remains a challenging problem. Embedding the cross-correlation operator as a separate layer into siamese networks is a popular choice to enhance the tracking accuracy. Being a key component of such a network, the correlation layer is updated online together with other parts of the network. Yet, when facing serious disturbance, fused trackers may still drift away from the target completely due to accumulated errors. To address these issues, we propose a coarse-to-fine tracking framework, which roughly infers the target state via an online-updating DCF module first and subsequently, finely locates the target through an offline-training asymmetric siamese network (ASN). Benefitting from the guidance of DCF and the learned channel weights obtained through exploiting the given ground-truth template, ASN refines feature representation and implements precise target localization. Systematic experiments on five popular tracking datasets demonstrate that the proposed DCF-ASN achieves the state-of-the-art performance while exhibiting good tracking efficiency.",0
"This paper presents a new approach for real-time visual tracking using discriminative correlation filters (DCF) and attentional siamese networks (Siamese Net). The proposed method, called DCF-ASN, combines the strengths of these two popular techniques into a coarse-to-fine framework that efficiently and accurately tracks objects in real time.  In recent years, DCF has been widely used due to its efficiency and accuracy in object tracking tasks. However, there are still challenges in handling occlusions, deformations, and changes in lighting conditions. To address these issues, we propose integrating an attention mechanism from Siamese Net to improve localization precision during tracking. Our proposed method uses a coarse-to-fine framework where initial bounding box detection is followed by fine-grained refinement of the prediction through our newly developed ASN model. We evaluate DCF-ASN on several benchmark datasets and demonstrate that it outperforms state-of-the-art methods while running at a high frame rate. Overall, our work advances the field of real-time visual tracking by providing an effective solution for robustly tracking objects under complex scenarios.",1
"HardWare-aware Neural Architecture Search (HW-NAS) has recently gained tremendous attention by automating the design of DNNs deployed in more resource-constrained daily life devices. Despite its promising performance, developing optimal HW-NAS solutions can be prohibitively challenging as it requires cross-disciplinary knowledge in the algorithm, micro-architecture, and device-specific compilation. First, to determine the hardware-cost to be incorporated into the NAS process, existing works mostly adopt either pre-collected hardware-cost look-up tables or device-specific hardware-cost models. Both of them limit the development of HW-NAS innovations and impose a barrier-to-entry to non-hardware experts. Second, similar to generic NAS, it can be notoriously difficult to benchmark HW-NAS algorithms due to their significant required computational resources and the differences in adopted search spaces, hyperparameters, and hardware devices. To this end, we develop HW-NAS-Bench, the first public dataset for HW-NAS research which aims to democratize HW-NAS research to non-hardware experts and make HW-NAS research more reproducible and accessible. To design HW-NAS-Bench, we carefully collected the measured/estimated hardware performance of all the networks in the search spaces of both NAS-Bench-201 and FBNet, on six hardware devices that fall into three categories (i.e., commercial edge devices, FPGA, and ASIC). Furthermore, we provide a comprehensive analysis of the collected measurements in HW-NAS-Bench to provide insights for HW-NAS research. Finally, we demonstrate exemplary user cases to (1) show that HW-NAS-Bench allows non-hardware experts to perform HW-NAS by simply querying it and (2) verify that dedicated device-specific HW-NAS can indeed lead to optimal accuracy-cost trade-offs. The codes and all collected data are available at https://github.com/RICE-EIC/HW-NAS-Bench.",0
"This paper presents ""HW-NAS-Bench,"" a benchmark designed to evaluate state-of-the art neural architecture search (NAS) methods on hardware-aware NAS problems. While traditional NAS focuses mainly on improving the accuracy of deep learning models across different datasets and network architectures, hardware-aware NAS also considers factors such as energy efficiency, latency, and device compatibility when searching for optimal networks. To address these additional dimensions, we introduce a benchmark that includes real-world constraints related to hardware resources, which can better reflect the complex tradeoffs that must be made during the design of machine learning systems. We provide results using six representative hardware-aware NAS methods and discuss how our benchmark can facilitate research into more effective use of specialized accelerators and other hardware resources for deploying high-performance deep learning applications. Finally, by comparing the strengths and weaknesses of current approaches on a variety of hardware configurations, we aim to stimulate new ideas and advancements in this rapidly evolving field.",1
"Few-shot action recognition has attracted increasing attention due to the difficulty in acquiring the properly labelled training samples. Current works have shown that preserving spatial information and comparing video descriptors are crucial for few-shot action recognition. However, the importance of preserving temporal information is not well discussed. In this paper, we propose a Contents and Length-based Temporal Attention (CLTA) model, which learns customized temporal attention for the individual video to tackle the few-shot action recognition problem. CLTA utilizes the Gaussian likelihood function as the template to generate temporal attention and trains the learning matrices to study the mean and standard deviation based on both frame contents and length. We show that even a not fine-tuned backbone with an ordinary softmax classifier can still achieve similar or better results compared to the state-of-the-art few-shot action recognition with precisely captured temporal attention.",0
"One major challenge in few-shot action recognition is how to effectively utilize limited training data. In this work, we propose using temporal attention mechanisms that take into account both video content and length to improve the accuracy of few-shot learning algorithms. We introduce two novel attentional modules, Content-Based Temporal Attention (CBA) and Length-Based Temporal Attention (LBA), which focus on relevant segments of videos based on their contents and lengths respectively. Our experimental results show that integrating these attentional modules significantly improves the performance of several state-of-the-art methods on popular benchmarks such as HMDB51, UCF101, and Kinetics datasets, demonstrating the effectiveness of our proposed approach in few-shot action recognition tasks.",1
"Automated visual inspection in the semiconductor industry aims to detect and classify manufacturing defects utilizing modern image processing techniques. While an earliest possible detection of defect patterns allows quality control and automation of manufacturing chains, manufacturers benefit from an increased yield and reduced manufacturing costs. Since classical image processing systems are limited in their ability to detect novel defect patterns, and machine learning approaches often involve a tremendous amount of computational effort, this contribution introduces a novel deep neural network based hybrid approach. Unlike classical deep neural networks, a multi-stage system allows the detection and classification of the finest structures in pixel size within high-resolution imagery. Consisting of stacked hybrid convolutional neural networks (SH-CNN) and inspired by current approaches of visual attention, the realized system draws the focus over the level of detail from its structures to more task-relevant areas of interest. The results of our test environment show that the SH-CNN outperforms current approaches of learning-based automated visual inspection, whereas a distinction depending on the level of detail enables the elimination of defect patterns in earlier stages of the manufacturing process.",0
"This paper presents a novel visual fault detection and classification system for semiconductor manufacturing using stacked hybrid convolutional neural networks (CNNs). The proposed system addresses the challenges faced by traditional methods used in the industry, such as human inspection and rule-based systems, which can be time-consuming, subjective, and error-prone. Our approach utilizes deep learning techniques to automatically detect and classify defects in semiconductor images captured during the manufacturing process. Specifically, we use two CNN architectures: VGG-16 and ResNet-50 pre-trained on ImageNet, which are then fine-tuned on our labeled dataset consisting of faulty and non-faulty chip images. We evaluate the performance of our method through experiments conducted on different datasets and demonstrate its superiority over state-of-the-art approaches in terms of accuracy, speed, and consistency. Finally, our visualization results show that the model can effectively capture high-level features and local details, making it well suited for industrial applications. Overall, our work provides a promising solution for enhancing quality control processes in semiconductor manufacturing while reducing costs and improving efficiency.",1
"Automated Machine Learning (AutoML) is the problem of automatically finding the pipeline with the best generalization performance on some given dataset. AutoML has received enormous attention in the last decade and has been addressed with sophisticated black-box optimization techniques such as Bayesian Optimization, Grammar-Based Genetic Algorithms, and tree search algorithms. In contrast to those approaches, we present Naive AutoML, a very simple solution to AutoML that exploits important meta-knowledge about machine learning problems and makes simplifying, yet, effective assumptions to quickly come to high-quality solutions. While Naive AutoML can be considered a baseline for the highly sophisticated black-box solvers, we empirically show that those solvers are not able to outperform Naive AutoML; sometimes the contrary is true. On the other hand, Naive AutoML comes with strong advantages such as interpretability and flexibility and poses a strong challenge to current tools.",0
"Title: ""Naive Automated Machine Learning - A Late Baseline for AutoML"" Abstract: Machine learning (ML) has become increasingly prevalent across all domains of science and industry. In recent years, there has been a growing trend towards automating various aspects of the ML pipeline including data preprocessing, model selection, hyperparameter tuning, and even model architecture design. These efforts have led to significant improvements in both speed and accuracy over manual pipelines but often come at the cost of interpretability, reproducibility, and human understanding. Our work presents a naive approach to automated machine learning that seeks to address these shortcomings by simplifying the entire pipeline into a single algorithm operating on raw input features without any form of preprocessing or feature engineering, relying solely on random search for hyperparameters, using ensembles as the final output model and returning interpretable models whenever possible. We evaluate our method against several state-of-the-art automatic methods using five benchmark datasets from multiple disciplines, demonstrating competitive performance while maintaining simplicity, transparency, and interpretability. By providing such late baseline performance, we encourage future researchers working in the field of AutoML to focus more closely on key properties like openness, traceability, adaptivity, collaboration and sustainability throughout the entire development process rather than only optimizing internal metrics. Finally, through a thorough ablation study, we provide insight into which components of our method contribute most significantly to the strong performance achieved. As a result of the above points, our overall conclusion is that while auto ml can produce high quality results on some tasks they require specialist knowledge and careful evaluation due to their opacity; a simple late-baseli",1
"We provide a general self-attention formulation to impose group equivariance to arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate consistent improvements of GSA-Nets over non-equivariant self-attention networks.",0
"Recently, self-attention mechanisms have become popular components of transformer models due to their ability to capture dependencies between input elements without any recurrence. Despite these advantages, they still face the problem of scalability: as the size of inputs grows, so do computational costs exponentially, which makes attention unwieldy for large-scale vision problems such as image classification on high-resolution images or video processing. In this paper we show how to apply group equivariance constraints to convolutional networks that employ standalone self-attention layers (which act independently from convolutions). Our method has a cost comparable to standard self-attention, while providing improved scaling properties. We demonstrate that our model outperforms strong baselines on several benchmark datasets using less parameters and achieves state-of-the art results on the task of human pose estimation even at very small scales. Finally, we prove its effectiveness when scaled up to larger sizes or combined with other methods from the literature that can benefit from attention.",1
"Multi-Task Learning (MTL) networks have emerged as a promising method for transferring learned knowledge across different tasks. However, MTL must deal with challenges such as: overfitting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Often, in Natural Language Processing (NLP), a separate model per task is needed to obtain the best performance. However, many fine-tuning approaches are both parameter inefficient, i.e., potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel Transformer architecture consisting of a new conditional attention mechanism as well as a set of task-conditioned modules that facilitate weight sharing. Through this construction, we achieve more efficient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model fixed. We also use a new multi-task data sampling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach, we are able to surpass single task fine-tuning methods while being parameter and data efficient (using around 66% of the data for weight updates). Compared to other BERT Large methods on GLUE, our 8-task model surpasses other Adapter methods by 2.8% and our 24-task model outperforms by 0.7-1.0% models that use MTL and single task fine-tuning. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets. Our code is publicly available at https://github.com/CAMTL/CA-MTL.",0
"In recent years, transfer learning has become an important technique in natural language processing (NLP), enabling models trained on large amounts of data in one task to improve performance on new tasks with limited training data. However, traditional multi-task learning approaches often suffer from negative transfer, where the model's ability to perform well on one task decreases after training on additional related tasks. This paper presents a novel approach called conditionally adaptive multi-task learning which improves transfer learning by leveraging unlabeled data from similar domains. Our method uses a regularization term that adjusts during training based on the similarity between the source and target datasets, resulting in improved generalization without requiring more labeled data. We demonstrate the effectiveness of our approach through experiments on three benchmark NLP datasets, achieving state-of-the-art results while using fewer parameters compared to previous methods. Overall, our work shows promising potential for further advancing transfer learning in NLP with real-world applications.",1
"Particle based optimization algorithms have recently been developed as sampling methods that iteratively update a set of particles to approximate a target distribution. In particular Stein variational gradient descent has gained attention in the approximate inference literature for its flexibility and accuracy. We empirically explore the ability of this method to sample from multi-modal distributions and focus on two important issues: (i) the inability of the particles to escape from local modes and (ii) the inefficacy in reproducing the density of the different regions. We propose an annealing schedule to solve these issues and show, through various experiments, how this simple solution leads to significant improvements in mode coverage, without invalidating any theoretical properties of the original algorithm.",0
"In recent years, deep learning has achieved unprecedented success across many fields thanks in large part to advances in optimization techniques such as gradient descent (GD). However, GD has been shown to converge slowly and often gets stuck in poor local minima. To address these issues, several algorithms have been proposed including the popular method stochastic gradient Langevin dynamics (SGLD) which adds noise to the gradients during training but can still suffer from slow convergence. In this work we propose annealed Stein variational gradient descent (ASVGD), an algorithm that adapts the temperature parameter used in SGLD to improve convergence speed by automatically reducing the amount of noise added during training over time. We demonstrate through experiments on benchmark datasets that ASVGD outperforms both vanilla GD and SGLD in terms of accuracy and efficiency. Our results suggest that ASVGD could provide a powerful new tool for practitioners looking to optimize their deep learning models.",1
"Primal heuristics play a crucial role in exact solvers for Mixed Integer Programming (MIP). While solvers are guaranteed to find optimal solutions given sufficient time, real-world applications typically require finding good solutions early on in the search to enable fast decision-making. While much of MIP research focuses on designing effective heuristics, the question of how to manage multiple MIP heuristics in a solver has not received equal attention. Generally, solvers follow hard-coded rules derived from empirical testing on broad sets of instances. Since the performance of heuristics is instance-dependent, using these general rules for a particular problem might not yield the best performance. In this work, we propose the first data-driven framework for scheduling heuristics in an exact MIP solver. By learning from data describing the performance of primal heuristics, we obtain a problem-specific schedule of heuristics that collectively find many solutions at minimal cost. We provide a formal description of the problem and propose an efficient algorithm for computing such a schedule. Compared to the default settings of a state-of-the-art academic MIP solver, we are able to reduce the average primal integral by up to 49% on a class of challenging instances.",0
"In this work, we propose a method for learning to schedule heuristics within branch-and-bound algorithms, leveraging deep reinforcement learning techniques to improve performance across a variety of domains. By modeling the search process as a Markov decision process (MDP), we develop an agent that learns optimal heuristic scheduling policies over time through trial and error. Our approach allows for effective exploration of solution spaces while minimizing computational overhead, leading to more efficient problem solving compared to traditional approaches relying on static heuristics alone. We evaluate our algorithm on a range of complex optimization problems including traveling salesman and vehicle routing, demonstrating significant reductions in computation times without sacrificing solution quality. Our results underscore the potential impact of integrating machine learning techniques into operations research practice, paving the way towards intelligent and adaptive solution methods.",1
"Self-attention networks have shown remarkable progress in computer vision tasks such as image classification. The main benefit of the self-attention mechanism is the ability to capture long-range feature interactions in attention-maps. However, the computation of attention-maps requires a learnable key, query, and positional encoding, whose usage is often not intuitive and computationally expensive. To mitigate this problem, we propose a novel self-attention module with explicitly modeled attention-maps using only a single learnable parameter for low computational overhead. The design of explicitly modeled attention-maps using geometric prior is based on the observation that the spatial context for a given pixel within an image is mostly dominated by its neighbors, while more distant pixels have a minor contribution. Concretely, the attention-maps are parametrized via simple functions (e.g., Gaussian kernel) with a learnable radius, which is modeled independently of the input content. Our evaluation shows that our method achieves an accuracy improvement of up to 2.2% over the ResNet-baselines in ImageNet ILSVRC and outperforms other self-attention methods such as AA-ResNet152 in accuracy by 0.9% with 6.4% fewer parameters and 6.7% fewer GFLOPs. This result empirically indicates the value of incorporating geometric prior into self-attention mechanism when applied in image classification.",0
"In recent years, attention mechanisms have become increasingly popular in deep learning-based computer vision tasks such as image classification. They allow models to focus on specific regions of images that contain relevant features by computing attentional weights over different spatial locations. Existing methods for producing attention maps are either indirect, relying on external sources (e.g., human annotations) or learned implicitly without explicit supervision. This paper proposes a novel approach to explicitly modeling attention maps for image classification. Our method trains a dedicated network branch to predict attention maps alongside the main prediction branch. We introduce a new loss function based on mutual information maximization between the predicted and ground truth attention maps. Experiments show that our approach significantly outperforms baseline models trained without attention and achieves competitive accuracy compared to state-of-the-art attention-based models. Additionally, we perform extensive qualitative analyses and visualizations to provide insights into how our model learns interpretable attention maps during training. Overall, our work demonstrates the effectiveness and importance of explicitly modeled attention mechanisms in improving image classification performance and interpretability.",1
"Graph convolutional neural networks (GCNNs) have received much attention recently, owing to their capability in handling graph-structured data. Among the existing GCNNs, many methods can be viewed as instances of a neural message passing motif; features of nodes are passed around their neighbors, aggregated and transformed to produce better nodes' representations. Nevertheless, these methods seldom use node transition probabilities, a measure that has been found useful in exploring graphs. Furthermore, when the transition probabilities are used, their transition direction is often improperly considered in the feature aggregation step, resulting in an inefficient weighting scheme. In addition, although a great number of GCNN models with increasing level of complexity have been introduced, the GCNNs often suffer from over-fitting when being trained on small graphs. Another issue of the GCNNs is over-smoothing, which tends to make nodes' representations indistinguishable. This work presents a new method to improve the message passing process based on node transition probabilities by properly considering the transition direction, leading to a better weighting scheme in nodes' features aggregation compared to the existing counterpart. Moreover, we propose a novel regularization method termed DropNode to address the over-fitting and over-smoothing issues simultaneously. DropNode randomly discards part of a graph, thus it creates multiple deformed versions of the graph, leading to data augmentation regularization effect. Additionally, DropNode lessens the connectivity of the graph, mitigating the effect of over-smoothing in deep GCNNs. Extensive experiments on eight benchmark datasets for node and graph classification tasks demonstrate the effectiveness of the proposed methods in comparison with the state of the art.",0
"This paper presents a novel approach to graph convolutional neural networks (GCNN) based on node transition probability-driven message passing and dropnode regularization. GCNN has been widely used in many fields such as computer vision, natural language processing, and bioinformatics due to their effectiveness at learning nonlinear features from irregular graphs. However, traditional methods have limitations that can hinder their performance in certain scenarios such as node sparsity, varying neighborhood sizes, and the presence of noise.  Our method addresses these issues by introducing two key components: node transition probability-driven message passing, which enables efficient communication among nodes even when they have different degrees; and dropnode regularization, which improves robustness to noisy data and helps maintain important structural information in the network during training. We evaluate our model on several benchmark datasets and demonstrate its superior performance compared to state-of-the-art approaches under challenging conditions. Our findings suggest that our framework is well suited for handling real-world problems where graph data may contain significant variations or inconsistencies. Overall, we believe that our work represents a significant step towards more effective GCNN models that can handle complex and varied graph structures.",1
"Explainability for machine learning models has gained considerable attention within our research community given the importance of deploying more reliable machine-learning systems. In computer vision applications, generative counterfactual methods indicate how to perturb a model's input to change its prediction, providing details about the model's decision-making. Current counterfactual methods make ambiguous interpretations as they combine multiple biases of the model and the data in a single counterfactual interpretation of the model's decision. Moreover, these methods tend to generate trivial counterfactuals about the model's decision, as they often suggest to exaggerate or remove the presence of the attribute being classified. For the machine learning practitioner, these types of counterfactuals offer little value, since they provide no new information about undesired model or data biases. In this work, we propose a counterfactual method that learns a perturbation in a disentangled latent space that is constrained using a diversity-enforcing loss to uncover multiple valuable explanations about the model's prediction. Further, we introduce a mechanism to prevent the model from producing trivial explanations. Experiments on CelebA and Synbols demonstrate that our model improves the success rate of producing high-quality valuable explanations when compared to previous state-of-the-art methods. We will publish the code.",0
"In this paper we propose a novel approach to explaining machine learning models beyond trivial counterfactual explanations based on local surrogate models. Our method captures diverse valuable explanations that go beyond individual feature attributions, enabling a deeper understanding of model behavior across datasets, tasks, and architectures. Through extensive experiments and user studies, we demonstrate the effectiveness of our approach in providing more informative, reliable, and actionable explanations for both laypersons and experts. Our contributions provide new opportunities for interpreting deep learning models and advancing their trustworthiness.",1
"The quality of the image representations obtained from self-supervised learning depends strongly on the type of data augmentations used in the learning formulation. Recent papers have ported these methods from still images to videos and found that leveraging both audio and video signals yields strong gains; however, they did not find that spatial augmentations such as cropping, which are very important for still images, work as well for videos. In this paper, we improve these formulations in two ways unique to the spatio-temporal aspect of videos. First, for space, we show that spatial augmentations such as cropping do work well for videos too, but that previous implementations, due to the high processing and memory cost, could not do this at a scale sufficient for it to work well. To address this issue, we first introduce Feature Crop, a method to simulate such augmentations much more efficiently directly in feature space. Second, we show that as opposed to naive average pooling, the use of transformer-based attention improves performance significantly, and is well suited for processing feature crops. Combining both of our discoveries into a new method, Space-time Crop & Attend (STiCA) we achieve state-of-the-art performance across multiple video-representation learning benchmarks. In particular, we achieve new state-of-the-art accuracies of 67.0% on HMDB-51 and 93.1% on UCF-101 when pre-training on Kinetics-400.",0
"This paper presents Space-Time Crop and Attend (STCA), a method that improves cross-modal video representation learning by addressing two key issues: the lack of spatial attention and poor temporal sampling during video encoding. STCA addresses these limitations through a novel architecture that consists of three components: space-time cropping, spatio-temporal attention, and modal interaction refinement. Our approach significantly outperforms state-of-the-art methods on several benchmark datasets, demonstrating its effectiveness in learning robust representations for downstream tasks such as action recognition, event detection, and activity forecasting. The results showcase the utility of our proposed method in advancing the current understanding of video representation learning. Overall, we believe STCA provides a solid foundation for future research in the field.",1
"We propose a conditional positional encoding (CPE) scheme for vision Transformers. Unlike previous fixed or learnable positional encodings, which are pre-defined and independent of input tokens, CPE is dynamically generated and conditioned on the local neighborhood of the input tokens. As a result, CPE can easily generalize to the input sequences that are longer than what the model has ever seen during training. Besides, CPE can keep the desired translation-invariance in the image classification task, resulting in improved classification accuracy. CPE can be effortlessly implemented with a simple Position Encoding Generator (PEG), and it can be seamlessly incorporated into the current Transformer framework. Built on PEG, we present Conditional Position encoding Vision Transformer (CPVT). We demonstrate that CPVT has visually similar attention maps compared to those with learned positional encodings. Benefit from the conditional positional encoding scheme, we obtain state-of-the-art results on the ImageNet classification task compared with vision Transformers to date. Our code will be made available at https://github.com/Meituan-AutoML/CPVT .",0
"In recent years, transformer architectures have revolutionized natural language processing tasks by providing state-of-the-art results on many benchmarks. Despite their impressive performance, applying transformer models to computer vision has been challenging due to their computational cost and limited efficiency compared to convolutional neural networks (CNNs). To address these limitations, conditional positional encodings were proposed as a method for reducing computations while improving model accuracy. This approach exploits the fact that image patches conditioned by contextual variables like object instances contain rich visual cue information. By encoding only those image regions where instances appear, we can significantly reduce computations without sacrificing expressiveness. Our experimental evaluations demonstrate that using conditional positional encodings leads to improved accuracy across several high-profile image classification benchmarks compared to full attention variants and other competitive CNN models. These findings highlight the potential utility of this new technique for efficient vision transformer design and offer promising directions for future research in CV applications of transformers.",1
"Accurate pedestrian classification and localization have received considerable attention due to their wide applications such as security monitoring, autonomous driving, etc. Although pedestrian detectors have made great progress in recent years, the fixed Intersection over Union (IoU) based assignment-regression manner still limits their performance. Two main factors are responsible for this: 1) the IoU threshold faces a dilemma that a lower one will result in more false positives, while a higher one will filter out the matched positives; 2) the IoU-based GT-Proposal assignment suffers from the inconsistent supervision problem that spatially adjacent proposals with similar features are assigned to different ground-truth boxes, which means some very similar proposals may be forced to regress towards different targets, and thus confuses the bounding-box regression when predicting the location results. In this paper, we first put forward the question that \textbf{Regression Direction} would affect the performance for pedestrian detection. Consequently, we address the weakness of IoU by introducing one geometric sensitive search algorithm as a new assignment and regression metric. Different from the previous IoU-based \textbf{one-to-one} assignment manner of one proposal to one ground-truth box, the proposed method attempts to seek a reasonable matching between the sets of proposals and ground-truth boxes. Specifically, we boost the MR-FPPI under R$_{75}$ by 8.8\% on Citypersons dataset. Furthermore, by incorporating this method as a metric into the state-of-the-art pedestrian detectors, we show a consistent improvement.",0
This is just a simple request without any context! Please provide more details. What kind of abstract are you looking for? Maybe I can assist with that instead. Is there a specific subject area you need help writing an abstract for?,1
"The introduction of Transformer model has led to tremendous advancements in sequence modeling, especially in text domain. However, the use of attention-based models for video understanding is still relatively unexplored. In this paper, we introduce Gated Adversarial Transformer (GAT) to enhance the applicability of attention-based models to videos. GAT uses a multi-level attention gate to model the relevance of a frame based on local and global contexts. This enables the model to understand the video at various granularities. Further, GAT uses adversarial training to improve model generalization. We propose temporal attention regularization scheme to improve the robustness of attention modules to adversarial examples. We illustrate the performance of GAT on the large-scale YoutTube-8M data set on the task of video categorization. We further show ablation studies along with quantitative and qualitative analysis to showcase the improvement.",0
"This paper explores how to use gated multi-level attention and temporal adversarial training to improve video understanding by transformers. With these new techniques, we show that we can achieve state-of-the-art results on several benchmark datasets for action recognition and scene classification. Our approach outperforms previous methods by more effectively capturing global dependencies among spatial regions and handling long-range temporal relationships. We conduct extensive experiments to demonstrate our modelâ€™s effectiveness and also provide a comprehensive analysis of our methodâ€™s components. Our work highlights the importance of incorporating both local and global interactions within deep learning models for high-quality video understanding.",1
"Given an incomplete image without additional constraint, image inpainting natively allows for multiple solutions as long as they appear plausible. Recently, multiplesolution inpainting methods have been proposed and shown the potential of generating diverse results. However, these methods have difficulty in ensuring the quality of each solution, e.g. they produce distorted structure and/or blurry texture. We propose a two-stage model for diverse inpainting, where the first stage generates multiple coarse results each of which has a different structure, and the second stage refines each coarse result separately by augmenting texture. The proposed model is inspired by the hierarchical vector quantized variational auto-encoder (VQ-VAE), whose hierarchical architecture isentangles structural and textural information. In addition, the vector quantization in VQVAE enables autoregressive modeling of the discrete distribution over the structural information. Sampling from the distribution can easily generate diverse and high-quality structures, making up the first stage of our model. In the second stage, we propose a structural attention module inside the texture generation network, where the module utilizes the structural information to capture distant correlations. We further reuse the VQ-VAE to calculate two feature losses, which help improve structure coherence and texture realism, respectively. Experimental results on CelebA-HQ, Places2, and ImageNet datasets show that our method not only enhances the diversity of the inpainting solutions but also improves the visual quality of the generated multiple images. Code and models are available at: https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting.",0
"This paper presents a novel approach for generating diverse structure for image inpainting using hierarchical Vector Quantization - Variational Autoencoder (VQ-VAE). The method utilizes a multi-scale representation of images that captures structural information at different scales, which enables better preservation of image details during inpainting. To achieve diversity in the generated structures, we introduce a modification to the decoding process, where multiple candidates are obtained and combined in a weighted manner based on their similarity to the input image. Experimental results demonstrate the effectiveness of our method in generating high quality image inpaintings with diverse structures that closely match the visual characteristics of real-world scenes. Additionally, qualitative analysis shows that our method outperforms state-of-the-art methods in terms of accuracy, efficiency, and adaptability to changing environments. Overall, our work represents an important step towards more generalizable and robust image inpainting solutions in complex scenarios.",1
"DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.",0
"This work presents Deformable DETR (Deformerable transformers for END-TO-END object detection), which leverages deformable convolutions within multi-head self attention layers to detect objects end-to-end. We show that applying continuous affine transformations can improve performance on tough cases. Extensive experiments on COCO and Cityscapes demonstrate both large improvements over previous state-of-the-art methods as well as significant robustness against common challenges such as scale variation, occlusion, truncation etc. Code will be released upon acceptance.",1
"We propose a new class of parameterizations for spatio-temporal point processes which leverage Neural ODEs as a computational method and enable flexible, high-fidelity models of discrete events that are localized in continuous time and space. Central to our approach is a combination of continuous-time neural networks with two novel neural architectures, i.e., Jump and Attentive Continuous-time Normalizing Flows. This approach allows us to learn complex distributions for both the spatial and temporal domain and to condition non-trivially on the observed event history. We validate our models on data sets from a wide variety of contexts such as seismology, epidemiology, urban mobility, and neuroscience.",0
"In this paper we propose neural spatio-temporal point processes (NSTPP), which are flexible models that capture complex patterns in event data over time and space. These models can represent dependencies among events at multiple scales by encoding local features as well as capturing global patterns through nonlinear interactions among neurons inspired by deep learning techniques such as convolutions. NSTPP provide interpretable representations of spatiotemporal structures by disentangling different aspects of complex spatial and temporal dependencies into separate networks of excitatory and inhibitory interconnections. We showcase the versatility of our approach on several real-world datasets involving geospatial crime incidents, human mobility, and cybersecurity intrusion detection systems, demonstrating how our model generates accurate predictions and enhances interpretability compared to current methods used in the field. Our work facilitates modeling spatially heterogeneous phenomena while integrating prior domain knowledge into deep learning frameworks paving the way for novel applications across various domains where spatiotemporal reasoning matters.",1
"In this work we propose a novel deep-learning approach for age estimation based on face images. We first introduce a dual image augmentation-aggregation approach based on attention. This allows the network to jointly utilize multiple face image augmentations whose embeddings are aggregated by a Transformer-Encoder. The resulting aggregated embedding is shown to better encode the face image attributes. We then propose a probabilistic hierarchical regression framework that combines a discrete probabilistic estimate of age labels, with a corresponding ensemble of regressors. Each regressor is particularly adapted and trained to refine the probabilistic estimate over a range of ages. Our scheme is shown to outperform contemporary schemes and provide a new state-of-the-art age estimation accuracy, when applied to the MORPH II dataset for age estimation. Last, we introduce a bias analysis of state-of-the-art age estimation results.",0
"This paper proposes a new method for estimating age and bias from images using hierarchical attention mechanisms. Our approach uses a deep neural network architecture that combines multiple scales of representation in order to accurately estimate both age and bias simultaneously. We demonstrate through experiments on two publicly available datasets that our method outperforms state-of-the-art methods in terms of accuracy and generalization ability. Additionally, we show that our method can effectively handle variations in illumination conditions, facial expressions, and head pose while providing more accurate estimates of both age and bias. Finally, we provide visualizations to illustrate how the network makes predictions at different layers of abstraction. Overall, our proposed method represents a significant advance in the field of age and bias estimation from images.",1
"Following the success of dot-product attention in Transformers, numerous approximations have been recently proposed to address its quadratic complexity with respect to the input length. However, all approximations thus far have ignored the contribution of the $\textit{value vectors}$ to the quality of approximation. In this work, we argue that research efforts should be directed towards approximating the true output of the attention sub-layer, which includes the value vectors. We propose a value-aware objective, and show theoretically and empirically that an optimal approximation of a value-aware objective substantially outperforms an optimal approximation that ignores values, in the context of language modeling. Moreover, we show that the choice of kernel function for computing attention similarity can substantially affect the quality of sparse approximations, where kernel functions that are less skewed are more affected by the value vectors.",0
"Attention mechanisms have become ubiquitous in natural language processing tasks due to their ability to weigh input features based on their relevance to the current context. However, computing exact attention weights can be computationally expensive and may not always lead to improved performance. In this work, we introduce value-aware approximate attention (VAAA), which approximates the softmax function used in traditional attention with randomized sampling and scaling techniques. Our method improves computational efficiency while maintaining competitive accuracy across a range of NLP benchmarks. We evaluate VAAA using both automatic metrics and human evaluations on three different datasets: WMT 2014 English-to-German translation, CNN/Daily Mail document ranking, and Quora question answering. Overall, our results demonstrate that VAAA offers a promising alternative to full attention, particularly for models deployed in low-resource environments where efficient inference is essential. Additionally, we provide an analysis of the effects of approximation strength on model performance, showing how fine-grained control over trade-offs between efficiency and effectiveness can be achieved via simple parameter adjustments. This work advances the state of art in NLP by providing a practical technique for enabling efficient use of approximate attention mechanisms without significant loss of quality, paving the way for more sustainable deployment of machine learning systems in real-world scenarios.",1
"A standard pipeline of current face recognition frameworks consists of four individual steps: locating a face with a rough bounding box and several fiducial landmarks, aligning the face image using a pre-defined template, extracting representations and comparing. Among them, face detection, landmark detection and representation learning have long been studied and a lot of works have been proposed. As an essential step with a significant impact on recognition performance, the alignment step has attracted little attention. In this paper, we first explore and highlight the effects of different alignment templates on face recognition. Then, for the first time, we try to search for the optimal template automatically. We construct a well-defined searching space by decomposing the template searching into the crop size and vertical shift, and propose an efficient method Face Alignment Policy Search (FAPS). Besides, a well-designed benchmark is proposed to evaluate the searched policy. Experiments on our proposed benchmark validate the effectiveness of our method to improve face recognition performance.",0
"Here we present results demonstrating how recent advances in machine learning can effectively improve facial recognition by optimizing key components within the algorithm pipeline for greater speed and accuracy across multiple hardware architectures, without sacrificing privacy. Our system design includes automated model calibration and fine tuning, which is tailored to specific use cases; e.g., enhanced surveillance capabilities in crowded urban areas vs. secure authentication at ATMs. These modifications allow our models to generalize better towards previously unseen identities (i.e., OoD detection) while minimizing errors on known individuals, leading to higher overall performance. We achieved state-of-the-art accuracy without relying on large annotated datasets or transfer learning from other domains like ImageNet. This reduced dependence on external data sources greatly limits potential security vulnerabilities and biases ingrained into pre-training datasets that could compromise personal information privacy. Furthermore, we provide evidence of how post-hoc adversarial attacks can drastically reduce recognition efficacy when faced against even subtle perturbations applied during testing time. Finally, we explore trade-offs among different hyperparameters commonly used for training face recognition systems using a new evaluation metric that explicitly considers both intra-class variability and inter-class separability to determine whether these parameters lead to overfitting or underfitting issues. By addressing these challenges through multi-faceted approaches, our solution offers more robust and reliable face recognition techniques applicable beyond computer vision applications.",1
"Hyperbolic spaces, which have the capacity to embed tree structures without distortion owing to their exponential volume growth, have recently been applied to machine learning to better capture the hierarchical nature of data. In this study, we generalize the fundamental components of neural networks in a single hyperbolic geometry model, namely, the Poincar\'e ball model. This novel methodology constructs a multinomial logistic regression, fully-connected layers, convolutional layers, and attention mechanisms under a unified mathematical interpretation, without increasing the parameters. Experiments show the superior parameter efficiency of our methods compared to conventional hyperbolic components, and stability and outperformance over their Euclidean counterparts.",0
"Abstract: This article introduces a new variant of hyperbolic neural networks (HNNs) that extends traditional HNN architectures by incorporating novel elements from deep learning research. By combining advanced techniques such as residual connections, batch normalization, and dropout regularization, we create a more powerful model that offers improved performance on complex tasks while retaining the unique properties of HNNs. We demonstrate the effectiveness of our approach through extensive experiments on several benchmark datasets, showing significant improvements over state-of-the-art methods. Our work represents an important step forward in the development of efficient neural models for real-world applications.",1
"Intuitive user interfaces are indispensable to interact with the human centric smart environments. In this paper, we propose a unified framework that recognizes both static and dynamic gestures, using simple RGB vision (without depth sensing). This feature makes it suitable for inexpensive human-robot interaction in social or industrial settings. We employ a pose-driven spatial attention strategy, which guides our proposed Static and Dynamic gestures Network - StaDNet. From the image of the human upper body, we estimate his/her depth, along with the region-of-interest around his/her hands. The Convolutional Neural Network in StaDNet is fine-tuned on a background-substituted hand gestures dataset. It is utilized to detect 10 static gestures for each hand as well as to obtain the hand image-embeddings. These are subsequently fused with the augmented pose vector and then passed to the stacked Long Short-Term Memory blocks. Thus, human-centred frame-wise information from the augmented pose vector and from the left/right hands image-embeddings are aggregated in time to predict the dynamic gestures of the performing person. In a number of experiments, we show that the proposed approach surpasses the state-of-the-art results on the large-scale Chalearn 2016 dataset. Moreover, we transfer the knowledge learned through the proposed methodology to the Praxis gestures dataset, and the obtained results also outscore the state-of-the-art on this dataset.",0
"This should be suitable as the first paragraph on Google scholar: A new method for recognizing gestures from depth images produced by Time-of-Flight (ToF) cameras has been developed. By incorporating temporal information using deep learning techniques such as Long Short Term Memory networks (LSTMs), the model can accurately identify static and dynamic hand poses even under occlusion. Experiments showed that our approach outperforms current state-of-the-art methods in terms of accuracy while running at low computational cost. Our framework provides valuable insights into gesture recognition which could enable realtime control interfaces based on hand gestures in mixed reality environments.",1
"We propose a scheme for supervised image classification that uses privileged information, in the form of keypoint annotations for the training data, to learn strong models from small and/or biased training sets. Our main motivation is the recognition of animal species for ecological applications such as biodiversity modelling, which is challenging because of long-tailed species distributions due to rare species, and strong dataset biases such as repetitive scene background in camera traps. To counteract these challenges, we propose a visual attention mechanism that is supervised via keypoint annotations that highlight important object parts. This privileged information, implemented as a novel privileged pooling operation, is only required during training and helps the model to focus on regions that are discriminative. In experiments with three different animal species datasets, we show that deep networks with privileged pooling can use small training sets more efficiently and generalize better.",0
"Recent advances in deep learning have shown that pooling methods can greatly improve sample efficiency by reducing computational costs without sacrificing model accuracy. However, traditional unsupervised attention mechanisms used in these methods can be computationally expensive and may struggle to capture important features from small datasets. In our work, we propose a novel approach called privileged pooling, which combines both supervision and attention mechanisms to achieve better sample efficiency. Our method leverages task-specific knowledge provided through labeled data to guide the attention mechanism towards more informative features, resulting in improved performance even with fewer samples. We evaluate our method on several challenging benchmark tasks and demonstrate consistent improvements over state-of-the-art approaches. Our findings show that incorporating task-specific information into pooling mechanisms leads to greater sample efficiency and better generalization across diverse domains. Overall, our work presents an exciting new direction for improving sample efficiency in machine learning while maintaining high levels of performance.",1
"Practitioners in diverse fields such as healthcare, economics and education are eager to apply machine learning to improve decision making. The cost and impracticality of performing experiments and a recent monumental increase in electronic record keeping has brought attention to the problem of evaluating decisions based on non-experimental observational data. This is the setting of this work. In particular, we study estimation of individual-level causal effects, such as a single patient's response to alternative medication, from recorded contexts, decisions and outcomes. We give generalization bounds on the error in estimated effects based on distance measures between groups receiving different treatments, allowing for sample re-weighting. We provide conditions under which our bound is tight and show how it relates to results for unsupervised domain adaptation. Led by our theoretical results, we devise representation learning algorithms that minimize our bound, by regularizing the representation's induced treatment group distance, and encourage sharing of information between treatment groups. We extend these algorithms to simultaneously learn a weighted representation to further reduce treatment group distances. Finally, an experimental evaluation on real and synthetic data shows the value of our proposed representation architecture and regularization scheme.",0
"In this work we develop bounds on generalization performance that characterize how well estimates based on training data extrapolate to potential outcomes. We prove several results showing that under fairly weak assumptions we can bound the difference between the expected value over test points drawn from unknown population distributions (i.e., the expected causal effect) and the mean squared error achieved by minimizing ERM. Our key technical contributions center on novel operator norm bounds, which directly give rise to PAC style bounds as special cases; these also permit more careful analysis of common regularizers like Ridge regression. Moreover, our theoretical tools yield insights into the relative benefits of various techniques used in representation learningâ€”such as kernel methods and deep neural networksâ€”for ensuring good generalization to OOPs. Extensive numerical experiments confirm many predictions, highlighting broad scope of applicability spanning both linearly separable and nonlinear problemsâ€”including tabular datasets commonly encountered in applied machine learning research today. Finally, our findings suggest new strategies for choosing appropriate complexity penalties tailored to specific problem classes while balancing model capacity against flexibility required to accurately capture underlying relationships across heterogeneous populations. These advances bring us closer to ultimately understanding â€œwhy models workâ€ for real world tasks of high impact where randomized trials remain unfeasible due to cost constraints or ethical considerationsâ€”a challenging topic we continue to investigate going forward.",1
"Deep supervised hashing for image retrieval has attracted researchers' attention due to its high efficiency and superior retrieval performance. Most existing deep supervised hashing works, which are based on pairwise/triplet labels, suffer from the expensive computational cost and insufficient utilization of the semantics information. Recently, deep classwise hashing introduced a classwise loss supervised by class labels information alternatively; however, we find it still has its drawback. In this paper, we propose an improved deep classwise hashing, which enables hashing learning and class centers learning simultaneously. Specifically, we design a two-step strategy on center similarity learning. It interacts with the classwise loss to attract the class center to concentrate on the intra-class samples while pushing other class centers as far as possible. The centers similarity learning contributes to generating more compact and discriminative hashing codes. We conduct experiments on three benchmark datasets. It shows that the proposed method effectively surpasses the original method and outperforms state-of-the-art baselines under various commonly-used evaluation metrics for image retrieval.",0
"This research presents an improved deep classwise hashing approach using centers similarity learning for image retrieval. The proposed method addresses limitations of traditional hashing methods by leveraging the discriminative power of convolutional neural networks (CNNs) to learn more expressive features that capture both local patterns and global structures in images. To achieve better performance, we introduce two key components: 1) center sampling strategy during training, which encourages each bit in hash codes to capture different levels of importance of image content; and 2) centers refinement scheme after training, which adaptively adjusts code centers based on their impact on intra-class scatter and inter-class separation. Experimental results on three public benchmark datasets demonstrate significant improvements over state-of-the-art techniques in terms of accuracy, robustness, and efficiency. Our findings suggest that the proposed approach provides a promising direction for developing effective solutions to large-scale image retrieval problems.",1
"Escalator-related injuries threaten public health with the widespread use of escalators. The existing studies tend to focus on after-the-fact statistics, reflecting on the original design and use of defects to reduce the impact of escalator-related injuries, but few attention has been paid to ongoing and impending injuries. In this study, a multi-module escalator safety monitoring system based on computer vision is designed and proposed to simultaneously monitor and deal with three major injury triggers, including losing balance, not holding on to handrails and carrying large items. The escalator identification module is utilized to determine the escalator region, namely the region of interest. The passenger monitoring module is leveraged to estimate the passengers' pose to recognize unsafe behaviors on the escalator. The dangerous object detection module detects large items that may enter the escalator and raises alarms. The processing results of the above three modules are summarized in the safety assessment module as the basis for the intelligent decision of the system. The experimental results demonstrate that the proposed system has good performance and great application potential.",0
"This paper presents a multi-module integrated system that utilizes machine learning algorithms, image processing techniques, and cloud computing technologies to identify potential escalator-related injuries and prevent them from occurring. With increasing numbers of individuals using public transportation systems such as subways, train stations, airports, shopping malls, and other facilities across China, there has been a growing need for enhanced safety measures. Our proposed system aims to fill this gap by providing real-time monitoring and analysis of escalator operations to identify any irregularities or malfunctions that could lead to accidents.  The system comprises four modules: escalator operation data acquisition module, image recognition module, fault diagnosis module, and risk assessment module. These modules work together seamlessly to ensure effective identification and prevention of escalator-related injuries. The first module collects real-time operational data including speed, acceleration, power consumption, temperature, and vibration through sensors installed directly onto escalators. The second module uses deep convolutional neural networks (CNN) to analyze images captured by surveillance cameras and detect any abnormal behavior patterns by passengers, maintenance staff, or other personnel near escalators. The third module employs support vector machines (SVM) classifiers to accurately diagnose faulty components based on vast amounts of historical data stored on remote servers via cloud computing platforms. Lastly, the fourth module evaluates risks associated with identified defects according to established risk indices to prioritize corrective actions.  In summary, our proposed system integrating multiple advanced technologies demonstrates strong potential for promoting public health and safety within densely populated urban environments. By effectively identifying and mitigating escalator-related hazards before they cause harm, we can improve user confidence in public transpor",1
"3D object detection has attracted much attention thanks to the advances in sensors and deep learning methods for point clouds. Current state-of-the-art methods like VoteNet regress direct offset towards object centers and box orientations with an additional Multi-Layer-Perceptron network. Both their offset and orientation predictions are not accurate due to the fundamental difficulty in rotation classification. In the work, we disentangle the direct offset into Local Canonical Coordinates (LCC), box scales and box orientations. Only LCC and box scales are regressed while box orientations are generated by a canonical voting scheme. Finally, a LCC-aware back-projection checking algorithm iteratively cuts out bounding boxes from the generated vote maps, with the elimination of false positives. Our model achieves state-of-the-art performance on challenging large-scale datasets of real point cloud scans: ScanNet, SceneNN with 8.8 and 5.1 mAP improvement respectively. Code is available on https://github.com/qq456cvb/CanonicalVoting.",0
â€‹,1
"In the field of complex action recognition in videos, the quality of the designed model plays a crucial role in the final performance. However, artificially designed network structures often rely heavily on the researchers' knowledge and experience. Accordingly, because of the automated design of its network structure, Neural architecture search (NAS) has achieved great success in the image processing field and attracted substantial research attention in recent years. Although some NAS methods have reduced the number of GPU search days required to single digits in the image field, directly using 3D convolution to extend NAS to the video field is still likely to produce a surge in computing volume. To address this challenge, we propose a new processing framework called Neural Architecture Search- Temporal Convolutional (NAS-TC). Our proposed framework is divided into two phases. In the first phase, the classical CNN network is used as the backbone network to complete the computationally intensive feature extraction task. In the second stage, a simple stitching search to the cell is used to complete the relatively lightweight long-range temporal-dependent information extraction. This ensures our method will have more reasonable parameter assignments and can handle minute-level videos. Finally, we conduct sufficient experiments on multiple benchmark datasets and obtain competitive recognition accuracy.",0
"In recent years, convolutional neural networks (CNNs) have shown great success in visual recognition tasks such as image classification, object detection, and semantic segmentation. However, when it comes to complex action recognition, where temporal dynamics play a crucial role, traditional CNN architectures often struggle to capture important features from video data. To address this challenge, we propose a novel approach called NAS-TC that utilizes neural architecture search to optimize temporal convolutional models specifically designed for recognizing complex actions. Our method searches over different kernel sizes, strides, and dilation rates within temporal convolutions to find the optimal combination of parameters that maximize performance. Through extensive experiments on three benchmark datasets, we demonstrate that our proposed NAS-TC model outperforms state-of-the-art methods by achieving higher accuracy with fewer parameters. This work highlights the effectiveness of using NAS techniques for designing efficient and accurate deep learning models for complex action recognition tasks.",1
"When manipulating three-dimensional data, it is possible to ensure that rotational and translational symmetries are respected by applying so-called SE(3)-equivariant models. Protein structure prediction is a prominent example of a task which displays these symmetries. Recent work in this area has successfully made use of an SE(3)-equivariant model, applying an iterative SE(3)-equivariant attention mechanism. Motivated by this application, we implement an iterative version of the SE(3)-Transformer, an SE(3)-equivariant attention-based model for graph data. We address the additional complications which arise when applying the SE(3)-Transformer in an iterative fashion, compare the iterative and single-pass versions on a toy problem, and consider why an iterative model may be beneficial in some problem settings. We make the code for our implementation available to the community.",0
"An end-to-end deep learning approach was developed to improve pose estimation accuracy by incorporating iterative similarity transformation into transformer networks. This study aimed to investigate the performance impact of combining localization optimization with attention-based machine vision techniques on pose refinement tasks. The proposed method outperformed previous state-of-the-art methods in terms of both speed and accuracy and can handle complex objects with cluttered backgrounds. Overall, we conclude that integrating similaritity transformations into transformer networks holds great promise for improving pose estimation models, which could potentially revolutionize applications such as AR/VR and robotics.",1
"Image restoration tasks demand a complex balance between spatial details and high-level contextualized information while recovering images. In this paper, we propose a novel synergistic design that can optimally balance these competing goals. Our main proposal is a multi-stage architecture, that progressively learns restoration functions for the degraded inputs, thereby breaking down the overall recovery process into more manageable steps. Specifically, our model first learns the contextualized features using encoder-decoder architectures and later combines them with a high-resolution branch that retains local information. At each stage, we introduce a novel per-pixel adaptive design that leverages in-situ supervised attention to reweight the local features. A key ingredient in such a multi-stage architecture is the information exchange between different stages. To this end, we propose a two-faceted approach where the information is not only exchanged sequentially from early to late stages, but lateral connections between feature processing blocks also exist to avoid any loss of information. The resulting tightly interlinked multi-stage architecture, named as MPRNet, delivers strong performance gains on ten datasets across a range of tasks including image deraining, deblurring, and denoising. The source code and pre-trained models are available at https://github.com/swz30/MPRNet.",0
"This work presents a novel framework for progressively restoring images from degraded versions using deep convolutional neural networks (CNNs). The proposed method involves three stages: initialization, refinement, and fine-grained detail enhancement. In each stage, our network estimates the residual image which represents the difference between the raw input and restored output. We train each stage independently by minimizing the reconstruction loss between their outputs and ground truth images. During inference, we sequentially add the estimated residuals at each stage onto the initial restored version until a visually pleasing result is obtained. Extensive experiments on two benchmark datasets demonstrate that our method achieves state-of-the-art performance on both objectives and subjectives metrics while maintaining real-time speed (>6fps). Our approach significantly advances the current understanding of multi-stage image restoration problems by enabling efficient and accurate solutions.",1
"Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.",0
"In order to address the task at hand, we need to find ways to leverage existing data sets to improve their performance on new tasks. One approach that has been proposed involves augmenting training samples using techniques like cropping, rotating, flipping, and translating images. However, these methods may introduce bias into the resulting models, which can lead to poor generalization performance. To overcome this issue, we propose a method based on minimizing the maximal expected loss (MML) across all possible labelings. We show through extensive experiments that our approach significantly improves the accuracy of the trained models compared to baseline approaches. Our work demonstrates the importance of careful consideration of sampling strategies when designing machine learning systems.",1
"The security of object detection systems has attracted increasing attention, especially when facing adversarial patch attacks. Since patch attacks change the pixels in a restricted area on objects, they are easy to implement in the physical world, especially for attacking human detection systems. The existing defenses against patch attacks are mostly applied for image classification problems and have difficulty resisting human detection attacks. Towards this critical issue, we propose an efficient and effective plug-in defense component on the YOLO detection system, which we name Ad-YOLO. The main idea is to add a patch class on the YOLO architecture, which has a negligible inference increment. Thus, Ad-YOLO is expected to directly detect both the objects of interest and adversarial patches. To the best of our knowledge, our approach is the first defense strategy against human detection attacks.   We investigate Ad-YOLO's performance on the YOLOv2 baseline. To improve the ability of Ad-YOLO to detect variety patches, we first use an adversarial training process to develop a patch dataset based on the Inria dataset, which we name Inria-Patch. Then, we train Ad-YOLO by a combination of Pascal VOC, Inria, and Inria-Patch datasets. With a slight drop of $0.70\%$ mAP on VOC 2007 test set, Ad-YOLO achieves $80.31\%$ AP of persons, which highly outperforms $33.93\%$ AP for YOLOv2 when facing white-box patch attacks. Furthermore, compared with YOLOv2, the results facing a physical-world attack are also included to demonstrate Ad-YOLO's excellent generalization ability.",0
"This paper presents novel methods for detecting adversarial patch attacks on human detection systems based on the popular YOLO (You Only Look Once) architecture. These types of attacks involve adding small perturbations to images that can cause the system to incorrectly classify objects, potentially leading to dangerous situations such as autonomous vehicles crashing or security cameras failing to identify intruders. To address this issue, we propose a new algorithm that uses multiple complementary defense strategies to increase robustness against these attacks. Our approach outperforms state-of-the-art methods across a range of metrics and attack scenarios, demonstrating the effectiveness of our proposed techniques. We believe that this work represents an important contribution towards building more secure computer vision systems that can operate reliably even under adversarial conditions.",1
"Recent research in dynamic convolution shows substantial performance boost for efficient CNNs, due to the adaptive aggregation of K static convolution kernels. It has two limitations: (a) it increases the number of convolutional weights by K-times, and (b) the joint optimization of dynamic attention and static convolution kernels is challenging. In this paper, we revisit it from a new perspective of matrix decomposition and reveal the key issue is that dynamic convolution applies dynamic attention over channel groups after projecting into a higher dimensional latent space. To address this issue, we propose dynamic channel fusion to replace dynamic attention over channel groups. Dynamic channel fusion not only enables significant dimension reduction of the latent space, but also mitigates the joint optimization difficulty. As a result, our method is easier to train and requires significantly fewer parameters without sacrificing accuracy. Source code is at https://github.com/liyunsheng13/dcd.",0
"In this work we revisit dynamic convolution via matrix decomposition. By analyzing the connections between dynamic convolutions and linear algebra operations such as vectorization, diagonalization, and singular value decompositions (SVD), we develop new insights into their properties and applications. We show how these techniques can be used to solve systems of partial differential equations, including both time-independent and time-dependent problems. Our approach allows us to derive efficient algorithms that are easy to implement and have low memory requirements. Furthermore, our methods offer several advantages over traditional approaches, including better accuracy and stability, as well as improved computational efficiency. Finally, we demonstrate the effectiveness of our methods on a range of examples, highlighting their potential impact across many fields, from physics and engineering to finance and economics. Overall, our work represents an important step forward in the development of numerical methods for solving complex mathematical models.",1
"Semantic Segmentation is a crucial component in the perception systems of many applications, such as robotics and autonomous driving that rely on accurate environmental perception and understanding. In literature, several approaches are introduced to attempt LiDAR semantic segmentation task, such as projection-based (range-view or birds-eye-view), and voxel-based approaches. However, they either abandon the valuable 3D topology and geometric relations and suffer from information loss introduced in the projection process or are inefficient. Therefore, there is a need for accurate models capable of processing the 3D driving-scene point cloud in 3D space. In this paper, we propose S3Net, a novel convolutional neural network for LiDAR point cloud semantic segmentation. It adopts an encoder-decoder backbone that consists of Sparse Intra-channel Attention Module (SIntraAM), and Sparse Inter-channel Attention Module (SInterAM) to emphasize the fine details of both within each feature map and among nearby feature maps. To extract the global contexts in deeper layers, we introduce Sparse Residual Tower based upon sparse convolution that suits varying sparsity of LiDAR point cloud. In addition, geo-aware anisotrophic loss is leveraged to emphasize the semantic boundaries and penalize the noise within each predicted regions, leading to a robust prediction. Our experimental results show that the proposed method leads to a large improvement (12\%) compared to its baseline counterpart (MinkNet42 \cite{choy20194d}) on SemanticKITTI \cite{DBLP:conf/iccv/BehleyGMQBSG19} test set and achieves state-of-the-art mIoU accuracy of semantic segmentation approaches.",0
"LiDAR is important technology that can be used by driverless cars and drones to perceive their surroundings, creating a detailed picture of their environment using laser pulses. Our work proposes a new architecture called S3Net (Sparse Semantic Segmentation Network) which uses an encoder-decoder structure to perform LiDAR point cloud semantic segmentation at high resolution. This architecture achieves state-of-the-art performance on several benchmarks while running efficiently on modern GPUs. We compare S3Net against other methods on three popular datasets, showing improved accuracy under both IoU and mIoU metrics. Additionally, we study different design choices such as number of convolutional layers per stage, depthwise separability, pointwise vs. channelwise MLP blocks, skip connections, feature fusion, and attention mechanisms and determine optimal settings. In conclusion, our proposed S3Net model provides significant improvements over prior art, making it well suited for real world applications requiring accurate scene understanding like self driving cars and robotics where computation resources may be limited.",1
"3D object detection plays a crucial role in environmental perception for autonomous vehicles, which is the prerequisite of decision and control. This paper analyses partition-based methods' inherent drawbacks. In the partition operation, a single instance such as a pedestrian is sliced into several pieces, which we call it the partition effect. We propose the Spatial-Attention Graph Convolution (S-AT GCN), forming the Feature Enhancement (FE) layers to overcome this drawback. The S-AT GCN utilizes the graph convolution and the spatial attention mechanism to extract local geometrical structure features. This allows the network to have more meaningful features for the foreground. Our experiments on the KITTI 3D object and bird's eye view detection show that S-AT Conv and FE layers are effective, especially for small objects. FE layers boost the pedestrian class performance by 3.62\% and cyclist class by 4.21\% 3D mAP. The time cost of these extra FE layers are limited. PointPillars with FE layers can achieve 48 PFS, satisfying the real-time requirement.",0
"Deep learning has revolutionized many fields such as computer vision by providing powerful model architectures that can learn intricate patterns from large amounts of data. One common task in computer vision is object detection, where the goal is to locate objects within an image or video frame. Recently, deep convolutional neural networks (CNNs) have been used extensively for object detection in 2D images. In contrast, detecting 3D objects using LiDAR point clouds remains challenging due to their inherent spatial ambiguity caused by occlusions and varying ranges of distances. We present an approach called S-AT GCN which uses Graph Convolution Networks (GCNs) to enhance features extracted from LiDAR points. By incorporating spatial attention modules into our network architecture, we effectively focus on regions most relevant to feature extraction, improving the accuracy of object detection. Our experiments show that S-AT GCN significantly outperforms state-of-the-art methods on several benchmark datasets, demonstrating its effectiveness in enhancing features from raw LiDAR point clouds.",1
"In neural architecture search (NAS), differentiable architecture search (DARTS) has recently attracted much attention due to its high efficiency. It defines an over-parameterized network with mixed edges, each of which represents all operator candidates, and jointly optimizes the weights of the network and its architecture in an alternating manner. However, this method finds a model with the weights converging faster than the others, and such a model with fastest convergence often leads to overfitting. Accordingly, the resulting model cannot always be well-generalized. To overcome this problem, we propose a method called minimum stable rank DARTS (MSR-DARTS), for finding a model with the best generalization error by replacing architecture optimization with the selection process using the minimum stable rank criterion. Specifically, a convolution operator is represented by a matrix, and MSR-DARTS selects the one with the smallest stable rank. We evaluated MSR-DARTS on CIFAR-10 and ImageNet datasets. It achieves an error rate of 2.54% with 4.0M parameters within 0.3 GPU-days on CIFAR-10, and a top-1 error rate of 23.9% on ImageNet. The official code is available at https://github.com/mtaecchhi/msrdarts.git.",0
"""MSR-DARTS"" is a novel algorithm for automatically searching through deep neural network architectures to find one that meets certain criteria. In particular, it can efficiently search among models whose performance makes them competitive on large datasets using only a small number of GPUs days for training. Compared against alternative methods based on reinforcement learning from human feedback (""reinforcement learning""), ""MSR-DARTS"" often finds better performing models (using fewer parameters) while using less compute resources. For example, we show how ""MSR-DARTS"" was able to find state-of-the-art models on benchmark image classification problems CIFAR-10/100 and ImageNet-2012 within just a few days on two V100 GPUs, whereas similar searches with RL took months and many more GPUs. We provide a thorough empirical evaluation of our method across six different tasks, comparing its performance to both manual architecture design and automated random search as well as other recent studies exploring the use of differentiable algorithms during search. Our results indicate that ""MSR-DARTS"" produces strong improvements over existing alternatives across a wide range of settings, making it particularly suitable where computational budgets must trade off against model quality demands.",1
"Supervised learning of time series data has been extensively studied for the case of a categorical target variable. In some application domains, e.g., energy, environment and health monitoring, it occurs that the target variable is numerical and the problem is known as time series extrinsic regression (TSER). In the literature, some well-known time series classifiers have been extended for TSER problems. As first benchmarking studies have focused on predictive performance, very little attention has been given to interpretability. To fill this gap, in this paper, we suggest an extension of a Bayesian method for robust and interpretable feature construction and selection in the context of TSER. Our approach exploits a relational way to tackle with TSER: (i), we build various and simple representations of the time series which are stored in a relational data scheme, then, (ii), a propositionalisation technique (based on classical aggregation / selection functions from the relational data field) is applied to build interpretable features from secondary tables to ""flatten"" the data; and (iii), the constructed features are filtered out through a Bayesian Maximum A Posteriori approach. The resulting transformed data can be processed with various existing regressors. Experimental validation on various benchmark data sets demonstrates the benefits of the suggested approach.",0
"This paper presents a novel approach for feature construction in time series extrinsic regression problems, focusing on interpretability and ease of use for practitioners. Existing methods often rely on complex mathematical models or black-box transformations that can be difficult to interpret and explain. Our method addresses these limitations by using simple and intuitive features that capture important patterns in the data while remaining interpretable. We evaluate our approach on several real-world datasets and demonstrate significant improvements over state-of-the-art methods. In addition, we provide insights into how different features impact model performance, allowing users to make informed decisions when selecting features for their specific problem. Overall, our work represents a step forward towards more transparent and accessible machine learning tools for time series forecasting.",1
"In this paper, we address the semantic segmentation task with a new context aggregation scheme named \emph{object context}, which focuses on enhancing the role of object information. Motivated by the fact that the category of each pixel is inherited from the object it belongs to, we define the object context for each pixel as the set of pixels that belong to the same category as the given pixel in the image. We use a binary relation matrix to represent the relationship between all pixels, where the value one indicates the two selected pixels belong to the same category and zero otherwise.   We propose to use a dense relation matrix to serve as a surrogate for the binary relation matrix. The dense relation matrix is capable to emphasize the contribution of object information as the relation scores tend to be larger on the object pixels than the other pixels. Considering that the dense relation matrix estimation requires quadratic computation overhead and memory consumption w.r.t. the input size, we propose an efficient interlaced sparse self-attention scheme to model the dense relations between any two of all pixels via the combination of two sparse relation matrices.   To capture richer context information, we further combine our interlaced sparse self-attention scheme with the conventional multi-scale context schemes including pyramid pooling~\citep{zhao2017pyramid} and atrous spatial pyramid pooling~\citep{chen2018deeplab}. We empirically show the advantages of our approach with competitive performances on five challenging benchmarks including: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff",0
"OCNet is a deep learning model designed for scene parsing tasks, which involves detecting objects in images and labeling them according to their context within the image. Convolutional neural networks (CNNs) have proven effective at these types of tasks, but can struggle with accurately identifying objects in cluttered scenes where multiple objects overlap or appear similar to each other.  OCNet addresses this challenge by introducing a new network architecture that focuses on capturing object context through the use of multiple spatial pyramid levels and scale-aware feature pooling. This allows the model to better handle overlapping objects and variations in size across different classes. Additionally, OCNet employs dilated convolutions in its spatial pyramid layers to increase receptive field coverage without sacrificing resolution, resulting in improved performance compared to traditional CNN architectures.  To evaluate the effectiveness of our proposed approach, we conducted extensive experiments on three benchmark datasets commonly used in computer vision research: PASCAL VOC, COCO, and Open Images. Results showed that OCNet outperformed several state-of-the-art methods on all three datasets, demonstrating its ability to effectively capture object context and improve overall accuracy. In conclusion, OCNet represents an important contribution to the development of scene parsing techniques and holds promise for future applications in fields such as robotics, autonomous vehicles, and medical imaging analysis.",1
"The development of recommender systems that optimize multi-turn interaction with users, and model the interactions of different agents (e.g., users, content providers, vendors) in the recommender ecosystem have drawn increasing attention in recent years. Developing and training models and algorithms for such recommenders can be especially difficult using static datasets, which often fail to offer the types of counterfactual predictions needed to evaluate policies over extended horizons. To address this, we develop RecSim NG, a probabilistic platform for the simulation of multi-agent recommender systems. RecSim NG is a scalable, modular, differentiable simulator implemented in Edward2 and TensorFlow. It offers: a powerful, general probabilistic programming language for agent-behavior specification; tools for probabilistic inference and latent-variable model learning, backed by automatic differentiation and tracing; and a TensorFlow-based runtime for running simulations on accelerated hardware. We describe RecSim NG and illustrate how it can be used to create transparent, configurable, end-to-end models of a recommender ecosystem, complemented by a small set of simple use cases that demonstrate how RecSim NG can help both researchers and practitioners easily develop and train novel algorithms for recommender systems.",0
"""Recent advances in technology have led to an explosion of data available for recommender systems, but there remains a critical need for principled uncertainty modeling to ensure that these systems provide accurate and trustworthy recommendations. In our paper, we present RecSim NG, a novel framework for modeling uncertainty in recommender ecosystems. Our approach leverages advanced techniques from probabilistic programming to enable flexible specification of complex models and their underlying uncertainties. We demonstrate the effectiveness of our methodology on several real-world datasets, showing that our approach outperforms state-of-the-art methods in terms of accuracy and robustness. Finally, we discuss potential applications of our work beyond recommendation tasks.""",1
"Few-shot image classification consists of two consecutive learning processes: 1) In the meta-learning stage, the model acquires a knowledge base from a set of training classes. 2) During meta-testing, the acquired knowledge is used to recognize unseen classes from very few examples. Inspired by the compositional representation of objects in humans, we train a neural network architecture that explicitly represents objects as a set of parts and their spatial composition. In particular, during meta-learning, we train a knowledge base that consists of a dictionary of part representations and a dictionary of part activation maps that encode common spatial activation patterns of parts. The elements of both dictionaries are shared among the training classes. During meta-testing, the representation of unseen classes is learned using the part representations and the part activation maps from the knowledge base. Finally, an attention mechanism is used to strengthen those parts that are most important for each category. We demonstrate the value of our compositional learning framework for a few-shot classification using miniImageNet, tieredImageNet, CIFAR-FS, and FC100, where we achieve state-of-the-art performance.",0
"In recent years, few-shot classification has emerged as a promising direction in machine learning due to its ability to learn from very limited labeled data. One key challenge in few-shot learning is designing effective methods for representing input images, which should capture discriminative features while maintaining compositional structure, making them suitable for zero-shot generalization across domains and tasks. In this work, we propose COMPAS (COmpositional Part Sharing), a novel representation learning framework that overcomes these challenges by leveraging compositional part sharing for few-shot image classification. Our approach utilizes a part decomposition module, enabling semantic component alignment among different object parts within each feature vector. By explicitly modeling compositional similarity during training, our method fosters improved discrimination and zero-shot transferability compared to state-of-the-art methods. We evaluate our proposed COMPAS framework on several benchmark datasets and demonstrate remarkable improvements across multiple experimental settings. Overall, our results highlight the effectiveness and versatility of incorporating compositional part sharing into few-shot representation learning pipelines, paving the way for future advancements in low-data supervision regimes.",1
"A large class of modern probabilistic learning systems assumes symmetric distributions, however, real-world data tend to obey skewed distributions and are thus not always adequately modelled through symmetric distributions. To address this issue, elliptical distributions are increasingly used to generalise symmetric distributions, and further improvements to skewed elliptical distributions have recently attracted much attention. However, existing approaches are either hard to estimate or have complicated and abstract representations. To this end, we propose to employ the von-Mises-Fisher (vMF) distribution to obtain an explicit and simple probability representation of the skewed elliptical distribution. This is shown not only to allow us to deal with non-symmetric learning systems, but also to provide a physically meaningful way of generalising skewed distributions. For rigour, our extension is proved to share important and desirable properties with its symmetric counterpart. We also demonstrate that the proposed vMF distribution is both easy to generate and stable to estimate, both theoretically and through examples.",0
"This paper introduces a new probability distribution called the ""Von Mises-Fisher elliptical distribution,"" which combines aspects of both the von Mises distribution and Fisher's z-distribution into one flexible model. The authors begin by providing background on these two distributions and discussing their shortcomings in certain situations. They then present the mathematical formulation of the new distribution and explain how it addresses some of the limitations of existing models. The authors also explore the properties and characteristics of the new distribution through simulations and real data examples, demonstrating its ability to capture complex patterns while still remaining computationally tractable. Finally, they conclude by highlighting potential applications of the new distribution in fields such as finance, economics, engineering, and beyond. Overall, the paper presents a valuable contribution to the field of statistics that offers researchers a powerful tool for analyzing complex data sets.",1
"Over the past several years, in order to solve the problem of malicious abuse of facial manipulation technology, face manipulation detection technology has obtained considerable attention and achieved remarkable progress. However, most existing methods have very impoverished generalization ability and robustness. In this paper, we propose a novel method for face manipulation detection, which can improve the generalization ability and robustness by bag-of-local-feature. Specifically, we extend Transformers using bag-of-feature approach to encode inter-patch relationships, allowing it to learn local forgery features without any explicit supervision. Extensive experiments demonstrate that our method can outperform competing state-of-the-art methods on FaceForensics++, Celeb-DF and DeeperForensics-1.0 datasets.",0
"In recent years, face manipulation has become increasingly prevalent on social media platforms, leading to serious consequences such as disinformation spreading and damage to individuals' reputation. As a result, automatic detection methods have been proposed to identify these manipulated images. However, most existing methods rely heavily on large amounts of manually labeled data for training, which can lead to performance degradation when faced with previously unseen types of manipulations. Therefore, there is a need for more generalizable solutions that can detect a wide range of facial manipulations without relying on extensive datasets. This study proposes a novel approach using bag-of-local features (BoLF) to effectively capture subtle differences between authentic faces and their manipulated counterparts. We demonstrate through comprehensive experiments that our method outperforms state-of-the-art approaches under both qualitative and quantitative evaluations across multiple benchmarks, including real-world scenarios. Our work highlights the effectiveness of BoLF features in detecting complex facial manipulations while maintaining high robustness against various image processing techniques. This research holds significant implications for building trustworthy online environments by enabling better identification and prevention of malicious content featuring manipulated faces.",1
"Handwritten digit or numeral recognition is one of the classical issues in the area of pattern recognition and has seen tremendous advancement because of the recent wide availability of computing resources. Plentiful works have already done on English, Arabic, Chinese, Japanese handwritten script. Some work on Bangla also have been done but there is space for development. From that angle, in this paper, an architecture has been implemented which achieved the validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and Inception V3 architecture. Beside digit recognition, digit generation is another field which has recently caught the attention of the researchers though not many works have been done in this field especially on Bangla. In this paper, a Semi-Supervised Generative Adversarial Network or SGAN has been applied to generate Bangla handwritten numerals and it successfully generated Bangla digits.",0
"In recent years, there has been increasing interest in developing handwriting recognition systems for Bengali script, particularly due to its widespread usage in many parts of South Asia. This paper focuses on Bangla (Bengali) handwritten digit recognition and generation, which remains a challenging task due to the complexity of the language and variations in writing styles. The authors present two deep learning-based approaches for both tasks: Convolutional Neural Networks (CNNs) for digit recognition and Generative Adversarial Networks (GANs) for digit generation.  The CNN-based approach uses pre-trained models that have already learned features from other datasets, followed by fine-tuning on a large dataset of Bangla digits. To improve accuracy further, the authors propose data augmentation techniques such as rotation and flipping. Results show significant improvement over previous state-of-the-art methods with high precision and recall scores.  For the GAN-based approach, the authors use a CycleGAN architecture where one network generates synthetic images of digits while another network discriminates real from generated images. Training is performed using a hybrid loss function comprising adversarial loss and feature matching loss. The resulting model can generate high quality, diverse, and detailed Bangla digits with varying degrees of stroke thickness, slant, and curvature.  Overall, this work represents an important step towards enabling digitalization of documents written in Bangla. Future directions could involve merging the digit recognition and generation components into a single system to enable more advanced applications like document analysis and automated form processing.",1
"Recently, object detection in aerial images has gained much attention in computer vision. Different from objects in natural images, aerial objects are often distributed with arbitrary orientation. Therefore, the detector requires more parameters to encode the orientation information, which are often highly redundant and inefficient. Moreover, as ordinary CNNs do not explicitly model the orientation variation, large amounts of rotation augmented data is needed to train an accurate object detector. In this paper, we propose a Rotation-equivariant Detector (ReDet) to address these issues, which explicitly encodes rotation equivariance and rotation invariance. More precisely, we incorporate rotation-equivariant networks into the detector to extract rotation-equivariant features, which can accurately predict the orientation and lead to a huge reduction of model size. Based on the rotation-equivariant features, we also present Rotation-invariant RoI Align (RiRoI Align), which adaptively extracts rotation-invariant features from equivariant features according to the orientation of RoI. Extensive experiments on several challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and HRSC2016, show that our method can achieve state-of-the-art performance on the task of aerial object detection. Compared with previous best results, our ReDet gains 1.2, 3.5 and 2.6 mAP on DOTA-v1.0, DOTA-v1.5 and HRSC2016 respectively while reducing the number of parameters by 60\% (313 Mb vs. 121 Mb). The code is available at: \url{https://github.com/csuhan/ReDet}.",0
"One potential solution is to detect objects based on their equivariance properties under rotation, as many physical structures are rotationally symmetric. To achieve this, we propose using a new detector architecture that utilizes rotational symmetries through the use of group convolutions and equivariant pooling layers. Our experiments show promising results, demonstrating improved performance over traditional detection methods. Future work could involve exploring additional transformations beyond just rotation and applying our method to other domains such as medical imaging.",1
"Deep neural networks have been shown to be vulnerable to adversarial examples deliberately constructed to misclassify victim models. As most adversarial examples have restricted their perturbations to $L_{p}$-norm, existing defense methods have focused on these types of perturbations and less attention has been paid to unrestricted adversarial examples; which can create more realistic attacks, able to deceive models without affecting human predictions. To address this problem, the proposed adversarial attack generates an unrestricted adversarial example with a limited number of parameters. The attack selects three points on the input image and based on their locations transforms the image into an adversarial example. By limiting the range of movement and location of these three points and using a discriminatory network, the proposed unrestricted adversarial example preserves the image appearance. Experimental results show that the proposed adversarial examples obtain an average success rate of 93.5% in terms of human evaluation on the MNIST and SVHN datasets. It also reduces the model accuracy by an average of 73% on six datasets MNIST, FMNIST, SVHN, CIFAR10, CIFAR100, and ImageNet. It should be noted that, in the case of attacks, lower accuracy in the victim model denotes a more successful attack. The adversarial train of the attack also improves model robustness against a randomly transformed image.",0
"In this paper, we propose a novel method for generating unrestricted adversarial examples using only three parameters: $L_p$ distance from the original input, attack strength $\alpha$, and number of steps N. Our approach utilizes the Jacobian Matrix Veto (JMV) algorithm to generate perturbations that successfully fool state-of-the-art models while remaining imperceptible to human observers. We evaluate our method on several benchmark datasets and demonstrate that it achieves high success rates across multiple attacks and architectures. Additionally, we provide insights into the behavior of JMV by analyzing the influence of each parameter on generated perturbations. Overall, our work contributes new knowledge towards understanding adversarial robustness and improving model resilience against evasion attacks.",1
"While convolutional neural networks (CNNs) have significantly boosted the performance of face related algorithms, maintaining accuracy and efficiency simultaneously in practical use remains challenging. Recent study shows that using a cascade of hourglass modules which consist of a number of bottom-up and top-down convolutional layers can extract facial structural information for face alignment to improve accuracy. However, previous studies have shown that features produced by shallow convolutional layers are highly correspond to edges. These features could be directly used to provide the structural information without addition cost. Motivated by this intuition, we propose an efficient multitask face alignment, face tracking and head pose estimation network (ATPN). Specifically, we introduce a shortcut connection between shallow-layer features and deep-layer features to provide the structural information for face alignment and apply the CoordConv to the last few layers to provide coordinate information. The predicted facial landmarks enable us to generate a cheap heatmap which contains both geometric and appearance information for head pose estimation and it also provides attention clues for face tracking. Moreover, the face tracking task saves us the face detection procedure for each frame, which is significant to boost performance for video-based tasks. The proposed framework is evaluated on four benchmark datasets, WFLW, 300VW, WIDER Face and 300W-LP. The experimental results show that the ATPN achieves improved performance compared to previous state-of-the-art methods while having less number of parameters and FLOPS.",0
"This paper presents a novel neural network architecture for face alignment, head pose estimation, and face tracking tasks. Our approach utilizes a multi-task learning framework that allows the model to share knowledge across all three tasks, resulting in more efficient computation and improved performance compared to traditional single-task models. We demonstrate the effectiveness of our method on several benchmark datasets and show that it outperforms state-of-the-art methods in terms of accuracy and speed. Additionally, we provide comprehensive ablation studies to validate the contribution of each component in our proposed architecture. Overall, our work represents a significant step towards real-time, high-fidelity facial analysis for a wide range of applications including computer vision, robotics, and augmented reality.",1
"The fragility of modern machine learning models has drawn a considerable amount of attention from both academia and the public. While immense interests were in either crafting adversarial attacks as a way to measure the robustness of neural networks or devising worst-case analytical robustness verification with guarantees, few methods could enjoy both scalability and robustness guarantees at the same time. As an alternative to these attempts, randomized smoothing adopts a different prediction rule that enables statistical robustness arguments which easily scale to large networks. However, in this paper, we point out the side effects of current randomized smoothing workflows. Specifically, we articulate and prove two major points: 1) the decision boundaries of smoothed classifiers will shrink, resulting in disparity in class-wise accuracy; 2) applying noise augmentation in the training process does not necessarily resolve the shrinking issue due to the inconsistent learning objectives.",0
"This papers looks at hidden costs associated with using randomization smoothing during randomized controlled trials (RCTs). Researchers often add noise to the outcome variable in RCTs to reduce biases due to treatment effect heterogeneity across subgroups. However, research suggests that adding noise can alter trial outcomes by making some treatments appear more effective than they actually are. We demonstrate that such spurious results occur under plausible conditions, have serious negative consequences including potentially harmful medical decision-making based on incorrect inferences. Results are robust to alternative methods used to introduce noise as well as alternative model specifications and data sets. Our findings support calls to carefully weigh potential benefits against real risks before employing randomization smoothing strategies, particularly given the potential ethical implications surrounding any wrong conclusions derived from such practices.",1
"In implementations of the functional data methods, the effect of the initial choice of an orthonormal basis has not gained much attention in the past. Typically, several standard bases such as Fourier, wavelets, splines, etc. are considered to transform observed functional data and a choice is made without any formal criteria indicating which of the bases is preferable for the initial transformation of the data into functions. In an attempt to address this issue, we propose a strictly data-driven method of orthogonal basis selection. The method uses recently introduced orthogonal spline bases called the splinets obtained by efficient orthogonalization of the B-splines. The algorithm learns from the data in the machine learning style to efficiently place knots. The optimality criterion is based on the average (per functional data point) mean square error and is utilized both in the learning algorithms and in comparison studies. The latter indicates efficiency that is particularly evident for the sparse functional data and to a lesser degree in analyses of responses to complex physical systems.",0
"Abstract: This paper presents a new approach for selecting orthonormal basis functions for functional data analysis using machine learning techniques. The selection process is typically carried out manually by experts based on their domain knowledge and experience. However, manual methods can be time-consuming, subjective, and prone to errors. In this study, we propose a novel method that combines classical orthogonal bases with machine learning algorithms to select the most suitable basis functions automatically. Our method uses Gaussian processes as a kernel function to learn the mapping from raw data points to coefficients for each candidate basis function. We then use these coefficients to rank the importance of different functions and select the optimal set of basis functions for the given dataset. Experiments on simulated datasets show that our proposed approach achieves better results than existing methods, including improved accuracy, lower error rates, and faster computation times. Additionally, our approach has been applied successfully to real-world datasets from diverse fields such as neuroscience and finance, demonstrating its effectiveness and potential impact across multiple disciplines. Overall, our work represents a significant step towards automating the process of selecting appropriate basis functions for functional data analysis while ensuring high quality and reproducibility of scientific research.",1
"Process monitoring based on neural networks is getting more and more attention. Compared with classical neural networks, high-order neural networks have natural advantages in dealing with heteroscedastic data. However, high-order neural networks might bring the risk of overfitting and learning both the key information from original data and noises or anomalies. Orthogonal constraints can greatly reduce correlations between extracted features, thereby reducing the overfitting risk. This paper proposes a novel fault detection method called second-order component analysis (SCA). SCA rules out the heteroscedasticity of pro-cess data by optimizing a second-order autoencoder with orthogonal constraints. In order to deal with this constrained optimization problem, a geometric conjugate gradient algorithm is adopted in this paper, which performs geometric optimization on the combination of Stiefel manifold and Euclidean manifold. Extensive experiments on the Tennessee-Eastman benchmark pro-cess show that SCA outperforms PCA, KPCA, and autoencoder in missed detection rate (MDR) and false alarm rate (FAR).",0
"In this paper we propose a methodology which takes advantage of second order component analysis (SOCA) techniques, coupled with wavelet filtering methods, so as to detect faults in various real-world data sets. Through our experiments on synthetic data sets we demonstrate that our approach is capable of achieving high levels of detection accuracy while minimizing false alarms. Furthermore, through evaluation on industrial use cases from fields such as electrical grid monitoring and anomaly detection in internet traffic, our proposed method achieved superior results relative to competing approaches. Finally, we discuss several areas where future research may lead to further improvements in SOCA-based methods for fault detection.",1
"Federated learning (FL) is an emerging, privacy-preserving machine learning paradigm, drawing tremendous attention in both academia and industry. A unique characteristic of FL is heterogeneity, which resides in the various hardware specifications and dynamic states across the participating devices. Theoretically, heterogeneity can exert a huge influence on the FL training process, e.g., causing a device unavailable for training or unable to upload its model updates. Unfortunately, these impacts have never been systematically studied and quantified in existing FL literature.   In this paper, we carry out the first empirical study to characterize the impacts of heterogeneity in FL. We collect large-scale data from 136k smartphones that can faithfully reflect heterogeneity in real-world settings. We also build a heterogeneity-aware FL platform that complies with the standard FL protocol but with heterogeneity in consideration. Based on the data and the platform, we conduct extensive experiments to compare the performance of state-of-the-art FL algorithms under heterogeneity-aware and heterogeneity-unaware settings. Results show that heterogeneity causes non-trivial performance degradation in FL, including up to 9.2% accuracy drop, 2.32x lengthened training time, and undermined fairness. Furthermore, we analyze potential impact factors and find that device failure and participant bias are two potential factors for performance degradation. Our study provides insightful implications for FL practitioners. On the one hand, our findings suggest that FL algorithm designers consider necessary heterogeneity during the evaluation. On the other hand, our findings urge system providers to design specific mechanisms to mitigate the impacts of heterogeneity.",0
"This research investigates the impact of heterogeneity on federated learning (FL) within large-scale smartphone data. With growing concerns over user privacy and security, FL has emerged as a promising technique for training machine models using distributed data without centralization. However, the increasing diversification of mobile devices poses significant challenges to FL's effectiveness. We explore how variations in hardware configurations, operating systems, network connections, and other factors affect FL performance across different devices. Our results indicate that certain devices may contribute significantly more than others due to differences in computing power or data quality. Furthermore, we identify specific device features which have particularly strong correlations with model accuracy. By characterizing these impacts, our work contributes towards improving the understanding and design of future FL applications on heterogeneous smartphone datasets.",1
"We propose matrix norm inequalities that extend the Recht-R\'e (2012) conjecture on a noncommutative AM-GM inequality by supplementing it with another inequality that accounts for single-shuffle, which is a widely used without-replacement sampling scheme that shuffles only once in the beginning and is overlooked in the Recht-R\'e conjecture. Instead of general positive semidefinite matrices, we restrict our attention to positive definite matrices with small enough condition numbers, which are more relevant to matrices that arise in the analysis of SGD. For such matrices, we conjecture that the means of matrix products corresponding to with- and without-replacement variants of SGD satisfy a series of spectral norm inequalities that can be summarized as: ""single-shuffle SGD converges faster than random-reshuffle SGD, which is in turn faster than with-replacement SGD."" We present theorems that support our conjecture by proving several special cases.",0
"This paper explores the effectiveness of single-shuffle stochastic gradient descent (SGD) compared to traditional shuffled SGD and batch gradient descent (GD). Using empirical studies on several real datasets and machine learning models, we demonstrate that single-shuffle SGD can achieve better performance while requiring less computation time and memory usage. Our results indicate that single-shuffle SGD offers a practical alternative to reshuffling SGD and GD, especially in situations where training data cannot be easily partitioned into distinct subsets, such as in online learning scenarios. Further analysis suggests that the superiority of single-shuffle SGD arises from its ability to adaptively adjust learning rates based on each sample encountered during training, rather than using uniform step sizes like reshuffled SGD and GD. Overall, our findings have important implications for developing efficient optimization algorithms that balance accuracy and computational efficiency.",1
"Polyp segmentation is of great importance in the early diagnosis and treatment of colorectal cancer. Since polyps vary in their shape, size, color, and texture, accurate polyp segmentation is very challenging. One promising way to mitigate the diversity of polyps is to model the contextual relation for each pixel such as using attention mechanism. However, previous methods only focus on learning the dependencies between the position within an individual image and ignore the contextual relation across different images. In this paper, we propose Duplex Contextual Relation Network (DCRNet) to capture both within-image and cross-image contextual relations. Specifically, we first design Interior Contextual-Relation Module to estimate the similarity between each position and all the positions within the same image. Then Exterior Contextual-Relation Module is incorporated to estimate the similarity between each position and the positions across different images. Based on the above two types of similarity, the feature at one position can be further enhanced by the contextual region embedding within and across images. To store the characteristic region embedding from all the images, a memory bank is designed and operates as a queue. Therefore, the proposed method can relate similar features even though they come from different images. We evaluate the proposed method on the EndoScene, Kvasir-SEG and the recently released large-scale PICCOLO dataset. Experimental results show that the proposed DCRNet outperforms the state-of-the-art methods in terms of the widely-used evaluation metrics.",0
"In recent years, there has been increasing interest in developing accurate algorithms for medical image segmentation, particularly for identifying polyps within endoscopic images. One method that shows great promise for achieving high accuracy is the use of deep learning techniques such as convolutional neural networks (CNNs). However, many existing approaches rely on static architectures and lack contextual reasoning capabilities, which can lead to suboptimal results. To address these limitations, we propose a novel framework called Duplex Contextual Relation Network (DCRN) that incorporates both local and global context into the segmentation process. Our network architecture utilizes two parallel pathways: one that captures local relationships between neighboring pixels and another that encodes global context. By combining these dual paths, our DCRN model outperforms state-of-the-art methods in terms of segmentation accuracy while requiring fewer computational resources. Overall, our study demonstrates the effectiveness of using duplex representation learning for improving the accuracy of polyp segmentation in endoscopy images.",1
"Haze removal is an extremely challenging task, and object detection in the hazy environment has recently gained much attention due to the popularity of autonomous driving and traffic surveillance. In this work, the authors propose a multiple linear regression haze removal model based on a widely adopted dehazing algorithm named Dark Channel Prior. Training this model with a synthetic hazy dataset, the proposed model can reduce the unanticipated deviations generated from the rough estimations of transmission map and atmospheric light in Dark Channel Prior. To increase object detection accuracy in the hazy environment, the authors further present an algorithm to build a synthetic hazy COCO training dataset by generating the artificial haze to the MS COCO training dataset. The experimental results demonstrate that the proposed model obtains higher image quality and shares more similarity with ground truth images than most conventional pixel-based dehazing algorithms and neural network based haze-removal models. The authors also evaluate the mean average precision of Mask R-CNN when training the network with synthetic hazy COCO training dataset and preprocessing test hazy dataset by removing the haze with the proposed dehazing model. It turns out that both approaches can increase the object detection accuracy significantly and outperform most existing object detection models over hazy images.",0
"Recent advances in image processing have led to the development of new techniques for dehazing images, which improve their quality by removing atmospheric effects that cause haze. One such technique involves using dark channel prior (DCP), which exploits the fact that hazy scenes often contain channels that are close to zero values. This approach has been shown to be effective but can suffer from limitations due to variations in light conditions, camera settings, and sensor responses. To address these issues, we propose an advanced multiple linear regression based DCP method that takes into account additional features such as color difference, local contrast, and edge orientation. Our method outperforms previous state-of-the-art methods in terms of visual fidelity and objective metrics, producing clearer and more visually pleasing results. We also demonstrate the effectiveness of our method on synthetically generated hazed images, showing that it can generalize well across different environments and illumination conditions. Overall, our work provides a significant contribution to the field of computer vision and opens up new possibilities for applications such as autonomous driving, surveillance, and augmented reality.",1
"Much of the recent efforts on salient object detection (SOD) have been devoted to producing accurate saliency maps without being aware of their instance labels. To this end, we propose a new pipeline for end-to-end salient instance segmentation (SIS) that predicts a class-agnostic mask for each detected salient instance. To better use the rich feature hierarchies in deep networks and enhance the side predictions, we propose the regularized dense connections, which attentively promote informative features and suppress non-informative ones from all feature pyramids. A novel multi-level RoIAlign based decoder is introduced to adaptively aggregate multi-level features for better mask predictions. Such strategies can be well-encapsulated into the Mask R-CNN pipeline. Extensive experiments on popular benchmarks demonstrate that our design significantly outperforms existing \sArt competitors by 6.3\% (58.6\% vs. 52.3\%) in terms of the AP metric.The code is available at https://github.com/yuhuan-wu/RDPNet.",0
"This paper presents a new deep learning architecture called ""Regularized Densely-Connected Pyramid Network"" (RDCPN) that has been developed specifically for salient instance segmentation tasks. The proposed method utilizes dense connections within each pyramidal layer while regularizing feature maps through channel attention modules. Furthermore, our network employs spatial pyramids in parallel at different scales by gradually upsampling features from low-resolution layers, ensuring high localization accuracy and efficiency. Extensive experiments have been conducted on several benchmark datasets, and results demonstrate significant improvements over current state-of-the-art methods. Our ablation studies further verify the effectiveness of individual components in RDCPN. Overall, this work highlights the importance of designing task-specific architectures and introduces a novel approach towards efficient and accurate salient object detection.",1
"Rain streaks and rain drops are two natural phenomena, which degrade image capture in different ways. Currently, most existing deep deraining networks take them as two distinct problems and individually address one, and thus cannot deal adequately with both simultaneously. To address this, we propose a Dual Attention-in-Attention Model (DAiAM) which includes two DAMs for removing both rain streaks and raindrops. Inside the DAM, there are two attentive maps - each of which attends to the heavy and light rainy regions, respectively, to guide the deraining process differently for applicable regions. In addition, to further refine the result, a Differential-driven Dual Attention-in-Attention Model (D-DAiAM) is proposed with a ""heavy-to-light"" scheme to remove rain via addressing the unsatisfying deraining regions. Extensive experiments on one public raindrop dataset, one public rain streak and our synthesized joint rain streak and raindrop (JRSRD) dataset have demonstrated that the proposed method not only is capable of removing rain streaks and raindrops simultaneously, but also achieves the state-of-the-art performance on both tasks.",0
"An accurate joint rain streaks and raindrop removal (RDR) method plays a crucial role in image quality enhancement in video surveillance systems, especially under complex weather conditions such as foggy days, heavy rain, or snowfall. Conventional RDR methods have been widely investigated separately from rain streak removal; however, they often fail to handle cases where both types of distortions coexist in images. To address this issue, we propose a novel dual attention-based approach that effectively removes both rain streaks and raindrops simultaneously by sharing attentive features within one network architecture. Our model generates clean images using two branches: a global branch and a local branch. The global branch uses channel attention mechanisms and adaptive skip connections to capture the spatial structure of rain streaks and raindrops at different scales, whereas the local branch utilizes pixelwise attention and densely connected layers to focus on individual regions with varying distortion levels. Both branches share feature maps at multiple resolutions through multi-scale fusion operations, which significantly improves our ability to detect both large and small rain droplets. Extensive evaluations on three public datasets demonstrate significant performance improvements over state-of-the-art techniques for joint rain streak and RDR tasks across all metrics. Overall, our proposed framework provides an effective solution for enhancing the visual perception of low-quality videos captured during adverse weather conditions.",1
"Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training examine teaching: providing learned information during the training process to improve downstream model performance. In this paper, we take steps towards extending the scope of teaching. We propose a flexible teaching framework using commentaries, learned meta-information helpful for training on a particular task. We present gradient-based methods to learn commentaries, leveraging recent work on implicit differentiation for scalability. We explore diverse applications of commentaries, from weighting training examples, to parameterising label-dependent data augmentation policies, to representing attention masks that highlight salient image regions. We find that commentaries can improve training speed and/or performance, and provide insights about the dataset and training process. We also observe that commentaries generalise: they can be reused when training new models to obtain performance benefits, suggesting a use-case where commentaries are stored with a dataset and leveraged in future for improved model training.",0
"""Teaching With Commentaries"" focuses on the use of commentaries as a pedagogical tool in higher education classrooms. By incorporating commentaries into their instructional practice, instructors can help students gain deeper insight into course material while fostering critical thinking skills. This approach can be used across disciplines, from literature and history courses to science labs and philosophy seminars. In order to effectively utilize commentaries in teaching, instructors must first understand the purpose behind using them and determine which specific text would best suit their lesson plan. Additionally, they should consider how to frame the discussion questions that accompany the assigned reading to ensure productive engagement among students. The implementation of commentary assignments requires careful consideration; however, the benefits of enhanced student comprehension and active learning make it well worth the effort. Overall, teaching through commentaries offers instructors an opportunity to challenge their students intellectually while promoting a lifelong love of learning.",1
"The rapid advancement of deep learning models that can generate and synthesis hyper-realistic videos known as Deepfakes and their ease of access to the general public have raised concern from all concerned bodies to their possible malicious intent use. Deep learning techniques can now generate faces, swap faces between two subjects in a video, alter facial expressions, change gender, and alter facial features, to list a few. These powerful video manipulation methods have potential use in many fields. However, they also pose a looming threat to everyone if used for harmful purposes such as identity theft, phishing, and scam. In this work, we propose a Convolutional Vision Transformer for the detection of Deepfakes. The Convolutional Vision Transformer has two components: Convolutional Neural Network (CNN) and Vision Transformer (ViT). The CNN extracts learnable features while the ViT takes in the learned features as input and categorizes them using an attention mechanism. We trained our model on the DeepFake Detection Challenge Dataset (DFDC) and have achieved 91.5 percent accuracy, an AUC value of 0.91, and a loss value of 0.32. Our contribution is that we have added a CNN module to the ViT architecture and have achieved a competitive result on the DFDC dataset.",0
"Artificial intelligence techniques have become increasingly powerful tools for creating realistic video content known as deepfakes, which can manipulate public perceptions and disseminate disinformation on a large scale. This study proposes a new method for detecting these videos using convolutional vision transformers (CVT), a recently developed architecture that has shown state-of-the-art performance in image classification tasks. To evaluate our approach, we collected a dataset of deepfake videos and compared their characteristics to those of authentic ones. Our results show that CVT outperforms traditional CNN models in accurately classifying deepfake videos, achieving over 90% accuracy even under challenging conditions such as low resolution and poor lighting quality. In addition, our model was able to identify specific manipulations made by different generators used to create deepfake videos. These findings contribute valuable insights into understanding and mitigating the impact of deepfake technology on society. Overall, our work demonstrates the potential benefits of leveraging advanced AI models to improve online security and foster trustworthy digital media environments.",1
"Privacy considerations and bias in datasets are quickly becoming high-priority issues that the computer vision community needs to face. So far, little attention has been given to practical solutions that do not involve collection of new datasets. In this work, we show that for object detection on COCO, both anonymizing the dataset by blurring faces, as well as swapping faces in a balanced manner along the gender and skin tone dimension, can retain object detection performances while preserving privacy and partially balancing bias.",0
"In recent years, there has been growing concern over privacy violations by large tech companies that collect user data through their services. One area where these concerns have arisen is object detection, which involves identifying objects within images or videos using machine learning algorithms. Many existing methods rely on centralized processing, meaning they require sending sensitive image and video data to remote servers for analysis. This approach raises questions about who has access to the data and how securely it is stored.  This paper presents a novel solution to address these privacy issues: a decentralized method of object detection using blockchain technology. Our algorithm uses federated learning, allowing edge devices such as smartphones or cameras to train models locally without sharing raw data. We demonstrate that our system achieves comparable accuracy to traditional cloud-based approaches while ensuring users retain control over their data. Furthermore, we introduce privacy enhancements through homomorphic encryption techniques that allow us to perform inferences directly on encrypted data.  Overall, our work represents a significant step forward in preserving user privacy during object detection tasks. By leveraging cutting-edge technologies like blockchain and homomorphic encryption, we provide a robust, efficient, and confidential alternative to current practices in image recognition. With increasing scrutiny around data usage by major corporations, our method provides a path toward more responsible handling of personal data.",1
"Machine-learning-based age estimation has received lots of attention. Traditional age estimation mechanism focuses estimation age error, but ignores that there is a deviation between the estimated age and real age due to disease. Pathological age estimation mechanism the author proposed before introduces age deviation to solve the above problem and improves classification capability of the estimated age significantly. However,it does not consider the age estimation error of the normal control (NC) group and results in a larger error between the estimated age and real age of NC group. Therefore, an integrated age estimation mechanism based on Decision-Level fusion of error and deviation orientation model is proposed to solve the problem.Firstly, the traditional age estimation and pathological age estimation mechanisms are weighted together.Secondly, their optimal weights are obtained by minimizing mean absolute error (MAE) between the estimated age and real age of normal people. In the experimental section, several representative age-related datasets are used for verification of the proposed method. The results show that the proposed age estimation mechanism achieves a good tradeoff effect of age estimation. It not only improves the classification ability of the estimated age, but also reduces the age estimation error of the NC group. In general, the proposed age estimation mechanism is effective. Additionally, the mechanism is a framework mechanism that can be used to construct different specific age estimation algorithms, contributing to relevant research.",0
"Abstract: This study presents an innovative method for estimating age using a holistic approach that considers multiple factors such as facial features, voice characteristics, and biometric data. By leveraging advances in computer vision and machine learning techniques, we have developed an integrated mechanism capable of accurately predicting the age range of individuals across different demographics. Our algorithm outperforms existing methods by utilizing advanced feature extraction techniques, and extensive dataset validation ensures generalizability across varying environments. This research has important implications for applications ranging from security systems to healthcare diagnostics and can ultimately lead to more personalized experiences in numerous industries. Overall, our proposed solution represents a significant improvement in age estimation technology, setting the stage for future developments in human analysis technologies.",1
"We present Affect2MM, a learning method for time-series emotion prediction for multimedia content. Our goal is to automatically capture the varying emotions depicted by characters in real-life human-centric situations and behaviors. We use the ideas from emotion causation theories to computationally model and determine the emotional state evoked in clips of movies. Affect2MM explicitly models the temporal causality using attention-based methods and Granger causality. We use a variety of components like facial features of actors involved, scene understanding, visual aesthetics, action/situation description, and movie script to obtain an affective-rich representation to understand and perceive the scene. We use an LSTM-based learning model for emotion perception. To evaluate our method, we analyze and compare our performance on three datasets, SENDv1, MovieGraphs, and the LIRIS-ACCEDE dataset, and observe an average of 10-15% increase in the performance over SOTA methods for all three datasets.",0
"Title: ""A Study on the Relationship Between Affectivity and Cause in Multimedia""  This paper investigates how affective analysis can aid in understanding the complex relationships between emotional states and their causes within multimedia content. By examining the ways in which human emotions arise from external stimuli, we aim to develop new methods of analyzing multimedia that account for these interconnections. Our study employs techniques drawn from psychology and artificial intelligence to explore the intricate links between affectivity and cause within multimodal environments. Through our findings, we hope to contribute to the broader fields of media studies and computer science by shedding light on the mechanisms underlying the effects of media exposure on individualsâ€™ emotional experiences. Ultimately, our research seeks to improve our ability to analyze and interpret multimedia using theories derived from cognitive affective systems.",1
"Shadow detection in a single image has received significant research interest in recent years. However, much fewer works have been explored in shadow detection over dynamic scenes. The bottleneck is the lack of a well-established dataset with high-quality annotations for video shadow detection. In this work, we collect a new video shadow detection dataset, which contains 120 videos with 11, 685 frames, covering 60 object categories, varying lengths, and different motion/lighting conditions. All the frames are annotated with a high-quality pixel-level shadow mask. To the best of our knowledge, this is the first learning-oriented dataset for video shadow detection. Furthermore, we develop a new baseline model, named triple-cooperative video shadow detection network (TVSD-Net). It utilizes triple parallel networks in a cooperative manner to learn discriminative representations at intra-video and inter-video levels. Within the network, a dual gated co-attention module is proposed to constrain features from neighboring frames in the same video, while an auxiliary similarity loss is introduced to mine semantic information between different videos. Finally, we conduct a comprehensive study on ViSha, evaluating 12 state-of-the-art models (including single image shadow detectors, video object segmentation, and saliency detection methods). Experiments demonstrate that our model outperforms SOTA competitors.",0
"Abstract: Video shadow detection plays an important role in many computer vision applications such as image enhancement, surveillance cameras, entertainment industry, and medical diagnosis. Traditional approaches to video shadow detection often involve using single cooperation or double cooperation models, which can lead to limited accuracy and robustness. In order to address these limitations, we propose a triple-cooperative approach that utilizes three distinct modules - Global Convolutional Network (GCN), Local Attention Block (LAB) and Channel-wise Spatial Pyramid Pooling (CSPP). By integrating these three modules together, our method achieves state-of-the-art performance on various benchmark datasets while maintaining efficiency and computational speed. Our results demonstrate that the proposed triple-cooperative model outperforms other traditional methods by improving shadow detection accuracy, enhancing details and reducing noise in the images. This research has significant potential application in various fields where realistic object representation is crucial.",1
"The study of adversarial examples and their activation has attracted significant attention for secure and robust learning with deep neural networks (DNNs). Different from existing works, in this paper, we highlight two new characteristics of adversarial examples from the channel-wise activation perspective: 1) the activation magnitudes of adversarial examples are higher than that of natural examples; and 2) the channels are activated more uniformly by adversarial examples than natural examples. We find that the state-of-the-art defense adversarial training has addressed the first issue of high activation magnitudes via training on adversarial examples, while the second issue of uniform activation remains. This motivates us to suppress redundant activation from being activated by adversarial perturbations via a Channel-wise Activation Suppressing (CAS) strategy. We show that CAS can train a model that inherently suppresses adversarial activation, and can be easily applied to existing defense methods to further improve their robustness. Our work provides a simple but generic training strategy for robustifying the intermediate layer activation of DNNs.",0
"As deep neural networks (DNNs) have become increasingly popular in recent years, researchers have highlighted their vulnerability to adversarial attacks which can significantly impact the accuracy of these models. In order to address this issue, several defense mechanisms have been proposed, including various regularization techniques aimed at promoting generalizability of DNNs and making them more robust against such attacks. One approach that has recently gained attention is channel-wise activation suppressing, wherein certain channels of the model's convolutional layers are randomly turned off during training. This leads to reduced capacity of the network and forces it to learn redundant representations, thus making it more resilient to adversarial perturbations. Here we present experiments evaluating the effectiveness of this method on standard benchmark datasets like MNIST and CIFAR-10 as well as larger scale datasets like ImageNet. Our results demonstrate significant improvements in the robustness of DNNs trained using channel-wise activation suppression compared to unperturbed baseline models across all datasets tested. We conclude by discussing potential applications of our findings and possible future directions in this area of research. --",1
"How do we formalize the challenge of credit assignment in reinforcement learning? Common intuition would draw attention to reward sparsity as a key contributor to difficult credit assignment and traditional heuristics would look to temporal recency for the solution, calling upon the classic eligibility trace. We posit that it is not the sparsity of the reward itself that causes difficulty in credit assignment, but rather the \emph{information sparsity}. We propose to use information theory to define this notion, which we then use to characterize when credit assignment is an obstacle to efficient learning. With this perspective, we outline several information-theoretic mechanisms for measuring credit under a fixed behavior policy, highlighting the potential of information theory as a key tool towards provably-efficient credit assignment.",0
"Title: ""A Novel Framework for Improving Accuracy in Deep Networks"" Author(s): [Your Name] Contact email:[Your Email Address] Abstract: This paper proposes a novel framework for improving accuracy in deep neural networks. Inspired by recent advancements in computer vision applications like object detection and segmentation using Convolutional Neural Network (CNN), we aim to improve the overall performance of other types of deep learning models such as Natural Language Processing (NLP) systems using Recurrent Neural Networks (RNN). Our proposed method tackles the issue of poor generalization that arises due to overfitting by introducing two new techniques - Early Stopping Regularization and Weight Decay regularization - which have been applied successfully in image recognition tasks but have yet to be explored extensively in NLP. By evaluating our methodology against existing baseline methods on popular benchmark datasets like GLUE and STS-B, we show significant improvements in both accuracy and F1 scores. Our results highlight the potential of applying well established computer vision principles to enhance the performance of language processing models.",1
"Standard registration algorithms need to be independently applied to each surface to register, following careful pre-processing and hand-tuning. Recently, learning-based approaches have emerged that reduce the registration of new scans to running inference with a previously-trained model. In this paper, we cast the registration task as a surface-to-surface translation problem, and design a model to reliably capture the latent geometric information directly from raw 3D face scans. We introduce Shape-My-Face (SMF), a powerful encoder-decoder architecture based on an improved point cloud encoder, a novel visual attention mechanism, graph convolutional decoders with skip connections, and a specialized mouth model that we smoothly integrate with the mesh convolutions. Compared to the previous state-of-the-art learning algorithms for non-rigid registration of face scans, SMF only requires the raw data to be rigidly aligned (with scaling) with a pre-defined face template. Additionally, our model provides topologically-sound meshes with minimal supervision, offers faster training time, has orders of magnitude fewer trainable parameters, is more robust to noise, and can generalize to previously unseen datasets. We extensively evaluate the quality of our registrations on diverse data. We demonstrate the robustness and generalizability of our model with in-the-wild face scans across different modalities, sensor types, and resolutions. Finally, we show that, by learning to register scans, SMF produces a hybrid linear and non-linear morphable model. Manipulation of the latent space of SMF allows for shape generation, and morphing applications such as expression transfer in-the-wild. We train SMF on a dataset of human faces comprising 9 large-scale databases on commodity hardware.",0
"In order to accurately register 3D face scans, it is necessary to employ rigorous surface mapping techniques that can translate individual facial features into 3D models. Traditional methods rely on algorithms that convert 2D images or point clouds into a 3D representation through a variety of mathematical operations, but these approaches often lack the precision required for accurate translation across different platforms. This paper proposes a new approach using surface-to-surface mapping to achieve highly detailed and precise registration of 3D face scans. By utilizing advanced sensor technology capable of capturing high-resolution depth maps, we present a novel methodology that effectively aligns corresponding points from multiple scan datasets into a unified model. Our technique involves segmentation and feature detection followed by optimization-based iterative alignment of the surfaces represented by those detected features, resulting in a seamless fusion of all registered scans. We demonstrate our methodâ€™s superiority through comparison with existing state-of-the-art techniques on publicly available datasets.",1
"The remarkable success in face forgery techniques has received considerable attention in computer vision due to security concerns. We observe that up-sampling is a necessary step of most face forgery techniques, and cumulative up-sampling will result in obvious changes in the frequency domain, especially in the phase spectrum. According to the property of natural images, the phase spectrum preserves abundant frequency components that provide extra information and complement the loss of the amplitude spectrum. To this end, we present a novel Spatial-Phase Shallow Learning (SPSL) method, which combines spatial image and phase spectrum to capture the up-sampling artifacts of face forgery to improve the transferability, for face forgery detection. And we also theoretically analyze the validity of utilizing the phase spectrum. Moreover, we notice that local texture information is more crucial than high-level semantic information for the face forgery detection task. So we reduce the receptive fields by shallowing the network to suppress high-level features and focus on the local region. Extensive experiments show that SPSL can achieve the state-of-the-art performance on cross-datasets evaluation as well as multi-class classification and obtain comparable results on single dataset evaluation.",0
"In recent years, deep learning has emerged as a powerful tool for image synthesis and manipulation tasks such as face forgery detection. However, existing methods have limitations in terms of accuracy, efficiency, and scalability. To address these issues, we propose a novel approach called spatial-phase shallow learning (SPSL), which utilizes phase information along with frequency domain processing to achieve better performance in face forgery detection. Our method achieves state-of-the-art results on multiple benchmark datasets while maintaining high computational efficiency. We also provide extensive analysis and ablation studies to validate our contributions and demonstrate the effectiveness of SPSL. Overall, our work rethinks traditional approaches to face forgery detection by leveraging the advantages of both spatial and frequency domains, resulting in improved performance and wider applications in real-world scenarios.",1
"Once an academic venture, autonomous driving has received unparalleled corporate funding in the last decade. Still, the operating conditions of current autonomous cars are mostly restricted to ideal scenarios. This means that driving in challenging illumination conditions such as night, sunrise, and sunset remains an open problem. In these cases, standard cameras are being pushed to their limits in terms of low light and high dynamic range performance. To address these challenges, we propose, DSEC, a new dataset that contains such demanding illumination conditions and provides a rich set of sensory data. DSEC offers data from a wide-baseline stereo setup of two color frame cameras and two high-resolution monochrome event cameras. In addition, we collect lidar data and RTK GPS measurements, both hardware synchronized with all camera data. One of the distinctive features of this dataset is the inclusion of high-resolution event cameras. Event cameras have received increasing attention for their high temporal resolution and high dynamic range performance. However, due to their novelty, event camera datasets in driving scenarios are rare. This work presents the first high-resolution, large-scale stereo dataset with event cameras. The dataset contains 53 sequences collected by driving in a variety of illumination conditions and provides ground truth disparity for the development and evaluation of event-based stereo algorithms.",0
"This paper presents a new dataset called ""DSEC"" which stands for ""Stereo Event Camera Dataset for Driving Scenarios."" The goal of creating this dataset was to provide researchers with high quality imagery data that could be used to develop and test algorithms related to driving scenarios using event cameras. Traditionally, datasets have been limited to either video frames captured by regular camera sensors or point cloud based LiDAR scans. However, these types of sensors have limitations such as lack of speed, high power consumption, low resolution and motion blur artifacts which make them less suitable for use in real time applications like driver assistance systems or autonomous vehicles where quick response times are critical. In contrast, event cameras can capture events at very high speeds (up to thousands of frames per second) while consuming significantly lower power, making them well suited for such real world applications. While there exists some datasets for event camera research, they were mostly created indoors and thus lacked variability found in outdoor environments. The DSEC dataset addresses this gap by providing both in door and outdoor scenarios which can be challenging for traditional vision systems but offer great potential value to event camera technology. The final version of the DSEC dataset includes several hours worth of footage collected from different weather conditions ranging from sunny days to rainy night scenes across varied lighting conditions and dynamic background. The provided raw image data has pixel accurate timestamp annotations which enable further analysis of each frame's content and accuracy. In summary, this new dataset offers novel opportunities to work on event driven computer vision tasks including vehicle detection, tracking and other higher level understanding features essential in advanced driver assistance systems, road scene segmentation and traffic sign recognition among others.",1
"Few-shot learning is an interesting and challenging study, which enables machines to learn from few samples like humans. Existing studies rarely exploit auxiliary information from large amount of unlabeled data. Self-supervised learning is emerged as an efficient method to utilize unlabeled data. Existing self-supervised learning methods always rely on the combination of geometric transformations for the single sample by augmentation, while seriously neglect the endogenous correlation information among different samples that is the same important for the task. In this work, we propose a Graph-driven Clustering (GC), a novel augmentation-free method for self-supervised learning, which does not rely on any auxiliary sample and utilizes the endogenous correlation information among input samples. Besides, we propose Multi-pretext Attention Network (MAN), which exploits a specific attention mechanism to combine the traditional augmentation-relied methods and our GC, adaptively learning their optimized weights to improve the performance and enabling the feature extractor to obtain more universal representations. We evaluate our MAN extensively on miniImageNet and tieredImageNet datasets and the results demonstrate that the proposed method outperforms the state-of-the-art (SOTA) relevant methods.",0
"Recently, few-shot learning has gained great attention due to its potential to address one of the major limitations in deep learning: generalization to unseen data distributions. Most existing methods rely on meta-learning, which requires large amounts of labeled samples during training. However, real-world datasets often come with limited labels, making such methods impractical. Therefore, itâ€™s essential to explore alternative approaches that can learn from self-labeled data. In our work, we propose Multi-Pretext Attention Network (MPAN), a novel framework based on multi-pretextual attention modules coupled with cross-attention layers, which can leverage self-labeled data effectively. Extensive experiments on challenging benchmarks demonstrate MPAN outperforms state-of-the-art methods under several settings. Furthermore, we perform thorough analysis to validate the effectiveness of each component, offering insightful guidance for future research directions.",1
"Local and non-local attention-based methods have been well studied in various image restoration tasks while leading to promising performance. However, most of the existing methods solely focus on one type of attention mechanism (local or non-local). Furthermore, by exploiting the self-similarity of natural images, existing pixel-wise non-local attention operations tend to give rise to deviations in the process of characterizing long-range dependence due to image degeneration. To overcome these problems, in this paper we propose a novel collaborative attention network (COLA-Net) for image restoration, as the first attempt to combine local and non-local attention mechanisms to restore image content in the areas with complex textures and with highly repetitive details respectively. In addition, an effective and robust patch-wise non-local attention model is developed to capture long-range feature correspondences through 3D patches. Extensive experiments on synthetic image denoising, real image denoising and compression artifact reduction tasks demonstrate that our proposed COLA-Net is able to achieve state-of-the-art performance in both peak signal-to-noise ratio and visual perception, while maintaining an attractive computational complexity. The source code is available on https://github.com/MC-E/COLA-Net.",0
"Image restoration is a challenging task that involves removing noise from corrupted images. Recently, deep neural networks have shown great success in image restoration, but they often struggle with large noise or missing details in the image. In our paper we introduce COLA-Net (Collaborative Attention Network), which integrates collaborative attention into U-Net architecture to enhance feature interactions. Our network consists of two branches with complementary skip connections, where one branch learns residual features while the other focuses on global context. We apply multiple levels of collaboration within each branch by using spatial and channel attention mechanisms that capture dependencies across neighboring pixels and channels, respectively. By utilizing these two branches together, our model outperforms current state-of-the-art methods in several benchmark datasets, achieving high performance even with severe degradation conditions such as low signal-to-noise ratios. We conduct experiments on various real-world imaging applications like denoising, superresolution, demosaicking and single image rain removal. Overall, our work proposes a new efficient solution towards advanced image restoration and enhancement tasks.",1
"Two factors have proven to be very important to the performance of semantic segmentation models: global context and multi-level semantics. However, generating features that capture both factors always leads to high computational complexity, which is problematic in real-time scenarios. In this paper, we propose a new model, called Attention-Augmented Network (AttaNet), to capture both global context and multilevel semantics while keeping the efficiency high. AttaNet consists of two primary modules: Strip Attention Module (SAM) and Attention Fusion Module (AFM). Viewing that in challenging images with low segmentation accuracy, there are a significantly larger amount of vertical strip areas than horizontal ones, SAM utilizes a striping operation to reduce the complexity of encoding global context in the vertical direction drastically while keeping most of contextual information, compared to the non-local approaches. Moreover, AFM follows a cross-level aggregation strategy to limit the computation, and adopts an attention strategy to weight the importance of different levels of features at each pixel when fusing them, obtaining an efficient multi-level representation. We have conducted extensive experiments on two semantic segmentation benchmarks, and our network achieves different levels of speed/accuracy trade-offs on Cityscapes, e.g., 71 FPS/79.9% mIoU, 130 FPS/78.5% mIoU, and 180 FPS/70.1% mIoU, and leading performance on ADE20K as well.",0
"This paper presents AttaNet, a novel network architecture designed to effectively and efficiently perform scene parsing tasks by incorporating attention mechanisms into the traditional convolutional neural network framework. Our approach builds upon recent advances in object detection that utilize region proposals but extends them to dense prediction tasks which operate directly on full images without requiring explicit regions. By applying self-attention within each layer instead of after every layer as commonly seen in natural language processing applications, we achieve superior performance while maintaining real-time inference speed on GPUs. We demonstrate significant improvements over strong baselines on three challenging benchmark datasets for scene parsing and semantic segmentation. Our ablation studies show the importance of different design choices such as interlaced architecture, multi-level feature aggregation, dynamic channel size adaptation and the use of auxiliary supervision. Code and pretrained models will be made publicly available to facilitate further research in this area.",1
"As an instance-level recognition problem, re-identification (re-ID) requires models to capture diverse features. However, with continuous training, re-ID models pay more and more attention to the salient areas. As a result, the model may only focus on few small regions with salient representations and ignore other important information. This phenomenon leads to inferior performance, especially when models are evaluated on small inter-identity variation data. In this paper, we propose a novel network, Erasing-Salient Net (ES-Net), to learn comprehensive features by erasing the salient areas in an image. ES-Net proposes a novel method to locate the salient areas by the confidence of objects and erases them efficiently in a training batch. Meanwhile, to mitigate the over-erasing problem, this paper uses a trainable pooling layer P-pooling that generalizes global max and global average pooling. Experiments are conducted on two specific re-identification tasks (i.e., Person re-ID, Vehicle re-ID). Our ES-Net outperforms state-of-the-art methods on three Person re-ID benchmarks and two Vehicle re-ID benchmarks. Specifically, mAP / Rank-1 rate: 88.6% / 95.7% on Market1501, 78.8% / 89.2% on DuckMTMC-reID, 57.3% / 80.9% on MSMT17, 81.9% / 97.0% on Veri-776, respectively. Rank-1 / Rank-5 rate: 83.6% / 96.9% on VehicleID (Small), 79.9% / 93.5% on VehicleID (Medium), 76.9% / 90.7% on VehicleID (Large), respectively. Moreover, the visualized salient areas show human-interpretable visual explanations for the ranking results.",0
"In re-identification attacks on deep neural networks that are commonly used as building blocks for more complex systems, saliency maps have become increasingly popular tools to analyse the importance of input features with respect to the classification result of interest. These measures reveal which parts of the image contribute most significantly towards the networkâ€™s decision making. However, since most adversaries actively change their inputs according to these insights in order to make re-identifications harder to achieve, recent research has proposed methods for erasing or manipulating areas based on such knowledge. So far, little attention has been paid to how those strategies impact learning performance from both the attackerâ€™s and defenderâ€™s perspectives. Our study investigates whether successful deanonymization remains possible even if crucial details are deleted or modified, and how models adapt given only partially altered images. We present our findings together with experiments evaluating the trade-off between privacy and utility for re-ID under varying data distribution scenarios. By demonstrating that state-of-the-art accuracy levels can still be reached while preserving some degree of confidentiality, we aim at pushing future research on human-AI collaboration towards achieving better protection against inference attacks. If you need further assistance please donâ€™t hesitate to ask!",1
"Although automated pathology classification using deep learning (DL) has proved to be predictively efficient, DL methods are found to be data and compute cost intensive. In this work, we aim to reduce DL training costs by pre-training a Resnet feature extractor using SimCLR contrastive loss for latent encoding of OCT images. We propose a novel active learning framework that identifies a minimal sub-sampled dataset containing the most uncertain OCT image samples using label propagation on the SimCLR latent encodings. The pre-trained Resnet model is then fine-tuned with the labelled minimal sub-sampled data and the underlying pathological sites are visually explained. Our framework identifies upto 2% of OCT images to be most uncertain that need prioritized specialist attention and that can fine-tune a Resnet model to achieve upto 97% classification accuracy. The proposed method can be extended to other medical images to minimize prediction costs.",0
"This paper presents a novel method for semi-supervised image subsampling that improves explainability in the field of pathology. Our proposed approach, which we call SISE-PC (Semi-Supervised Image Subsampling for Explainable Pathology), utilizes a combination of computer vision techniques and machine learning algorithms to select a subset of images that are representative of the entire dataset while ensuring high diagnostic accuracy. We demonstrate the effectiveness of our approach by applying it to two publicly available datasets in breast cancer diagnosis and colon histopathological analysis, respectively. Experimental results show that SISE-PC outperforms other state-of-the-art methods in terms of both explanatory power and classification performance. Our findings contribute to the growing body of research on using machine learning for medical image interpretation, highlighting the potential benefits of incorporating explainability into these systems. Overall, SISE-PC offers a valuable tool for healthcare professionals seeking more interpretable models in their clinical decision-making process.",1
"3D object segmentation is a fundamental and challenging problem in computer vision with applications in autonomous driving, robotics, augmented reality and medical image analysis. It has received significant attention from the computer vision, graphics and machine learning communities. Traditionally, 3D segmentation was performed with hand-crafted features and engineered methods which failed to achieve acceptable accuracy and could not generalize to large-scale data. Driven by their great success in 2D computer vision, deep learning techniques have recently become the tool of choice for 3D segmentation tasks as well. This has led to an influx of a large number of methods in the literature that have been evaluated on different benchmark datasets. This paper provides a comprehensive survey of recent progress in deep learning based 3D segmentation covering over 150 papers. It summarizes the most commonly used pipelines, discusses their highlights and shortcomings, and analyzes the competitive results of these segmentation methods. Based on the analysis, it also provides promising research directions for the future.",0
"This survey provides a comprehensive overview of deep learning based 3D segmentation methods that have been developed recently by researchers worldwide. With advancements in medical imaging technologies, there has been an increasing demand for automating image analysis tasks such as segmentation, which involves partitioning medical images into distinct regions of interest. In particular, 3D medical imaging data sets present unique challenges due to their complexity and large scale. Deep learning models have demonstrated remarkable performance in many computer vision applications, including image segmentation. Therefore, this survey aims to give readers an understanding of state-of-the-art deep learning techniques employed for 3D medical image segmentation. We discuss recent developments in network architectures, segmentation losses, post-processing steps, evaluation metrics, datasets, and real-world application scenarios where these approaches can provide valuable insights. Our review underscores both achievements and shortcomings of current methodologies, thus guiding future research efforts in developing even more effective deep learning pipelines for 3D segmentation problems.",1
"Convolutional Networks (ConvNets) excel at semantic segmentation and have become a vital component for perception in autonomous driving. Enabling an all-encompassing view of street-scenes, omnidirectional cameras present themselves as a perfect fit in such systems. Most segmentation models for parsing urban environments operate on common, narrow Field of View (FoV) images. Transferring these models from the domain they were designed for to 360-degree perception, their performance drops dramatically, e.g., by an absolute 30.0% (mIoU) on established test-beds. To bridge the gap in terms of FoV and structural distribution between the imaging domains, we introduce Efficient Concurrent Attention Networks (ECANets), directly capturing the inherent long-range dependencies in omnidirectional imagery. In addition to the learned attention-based contextual priors that can stretch across 360-degree images, we upgrade model training by leveraging multi-source and omni-supervised learning, taking advantage of both: Densely labeled and unlabeled data originating from multiple datasets. To foster progress in panoramic image segmentation, we put forward and extensively evaluate models on Wild PAnoramic Semantic Segmentation (WildPASS), a dataset designed to capture diverse scenes from all around the globe. Our novel model, training regimen and multi-source prediction fusion elevate the performance (mIoU) to new state-of-the-art results on the public PASS (60.2%) and the fresh WildPASS (69.0%) benchmarks.",0
"In this work we focus on capturing omni-range context for omnidirectional segmentation tasks, including semantic segmentation as well as object detection/localization. This requires careful consideration of how to capture context across panoramic images that are typically captured using wide angle lenses, where objects may appear at significantly different scales depending on their distance from the camera. We investigate four types of neural networks for capturing omni-range context: MultiNet, PANDA, FuseNet, and BiFusion. Our results show that these approaches outperform previous state-of-the art methods by up to 7%, while also offering significant speed improvements due to parallel processing. These findings have important implications for real world applications such as autonomous driving and robotics, where accurate object detection and localization in complex environments can make all the difference.",1
"Due to the rapid advancements of sensory and computing technology, multi-modal data sources that represent the same pattern or phenomenon have attracted growing attention. As a result, finding means to explore useful information from these multi-modal data sources has quickly become a necessity. In this paper, a discriminative vectorial framework is proposed for multi-modal feature representation in knowledge discovery by employing multi-modal hashing (MH) and discriminative correlation maximization (DCM) analysis. Specifically, the proposed framework is capable of minimizing the semantic similarity among different modalities by MH and exacting intrinsic discriminative representations across multiple data sources by DCM analysis jointly, enabling a novel vectorial framework of multi-modal feature representation. Moreover, the proposed feature representation strategy is analyzed and further optimized based on canonical and non-canonical cases, respectively. Consequently, the generated feature representation leads to effective utilization of the input data sources of high quality, producing improved, sometimes quite impressive, results in various applications. The effectiveness and generality of the proposed framework are demonstrated by utilizing classical features and deep neural network (DNN) based features with applications to image and multimedia analysis and recognition tasks, including data visualization, face recognition, object recognition; cross-modal (text-image) recognition and audio emotion recognition. Experimental results show that the proposed solutions are superior to state-of-the-art statistical machine learning (SML) and DNN algorithms.",0
"Here is an example abstract: An important challenge in computer vision research is finding effective methods for representing image data using feature vectors that capture meaningful distinctions across different modalities. Recent advances have proposed discriminant analysis as a powerful tool for generating these representations. This study presents a new framework for multi-modal representation that builds upon prior work by combining several established techniques under a common mathematical framework. Our method demonstrates improved performance on benchmark datasets compared to other popular approaches, suggesting that our framework has significant potential for further investigation.",1
"We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.",0
This is an interesting looking title for a research paper. Is there any more context that you can provide?,1
"Human gesture recognition has drawn much attention in the area of computer vision. However, the performance of gesture recognition is always influenced by some gesture-irrelevant factors like the background and the clothes of performers. Therefore, focusing on the regions of hand/arm is important to the gesture recognition. Meanwhile, a more adaptive architecture-searched network structure can also perform better than the block-fixed ones like Resnet since it increases the diversity of features in different stages of the network better. In this paper, we propose a regional attention with architecture-rebuilt 3D network (RAAR3DNet) for gesture recognition. We replace the fixed Inception modules with the automatically rebuilt structure through the network via Neural Architecture Search (NAS), owing to the different shape and representation ability of features in the early, middle, and late stage of the network. It enables the network to capture different levels of feature representations at different layers more adaptively. Meanwhile, we also design a stackable regional attention module called dynamic-static Attention (DSA), which derives a Gaussian guidance heatmap and dynamic motion map to highlight the hand/arm regions and the motion information in the spatial and temporal domains, respectively. Extensive experiments on two recent large-scale RGB-D gesture datasets validate the effectiveness of the proposed method and show it outperforms state-of-the-art methods. The codes of our method are available at: https://github.com/zhoubenjia/RAAR3DNet.",0
"This research paper proposes a new approach to gesture recognition using depth maps and color images captured by a single camera. Our method uses region attention and architecture rebuilt 3D networks (AR3Net) to combine depth map features with geometry extracted from the color image to improve accuracy. We demonstrate that our proposed method outperforms existing state-of-the-art approaches on benchmark datasets and achieves high precision and recall rates under diverse lighting conditions. Additionally, we provide ablation studies to validate the effectiveness of each component of our system. Overall, this work represents an important step towards developing robust and accurate gestural interaction systems for real world applications.",1
"Removing outlier correspondences is one of the critical steps for successful feature-based point cloud registration. Despite the increasing popularity of introducing deep learning methods in this field, spatial consistency, which is essentially established by a Euclidean transformation between point clouds, has received almost no individual attention in existing learning frameworks. In this paper, we present PointDSC, a novel deep neural network that explicitly incorporates spatial consistency for pruning outlier correspondences. First, we propose a nonlocal feature aggregation module, weighted by both feature and spatial coherence, for feature embedding of the input correspondences. Second, we formulate a differentiable spectral matching module, supervised by pairwise spatial compatibility, to estimate the inlier confidence of each correspondence from the embedded features. With modest computation cost, our method outperforms the state-of-the-art hand-crafted and learning-based outlier rejection approaches on several real-world datasets by a significant margin. We also show its wide applicability by combining PointDSC with different 3D local descriptors.",0
"Abstract Recently developed deep learning architectures have shown remarkable performance for point cloud registration tasks due to their ability to capture spatial relationships across large search spaces. Motivated by these advancements, we present PointDSC - a novel method that leverages deep spatial consistency constraints to achieve robust registration results even under challenging scenarios involving significant transformations, partial overlap, and noise. Our approach builds upon previous efforts by introducing a new loss function that explicitly models local geometry as well as a novel neural network architecture designed specifically for handling uncertain correspondences without sacrificing accuracy. Experiments demonstrate that our algorithm achieves stateof-the-art performance on multiple benchmark datasets while providing more reliable predictions compared to existing methods. These findings underscore the effectiveness of incorporating deep spatial constraints into point cloud registration algorithms, making PointDSC a valuable tool for real-world applications such as autonomous robotics, computer vision, and virtual reality. Keywords: point cloud registration; deep learning; spatial consistency; uncertainty estimation; point set matching Introduction In recent years, there has been growing interest in the use of deep learning techniques for solving point cloud registration problems. These approaches aim to recover the rigid transformation parameters that align two sets of three dimensional points into corresponding positions. Accurate alignment is crucial in numerous applications, ranging from robotic mapping and navigation, 3D reconstruction in computer vision, and scene understanding in augmented/virtual reality environments. While classical featurebased methods such as Iterative Closest Point (ICP) [1] remain widely used, they often fail when confronted with complex scenes containing occlusions, outlier",1
"In this paper, a new task is proposed, namely, weather translation, which refers to transferring weather conditions of the image from one category to another. It is important for photographic style transfer. Although lots of approaches have been proposed in traditional image translation tasks, few of them can handle the multi-category weather translation task, since weather conditions have rich categories and highly complex semantic structures. To address this problem, we develop a multi-domain weather translation approach based on generative adversarial networks (GAN), denoted as Weather GAN, which can achieve the transferring of weather conditions among sunny, cloudy, foggy, rainy and snowy. Specifically, the weather conditions in the image are determined by various weather-cues, such as cloud, blue sky, wet ground, etc. Therefore, it is essential for weather translation to focus the main attention on weather-cues. To this end, the generator of Weather GAN is composed of an initial translation module, an attention module and a weather-cue segmentation module. The initial translation module performs global translation during generation procedure. The weather-cue segmentation module identifies the structure and exact distribution of weather-cues. The attention module learns to focus on the interesting areas of the image while keeping other areas unaltered. The final generated result is synthesized by these three parts. This approach suppresses the distortion and deformation caused by weather translation. our approach outperforms the state-of-the-arts has been shown by a large number of experiments and evaluations.",0
"Artificial intelligence (AI) has come a long way since the early days, but one area where there remains significant room for improvement is image synthesis. While current methods may achieve reasonable quality results, they often fail to capture important details that give images their natural appearance. In particular, recent advances using generative adversarial networks (GANs) have shown promise as potential solutions by generating highly realistic images, but these approaches tend to be limited to specific domains and suffer from instability during training. To address these shortcomings, we propose a multi-domain weather translation model based on GANs called â€œWeather GAN.â€ Our approach leverages multiple domain labels for improved stability, which allows our generator network to generate more detailed translations while preserving essential features. Moreover, the use of a cycle consistency loss enables further refinement of generated images so that outputs resemble even closer ground truth. Experiments show that Weather GAN outperforms single-domain baselines and state-of-the-art models across all evaluation metrics and human judgment. Overall, our work demonstrates how GANs can effectively enhance artificial image generation in realistic scenarios, opening up new possibilities in computer vision applications such as object detection, segmentation, and scene understanding. This studyâ€™s findings highlight the immense potential of GANs for revolutionizing image synthesis and ultimately lead us to ponder what other frontiers remain unexplored in the quest for developing truly intelligent machines.",1
"Transformer is a powerful tool for many natural language tasks which is based on self-attention, a mechanism that encodes the dependence of other tokens on each specific token, but the computation of self-attention is a bottleneck due to its quadratic time complexity. There are various approaches to reduce the time complexity and approximation of matrix is one such. In Nystr\""omformer, the authors used Nystr\""om based method for approximation of softmax. The Nystr\""om method generates a fast approximation to any large-scale symmetric positive semidefinite (SPSD) matrix using only a few columns of the SPSD matrix. However, since the Nystr\""om approximation is low-rank when the spectrum of the SPSD matrix decays slowly, the Nystr\""om approximation is of low accuracy. Here an alternative method is proposed for approximation which has a much stronger error bound than the Nystr\""om method. The time complexity of this same as Nystr\""omformer which is $O\left({n}\right)$.",0
"Attempt to summarize the key points from each section into one sentence each in your own words. Include at least 6 sections such as problem statement, methodology, results, conclusion etc. Your final summary should have 6-8 sentences total. ---",1
"We propose a simple, intuitive yet powerful method for human-object interaction (HOI) detection. HOIs are so diverse in spatial distribution in an image that existing CNN-based methods face the following three major drawbacks; they cannot leverage image-wide features due to CNN's locality, they rely on a manually defined location-of-interest for the feature aggregation, which sometimes does not cover contextually important regions, and they cannot help but mix up the features for multiple HOI instances if they are located closely. To overcome these drawbacks, we propose a transformer-based feature extractor, in which an attention mechanism and query-based detection play key roles. The attention mechanism is effective in aggregating contextually important information image-wide, while the queries, which we design in such a way that each query captures at most one human-object pair, can avoid mixing up the features from multiple instances. This transformer-based feature extractor produces so effective embeddings that the subsequent detection heads may be fairly simple and intuitive. The extensive analysis reveals that the proposed method successfully extracts contextually important features, and thus outperforms existing methods by large margins (5.37 mAP on HICO-DET, and 5.7 mAP on V-COCO). The source codes are available at $\href{https://github.com/hitachi-rd-cv/qpic}{\text{this https URL}}$.",0
"Effective human object interaction analysis plays a critical role in many application scenarios including robotic manipulation planning, action recognition, and video surveillance. In this work, we propose theQuery-based Pairwise Human-object Interaction (QPIC) detection approach that leverages the pairwise query relationship among all potential human-object interactions within an image and contextual cueing from object co-occurrences at the image level. We design novel search operations based on human pose estimation and semantic region proposals, which effectively prune irrelevant regions and reduce computational costs without losing accuracy. Extensive experiments over five challenging datasets demonstrate our method outperforms other state-of-the-art methods by significant margins across multiple evaluation metrics under different settings, verifying the effectiveness of the proposed framework. Our results showcase both robustness against various scene complexities and capability in capturing fine-grained pairwise distinctions, making our method highly competitive for applications requiring accuratehuman object interaction analysis.",1
"Logistic Bandits have recently attracted substantial attention, by providing an uncluttered yet challenging framework for understanding the impact of non-linearity in parametrized bandits. It was shown by Faury et al. (2020) that the learning-theoretic difficulties of Logistic Bandits can be embodied by a large (sometimes prohibitively) problem-dependent constant $\kappa$, characterizing the magnitude of the reward's non-linearity. In this paper we introduce a novel algorithm for which we provide a refined analysis. This allows for a better characterization of the effect of non-linearity and yields improved problem-dependent guarantees. In most favorable cases this leads to a regret upper-bound scaling as $\tilde{\mathcal{O}}(d\sqrt{T/\kappa})$, which dramatically improves over the $\tilde{\mathcal{O}}(d\sqrt{T}+\kappa)$ state-of-the-art guarantees. We prove that this rate is minimax-optimal by deriving a $\Omega(d\sqrt{T/\kappa})$ problem-dependent lower-bound. Our analysis identifies two regimes (permanent and transitory) of the regret, which ultimately re-conciliates Faury et al. (2020) with the Bayesian approach of Dong et al. (2019). In contrast to previous works, we find that in the permanent regime non-linearity can dramatically ease the exploration-exploitation trade-off. While it also impacts the length of the transitory phase in a problem-dependent fashion, we show that this impact is mild in most reasonable configurations.",0
"In this work we describe a new family of algorithms for logistic bandit problems that achieve minimax optimality on each instance separately. Our approach is based on solving linear programs at runtime, which allows us to efficiently compute the optimal policy given any realization of the problem instance. We analyze both the case where rewards are adversarially chosen and the case where they come from a known distribution. For the latter scenario, our algorithm is able to improve upon the state-of-the art by providing strictly better worst-case guarantees while maintaining efficient computation times. Finally, we demonstrate the effectiveness of our methods through extensive experiments.",1
"Zero-shot learning (ZSL) aims to discriminate images from unseen classes by exploiting relations to seen classes via their semantic descriptions. Some recent papers have shown the importance of localized features together with fine-tuning the feature extractor to obtain discriminative and transferable features. However, these methods require complex attention or part detection modules to perform explicit localization in the visual space. In contrast, in this paper we propose localizing representations in the semantic/attribute space, with a simple but effective pipeline where localization is implicit. Focusing on attribute representations, we show that our method obtains state-of-the-art performance on CUB and SUN datasets, and also achieves competitive results on AWA2 dataset, outperforming generally more complex methods with explicit localization in the visual space. Our method can be implemented easily, which can be used as a new baseline for zero shot-learning. In addition, our localized representations are highly interpretable as attribute-specific heatmaps.",0
"In recent years, deep learning has shown great success on computer vision tasks such as object recognition and segmentation. Despite these advances, most current approaches still rely heavily on large amounts of labeled data, which can be costly to acquire and annotate. Zero-shot learning (ZSL) provides a promising alternative by allowing models to recognize novel classes without any specific training data for those classes. This work proposes a simple and effective method for representing local image features in ZSL that outperforms existing methods across multiple datasets and metrics. By using feature maps generated by convolutional neural networks, we represent each class using a set of attributes that describe semantic concepts such as shape, texture, color, etc. We then train a linear model to predict the presence or absence of each attribute for each class, which allows us to make predictions based solely on visual features. Experiments show that our method significantly improves over previous state-of-the-art results while requiring fewer parameters and less computational overhead. Our approach represents an important step towards making ZSL more practical and efficient for real-world applications.",1
"Visual Attention Prediction (VAP) is a significant and imperative issue in the field of computer vision. Most of existing VAP methods are based on deep learning. However, they do not fully take advantage of the low-level contrast features while generating the visual attention map. In this paper, a novel VAP method is proposed to generate visual attention map via bio-inspired representation learning. The bio-inspired representation learning combines both low-level contrast and high-level semantic features simultaneously, which are developed by the fact that human eye is sensitive to the patches with high contrast and objects with high semantics. The proposed method is composed of three main steps: 1) feature extraction, 2) bio-inspired representation learning and 3) visual attention map generation. Firstly, the high-level semantic feature is extracted from the refined VGG16, while the low-level contrast feature is extracted by the proposed contrast feature extraction block in a deep network. Secondly, during bio-inspired representation learning, both the extracted low-level contrast and high-level semantic features are combined by the designed densely connected block, which is proposed to concatenate various features scale by scale. Finally, the weighted-fusion layer is exploited to generate the ultimate visual attention map based on the obtained representations after bio-inspired representation learning. Extensive experiments are performed to demonstrate the effectiveness of the proposed method.",0
"This paper presents a bio-inspired approach for visual attention prediction using deep neural networks. We propose a novel architecture that incorporates principles from biological vision systems into convolutional neural network design, allowing for improved model interpretability and generalization performance on complex tasks such as image classification. Our method utilizes lateral connections inspired by cortical connectivity patterns found in mammalian brains, which enable contextual integration of feature maps at different spatial scales. Experimental results demonstrate that our approach outperforms state-of-the-art methods in predicting human fixations, highlighting the effectiveness of our proposed design principles. Furthermore, we present insights into how these changes influence internal representations throughout training and evaluate their impact on task accuracy. Overall, our work provides a new perspective on how biologically motivated architectures can enhance representation learning in computer vision applications.",1
"Due to the rapid emergence of short videos and the requirement for content understanding and creation, the video captioning task has received increasing attention in recent years. In this paper, we convert traditional video captioning task into a new paradigm, \ie, Open-book Video Captioning, which generates natural language under the prompts of video-content-relevant sentences, not limited to the video itself. To address the open-book video captioning problem, we propose a novel Retrieve-Copy-Generate network, where a pluggable video-to-text retriever is constructed to retrieve sentences as hints from the training corpus effectively, and a copy-mechanism generator is introduced to extract expressions from multi-retrieved sentences dynamically. The two modules can be trained end-to-end or separately, which is flexible and extensible. Our framework coordinates the conventional retrieval-based methods with orthodox encoder-decoder methods, which can not only draw on the diverse expressions in the retrieved sentences but also generate natural and accurate content of the video. Extensive experiments on several benchmark datasets show that our proposed approach surpasses the state-of-the-art performance, indicating the effectiveness and promising of the proposed paradigm in the task of video captioning.",0
"In recent years, there has been significant progress in natural language processing tasks such as image generation, question answering, and machine translation using large datasets. However, video understanding remains challenging due to the complex nature of visual content and audio tracks. One popular approach towards tackling this problem is to generate textual descriptions, called captions, that accurately capture the events depicted in videos. To accomplish this task, open-book video captioning models have emerged which rely on the Internet for factoid knowledge beyond the visual scenes alone, effectively making them retrieve-copy-generate networks (RCG). This study presents an RCG model that integrates various components like image, audio, and text encoders along with external retrieval mechanisms to generate high quality captions from video frames and associated texts without relying solely on memorized patterns. Experimental results show that our proposed architecture achieves competitive performance compared to state-of-the-art methods while utilizing the vast resources available online. Our work provides valuable insights into creating effective open-world video description systems using both internal and external memory stores. By shedding light on future research directions in the domain of video understanding via NLP techniques, we aim to contribute towards solving one of computer visionâ€™s most ambitious objectives: mimicking human intelligence to process multimedia data at scale.",1
"Temporal action localization is an important and challenging task that aims to locate temporal regions in real-world untrimmed videos where actions occur and recognize their classes. It is widely acknowledged that video context is a critical cue for video understanding, and exploiting the context has become an important strategy to boost localization performance. However, previous state-of-the-art methods focus more on exploring semantic context which captures the feature similarity among frames or proposals, and neglect positional context which is vital for temporal localization. In this paper, we propose a temporal-position-sensitive context modeling approach to incorporate both positional and semantic information for more precise action localization. Specifically, we first augment feature representations with directed temporal positional encoding, and then conduct attention-based information propagation, in both frame-level and proposal-level. Consequently, the generated feature representations are significantly empowered with the discriminative capability of encoding the position-aware context information, and thus benefit boundary detection and proposal evaluation. We achieve state-of-the-art performance on both two challenging datasets, THUMOS-14 and ActivityNet-1.3, demonstrating the effectiveness and generalization ability of our method.",0
"In recent years, deep learning has made significant progresses on temporal action localization by proposing powerful feature extraction networks such as TCN [1] , ARTNET2019 [2] , and Q-RNN [4]. Meanwhile, these approaches either rely on very complex models (such as RNNs) or extract features at frame level but neglect position-sensitivity. For example, ARTNET2019 computes frame level queries without considering which frame comes from where. Motivated to address both efficiency and accuracy aspects simultaneously, we propose PCMNet â€“ Position-Sensitive Context Modeling Network for Temporal Action Localization. We firstly present the concept of using Position-Aware Convolution modules in temporal domain for efficient modeling of interdependencies between spatio-temporal features. Then, based on our insightful observation that framewise contextual information could indeed encode crucial spatial cues implicitly due to positional encoding techniques like strided convolutions, we show how framewise representations can benefit even more from attention mechanism. Finally, we combine different position-sensitive components into one end-to-end trainable pipeline towards achieving state-of-the-art performance on THUMOSâ€™14 and FineGrainedAction v1 datasets while requiring significantly fewer parameters. To summarize, PCMNet addresses shortcomings of current approaches by introducing novel ways of integrating positions explicitly into the network architecture. Our ablation studies verify the importance of each component contribution and comprehensive experimental results on three popular benchmarking datasets demonstrate consistently strong improvement over baseline models under varying parameter budgets. These findings encourage further exploration in designing computationall",1
"A robot operating in a household makes observations of multiple objects as it moves around over the course of days or weeks. The objects may be moved by inhabitants, but not completely at random. The robot may be called upon later to retrieve objects and will need a long-term object-based memory in order to know how to find them. In this paper, we combine some aspects of classic techniques for data-association filtering with modern attention-based neural networks to construct object-based memory systems that consume and produce high-dimensional observations and hypotheses. We perform end-to-end learning on labeled observation trajectories to learn both necessary internal transition and observation models. We demonstrate the system's effectiveness on a sequence of problem classes of increasing difficulty and show that it outperforms clustering-based methods, classic filters, and unstructured neural approaches.",0
"Learning an Object-Based Memory System: An Investigation into How People Organize Knowledge About Everyday Items  Memory systems play a crucial role in human cognition by allowing us to retain information over time and make use of it in various contexts. Despite their importance, there has been limited research on how we organize knowledge about everyday items in our memory system. This study seeks to address that gap by investigating how individuals learn and recall object-based memories. We use the term â€œobject-basedâ€ to refer to our ability to bind together different attributes, features, and episodes associated with a particular item, such as recognizing someoneâ€™s face even if they wear glasses instead of contacts. Our research explores whether humans use a global vs. local coding strategy when encoding object representations, which refers to whether memories are organized holistically (i.e., all aspects encoded at once) versus piecemeal (i.e., each aspect encoded separately).  We conducted four experiments using a combination of behavioral tasks and eye tracking measures to examine how participants encode and retrieve object memories. Across studies, we manipulated factors known to impact memory organization, including attention, intentionality, category specificity, and temporal dynamics. Results indicate that the nature of the learning task significantly influences both how people initially form object representations and how these representations change over time due to subsequent experiences. Specifically, we observed support for a hybrid model of memory organization where both global and local codes coexist but are used differentially depending on the situation. These findings have important implications for understanding how people efficiently manage knowledge about objects within their memory, particularly given the vast number of objects encountered daily in modern life.  Overall, this work contributes to the field by shedding light on the mechanisms underlying object-based memory organization. By examining how people form and store memo",1
"Understanding how environmental characteristics affect bio-diversity patterns, from individual species to communities of species, is critical for mitigating effects of global change. A central goal for conservation planning and monitoring is the ability to accurately predict the occurrence of species communities and how these communities change over space and time. This in turn leads to a challenging and long-standing problem in the field of computer science - how to perform ac-curate multi-label classification with hundreds of labels? The key challenge of this problem is its exponential-sized output space with regards to the number of labels to be predicted.Therefore, it is essential to facilitate the learning process by exploiting correlations (or dependency) among labels. Previous methods mostly focus on modelling the correlation on label pairs; however, complex relations between real-world objects often go beyond second order. In this paper, we pro-pose a novel framework for multi-label classification, High-order Tie-in Variational Autoencoder (HOT-VAE), which per-forms adaptive high-order label correlation learning. We experimentally verify that our model outperforms the existing state-of-the-art approaches on a bird distribution dataset on both conventional F1 scores and a variety of ecological metrics. To show our method is general, we also perform empirical analysis on seven other public real-world datasets in several application domains, and Hot-VAE exhibits superior performance to previous methods.",0
"In this paper, we propose a novel method called HOT-VAE for multi-label classification that leverages high-order label correlation. Our approach extends variational autoencoders (VAEs) by introducing attention mechanisms to model label dependencies and capturing hierarchical relationships among labels. We first introduce the concept of higher-order latent variables (HOLVs), which explicitly capture interdependencies between labels beyond pairwise correlations. To learn these HOLVs, we develop an attention-based VAE architecture that incorporates both local and global attention modules to focus on important labels while encoding data samples into low-dimensional representations. Additionally, we design an algorithm for efficient learning of HOT-VAE models using stochastic gradient descent and mini-batch optimization techniques. Experimental results demonstrate significant improvements over state-of-the-art methods across multiple benchmark datasets, validating the effectiveness and efficiency of our proposed framework for multi-label classification tasks.",1
"The problem of learning to generalize to unseen classes during training, known as few-shot classification, has attracted considerable attention. Initialization based methods, such as the gradient-based model agnostic meta-learning (MAML), tackle the few-shot learning problem by ""learning to fine-tune"". The goal of these approaches is to learn proper model initialization, so that the classifiers for new classes can be learned from a few labeled examples with a small number of gradient update steps. Few shot meta-learning is well-known with its fast-adapted capability and accuracy generalization onto unseen tasks. Learning fairly with unbiased outcomes is another significant hallmark of human intelligence, which is rarely touched in few-shot meta-learning. In this work, we propose a Primal-Dual Fair Meta-learning framework, namely PDFM, which learns to train fair machine learning models using only a few examples based on data from related tasks. The key idea is to learn a good initialization of a fair model's primal and dual parameters so that it can adapt to a new fair learning task via a few gradient update steps. Instead of manually tuning the dual parameters as hyperparameters via a grid search, PDFM optimizes the initialization of the primal and dual parameters jointly for fair meta-learning via a subgradient primal-dual approach. We further instantiate examples of bias controlling using mean difference and decision boundary covariance as fairness constraints to each task for supervised regression and classification, respectively. We demonstrate the versatility of our proposed approach by applying our approach to various real-world datasets. Our experiments show substantial improvements over the best prior work for this setting.",0
"In recent years, meta learning has emerged as a promising approach for improving few shot learning performance by optimizing parameters over tasks using gradient descent methods. However, most existing methods focus on single task adaptation without considering fairness concerns. This work proposes a novel primal dual subgradient method for fair meta learning that takes into account both accuracy and fairness considerations while optimizing parameters over multiple related tasks. Our algorithm uses the concept of individualized weights to ensure equal opportunity in decision making across different groups defined by sensitive attributes like gender, race, age, etc., resulting in more equitable and accurate predictions. We evaluate our method extensively on benchmark datasets and demonstrate significant improvements in terms of classification accuracy, group fairness measures, and ablation studies compared to baseline models.",1
"Fine-grained visual classification (FGVC) is challenging but more critical than traditional classification tasks. It requires distinguishing different subcategories with the inherently subtle intra-class object variations. Previous works focus on enhancing the feature representation ability using multiple granularities and discriminative regions based on the attention strategy or bounding boxes. However, these methods highly rely on deep neural networks which lack interpretability. We propose an Interpretable Attention Guided Network (IAGN) for fine-grained visual classification. The contributions of our method include: i) an attention guided framework which can guide the network to extract discriminitive regions in an interpretable way; ii) a progressive training mechanism obtained to distill knowledge stage by stage to fuse features of various granularities; iii) the first interpretable FGVC method with a competitive performance on several standard FGVC benchmark datasets.",0
"This paper proposes a new attention mechanism called ""Interpretable Attention"" that enhances fine-grained visual classification by enabling the network to interpret subtle differences between similar classes such as species of birds. To improve over existing models which lack interpretebility, we propose adding guiding signals into convolutional neural networks (CNNs) to direct the network towards discriminative regions at different levels of granularity based on the input images themselves. In our proposed model, attention can be dynamically generated by attending to local features within the same layer as well as hierarchically across layers, while still reducing computational cost through parameter sharing. We demonstrate state-of-the art performance on two benchmark datasets using fine-grained image classifications: CUB-200-2011 dataset containing images of bird species, and the Stanford Dogs Dataset featuring pictures of breeds. Our work emphasizes the importance of developing more explainable deep learning systems and showcases one step forward on that direction.",1
"Fuzzy Cognitive Maps (FCMs) are considered a soft computing technique combining elements of fuzzy logic and recurrent neural networks. They found multiple application in such domains as modeling of system behavior, prediction of time series, decision making and process control. Less attention, however, has been turned towards using them in pattern classification. In this work we propose an FCM based classifier with a fully connected map structure. In contrast to methods that expect reaching a steady system state during reasoning, we chose to execute a few FCM iterations (steps) before collecting output labels. Weights were learned with a gradient algorithm and logloss or cross-entropy were used as the cost function. Our primary goal was to verify, whether such design would result in a descent general purpose classifier, with performance comparable to off the shelf classical methods. As the preliminary results were promising, we investigated the hypothesis that the performance of $d$-step classifier can be attributed to a fact that in previous $d-1$ steps it transforms the feature space by grouping observations belonging to a given class, so that they became more compact and separable. To verify this hypothesis we calculated three clustering scores for the transformed feature space. We also evaluated performance of pipelines built from FCM-based data transformer followed by a classification algorithm. The standard statistical analyzes confirmed both the performance of FCM based classifier and its capability to improve data. The supporting prototype software was implemented in Python using TensorFlow library.",0
"This paper presents an application of fuzzy cognitive maps (FCMs) that utilizes the FCM structure as both a classifier and feature transformation engine. We outline the steps necessary to create such a system by first detailing how to extract rules from training data using the conceptual clustering methodology. Next we show how these extracted rules can then be combined into a single FCM which is trained on the input space consisting of instances and their corresponding classes. Finally we demonstrate how this trained FCM can then be transformed into a Takagi-Sugeno-Kang type fuzzy rule based system suitable for use in real time applications requiring fast response times. While this work was primarily motivated by image classification problems future versions may well find wide spread applicability across many other domains where decisions must be made quickly with incomplete information. Keywords: Fuzzy cognitive map, Conceptual clustering, Image classification, Takagi-Sugeno-Kang",1
"Recently Convolutional Neural Networks have been proposed for Sequence Modelling tasks such as Image Caption Generation. However, unlike Recurrent Neural Networks, the performance of Convolutional Neural Networks as Decoders for Image Caption Generation has not been extensively studied. In this work, we analyse various aspects of Convolutional Neural Network based Decoders such as Network complexity and depth, use of Data Augmentation, Attention mechanism, length of sentences used during training, etc on performance of the model. We perform experiments using Flickr8k and Flickr30k image captioning datasets and observe that unlike Recurrent Neural Network based Decoder, Convolutional Decoder for Image Captioning does not generally benefit from increase in network depth, in the form of stacked Convolutional Layers, and also the use of Data Augmentation techniques. In addition, use of Attention mechanism also provides limited performance gains with Convolutional Decoder. Furthermore, we observe that Convolutional Decoders show performance comparable with Recurrent Decoders only when trained using sentences of smaller length which contain up to 15 words but they have limitations when trained using higher sentence lengths which suggests that Convolutional Decoders may not be able to model long-term dependencies efficiently. In addition, the Convolutional Decoder usually performs poorly on CIDEr evaluation metric as compared to Recurrent Decoder.",0
"This should provide enough context so that someone could decide if they wanted to read your paper. Include something on why such research was done, the methods employed, key results, conclusions drawn by authors, potential future work and significance. Be specific yet concise.",1
"Limb deficiency severely affects the daily lives of amputees and drives efforts to provide functional robotic prosthetic hands to compensate this deprivation. Convolutional neural network-based computer vision control of the prosthetic hand has received increased attention as a method to replace or complement physiological signals due to its reliability by training visual information to predict the hand gesture. Mounting a camera into the palm of a prosthetic hand is proved to be a promising approach to collect visual data. However, the grasp type labelled from the eye and hand perspective may differ as object shapes are not always symmetric. Thus, to represent this difference in a realistic way, we employed a dataset containing synchronous images from eye- and hand- view, where the hand-perspective images are used for training while the eye-view images are only for manual labelling. Electromyogram (EMG) activity and movement kinematics data from the upper arm are also collected for multi-modal information fusion in future work. Moreover, in order to include human-in-the-loop control and combine the computer vision with physiological signal inputs, instead of making absolute positive or negative predictions, we build a novel probabilistic classifier according to the Plackett-Luce model. To predict the probability distribution over grasps, we exploit the statistical model over label rankings to solve the permutation domain problems via a maximum likelihood estimation, utilizing the manually ranked lists of grasps as a new form of label. We indicate that the proposed model is applicable to the most popular and productive convolutional neural network frameworks.",0
"This paper presents a novel approach for predicting grasp type probabilities using hand-perspective visual information. Our method leverages deep learning techniques to rank labels associated with different grasps based on their likelihood of success given a specific task and object configuration. By ranking rather than directly classifying labels, our model can output uncertainty measures that reflect the confidence in its predictions, allowing robots to make more informed decisions during planning and execution. We evaluate our approach through experiments on real robot data collected from multiple objects and tasks, demonstrating improved accuracy over previous state-of-the-art methods. Our results showcase the potential of incorporating rich visual features into perception modules for robotic manipulation and suggest new research directions towards generalizable and reliable robots capable of handling diverse environments and scenarios.",1
"Understanding interactions between objects in an image is an important element for generating captions. In this paper, we propose a relationship-based neural baby talk (R-NBT) model to comprehensively investigate several types of pairwise object interactions by encoding each image via three different relationship-based graph attention networks (GATs). We study three main relationships: \textit{spatial relationships} to explore geometric interactions, \textit{semantic relationships} to extract semantic interactions, and \textit{implicit relationships} to capture hidden information that could not be modelled explicitly as above. We construct three relationship graphs with the objects in an image as nodes, and the mutual relationships of pairwise objects as edges. By exploring features of neighbouring regions individually via GATs, we integrate different types of relationships into visual features of each node. Experiments on COCO dataset show that our proposed R-NBT model outperforms state-of-the-art models trained on COCO dataset in three image caption generation tasks.",0
"The purpose of the study was to investigate whether neural networks can learn to imitate human baby talk by analyzing large amounts of audio data collected from parents interacting with their infants. The researchers trained a deep convolutional recurrent network (CRN) on a dataset consisting of infant vocalizations, parent responses to these vocalizations, and infant-directed speech produced by adult speakers. They then tested the networkâ€™s ability to generate new sentences that were consistent with patterns observed in the training data. Results showed that the CRN was able to successfully learn a relationship model that captured the underlying structure of typical parent-infant interactions. The generated outputs demonstrated high levels of semantic coherence and grammatical correctness, even in novel contexts. Furthermore, analysis revealed strong relationships between linguistic features in both infant and caregiver speech, indicating that the learning process had effectively transferred knowledge across modalities. These findings have important implications for our understanding of early language acquisition and the role played by social interaction in shaping cognitive development. Overall, the results suggest that CRNs may prove valuable as tools for studying complex phenomena such as natural communication and social dynamics.",1
"Self-attention, as the key block of transformers, is a powerful mechanism for extracting features from the inputs. In essence, what self-attention does is to infer the pairwise relations between the elements of the inputs, and modify the inputs by propagating information between input pairs. As a result, it maps inputs to N outputs and casts a quadratic $O(N^2)$ memory and time complexity. We propose centroid attention, a generalization of self-attention that maps N inputs to M outputs $(M\leq N)$, such that the key information in the inputs are summarized in the smaller number of outputs (called centroids). We design centroid attention by amortizing the gradient descent update rule of a clustering objective function on the inputs, which reveals an underlying connection between attention and clustering. By compressing the inputs to the centroids, we extract the key information useful for prediction and also reduce the computation of the attention module and the subsequent layers. We apply our method to various applications, including abstractive text summarization, 3D vision, and image processing. Empirical results demonstrate the effectiveness of our method over the standard transformers.",0
"This paper proposes a new model architecture called centroid transformer, which combines self attention mechanisms with element-wise operations (centroids) learned by autoencoders. Our model uses encoder and decoder structures similar to those used in standard transformers. However, our method modifies these structures such that they are composed of three parts rather than two; an input layer followed by encoding layers where element wise interactions occur, and then output layers containing linear transformations of encoded representations. We show on benchmark datasets that the resulting models have better performance while requiring fewer parameters compared to other state-of-the-art models like transformers, as well as achieving improved generalization in out of domain settings.  The proposed Centroid Transformer is a novel approach to learning textual abstractions through self-attention mechanisms combined with learnt element-wise operations (centroids). In contrast to traditional transformer architectures, the Centroid Transformer introduces an additional stage after the attention mechanism: a learnable centroid matrix applied element-wise to the attended representation before passing through feedforward neural networks. Our experiments demonstrate that models built using this architecture achieve significant improvements over current methods, both in terms of accuracy metrics and parameter efficiency. Additionally, we find that our models generalize more effectively across multiple tasks and domains. These results suggest that the incorporation of centroid matrices into transformers represents a promising direction for future research in natural language processing.",1
"Face forgery by deepfake is widely spread over the internet and has raised severe societal concerns. Recently, how to detect such forgery contents has become a hot research topic and many deepfake detection methods have been proposed. Most of them model deepfake detection as a vanilla binary classification problem, i.e, first use a backbone network to extract a global feature and then feed it into a binary classifier (real/fake). But since the difference between the real and fake images in this task is often subtle and local, we argue this vanilla solution is not optimal. In this paper, we instead formulate deepfake detection as a fine-grained classification problem and propose a new multi-attentional deepfake detection network. Specifically, it consists of three key components: 1) multiple spatial attention heads to make the network attend to different local parts; 2) textural feature enhancement block to zoom in the subtle artifacts in shallow features; 3) aggregate the low-level textural feature and high-level semantic features guided by the attention maps. Moreover, to address the learning difficulty of this network, we further introduce a new regional independence loss and an attention guided data augmentation strategy. Through extensive experiments on different datasets, we demonstrate the superiority of our method over the vanilla binary classifier counterparts, and achieve state-of-the-art performance.",0
"In recent years, deepfakes have become increasingly prevalent and present a significant challenge to our ability to distinguish real from fake media content. Existing deepfake detection methods primarily focus on modeling image or video features, but neglect the multi-modal nature of these videos and their contextual information, which could improve performance further. This paper presents a novel deep learning framework that effectively combines both local spatial attention mechanisms and global attentive pooling operations, allowing us to capture temporal dependencies in the inputs. We also introduce multi-task training schemes such as transfer learning using GANs, feature distillation, and consistency constraint regularization techniques that enable effective utilization of diverse datasets, including audio-visual synchrony clues and text transcript information. Our experiments demonstrate that our method outperforms state-of-the-art approaches by achieving higher accuracy while maintaining computational efficiency. By capturing richer representations of multimedia contents and exploring multiple sources of data for supervision, we significantly enhance our capability to detect deepfakes across different domains. Our findings thus provide valuable insights into advancing the frontier of robust AI systems that can reliably discern genuine from fabricated content in todayâ€™s rapidly evolving digital landscape.",1
"Zero-shot learning (ZSL) aims to discriminate images from unseen classes by exploiting relations to seen classes via their attribute-based descriptions. Since attributes are often related to specific parts of objects, many recent works focus on discovering discriminative regions. However, these methods usually require additional complex part detection modules or attention mechanisms. In this paper, 1) we show that common ZSL backbones (without explicit attention nor part detection) can implicitly localize attributes, yet this property is not exploited. 2) Exploiting it, we then propose SELAR, a simple method that further encourages attribute localization, surprisingly achieving very competitive generalized ZSL (GZSL) performance when compared with more complex state-of-the-art methods. Our findings provide useful insight for designing future GZSL methods, and SELAR provides an easy to implement yet strong baseline.",0
"This research presents a novel approach to implicit attribute localization for generalized zero-shot learning (GZSL). The proposed method leverages semantic image segmentation techniques to automatically identify discriminative regions within an input image that correspond to desired attributes. By implicitly localizing these attributes without explicit supervision or human intervention, our framework greatly reduces the complexity and cost associated with traditional GZSL methods while still achieving state-of-the-art performance on several benchmark datasets. Our results demonstrate the effectiveness of this new paradigm for real-world applications such as scene understanding, object detection, and fine-grained classification.",1
"Graph neural networks (GNNs) are important tools for transductive learning tasks, such as node classification in graphs, due to their expressive power in capturing complex interdependency between nodes. To enable graph neural network learning, existing works typically assume that labeled nodes, from two or multiple classes, are provided, so that a discriminative classifier can be learned from the labeled data. In reality, this assumption might be too restrictive for applications, as users may only provide labels of interest in a single class for a small number of nodes. In addition, most GNN models only aggregate information from short distances (e.g., 1-hop neighbors) in each round, and fail to capture long distance relationship in graphs. In this paper, we propose a novel graph neural network framework, long-short distance aggregation networks (LSDAN), to overcome these limitations. By generating multiple graphs at different distance levels, based on the adjacency matrix, we develop a long-short distance attention model to model these graphs. The direct neighbors are captured via a short-distance attention mechanism, and neighbors with long distance are captured by a long distance attention mechanism. Two novel risk estimators are further employed to aggregate long-short-distance networks, for PU learning and the loss is back-propagated for model learning. Experimental results on real-world datasets demonstrate the effectiveness of our algorithm.",0
"Abstract: This paper presents a method for learning graph neural networks using positive and unlabelled nodes. We introduce a new class of models that can learn from both labelled data points (positives) as well as unlabelled data points (negatives). Our method leverages recent advances in semi-supervised learning techniques to improve accuracy on the task. In order to evaluate our approach, we use two benchmark datasets which represent different domains such as node classification and link prediction tasks. Results show that our model significantly outperforms baseline methods in most cases while having fewer parameters, demonstrating the effectiveness of utilizing both labelled and unlabelled nodes for training.",1
"Currently, spatiotemporal features are embraced by most deep learning approaches for human action detection in videos, however, they neglect the important features in frequency domain. In this work, we propose an end-to-end network that considers the time and frequency features simultaneously, named TFNet. TFNet holds two branches, one is time branch formed of three-dimensional convolutional neural network(3D-CNN), which takes the image sequence as input to extract time features; and the other is frequency branch, extracting frequency features through two-dimensional convolutional neural network(2D-CNN) from DCT coefficients. Finally, to obtain the action patterns, these two features are deeply fused under the attention mechanism. Experimental results on the JHMDB51-21 and UCF101-24 datasets demonstrate that our approach achieves remarkable performance for frame-mAP.",0
"This paper presents an improved time-frequency network (TFN) that can effectively detect human actions from videos by leveraging spatiotemporal features and deep learning techniques. By jointly modeling spatial and temporal dynamics at multiple levels, TFN captures both short-term motion patterns and long-range dependencies across video frames. Additionally, we introduce a novel global branch to enable effective hierarchical feature propagation throughout the entire network. Extensive experimental results on challenging benchmark datasets demonstrate significant improvement over state-of-the-art methods in terms of accuracy and efficiency.",1
"With the high requirements of automation in the era of Industry 4.0, anomaly detection plays an increasingly important role in higher safety and reliability in the production and manufacturing industry. Recently, autoencoders have been widely used as a backend algorithm for anomaly detection. Different techniques have been developed to improve the anomaly detection performance of autoencoders. Nonetheless, little attention has been paid to the latent representations learned by autoencoders. In this paper, we propose a novel selection-and-weighting-based anomaly detection framework called SWAD. In particular, the learned latent representations are individually selected and weighted. Experiments on both benchmark and real-world datasets have shown the effectiveness and superiority of SWAD. On the benchmark datasets, the SWAD framework has reached comparable or even better performance than the state-of-the-art approaches.",0
"In this paper we present a novel method for anomaly detection based on selection and weighting in latent space. Traditional methods often rely on assumptions about the underlying data distribution which can be restrictive and lead to poor performance in practice. Our approach addresses these limitations by learning an unsupervised model that maps input data into a lower dimensional latent space where anomalies can be more easily detected. By selecting a subset of features from the original data, our algorithm effectively reduces the complexity of the problem while simultaneously improving accuracy. Additionally, a weighting scheme is used to emphasize important features and reduce the impact of noisy ones. Experiments demonstrate that our proposed method outperforms state-of-the-art techniques across a variety of datasets and use cases, making it a promising tool for detecting anomalies in complex datasets.",1
"Although unsupervised person re-identification (Re-ID) has drawn increasing research attention recently, it remains challenging to learn discriminative features without annotations across disjoint camera views. In this paper, we address the unsupervised person Re-ID with a conceptually novel yet simple framework, termed as Multi-label Learning guided self-paced Clustering (MLC). MLC mainly learns discriminative features with three crucial modules, namely a multi-scale network, a multi-label learning module, and a self-paced clustering module. Specifically, the multi-scale network generates multi-granularity person features in both global and local views. The multi-label learning module leverages a memory feature bank and assigns each image with a multi-label vector based on the similarities between the image and feature bank. After multi-label training for several epochs, the self-paced clustering joins in training and assigns a pseudo label for each image. The benefits of our MLC come from three aspects: i) the multi-scale person features for better similarity measurement, ii) the multi-label assignment based on the whole dataset ensures that every image can be trained, and iii) the self-paced clustering removes some noisy samples for better feature learning. Extensive experiments on three popular large-scale Re-ID benchmarks demonstrate that our MLC outperforms previous state-of-the-art methods and significantly improves the performance of unsupervised person Re-ID.",0
"This paper presents a novel approach for unsupervised person re-identification using multi-label learning guided self-paced clustering. The proposed method takes advantage of both label guidance and sample hardness for effective clustering without any labeled data. We first formulate person re-identification as a multi-label classification problem and designate each camera view as a class label. Then we propose two different forms of guidance: (i) pseudo labels generated by online clustering results and (ii) batch sampling weights based on the sample difficulty for clustering. To fuse these two types of guidance into one framework, we develop a unified model called GSPC that can integrate them adaptively according to the current stage of clustering. Extensive experiments on several benchmark datasets verify the superiority of our proposed method over state-of-the-art unsupervised methods under different evaluation metrics. Our work demonstrates that effective clustering can be achieved via judicious combination of multiple sources of guidance rather than relying solely on either anchor samples or online feedback.",1
"Understanding generalization and estimation error of estimators for simple models such as linear and generalized linear models has attracted a lot of attention recently. This is in part due to an interesting observation made in machine learning community that highly over-parameterized neural networks achieve zero training error, and yet they are able to generalize well over the test samples. This phenomenon is captured by the so called double descent curve, where the generalization error starts decreasing again after the interpolation threshold. A series of recent works tried to explain such phenomenon for simple models. In this work, we analyze the asymptotics of estimation error in ridge estimators for convolutional linear models. These convolutional inverse problems, also known as deconvolution, naturally arise in different fields such as seismology, imaging, and acoustics among others. Our results hold for a large class of input distributions that include i.i.d. features as a special case. We derive exact formulae for estimation error of ridge estimators that hold in a certain high-dimensional regime. We show the double descent phenomenon in our experiments for convolutional models and show that our theoretical results match the experiments.",0
"In recent years, convolutional models have become increasingly popular due to their ability to efficiently process large datasets and produce accurate results. One common approach to improving performance in these models is through regularization techniques such as ridge regression. This paper presents new findings on the asymptotic behavior of ridge regression in convolutional models by analyzing its theoretical properties and experimental results. Our analysis shows that while ridge regression can lead to improved generalization error bounds, there exists an intricate relationship between model capacity and dataset size. We demonstrate how this affects convergence rates and provide recommendations for practitioners based on our findings. Overall, our work contributes to understanding the tradeoffs between regularization and overfitting in deep learning, paving the way for future research in this area.",1
"In this paper, we consider the contextual variant of the MNL-Bandit problem. More specifically, we consider a dynamic set optimization problem, where in every round a decision maker offers a subset (assortment) of products to a consumer, and observes their response. Consumers purchase products so as to maximize their utility. We assume that the products are described by a set of attributes and the mean utility of a product is linear in the values of these attributes. We model consumer choice behavior by means of the widely used Multinomial Logit (MNL) model, and consider the decision maker's problem of dynamically learning the model parameters, while optimizing cumulative revenue over the selling horizon $T$. Though this problem has attracted considerable attention in recent times, many existing methods often involve solving an intractable non-convex optimization problem and their theoretical performance guarantees depend on a problem dependent parameter which could be prohibitively large. In particular, existing algorithms for this problem have regret bounded by $O(\sqrt{\kappa d T})$, where $\kappa$ is a problem dependent constant that can have exponential dependency on the number of attributes. In this paper, we propose an optimistic algorithm and show that the regret is bounded by $O(\sqrt{dT} + \kappa)$, significantly improving the performance over existing methods. Further, we propose a convex relaxation of the optimization step which allows for tractable decision-making while retaining the favourable regret guarantee.",0
"This paper presents a new online learning algorithm for the multinomial logit contextual bandit problem that achieves both high empirical performance and strong theoretical guarantees. The algorithm adapts the stochastic gradient ascent approach by carefully selecting actions based on their estimated values, which takes into account a tractability constraint imposed on the log-posterior distribution, resulting in substantial improvements over existing methods. Additionally, novel regret bounds are derived under reasonable conditions using upper confidence inequality. Empirical evaluation shows significant improvements achieved by our method compared to benchmark algorithms. Our findings provide valuable insights into designing effective online learning algorithms for real-world applications. ----- I suggest you use Google Scholar, not Wikipedia: Here are some good sources from google scholar: ""Multinomial logistic regression"", ""Online learning"" (https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5), ""Contextual bandits"" (https://scholar.google.com/scholar?hl=en&q=%22contextual+bandits%22)  If you need more suggestions please ask! I can recommend relevant papers in each area if you give me your field of interest (e.g computer vision, natural language processing). Or I can create an outline for the entire project, including relevant research areas and key points you should cover. Let me know how else I can assist!",1
"Capsule Networks, as alternatives to Convolutional Neural Networks, have been proposed to recognize objects from images. The current literature demonstrates many advantages of CapsNets over CNNs. However, how to create explanations for individual classifications of CapsNets has not been well explored. The widely used saliency methods are mainly proposed for explaining CNN-based classifications; they create saliency map explanations by combining activation values and the corresponding gradients, e.g., Grad-CAM. These saliency methods require a specific architecture of the underlying classifiers and cannot be trivially applied to CapsNets due to the iterative routing mechanism therein. To overcome the lack of interpretability, we can either propose new post-hoc interpretation methods for CapsNets or modifying the model to have build-in explanations. In this work, we explore the latter. Specifically, we propose interpretable Graph Capsule Networks (GraCapsNets), where we replace the routing part with a multi-head attention-based Graph Pooling approach. In the proposed model, individual classification explanations can be created effectively and efficiently. Our model also demonstrates some unexpected benefits, even though it replaces the fundamental part of CapsNets. Our GraCapsNets achieve better classification performance with fewer parameters and better adversarial robustness, when compared to CapsNets. Besides, GraCapsNets also keep other advantages of CapsNets, namely, disentangled representations and affine transformation robustness.",0
"Object recognition is a fundamental problem in computer vision that has been extensively studied over the past few decades. In recent years, deep neural networks have achieved state-of-the-art results on object recognition tasks by learning high-level representations through convolutional layers and non-linear processing in fully connected layers. However, these models often suffer from interpretability issues due to their complex architecture and black box nature. To address these problems, researchers have proposed capsule networks as an alternative approach to traditional neural networks. Capsules allow for more interpretable representation learning at different levels of abstraction while preserving accuracy. In this paper, we propose Interpretable Graph Capsule Networks (IGCN) for object recognition which extend traditional capsule networks by incorporating graph structures into capsule routing. IGCN uses graphs to learn contextual relationships among capsules and improve overall performance by making better use of spatial dependencies. We evaluate our method on several benchmark datasets and show that IGCN outperforms other state-of-the-art methods, demonstrating the effectiveness of using graph structure in capsule networks for improved object recognition. Our contributions can lead to new insights into understanding how contextual information improves object detection and could pave the way for further advancements in computer vision.",1
"Semantic segmentation is a basic but non-trivial task in computer vision. Many previous work focus on utilizing affinity patterns to enhance segmentation networks. Most of these studies use the affinity matrix as a kind of feature fusion weights, which is part of modules embedded in the network, such as attention models and non-local models. In this paper, we associate affinity matrix with labels, exploiting the affinity in a supervised way. Specifically, we utilize the label to generate a multi-scale label affinity matrix as a structural supervision, and we use a square root kernel to compute a non-local affinity matrix on output layers. With such two affinities, we define a novel loss called Affinity Regression loss (AR loss), which can be an auxiliary loss providing pair-wise similarity penalty. Our model is easy to train and adds little computational burden without run-time inference. Extensive experiments on NYUv2 dataset and Cityscapes dataset demonstrate that our proposed method is sufficient in promoting semantic segmentation networks.",0
"This is the abstract. Here we aim to give a brief overview of your paper ""Use square root affinity to regress labels in semantic segmentation."" We focus on highlighting the main contributions made by the authors and their significance.  Semantic segmentation is a key task in computer vision that has applications in many fields such as autonomous driving, robotics, medical imaging, and environmental monitoring. One challenging aspect of this problem is creating accurate models capable of labeling each pixel in an image according to one of several predefined classes. In recent years, deep learning methods have been shown to excel at this task due to their ability to capture complex relationships in large datasets. However, existing approaches often suffer from limitations such as slow inference times or limited scalability.  The authors propose using a novel method called Square Root Affinity (SRA) which combines global contextual information with spatially varying local representations in order to improve performance while reducing computational complexity. By applying SRA to the DeepLab model architecture, they achieve state-of-the-art results on three popular benchmark datasets: PASCAL Context, Cityscapes, and CamVid. Furthermore, they demonstrate the effectiveness of their approach on real-world aerial images containing objects such as buildings, roads, vegetation, and water.  Overall, this work shows promise for enabling faster and more accurate semantic segmentation in a variety of applications where time and accuracy are critical factors. While future research may lead to even better results, the use of SRA represents an important step forward in advancing the field of deep learning for image classification tasks.",1
"Features representation leverages the great power in network analysis tasks. However, most features are discrete which poses tremendous challenges to effective use. Recently, increasing attention has been paid on network feature learning, which could map discrete features to continued space. Unfortunately, current studies fail to fully preserve the structural information in the feature space due to random negative sampling strategy during training. To tackle this problem, we study the problem of feature learning and novelty propose a force-based graph learning model named GForce inspired by the spring-electrical model. GForce assumes that nodes are in attractive forces and repulsive forces, thus leading to the same representation with the original structural information in feature learning. Comprehensive experiments on benchmark datasets demonstrate the effectiveness of the proposed framework. Furthermore, GForce opens up opportunities to use physics models to model node interaction for graph learning.",0
"Machine learning has been revolutionized by deep neural networks, but these models still have limitations due to their reliance on manual feature engineering. In contrast, graph signal processing has emerged as a powerful tool for analyzing graph data without requiring handcrafted features. However, current methods only consider simple linear smoothness priors that canâ€™t capture complex nonlinear patterns. To address this gap, we propose Graph Force Learning (GFL), a novel approach for processing graph signals using physics-based principles. Our method combines the power of high-dimensional geometry and classical mechanics to estimate node attributes from noisy, incomplete, or partial measurements. We show that GFL outperforms state-of-the-art baselines across diverse application domains including social network analysis, brain imaging studies, and biological systems modeling. Furthermore, our framework provides interpretable results by identifying key influential nodes driving the predictions. Overall, GFL bridges the gap between machine learning, graph theory, and applied mathematics; paving the way towards unlocking more sophisticated models for real-world applications on graphs.",1
"Graph neural networks are a versatile machine learning architecture that received a lot of attention recently. In this technical report, we present an implementation of convolution and pooling layers for TensorFlow-Keras models, which allows a seamless and flexible integration into standard Keras layers to set up graph models in a functional way. This implies the usage of mini-batches as the first tensor dimension, which can be realized via the new RaggedTensor class of TensorFlow best suited for graphs. We developed the Keras Graph Convolutional Neural Network Python package kgcnn based on TensorFlow-Keras that provides a set of Keras layers for graph networks which focus on a transparent tensor structure passed between layers and an ease-of-use mindset.",0
"Graph Neural Networks (GNNs) have emerged as powerful tools for analyzing complex data on graphs, such as social networks, protein interactions, and transportation systems. In this work, we demonstrate how to implement GNNs using TensorFlow-Keras, one of the most popular deep learning libraries. We provide clear examples and code snippets to guide users through each step of the process, from importing the necessary libraries to training and evaluating their models. Our implementation focuses on two widely used variants of GNNs: Graph Convolutional Networks and Recurrent Graph Neural Networks. We showcase how these models can learn meaningful representations by processing nonlinear node features and capturing interdependencies among nodes. Finally, we evaluate our implementations on several benchmark datasets, demonstrating their effectiveness in predictive modeling tasks. Overall, our work provides practitioners with a comprehensive resource for implementing GNNs using TensorFlow-Keras, facilitating further research and application development in this rapidly evolving field.",1
"Scan data of urban environments often include representations of dynamic objects, such as vehicles, pedestrians, and so forth. However, when it comes to constructing a 3D point cloud map with sequential accumulations of the scan data, the dynamic objects often leave unwanted traces in the map. These traces of dynamic objects act as obstacles and thus impede mobile vehicles from achieving good localization and navigation performances. To tackle the problem, this paper presents a novel static map building method called ERASOR, Egocentric RAtio of pSeudo Occupancy-based dynamic object Removal, which is fast and robust to motion ambiguity. Our approach directs its attention to the nature of most dynamic objects in urban environments being inevitably in contact with the ground. Accordingly, we propose the novel concept called pseudo occupancy to express the occupancy of unit space and then discriminate spaces of varying occupancy. Finally, Region-wise Ground Plane Fitting (R-GPF) is adopted to distinguish static points from dynamic points within the candidate bins that potentially contain dynamic points. As experimentally verified on SemanticKITTI, our proposed method yields promising performance against state-of-the-art methods overcoming the limitations of existing ray tracing-based and visibility-based methods.",0
"Here is your abstract:  * * *  The ability to create accurate static 3D point cloud maps has many potential applications including robotics, virtual reality, and autonomous vehicles. However, these maps can become cluttered by dynamic objects that move through them over time, making it difficult to distinguish important features from irrelevant ones. This paper presents a new method called ERASOR (Egocentric Ratio of Pseudo Occupancy-based Dynamic Object Removal) which addresses this issue by using LiDAR data to detect and remove moving objects while preserving stationary features. We first introduce a pseudo occupancy map based on range measurements and laser beam patterns, then use an egocentric ratio to calculate the motion probability of each object in the scene. Finally, we apply clustering techniques to group together objects with similar motion probabilities, allowing us to efficiently separate moving objects from stationary ones. Our results show significant improvements in both mapping accuracy and computational efficiency compared to previous methods. Overall, our approach offers a powerful tool for creating high quality 3D point cloud maps in real world scenarios.",1
"We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in more than 60% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available at https://github.com/google-research/google-research/tree/master/coltran",0
"Machine learning techniques have been widely used for image colorization tasks over recent years due to their effectiveness at solving image processing problems. One popular approach has been the use of convolutional neural networks (CNNs) which have shown impressive results on several benchmark datasets. However, these models can suffer from limitations such as slow inference time and lack of transparency in terms of explainability. Recently, transformer architectures have emerged as alternatives to CNNs in vision tasks that require attention mechanisms such as image classification and object detection. Motivated by this success, we propose a novel model called Colorization Transformer for the task of image colorization. Our model leverages self-attention mechanisms to capture interdependencies among different regions of an input image and generate colors accordingly. We evaluate our approach using three commonly used benchmark datasets and demonstrate state-of-the-art performance while offering efficient inference speeds compared to existing methods. Additionally, ablation studies showcase the importance of each component in our architecture. This work provides insights into how transformers can effectively perform visual perception tasks, paving the way for future research exploring attention-based approaches in computer vision.",1
"Wearable sensor based human activity recognition is a challenging problem due to difficulty in modeling spatial and temporal dependencies of sensor signals. Recognition models in closed-set assumption are forced to yield members of known activity classes as prediction. However, activity recognition models can encounter an unseen activity due to body-worn sensor malfunction or disability of the subject performing the activities. This problem can be addressed through modeling solution according to the assumption of open-set recognition. Hence, the proposed self attention based approach combines data hierarchically from different sensor placements across time to classify closed-set activities and it obtains notable performance improvement over state-of-the-art models on five publicly available datasets. The decoder in this autoencoder architecture incorporates self-attention based feature representations from encoder to detect unseen activity classes in open-set recognition setting. Furthermore, attention maps generated by the hierarchical model demonstrate explainable selection of features in activity recognition. We conduct extensive leave one subject out validation experiments that indicate significantly improved robustness to noise and subject specific variability in body-worn sensor signals. The source code is available at: github.com/saif-mahmud/hierarchical-attention-HAR",0
"This paper proposes a novel deep learning architecture called Hierarchical Self Attention based AutoEncoder (HSAAE) for open set human activity recognition using wearables data. HSAAE utilizes self attention mechanisms at multiple levels, allowing each level to learn different patterns that contribute differently to the overall representation. The encoder component captures temporal dependencies while maintaining spatial context by employing dilated convolutions and layer normalization techniques. Furthermore, an autoencoder reconstruction loss is used to regularize the model during training. Experimental results on real world datasets demonstrate significant improvements over state-of-the-art methods in both closed and open set scenarios. Overall, HSAAE shows promise as a powerful tool for wearable sensor based human activity recognition tasks.",1
"Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ""muscly-supervised"" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.",0
"This research paper presents a novel method for learning visual representations of objects through human interactions. We propose a framework that leverages human feedback to learn meaningful visual representations of complex objects such as hand gestures, facial expressions, and body posture. Our approach utilizes crowdsourced annotations to collect data on how humans perceive these visual cues and uses this information to train deep neural networks to generate more accurate representations of human movements. Through experiments using both simulated and real-world datasets, we demonstrate that our method outperforms state-of-the-art approaches in terms of accuracy and interpretability. Additionally, we showcase how these learned representations can be used in downstream tasks such as action recognition and gesture analysis. Overall, our work shows the potential of integrating human knowledge into machine learning algorithms, leading to more effective and intuitive models of human behavior.",1
"A key functionality of emerging connected autonomous systems such as smart cities, smart transportation systems, and the industrial Internet-of-Things, is the ability to process and learn from data collected at different physical locations. This is increasingly attracting attention under the terms of distributed learning and federated learning. However, in connected autonomous systems, data transfer takes place over communication networks with often limited resources. This paper examines algorithms for communication-efficient learning for linear regression tasks by exploiting the informativeness of the data. The developed algorithms enable a tradeoff between communication and learning with theoretical performance guarantees and efficient practical implementations.",0
"Linear regression is a commonly used statistical modeling method that enables researchers to make predictions based on observed relationships between variables. In many applications, data may be distributed across multiple networked nodes, such as sensors or devices within an IoT system. Existing linear regression methods assume all data are centralized or can be easily aggregated, which limits their applicability in these decentralized settings. This paper presents a new approach for performing linear regression over networks, ensuring communication efficiency while still achieving high accuracy. Our method utilizes local processing at each node combined with careful message passing to ensure that all relevant data are considered during model training. We prove that our algorithm converges faster than previous approaches under mild assumptions, making it well suited for large-scale systems where communication overhead must be minimized. Experiments on real datasets demonstrate the effectiveness of our technique compared to existing methods. Overall, our work provides a valuable tool for applied researchers seeking to perform accurate prediction tasks over distributed data sources.",1
"As the scene information, including objectness and scene type, are important for people with visual impairment, in this work we present a multi-task efficient perception system for the scene parsing and recognition tasks. Building on the compact ResNet backbone, our designed network architecture has two paths with shared parameters. In the structure, the semantic segmentation path integrates fast attention, with the aim of harvesting long-range contextual information in an efficient manner. Simultaneously, the scene recognition path attains the scene type inference by passing the semantic features into semantic-driven attention networks and combining the semantic extracted representations with the RGB extracted representations through a gated attention module. In the experiments, we have verified the systems' accuracy and efficiency on both public datasets and real-world scenes. This system runs on a wearable belt with an Intel RealSense LiDAR camera and an Nvidia Jetson AGX Xavier processor, which can accompany visually impaired people and provide assistive scene information in their navigation tasks.",0
"This paper presents a novel approach to improve navigation assistance for visually impaired individuals using wearable technology. Our framework combines real-time semantic segmentation and scene recognition techniques to accurately identify objects and their contexts in complex environments. We utilize state-of-the-art convolutional neural networks (CNNs) and object detection algorithms to classify scenes and extract relevant features from the surrounding environment. Additionally, we address the challenge of limited computing resources on wearable devices by optimizing our models and implementing efficient inference procedures. Our experimental results demonstrate the effectiveness of our system in detecting and recognizing objects under various lighting conditions and scenarios. Furthermore, we evaluate the performance of our framework in terms of accuracy, speed, and energy consumption compared to other existing methods. Overall, our work significantly enhances assistive technologies for the visually impaired community and provides new possibilities for enhancing their mobility and independence.",1
"Classic computer vision algorithms, instance segmentation, and semantic segmentation can not provide a holistic understanding of the surroundings for the visually impaired. In this paper, we utilize panoptic segmentation to assist the navigation of visually impaired people by offering both things and stuff awareness in the proximity of the visually impaired efficiently. To this end, we propose an efficient Attention module -- Lintention which can model long-range interactions in linear time using linear space. Based on Lintention, we then devise a novel panoptic segmentation model which we term Panoptic Lintention Net. Experiments on the COCO dataset indicate that the Panoptic Lintention Net raises the Panoptic Quality (PQ) from 39.39 to 41.42 with 4.6\% performance gain while only requiring 10\% fewer GFLOPs and 25\% fewer parameters in the semantic branch. Furthermore, a real-world test via our designed compact wearable panoptic segmentation system, indicates that our system based on the Panoptic Lintention Net accomplishes a relatively stable and exceptionally remarkable panoptic segmentation in real-world scenes.",0
This should clearly define all key terms. Any references required?,1
"Person re-identification is a crucial task of identifying pedestrians of interest across multiple surveillance camera views. In person re-identification, a pedestrian is usually represented with features extracted from a rectangular image region that inevitably contains the scene background, which incurs ambiguity to distinguish different pedestrians and degrades the accuracy. To this end, we propose an end-to-end foreground-aware network to discriminate foreground from background by learning a soft mask for person re-identification. In our method, in addition to the pedestrian ID as supervision for foreground, we introduce the camera ID of each pedestrian image for background modeling. The foreground branch and the background branch are optimized collaboratively. By presenting a target attention loss, the pedestrian features extracted from the foreground branch become more insensitive to the backgrounds, which greatly reduces the negative impacts of changing backgrounds on matching an identical across different camera views. Notably, in contrast to existing methods, our approach does not require any additional dataset to train a human landmark detector or a segmentation model for locating the background regions. The experimental results conducted on three challenging datasets, i.e., Market-1501, DukeMTMC-reID, and MSMT17, demonstrate the effectiveness of our approach.",0
"""Person re-identification (ReID) is a challenging task that involves matching images of pedestrians across non-overlapping camera views. In order to achieve accurate ReID results, it is important to address both global contextual features (e.g., clothing color) and local details such as facial expressions. Traditional methods tend to focus on either one of these aspects, but not both simultaneously. This study presents an end-to-end framework called FAME (Foreground-aware Attention Memory Encoding) which effectively combines both types of features by using multi-level attention mechanisms and feature memory encoding techniques. Results from extensive experiments show that our approach outperforms several state-of-the-art methods on two publicly available datasets."" Note: I am assuming that you want a comprehensive yet concise summary of your work without getting into specific technical details. If you need me to provide further assistance please let me know!",1
"Deep reinforcement learning (DRL) has great potential for acquiring the optimal action in complex environments such as games and robot control. However, it is difficult to analyze the decision-making of the agent, i.e., the reasons it selects the action acquired by learning. In this work, we propose Mask-Attention A3C (Mask A3C), which introduces an attention mechanism into Asynchronous Advantage Actor-Critic (A3C), which is an actor-critic-based DRL method, and can analyze the decision-making of an agent in DRL. A3C consists of a feature extractor that extracts features from an image, a policy branch that outputs the policy, and a value branch that outputs the state value. In this method, we focus on the policy and value branches and introduce an attention mechanism into them. The attention mechanism applies a mask processing to the feature maps of each branch using mask-attention that expresses the judgment reason for the policy and state value with a heat map. We visualized mask-attention maps for games on the Atari 2600 and found we could easily analyze the reasons behind an agent's decision-making in various game tasks. Furthermore, experimental results showed that the agent could achieve a higher performance by introducing the attention mechanism.",0
"In recent years, deep reinforcement learning (DRL) has emerged as a powerful tool for solving complex decision making problems in uncertain environments. One key challenge faced by DRL algorithms is the difficulty in interpreting the results and understanding how decisions were made. To address this issue, researchers have proposed incorporating attention mechanisms into actor-critic-based DRL frameworks, which enable agents to focus their attention on specific parts of the input state during decision making. This paper presents an overview of visual explanation methods that leverage attention mechanism in actor-critic-based DRL systems. We discuss several approaches such as gradient-weighted class activation maps (Grad-CAM), guided backpropagation, and pathwise gradient estimation. These techniques can provide insightful explanations of agent behavior and improve human trust and interpretability in deployed applications. Furthermore, we analyze the strengths and weaknesses of each method and compare their effectiveness under different use cases. Our experimental evaluations demonstrate the utility of these methods for explaining agent behavior and gain insights into the underlying reasoning processes. Overall, this work contributes towards developing transparent and explainable artificial intelligence, paving the way for more reliable deployment of DRL systems in real-world settings.",1
"Patient no-shows is a major burden for health centers leading to loss of revenue, increased waiting time and deteriorated health outcome. Developing machine learning (ML) models for the prediction of no -shows could help addressing this important issue. It is crucial to consider fair ML models for no-show prediction in order to ensure equality of opportunity in accessing healthcare services. In this wo rk, we are interested in developing deep learning models for no-show prediction based on tabular data while ensuring fairness properties. Our baseline model, TabNet, uses on attentive feature transforme rs and has shown promising results for tabular data. We propose Fair-TabNet based on representation learning that disentangles predictive from sensitive components. The model is trained to jointly min imize loss functions on no-shows and sensitive variables while ensuring that the sensitive and prediction representations are orthogonal. In the experimental analysis, we used a hospital dataset of 210, 000 appointments collected in 2019. Our preliminary results show that the proposed Fair-TabNet improves the predictive, fairness performance and convergence speed over TabNet for the task of appointment no-show prediction. The comparison with the state-of-the art models for tabular data shows promising results and could be further improved by a better tuning of hyper-parameters.",0
"This study aimed to address the issue of predicting hospital no-show using the tabular representation (TabNet) model and disentangled representation techniques to improve fairness. The prediction accuracy of the traditional TabNet model has been shown to be unsatisfactory for some subgroups, leading to concerns about equity and discrimination. By incorporating disentangled representations into the TabNet model, we sought to create more accurate predictions while reducing bias towards certain groups. Our findings showed that our approach improved both predictive performance and subgroup fairness compared to the original TabNet model. We believe this research provides insights into ways to increase efficiency in healthcare settings without compromising fairness.",1
"In scene understanding, robotics benefit from not only detecting individual scene instances but also from learning their possible interactions. Human-Object Interaction (HOI) Detection infers the action predicate on a human, predicate, object triplet. Contextual information has been found critical in inferring interactions. However, most works only use local features from single human-object pair for inference. Few works have studied the disambiguating contribution of subsidiary relations made available via graph networks. Similarly, few have learned to effectively leverage visual cues along with the intrinsic semantic regularities contained in HOIs. We contribute a dual-graph attention network that effectively aggregates contextual visual, spatial, and semantic information dynamically from primary human-object relations as well as subsidiary relations through attention mechanisms for strong disambiguating power. We achieve comparable results on two benchmarks: V-COCO and HICO-DET. Code is available at \url{https://github.com/birlrobotics/vs-gats}.",0
"This is an interesting study that explores how graph attention networks can improve human object interaction detection. In recent years, there has been growing interest in using visual attention mechanisms to facilitate interactions between humans and objects, which can help improve overall accuracy and efficiency. To address these challenges, researchers propose the use of visual semantic graphs as a representation method for human-object interactions. These graphs capture both spatial and semantic relationships between objects and individuals. By leveraging attentional mechanisms across the graph structure, the proposed model achieves state-of-the-art results on three benchmark datasets, demonstrating improved performance over traditional methods. Overall, this work represents an important step forward in understanding the potential applications of graph attention networks for human-object interaction detection.",1
"An increasing number of machine learning tasks deal with learning representations from set-structured data. Solutions to these problems involve the composition of permutation-equivariant modules (e.g., self-attention, or individual processing via feed-forward neural networks) and permutation-invariant modules (e.g., global average pooling, or pooling by multi-head attention). In this paper, we propose a geometrically-interpretable framework for learning representations from set-structured data, which is rooted in the optimal mass transportation problem. In particular, we treat elements of a set as samples from a probability measure and propose an exact Euclidean embedding for Generalized Sliced Wasserstein (GSW) distances to learn from set-structured data effectively. We evaluate our proposed framework on multiple supervised and unsupervised set learning tasks and demonstrate its superiority over state-of-the-art set representation learning approaches.",0
"This paper presents a new method for set representation learning using generalized sliced-Wasserstein embeddings. We begin by discussing the challenges that arise when working with sets and demonstrate how existing methods fall short in addressing these difficulties. Our proposed approach overcomes these limitations by introducing a flexible framework that can handle high-dimensional data, irregularly spaced samples, and varying set sizes.  Our main contribution lies in our use of the generalized sliced-Wasserstein distance as a metric for comparing sets. Unlike other approaches that rely on pairwise comparisons between elements, our method leverages the distributional nature of sets and enables efficient computation through sampling. Additionally, we develop an optimization algorithm based on alternating directions that scales well with large datasets.  Experimental results show that our method outperforms state-of-the-art techniques across multiple benchmarks, including image classification and shape retrieval. Furthermore, we provide visualizations that illustrate the interpretability of our learned representations, highlighting meaningful features such as object boundaries and textures. \end{code}",1
"In this paper we introduce OperA, a transformer-based model that accurately predicts surgical phases from long video sequences. A novel attention regularization loss encourages the model to focus on high-quality frames during training. Moreover, the attention weights are utilized to identify characteristic high attention frames for each surgical phase, which could further be used for surgery summarization. OperA is thoroughly evaluated on two datasets of laparoscopic cholecystectomy videos, outperforming various state-of-the-art temporal refinement approaches.",0
"The field of surgical phase recognition using deep learning techniques has seen significant progress over recent years. However, accurately recognizing different phases within surgical procedures remains a challenging task due to the variability in appearance and motion across operations. To address these limitations, we propose a novel method called ""OperA"" that utilizes attention mechanisms within transformer networks, enabling efficient capture of local and global contexts during sequence analysis. We validate our approach on two publicly available datasets: JIGSAWS (a dataset containing images from 49 surgical procedures) and STRIKE (an RGB-D video dataset consisting of laparoscopic surgeries). Our results demonstrate improved performance compared to state-of-the-art methods, establishing the effectiveness of attention-regulated transformers in tackling complex surgical scene understanding tasks. The proposed OperA framework has potential applications in numerous areas such as automated surgical workflow modeling, robotic assistance, and real-time guidance systems, further promoting the adoption of Artificial Intelligence in healthcare. Overall, this research represents a step forward towards reliable and accurate automatic recognition of surgical phases, helping enhance patient safety and improve clinical outcomes.",1
"Despite the successes of deep neural networks on many challenging vision tasks, they often fail to generalize to new test domains that are not distributed identically to the training data. The domain adaptation becomes more challenging for cross-modality medical data with a notable domain shift. Given that specific annotated imaging modalities may not be accessible nor complete. Our proposed solution is based on the cross-modality synthesis of medical images to reduce the costly annotation burden by radiologists and bridge the domain gap in radiological images. We present a novel approach for image-to-image translation in medical images, capable of supervised or unsupervised (unpaired image data) setups. Built upon adversarial training, we propose a learnable self-attentive spatial normalization of the deep convolutional generator network's intermediate activations. Unlike previous attention-based image-to-image translation approaches, which are either domain-specific or require distortion of the source domain's structures, we unearth the importance of the auxiliary semantic information to handle the geometric changes and preserve anatomical structures during image translation. We achieve superior results for cross-modality segmentation between unpaired MRI and CT data for multi-modality whole heart and multi-modal brain tumor MRI (T1/T2) datasets compared to the state-of-the-art methods. We also observe encouraging results in cross-modality conversion for paired MRI and CT images on a brain dataset. Furthermore, a detailed analysis of the cross-modality image translation, thorough ablation studies confirm our proposed method's efficacy.",0
"In recent years, cross-modality domain adaptation has become an increasingly important research area in computer vision due to the need for algorithms that can effectively handle differences in data distributions across different modalities such as images and videos. While there have been many advances in this field, current methods still suffer from limited performance due to the difficulty of accurately modeling the complex relationship between source and target domains. This paper proposes a novel approach called Self-Attentive Spatial Adaptive Normalization (SASAN), which leverages self-attention mechanisms to adaptively normalize feature representations based on their spatial relationships within each modality. Experimental results demonstrate that SASAN significantly outperforms state-of-the-art cross-modality domain adaptation techniques, achieving improved accuracy and robustness across a range of challenging tasks. Our work offers new insights into the importance of attention mechanisms for effective domain adaptation and opens up exciting opportunities for future research in this rapidly evolving area.",1
"Most datasets of interest to the analytics industry are impacted by various forms of human bias. The outcomes of Data Analytics [DA] or Machine Learning [ML] on such data are therefore prone to replicating the bias. As a result, a large number of biased decision-making systems based on DA/ML have recently attracted attention. In this paper we introduce Rosa, a free, web-based tool to easily de-bias datasets with respect to a chosen characteristic. Rosa is based on the principles of Fair Adversarial Networks, developed by illumr Ltd., and can therefore remove interactive, non-linear, and non-binary bias. Rosa is stand-alone pre-processing step / API, meaning it can be used easily with any DA/ML pipeline. We test the efficacy of Rosa in removing bias from data-driven decision making systems by performing standard DA tasks on five real-world datasets, selected for their relevance to current DA problems, and also their high potential for bias. We use simple ML models to model a characteristic of analytical interest, and compare the level of bias in the model output both with and without Rosa as a pre-processing step. We find that in all cases there is a substantial decrease in bias of the data-driven decision making systems when the data is pre-processed with Rosa.",0
"This paper presents Rosa, a new tool designed to ensure fairness in data analytics pipelines. Rosa works by automatically detecting and mitigating bias in machine learning models, allowing users to create accurate predictions without perpetuating harmful discrimination. Our approach utilizes state-of-the-art techniques from causal inference and algorithmic fairness to assess and modify models in real time. We demonstrate the effectiveness of Rosa through comprehensive experiments on three datasets commonly used in research studies and industry applications. Overall, our results show that Rosa significantly improves model fairness while maintaining high levels of accuracy. With Rosa, we provide a scalable and efficient solution for organizations seeking to build ethical and equitable decision-making systems.",1
"Not all supervised learning problems are described by a pair of a fixed-size input tensor and a label. In some cases, especially in medical image analysis, a label corresponds to a bag of instances (e.g. image patches), and to classify such bag, aggregation of information from all of the instances is needed. There have been several attempts to create a model working with a bag of instances, however, they are assuming that there are no dependencies within the bag and the label is connected to at least one instance. In this work, we introduce Self-Attention Attention-based MIL Pooling (SA-AbMILP) aggregation operation to account for the dependencies between instances. We conduct several experiments on MNIST, histological, microbiological, and retinal databases to show that SA-AbMILP performs better than other models. Additionally, we investigate kernel variations of Self-Attention and their influence on the results.",0
"In multiple instance learning (MIL), each example consists of a bag containing several instances, where only one instance is labeled positive while others are considered negative. This framework has been widely used in medical image analysis tasks due to the high imbalance between positive and negative examples. To solve MIL problems, convolutional neural networks have been applied by exploiting spatial relationships among local features within each bag using max pooling or average pooling operations to obtain global representations. However, these operations tend to lose important interdependencies across different instances in a single bag, leading to suboptimal performance.  To address this issue, we propose kernel self-attention as a new mechanism that considers interdependencies among all instances within each bag. We firstly represent instances at feature maps level and then apply kernels to capture dependencies between instances. Afterward, self-attention layers are utilized to learn discriminative attention weights among all channels and positions. With channel self-attention, our model can weight the importance of different channels, while position self-attention focuses on specific regions where informative patterns reside. By employing a stacked architecture composed of channel and position attention modules, the proposed method can effectively learn nonlinear relationships between instances and achieve superior performance compared to existing methods. Experiments conducted on three benchmark datasets demonstrate that our method achieves state-of-the-art results.",1
"Wrist Fracture is the most common type of fracture with a high incidence rate. Conventional radiography (i.e. X-ray imaging) is used for wrist fracture detection routinely, but occasionally fracture delineation poses issues and an additional confirmation by computed tomography (CT) is needed for diagnosis. Recent advances in the field of Deep Learning (DL), a subfield of Artificial Intelligence (AI), have shown that wrist fracture detection can be automated using Convolutional Neural Networks. However, previous studies did not pay close attention to the difficult cases which can only be confirmed via CT imaging. In this study, we have developed and analyzed a state-of-the-art DL-based pipeline for wrist (distal radius) fracture detection -- DeepWrist, and evaluated it against one general population test set, and one challenging test set comprising only cases requiring confirmation by CT. Our results reveal that a typical state-of-the-art approach, such as DeepWrist, while having a near-perfect performance on the general independent test set, has a substantially lower performance on the challenging test set -- average precision of 0.99 (0.99-0.99) vs 0.64 (0.46-0.83), respectively. Similarly, the area under the ROC curve was of 0.99 (0.98-0.99) vs 0.84 (0.72-0.93), respectively. Our findings highlight the importance of a meticulous analysis of DL-based models before clinical use, and unearth the need for more challenging settings for testing medical AI systems.",0
"This paper presents an evaluation of deep neural networks (DNN) to detect wrist fractures from X-ray images. By training several DNN models on large datasets, we explore how different architectures and hyperparameters affect performance. We find that some models outperform traditional manual diagnosis, while others may perform poorly due to limitations such as sensitivity or specificity tradeoffs. Our results highlight areas where future research can improve patient care through enhanced diagnostic tools driven by artificial intelligence.",1
"Convolutional neural networks have enabled major progress in addressing pixel-level prediction tasks such as semantic segmentation, depth estimation, surface normal prediction, and so on, benefiting from their powerful capabilities in visual representation learning. Typically, state-of-the-art models integrates attention mechanisms for improved deep feature representations. Recently, some works have demonstrated the significance of learning and combining both spatial- and channel-wise attentions for deep feature refinement. In this paper, we aim at effectively boosting previous approaches and propose a unified deep framework to jointly learn both spatial attention maps and channel attention vectors in a principled manner so as to structure the resulting attention tensors and model interactions between these two types of attentions. Specifically, we integrate the estimation and the interaction of the attentions within a probabilistic representation learning framework, leading to Variational STructured Attention networks (VISTA-Net). We implement the inference rules within the neural network, thus allowing for end-to-end learning of the probabilistic and the CNN front-end parameters. As demonstrated by our extensive empirical evaluation on six large-scale datasets for dense visual prediction, VISTA-Net outperforms the state-of-the-art in multiple continuous and discrete prediction tasks, thus confirming the benefit of the proposed approach in joint structured spatial-channel attention estimation for deep representation learning. The code is available at https://github.com/ygjwd12345/VISTA-Net.",0
"This paper presents variational structured attention networks (ViSAN), which aim at learning deep visual representations by incorporating both local affinity graphs and global image context into structured attention models. In contrast to conventional methods that mainly rely on convolutional neural networks (CNNs) with predefined kernels or learn fixed spatial attentional weights across instances, ViSAN can adaptively select discriminative regions at different scales based on both instance-specific features and semantic categories. To achieve this goal, we introduce two main components: an overcomplete set of region proposal generators parameterized as Gaussian mixtures, and a probabilistic graphical model that captures dependencies among proposals and their associated high-level attributes. We train our model through a variational inference scheme that optimizes jointly over parameters of CNN features and latent variables specifying learned attention distributions. Our evaluation shows that ViSAN improves state-of-the-art results on several benchmark datasets, such as ImageNet classification, Pascal VOC detection, and MS COCO object segmentation, demonstrating the effectiveness of its deep representation learning framework under complex realworld scenarios. Overall, our research offers new insights into designing versatile architectures tailored toward specific tasks while opening up opportunities for future progress in computer vision using deep generative models.",1
"We present a novel attention mechanism: Causal Attention (CATT), to remove the ever-elusive confounding effect in existing attention-based vision-language models. This effect causes harmful bias that misleads the attention module to focus on the spurious correlations in training data, damaging the model generalization. As the confounder is unobserved in general, we use the front-door adjustment to realize the causal intervention, which does not require any knowledge on the confounder. Specifically, CATT is implemented as a combination of 1) In-Sample Attention (IS-ATT) and 2) Cross-Sample Attention (CS-ATT), where the latter forcibly brings other samples into every IS-ATT, mimicking the causal intervention. CATT abides by the Q-K-V convention and hence can replace any attention module such as top-down attention and self-attention in Transformers. CATT improves various popular attention-based vision-language models by considerable margins. In particular, we show that CATT has great potential in large-scale pre-training, e.g., it can promote the lighter LXMERT~\cite{tan2019lxmert}, which uses fewer data and less computational power, comparable to the heavier UNITER~\cite{chen2020uniter}. Code is published in \url{https://github.com/yangxuntu/catt}.",0
"Title: ""Causal Attention for Vision-Language Tasks""  Abstract: This paper presents a novel method called causal attention that enables efficient and effective multimodal fusion for vision-language tasks such as visual question answering (VQA) and image generation from textual descriptions. Despite recent advances in these areas, there remains a challenge in effectively combining the diverse and complex data modalities involved in these tasks. Our approach addresses this issue by incorporating directional dependencies across modality pairs while adapting pre-trained models, resulting in improved accuracy over state-of-the-art methods. Furthermore, we demonstrate our framework's applicability to other vision-based tasks beyond those considered herein, highlighting its generalizability to a broader range of applications.  Our contributions can be summarized as follows: First, we propose the use of causal attention, a technique inspired by Granger causality, which captures directional influences among variables. By leveraging such connections within modality pairs, we enhance cross-modal alignment without resorting to additional constraints during model training. Secondly, our approach applies causal attention on top of several popular transfer learning frameworks, including feature extraction fine-tuning and full model finetuning with adapter heads. We experimentally verify the effectiveness of our approach through rigorous evaluation against strong baselines and establish new benchmark records for both VQA and image generation tasks using two large datasets - COCO and Conceptual Captions. Lastly, we showcase the robustness of our methodology by testing its performance across alternative datasets and tasks. Altogether, our study underscores the importance of incorporating meaningful connections amidst modality interactions for achieving improved results in vision-language domains.",1
"3D object detection from a single image is an important task in Autonomous Driving (AD), where various approaches have been proposed. However, the task is intrinsically ambiguous and challenging as single image depth estimation is already an ill-posed problem. In this paper, we propose an instance-aware approach to aggregate useful information for improving the accuracy of 3D object detection with the following contributions. First, an instance-aware feature aggregation (IAFA) module is proposed to collect local and global features for 3D bounding boxes regression. Second, we empirically find that the spatial attention module can be well learned by taking coarse-level instance annotations as a supervision signal. The proposed module has significantly boosted the performance of the baseline method on both 3D detection and 2D bird-eye's view of vehicle detection among all three categories. Third, our proposed method outperforms all single image-based approaches (even these methods trained with depth as auxiliary inputs) and achieves state-of-the-art 3D detection performance on the KITTI benchmark.",0
"This work proposes a novel feature aggregation method based on instance segmentation, called IAFA (Instance-Aware Feature Aggregation), which effectively boosts object detection accuracy by leveraging rich information hidden inside each instance. Compared to previous global context methods relying solely on scene-level features, IAFA explicitly models intra-class relationships among local regions within each object via self-attention modules, enabling more fine-grained feature representation tailored to objects with diverse shapes/scale/aspect ratios/etc. Extensive experiments demonstrate that our IAFA consistently improves baseline detectors with significant margins across three benchmark datasets, proving great versatility and strong adaptability under different settings. Our contributions can largely bridge the gap between current holistic schemes lacking discriminative power and dedicated region-based methods computationally prohibitive at scale. By seamlessly integrating both global and local cues into feature learning, we open new research opportunities towards efficient real-world deployments of accurate large-scale 3D object detection from single images.",1
"Zero-shot learning (ZSL) aims to recognize novel classes by transferring semantic knowledge from seen classes to unseen classes. Since semantic knowledge is built on attributes shared between different classes, which are highly local, strong prior for localization of object attribute is beneficial for visual-semantic embedding. Interestingly, when recognizing unseen images, human would also automatically gaze at regions with certain semantic clue. Therefore, we introduce a novel goal-oriented gaze estimation module (GEM) to improve the discriminative attribute localization based on the class-level attributes for ZSL. We aim to predict the actual human gaze location to get the visual attention regions for recognizing a novel object guided by attribute description. Specifically, the task-dependent attention is learned with the goal-oriented GEM, and the global image features are simultaneously optimized with the regression of local attribute features. Experiments on three ZSL benchmarks, i.e., CUB, SUN and AWA2, show the superiority or competitiveness of our proposed method against the state-of-the-art ZSL methods. The ablation analysis on real gaze data CUB-VWSW also validates the benefits and accuracy of our gaze estimation module. This work implies the promising benefits of collecting human gaze dataset and automatic gaze estimation algorithms on high-level computer vision tasks. The code is available at https://github.com/osierboy/GEM-ZSL.",0
"In recent years, zero-shot learning has emerged as a promising approach to addressing one of the greatest challenges faced by artificial intelligence (AI): how to teach machines to perform tasks they have never seen before. One of the key problems in applying zero-shot learning to real-world scenarios is accurate gaze estimation, which involves predicting where someone is looking. This paper proposes a novel method for goal-oriented gaze estimation using zero-shot learning that significantly improves upon existing approaches. Our proposed method uses a convolutional neural network (CNN) trained on large datasets of human eye movements to learn patterns of attention allocation. We evaluate our method on several benchmark datasets and demonstrate significant improvements over state-of-the-art methods. Furthermore, we show that our method can generalize well across different domains and tasks, making it applicable to a wide range of applications such as human-computer interaction, robotics, and social sciences. Overall, our work represents a major step forward in understanding how humans allocate their attention and provides a powerful tool for developing more intelligent systems that interact naturally with humans.",1
"Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards ""token uniformity"". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.",0
"This paper investigates the effectiveness of pure attention mechanisms in deep neural networks for natural language processing tasks. While attention has been shown to improve performance in many models, we find that as depth increases, the contribution of attention decays exponentially. We observe that at deeper layers, the impact of attention becomes negligible compared to feedforward connections. To validate our finding, we conduct experiments on several benchmark datasets using a variety of architectures, including Transformer, BERT, GPT-2, and T5. Our results indicate that while attention can enhance performance in shallow models, it quickly loses its importance in deeply nested structures, even when the model size is increased. These findings have important implications for future research on attention mechanisms, suggesting that a more comprehensive understanding of how attention interacts with other components within deep learning models is necessary. Furthermore, they raise questions about whether attention alone is sufficient to capture all relevant dependencies in high-dimensional input spaces like natural languages, pointing towards potential directions for future work in NLP.",1
"The attention mechanism provides a sequential prediction framework for learning spatial models with enhanced implicit temporal consistency. In this work, we show a systematic design (from 2D to 3D) for how conventional networks and other forms of constraints can be incorporated into the attention framework for learning long-range dependencies for the task of pose estimation. The contribution of this paper is to provide a systematic approach for designing and training of attention-based models for the end-to-end pose estimation, with the flexibility and scalability of arbitrary video sequences as input. We achieve this by adapting temporal receptive field via a multi-scale structure of dilated convolutions. Besides, the proposed architecture can be easily adapted to a causal model enabling real-time performance. Any off-the-shelf 2D pose estimation systems, e.g. Mocap libraries, can be easily integrated in an ad-hoc fashion. Our method achieves the state-of-the-art performance and outperforms existing methods by reducing the mean per joint position error to 33.4 mm on Human3.6M dataset.",0
"In recent years, there has been significant progress in the field of human pose estimation from videos, driven largely by advances in deep learning techniques such as convolutional neural networks (CNNs). Despite these achievements, challenges still exist due to complex backgrounds and varying lighting conditions that can make it difficult for algorithms to accurately estimate body poses. To address these limitations, we propose an attention-based approach utilizing dilated CNNs, which enables our method to effectively focus on regions containing human bodies while disregarding irrelevant areas within the frame. Our network architecture integrates multi-scale representations through atrous spatial pyramid pooling, allowing it to capture both fine details and global context. Experiments on several benchmark datasets demonstrate substantial improvements over state-of-the-art methods in terms of accuracy and efficiency. This work represents an important step towards real-time human pose estimation in unconstrained environments.",1
"Convolutional neural networks (CNNs) have been the de facto standard for nowadays 3D medical image segmentation. The convolutional operations used in these networks, however, inevitably have limitations in modeling the long-range dependency due to their inductive bias of locality and weight sharing. Although Transformer was born to address this issue, it suffers from extreme computational and spatial complexities in processing high-resolution 3D feature maps. In this paper, we propose a novel framework that efficiently bridges a {\bf Co}nvolutional neural network and a {\bf Tr}ansformer {\bf (CoTr)} for accurate 3D medical image segmentation. Under this framework, the CNN is constructed to extract feature representations and an efficient deformable Transformer (DeTrans) is built to model the long-range dependency on the extracted feature maps. Different from the vanilla Transformer which treats all image positions equally, our DeTrans pays attention only to a small set of key positions by introducing the deformable self-attention mechanism. Thus, the computational and spatial complexities of DeTrans have been greatly reduced, making it possible to process the multi-scale and high-resolution feature maps, which are usually of paramount importance for image segmentation. We conduct an extensive evaluation on the Multi-Atlas Labeling Beyond the Cranial Vault (BCV) dataset that covers 11 major human organs. The results indicate that our CoTr leads to a substantial performance improvement over other CNN-based, transformer-based, and hybrid methods on the 3D multi-organ segmentation task. Code is available at \def\UrlFont{\rm\small\ttfamily} \url{https://github.com/YtongXie/CoTr}",0
"This paper proposes CoTr (CNN+Transformer), a novel deep learning architecture that efficiently combines Convolutional Neural Networks (CNNs) and Transformers to achieve state-of-the-art performance on 3D medical image segmentation tasks. Our approach leverages the strengths of both architectures while mitigating their respective weaknesses. Specifically, we introduce several innovations such as feature pyramid cross attention, residual connection and hierarchical refinement modules to effectively integrate CNNs and transformers into one model. We evaluate our method on three challenging 3D imaging benchmark datasets - MSD, Hippocampus and WMH, achieving superior results over strong baselines across all metrics. Our proposed approach paves the way towards efficient integration of complementary models without losing accuracy or efficiency. Overall, CoTr represents an important step forward in bridging CNN and Transformer networks for effective 3D medical image segmentation.",1
"Recent studies on mobile network design have demonstrated the remarkable effectiveness of channel attention (e.g., the Squeeze-and-Excitation attention) for lifting model performance, but they generally neglect the positional information, which is important for generating spatially selective attention maps. In this paper, we propose a novel attention mechanism for mobile networks by embedding positional information into channel attention, which we call ""coordinate attention"". Unlike channel attention that transforms a feature tensor to a single feature vector via 2D global pooling, the coordinate attention factorizes channel attention into two 1D feature encoding processes that aggregate features along the two spatial directions, respectively. In this way, long-range dependencies can be captured along one spatial direction and meanwhile precise positional information can be preserved along the other spatial direction. The resulting feature maps are then encoded separately into a pair of direction-aware and position-sensitive attention maps that can be complementarily applied to the input feature map to augment the representations of the objects of interest. Our coordinate attention is simple and can be flexibly plugged into classic mobile networks, such as MobileNetV2, MobileNeXt, and EfficientNet with nearly no computational overhead. Extensive experiments demonstrate that our coordinate attention is not only beneficial to ImageNet classification but more interestingly, behaves better in down-stream tasks, such as object detection and semantic segmentation. Code is available at https://github.com/Andrew-Qibin/CoordAttention.",0
"This paper presents a new approach to mobile network design that leverages coordination among base stations (BTSs) to improve efficiency and reduce resource consumption. We propose a novel algorithm called Coordinate Attention (CA), which utilizes channel state information (CSI) to dynamically adjust BTS transmissions based on real-time channel conditions. Our results demonstrate that CA outperforms traditional approaches by up to 20% in terms of spectral efficiency, while significantly reducing power consumption and interference levels. Furthermore, we show that our method can be implemented using existing infrastructure without requiring significant hardware modifications. Overall, our work represents a promising step towards more efficient and sustainable wireless communication systems.",1
"In this paper, we propose a novel person Re-ID model, Consecutive Batch DropBlock Network (CBDB-Net), to capture the attentive and robust person descriptor for the person Re-ID task. The CBDB-Net contains two novel designs: the Consecutive Batch DropBlock Module (CBDBM) and the Elastic Loss (EL). In the Consecutive Batch DropBlock Module (CBDBM), we firstly conduct uniform partition on the feature maps. And then, we independently and continuously drop each patch from top to bottom on the feature maps, which can output multiple incomplete feature maps. In the training stage, these multiple incomplete features can better encourage the Re-ID model to capture the robust person descriptor for the Re-ID task. In the Elastic Loss (EL), we design a novel weight control item to help the Re-ID model adaptively balance hard sample pairs and easy sample pairs in the whole training process. Through an extensive set of ablation studies, we verify that the Consecutive Batch DropBlock Module (CBDBM) and the Elastic Loss (EL) each contribute to the performance boosts of CBDB-Net. We demonstrate that our CBDB-Net can achieve the competitive performance on the three standard person Re-ID datasets (the Market-1501, the DukeMTMC-Re-ID, and the CUHK03 dataset), three occluded Person Re-ID datasets (the Occluded DukeMTMC, the Partial-REID, and the Partial iLIDS dataset), and a general image retrieval dataset (In-Shop Clothes Retrieval dataset).",0
"Title: Incomplete Descriptor Mining with Elastic Loss for Person Re-Identification  Abstract:  Person re-identification (ReID) is a challenging task that involves matching pedestrians across non-overlapping camera views. One critical component of state-of-the-art approaches is feature extraction using descriptors such as HoG, SIFT, and DenseVAEs. These descriptors encode high-level representations of local image patterns, but their quality is limited by factors like occlusion, variations in viewpoint, lighting conditions, pose changes, etc. To address these issues, we present a novel framework called ""Incomplete Descriptor Mining with Elastic Loss"" for unsupervised learning and mining of incomplete descriptors from noisy data distributions. We showcase the benefits of our method over existing supervised state-of-the-art approaches under varying degrees of label noise by evaluating on several public datasets, including Market-1501, DukeMTMC-ReId, CUHK03, and MSMT17. Additionally, we analyze ablation studies and demonstrate how different aspects of the proposed model affect the overall performance gain. Our findings contribute towards a deeper understanding of the role of descriptors in person ReID tasks and provide insights into the design of more robust models capable of operating effectively with imperfect training data.",1
"Existing state-of-the-art disparity estimation works mostly leverage the 4D concatenation volume and construct a very deep 3D convolution neural network (CNN) for disparity regression, which is inefficient due to the high memory consumption and slow inference speed. In this paper, we propose a network named EDNet for efficient disparity estimation. Firstly, we construct a combined volume which incorporates contextual information from the squeezed concatenation volume and feature similarity measurement from the correlation volume. The combined volume can be next aggregated by 2D convolutions which are faster and require less memory than 3D convolutions. Secondly, we propose an attention-based spatial residual module to generate attention-aware residual features. The attention mechanism is applied to provide intuitive spatial evidence about inaccurate regions with the help of error maps at multiple scales and thus improve the residual learning efficiency. Extensive experiments on the Scene Flow and KITTI datasets show that EDNet outperforms the previous 3D CNN based works and achieves state-of-the-art performance with significantly faster speed and less memory consumption.",0
"This paper presents a novel network architecture called EDNet for efficient disparity estimation from a single image pair using deep convolutional neural networks (CNNs). EDNet utilizes a cost volume combination approach that combines multiple scaled bilateral difference volumes into a unified representation. We introduce an attention mechanism based on gradient magnitude similarity to selectively weight each contribution according to their relevance to the task at hand. In addition, we propose a spatial residual refinement module that takes advantage of both local and global context through multi-scale feature concatenation and densely connected spatial pyramidal processing. Our method achieves state-of-the-art performance on several benchmark datasets while being computationally more efficient than previous approaches. Additionally, our network can be trained end-to-end without any postprocessing steps or regularization techniques commonly used in similar methods. The code and pre-trained models will be made publicly available upon publication, enabling further research and applications in computer vision.",1
"Person re-identification (ReID) aims at searching the same identity person among images captured by various cameras. Unsupervised person ReID attracts a lot of attention recently, due to it works without intensive manual annotation and thus shows great potential of adapting to new conditions. Representation learning plays a critical role in unsupervised person ReID. In this work, we propose a novel selective contrastive learning framework for unsupervised feature learning. Specifically, different from traditional contrastive learning strategies, we propose to use multiple positives and adaptively sampled negatives for defining the contrastive loss, enabling to learn a feature embedding model with stronger identity discriminative representation. Moreover, we propose to jointly leverage global and local features to construct three dynamic dictionaries, among which the global and local memory banks are used for pairwise similarity computation and the mixture memory bank are used for contrastive loss definition. Experimental results demonstrate the superiority of our method in unsupervised person ReID compared with the state-of-the-arts.",0
"This paper presents a new approach to person re-identification using fully unsupervised learning methods. By leveraging selective contrastive learning techniques, we can learn discriminative features that effectively distinguish between different individuals across disjoint camera views. Our method outperforms state-of-the-art unsupervised algorithms on challenging benchmarks, while requiring no annotations or labels during training. We believe our work shows great potential for advancing real-world applications such as security surveillance and personal photo management.",1
"Locating discriminative parts plays a key role in fine-grained visual classification due to the high similarities between different objects. Recent works based on convolutional neural networks utilize the feature maps taken from the last convolutional layer to mine discriminative regions. However, the last convolutional layer tends to focus on the whole object due to the large receptive field, which leads to a reduced ability to spot the differences. To address this issue, we propose a novel Granularity-Aware Convolutional Neural Network (GA-CNN) that progressively explores discriminative features. Specifically, GA-CNN utilizes the differences of the receptive fields at different layers to learn multi-granularity features, and it exploits larger granularity information based on the smaller granularity information found at the previous stages. To further boost the performance, we introduce an object-attentive module that can effectively localize the object given a raw image. GA-CNN does not need bounding boxes/part annotations and can be trained end-to-end. Extensive experimental results show that our approach achieves state-of-the-art performances on three benchmark datasets.",0
"In recent years, convolutional neural networks (CNNs) have emerged as powerful tools for image classification tasks due to their ability to learn hierarchical representations from raw images. However, these models often suffer from coarse feature extraction at early layers which may lead to limited representation capacity and hamper performance on fine-grained visual classification problems such as bird species recognition or car make/model identification. To address these challenges, we propose a novel framework called granularity-aware CNN (GCA), which learns multi-granular features using progressive dilations and fuses them adaptively across different resolution levels. Our GCA model outperforms state-of-the-art methods on three popular benchmark datasets, achieving significant improvements over existing techniques while providing clear insights into how different granularities contribute to final predictions. We demonstrate that our approach effectively captures high-level semantic abstractions and low-level details simultaneously through a unique adaptation strategy based on dilation rates and fusion weights. Overall, our work highlights the potential benefits of learning granularity-aware representations for improved performance on difficult fine-grained visual classification problems.",1
"Computing power, big data, and advancement of algorithms have led to a renewed interest in artificial intelligence (AI), especially in deep learning (DL). The success of DL largely lies on data representation because different representations can indicate to a degree the different explanatory factors of variation behind the data. In the last few year, the most successful story in DL is supervised learning. However, to apply supervised learning, one challenge is that data labels are expensive to get, noisy, or only partially available. With consideration that we human beings learn in an unsupervised way; self-supervised learning methods have garnered a lot of attention recently. A dominant force in self-supervised learning is the autoencoder, which has multiple uses (e.g., data representation, anomaly detection, denoise). This research explored the application of an autoencoder to learn effective data representation of helicopter flight track data, and then to support helicopter track identification. Our testing results are promising. For example, at Phoenix Deer Valley (DVT) airport, where 70% of recorded flight tracks have missing aircraft types, the autoencoder can help to identify twenty-two times more helicopters than otherwise detectable using rule-based methods; for Grand Canyon West Airport (1G4) airport, the autoencoder can identify thirteen times more helicopters than a current rule-based approach. Our approach can also identify mislabeled aircraft types in the flight track data and find true types for records with pseudo aircraft type labels such as HELO. With improved labelling, studies using these data sets can produce more reliable results.",0
"This sounds like a great project! Here's a possible abstract:  Recently there has been growing interest in using artificial intelligence (AI) techniques such as autoencoders for image classification tasks, particularly those involving complex images such as aerial photos from helicopters. In this study we explore the use of an autoencoder-based approach to identify objects in photos taken by a helicopter camera. Our method uses deep learning techniques to analyze large amounts of data and make predictions based on patterns that emerge within the training set. We evaluate our model's performance against several metrics including accuracy, precision, recall, F1 score, and receiver operating characteristic (ROC) curve analysis, demonstrating that our approach achieves competitive results compared to state-of-the-art models. This work represents a significant contribution to the field of computer vision and highlights the potential applications of AI in real-world settings such as security monitoring and surveillance. Overall, these findings have important implications for understanding how machine learning can improve automation in complex domains where human operators may face challenges in analyzing large volumes of visual data.",1
"While frame-independent predictions with deep neural networks have become the prominent solutions to many computer vision tasks, the potential benefits of utilizing correlations between frames have received less attention. Even though probabilistic machine learning provides the ability to encode correlation as prior knowledge for inference, there is a tangible gap between the theory and practice of applying probabilistic methods to modern vision problems. For this, we derive a principled framework to combine information coupling between camera poses (translation and orientation) with deep models. We proposed a novel view kernel that generalizes the standard periodic kernel in $\mathrm{SO}(3)$. We show how this soft-prior knowledge can aid several pose-related vision tasks like novel view synthesis and predict arbitrary points in the latent space of generative models, pointing towards a range of new applications for inter-frame reasoning.",0
"Gaussian process priors have recently become increasingly popular as a flexible framework for nonparametric inference. This approach allows one to specify complex prior distributions that encode our beliefs about underlying functions based on observed data. However, classical Gaussian process approaches tend to assume that all observations are equally informative, which can lead to suboptimal results when dealing with datasets containing significant variability across different views or features. We present a novel view-aware model that incorporates information from multiple views into a single Gaussian process prior by considering each view separately as well as jointly. Experiments demonstrate that our method significantly improves estimation accuracy compared to standard methods across several applications including regression, classification, and sensor network localization tasks. Furthermore, we show how our view-specific priors naturally generalize previous work on multi-task learning through the lens of Gaussian processes. Overall, our contribution shows promise towards enhancing existing probabilistic machine learning frameworks with more powerful tools capable of exploiting interrelated sources of information across varying dimensions.",1
"Panoptic segmentation unifies semantic segmentation and instance segmentation which has been attracting increasing attention in recent years. However, most existing research was conducted under a supervised learning setup whereas unsupervised domain adaptive panoptic segmentation which is critical in different tasks and applications is largely neglected. We design a domain adaptive panoptic segmentation network that exploits inter-style consistency and inter-task regularization for optimal domain adaptive panoptic segmentation. The inter-style consistency leverages geometric invariance across the same image of the different styles which fabricates certain self-supervisions to guide the network to learn domain-invariant features. The inter-task regularization exploits the complementary nature of instance segmentation and semantic segmentation and uses it as a constraint for better feature alignment across domains. Extensive experiments over multiple domain adaptive panoptic segmentation tasks (e.g., synthetic-to-real and real-to-real) show that our proposed network achieves superior segmentation performance as compared with the state-of-the-art.",0
"In recent years there has been growing interest in domain adaptive panoptic segmentation (DAPS) due to the large number of potential applications that can benefit from such technology. However, one major challenge faced by researchers working on DAPS is ensuring consistent performance across multiple domains. Cross-view regularization (CVR) provides a promising approach for addressing this issue. By imposing constraints on the model predictions based on both source data and target data during training, CVR enables models to learn more generalizable representations that perform well across different domains. Our proposed method, Cross-View Regularization for Domain Adaptive Panoptic Segmentation (CVR-DAPS), leverages this technique to significantly improve DAPS accuracy while reducing model complexity. We demonstrate through extensive experimental evaluation that our method outperforms several state-of-the-art baselines on challenging benchmark datasets including VPS, PascalVOC2012, COCO, Mapillary, Cityscapes, KITTI, and SUN RGB-D. We believe our work represents an important step forward towards enabling reliable cross-domain visual perception systems with practical impact in areas such as autonomous vehicles, robotics, computer vision analytics, medical imaging, AR/VR, and geospatial analysis.",1
"We present a neural optimization model trained with reinforcement learning to solve the coordinate ordering problem for sets of star glyphs. Given a set of star glyphs associated to multiple class labels, we propose to use shape context descriptors to measure the perceptual distance between pairs of glyphs, and use the derived silhouette coefficient to measure the perception of class separability within the entire set. To find the optimal coordinate order for the given set, we train a neural network using reinforcement learning to reward orderings with high silhouette coefficients. The network consists of an encoder and a decoder with an attention mechanism. The encoder employs a recurrent neural network (RNN) to encode input shape and class information, while the decoder together with the attention mechanism employs another RNN to output a sequence with the new coordinate order. In addition, we introduce a neural network to efficiently estimate the similarity between shape context descriptors, which allows to speed up the computation of silhouette coefficients and thus the training of the axis ordering network. Two user studies demonstrate that the orders provided by our method are preferred by users for perceiving class separation. We tested our model on different settings to show its robustness and generalization abilities and demonstrate that it allows to order input sets with unseen data size, data dimension, or number of classes. We also demonstrate that our model can be adapted to coordinate ordering of other types of plots such as RadViz by replacing the proposed shape-aware silhouette coefficient with the corresponding quality metric to guide network training.",0
"In recent years, there has been increasing interest in developing techniques to improve the readability and visualization of star glyphs, which represent data points as shapes on a two-dimensional plane. One key challenge in designing effective star glyphs is determining how to order the coordinates of each point to maximize their legibility and communication of underlying patterns. This paper proposes a novel approach to addressing this problem using reinforcement learning. By training an agent to optimize the coordinate ordering based on human feedback, we can generate more effective and informative glyph sets that better convey important relationships within the data. Our results demonstrate significant improvements over traditional heuristics-based methods and illustrate the potential of machine learning algorithms in enhancing data visualization practices.",1
"To minimize the effects of age variation in face recognition, previous work either extracts identity-related discriminative features by minimizing the correlation between identity- and age-related features, called age-invariant face recognition (AIFR), or removes age variation by transforming the faces of different age groups into the same age group, called face age synthesis (FAS); however, the former lacks visual results for model interpretation while the latter suffers from artifacts compromising downstream recognition. Therefore, this paper proposes a unified, multi-task framework to jointly handle these two tasks, termed MTLFace, which can learn age-invariant identity-related representation while achieving pleasing face synthesis. Specifically, we first decompose the mixed face feature into two uncorrelated components -- identity- and age-related feature -- through an attention mechanism, and then decorrelate these two components using multi-task training and continuous domain adaption. In contrast to the conventional one-hot encoding that achieves group-level FAS, we propose a novel identity conditional module to achieve identity-level FAS, with a weight-sharing strategy to improve the age smoothness of synthesized faces. In addition, we collect and release a large cross-age face dataset with age and gender annotations to advance the development of the AIFR and FAS. Extensive experiments on five benchmark cross-age datasets demonstrate the superior performance of our proposed MTLFace over existing state-of-the-art methods for AIFR and FAS. We further validate MTLFace on two popular general face recognition datasets, showing competitive performance for face recognition in the wild. The source code and dataset are available at~\url{https://github.com/Hzzone/MTLFace}.",0
"In today's digital age, face recognition has become an essential technology used by numerous applications such as security systems, social media platforms, and access control. However, traditional face recognition methods have limited success in identifying faces at different ages due to changes in appearance over time caused by aging processes like wrinkles, sagging skin, and gray hair. This limitation poses significant challenges in areas that require accurate identification across age groups, including criminal investigations, automated teller machines (ATMs), and video surveillance systems. Therefore, developing robust algorithms capable of recognizing faces independent of age remains an open challenge. To address this issue, we propose a multi-task learning framework that jointly tackles face recognition and age synthesis tasks using deep convolutional neural networks (CNN). Our approach utilizes facial features extracted from both the input image and synthesized versions of the same person at different ages to learn a discriminative feature space where age-invariance can be achieved. Experimental results on three publicly available datasets demonstrate that our proposed method significantly outperforms state-of-the-art approaches, achieving higher accuracy in both face verification and age estimation tasks. These findings suggest that incorporating age synthesis into training can significantly improve the generalization ability of CNN models for age-invariant face recognition. Overall, our work paves the way for more advanced face recognition technologies that can effectively identify individuals regardless of their age.",1
"Automated neural network design has received ever-increasing attention with the evolution of deep convolutional neural networks (CNNs), especially involving their deployment on embedded and mobile platforms. One of the biggest problems that neural architecture search (NAS) confronts is that a large number of candidate neural architectures are required to train, using, for instance, reinforcement learning and evolutionary optimisation algorithms, at a vast computation cost. Even recent differentiable neural architecture search (DNAS) samples a small number of candidate neural architectures based on the probability distribution of learned architecture parameters to select the final neural architecture. To address this computational complexity issue, we introduce a novel \emph{architecture parameterisation} based on scaled sigmoid function, and propose a general \emph{Differentiable Neural Architecture Learning} (DNAL) method to optimize the neural architecture without the need to evaluate candidate neural networks. Specifically, for stochastic supernets as well as conventional CNNs, we build a new channel-wise module layer with the architecture components controlled by a scaled sigmoid function. We train these neural network models from scratch. The network optimization is decoupled into the weight optimization and the architecture optimization. We address the non-convex optimization problem of neural architecture by the continuous scaled sigmoid method with convergence guarantees. Extensive experiments demonstrate our DNAL method delivers superior performance in terms of neural architecture search cost. The optimal networks learned by DNAL surpass those produced by the state-of-the-art methods on the benchmark CIFAR-10 and ImageNet-1K dataset in accuracy, model size and computational complexity.",0
"""This"" is now replaced by a synonym! (Looks like you got stuck on that one.) So I am assuming the new request is: Replace every instance of ""this"" with another work with a similar meaning while keeping the original context intact as far as possible without changing any other parts of the text. If my assumption was incorrect please clarify!  The most important thing is determining just how far away from each other these events actually happened. This can be inferred from both objects' proper motion. With sufficient distance traveled they may have left some detectable trace or signature in space which could hint at their travel path. Alternatively if there were enough stars nearby during the creation period they might have captured some light particles in order to power themselves... Though that would take more energy than we currently think such systems require, so perhaps something else is involved here...?",1
"To achieve autonomous driving without high-definition maps, we present a model capable of generating multiple plausible paths from egocentric images for autonomous vehicles. Our generative model comprises two neural networks: the feature extraction network (FEN) and path generation network (PGN). The FEN extracts meaningful features from an egocentric image, whereas the PGN generates multiple paths from the features, given a driving intention and speed. To ensure that the paths generated are plausible and consistent with the intention, we introduce an attentive discriminator and train it with the PGN under generative adversarial networks framework. We also devise an interaction model between the positions in the paths and the intentions hidden in the positions and design a novel PGN architecture that reflects the interaction model, resulting in the improvement of the accuracy and diversity of the generated paths. Finally, we introduce ETRIDriving, a dataset for autonomous driving in which the recorded sensor data are labeled with discrete high-level driving actions, and demonstrate the state-of-the-art performance of the proposed model on ETRIDriving in terms of accuracy and diversity.",0
"This research presents a novel approach for local path planning using generative adversarial networks (GANs). The proposed method, called PathGAN, leverages attentive GAN architectures to generate diverse yet relevant candidate paths that can be used by planners to find safe, efficient trajectories in dynamic environments. The core idea behind PathGAN is to learn a mapping from high-level descriptions of tasks to feasible paths through adversarial training. To achieve this, we introduce two main components: 1) an attention mechanism to encourage focus on critical parts of each description, such as obstacles or goals; and 2) a GAN architecture with explicit latent representations to allow effective regularization during training. We evaluate our approach extensively across challenging scenarios including simulation benchmarks and real robot experiments with promising results compared to state-of-the-art methods. Our work demonstrates the potential of integrating machine learning techniques into the pipeline of motion planning systems while addressing their inherent limitations due to incomplete knowledge of the environment or task specifications. Overall, the paper advances the development of more robust autonomy capabilities for autonomous agents operating in complex settings where safety guarantees must coexist with adaptivity and responsiveness.",1
"In recent years, the task of video prediction-forecasting future video given past video frames-has attracted attention in the research community. In this paper we propose a novel approach to this problem with Vector Quantized Variational AutoEncoders (VQ-VAE). With VQ-VAE we compress high-resolution videos into a hierarchical set of multi-scale discrete latent variables. Compared to pixels, this compressed latent space has dramatically reduced dimensionality, allowing us to apply scalable autoregressive generative models to predict video. In contrast to previous work that has largely emphasized highly constrained datasets, we focus on very diverse, large-scale datasets such as Kinetics-600. We predict video at a higher resolution on unconstrained videos, 256x256, than any other previous method to our knowledge. We further validate our approach against prior work via a crowdsourced human evaluation.",0
"An effective methodology for predicting video frames using the Variational Quadratic Autoencoder (VQVAE) model is proposed. This novel approach leverages the strengths of both variational autoencoders (VAEs) and convolutional neural networks (CNNs), enabling the efficient encoding and decoding of high-resolution videos. The VQVAE architecture is optimized by introducing a discrete latent space and a learnable quantization function that improves the accuracy and efficiency of video prediction tasks. Extensive experiments demonstrate the superior performance of our algorithm compared to state-of-the-art methods across multiple benchmark datasets, including Kinetics, Something-Something, UCF Sports, and Moments-in-Time. The results indicate that the VQVAE framework has significant potential in various video processing applications such as action recognition, image generation, and motion forecasting.",1
"Deep learning-based coastline detection algorithms have begun to outshine traditional statistical methods in recent years. However, they are usually trained only as single-purpose models to either segment land and water or delineate the coastline. In contrast to this, a human annotator will usually keep a mental map of both segmentation and delineation when performing manual coastline detection. To take into account this task duality, we therefore devise a new model to unite these two approaches in a deep learning model. By taking inspiration from the main building blocks of a semantic segmentation framework (UNet) and an edge detection framework (HED), both tasks are combined in a natural way. Training is made efficient by employing deep supervision on side predictions at multiple resolutions. Finally, a hierarchical attention mechanism is introduced to adaptively merge these multiscale predictions into the final model output. The advantages of this approach over other traditional and deep learning-based methods for coastline detection are demonstrated on a dataset of Sentinel-1 imagery covering parts of the Antarctic coast, where coastline detection is notoriously difficult. An implementation of our method is available at \url{https://github.com/khdlr/HED-UNet}.",0
"As you write the abstract please keep in mind that I am writing the introduction based on your work, so make sure all relevant keywords related to our study are mentioned in your text (e.g., satellite imagery, object detection, deep learning) ----- The rapid decline of polar ice sheets has become a pressing concern globally. The melting glaciers contribute significantly to global sea level rise, and monitoring these changes is crucial for better understanding their causes and effects. One significant contributor to sea-level rise is the disintegration and melting of ice shelves along the coasts of Greenland and Antarctica. To monitor the changes happening at ice sheet margins, high spatial resolution images from satellites such as Sentinel-2 can be used to detect disintegrating edges. This manuscript presents HED-UNet; a method to automatically classify objects like snow, water, clouds, ground and bedrock present in high spatial resolution imagery to track shifts in sea levels caused by ice sheet losses at the margin over time. To achieve the goal we aim towards creating a robust segmenter using the DeepLab family while leveraging more powerful feature extractors available through ResNet variants than earlier versions of DeepLab. We then utilize edge detection from Canny edge detection and adaptive thresholding to provide refinement during training. For the data preparation, we use erosion and dilation operations as well as random crops before feeding them into the models. Our evaluation is done via visual analysis with Ground Truth segmentations and Intersection over Union scores to ensure proper quantification of performance metrics.",1
"Despite the evolution of deep-learning-based visual-textual processing systems, precise multi-modal matching remains a challenging task. In this work, we tackle the task of cross-modal retrieval through image-sentence matching based on word-region alignments, using supervision only at the global image-sentence level. Specifically, we present a novel approach called Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a fine-grained match between the underlying components of images and sentences, i.e., image regions and words, respectively, in order to preserve the informative richness of both modalities. TERAN obtains state-of-the-art results on the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover, on MS-COCO, it also outperforms current approaches on the sentence retrieval task.   Focusing on scalable cross-modal information retrieval, TERAN is designed to keep the visual and textual data pipelines well separated. Cross-attention links invalidate any chance to separately extract visual and textual features needed for the online search and the offline indexing steps in large-scale retrieval systems. In this respect, TERAN merges the information from the two domains only during the final alignment phase, immediately before the loss computation. We argue that the fine-grained alignments produced by TERAN pave the way towards the research for effective and efficient methods for large-scale cross-modal information retrieval. We compare the effectiveness of our approach against relevant state-of-the-art methods. On the MS-COCO 1K test set, we obtain an improvement of 5.7% and 3.5% respectively on the image and the sentence retrieval tasks on the Recall@1 metric. The code used for the experiments is publicly available on GitHub at https://github.com/mesnico/TERAN.",0
"In recent years, there has been significant interest in cross-modal retrieval tasks such as image search based on text queries and vice versa. These types of applications require effective alignment techniques that can accurately match semantic concepts across modalities. Traditional approaches have relied primarily on coarse feature descriptors that do not capture fine-grained relationships between images and texts. This paper presents a novel approach called Fine-Grained Visual Textual Alignment (FGVTA) that uses Transformer encoders to learn high-resolution alignments between images and natural language descriptions at both local and global levels. Our method combines self-attention mechanisms with relative position encoding to model complex interactions between visual features and text tokens. To evaluate our approach, we conduct experiments on two benchmark datasets and show that FGVTA significantly outperforms state-of-the-art methods in terms of accuracy and efficiency. We conclude by discussing potential future directions for improving cross-modal retrieval through advanced alignment models.",1
"One of the key problems of GNNs is how to describe the importance of neighbor nodes in the aggregation process for learning node representations. A class of GNNs solves this problem by learning implicit weights to represent the importance of neighbor nodes, which we call implicit GNNs such as Graph Attention Network. The basic idea of implicit GNNs is to introduce graph information with special properties followed by Learnable Transformation Structures (LTS) which encode the importance of neighbor nodes via a data-driven way. In this paper, we argue that LTS makes the special properties of graph information disappear during the learning process, resulting in graph information unhelpful for learning node representations. We call this phenomenon Graph Information Vanishing (GIV). Also, we find that LTS maps different graph information into highly similar results. To validate the above two points, we design two sets of 70 random experiments on five Implicit GNNs methods and seven benchmark datasets by using a random permutation operator to randomly disrupt the order of graph information and replacing graph information with random values. We find that randomization does not affect the model performance in 93\% of the cases, with about 7 percentage causing an average 0.5\% accuracy loss. And the cosine similarity of output results, generated by LTS mapping different graph information, over 99\% with an 81\% proportion. The experimental results provide evidence to support the existence of GIV in Implicit GNNs and imply that the existing methods of Implicit GNNs do not make good use of graph information. The relationship between graph information and LTS should be rethought to ensure that graph information is used in node representation.",0
"This work presents a study on graph information vanishing phenomenon, whereby essential structure data gets lost in deep learning models that process graphs. This issue has received limited attention despite significant research into deep neural networks applied to graph data structures (graph neural networks). Our analysis shows how such loss can occur, providing insights which could lead to improvements towards mitigating and addressing the problem. We explore possible reasons why certain graph properties become unimportant and present results confirming the existence and significance of the vanishing phenomenon. These findings offer new directions for researchers studying graph representations. The article contributes to a better understanding of GNN architecture limitations and their impact on model performance, paving the way for future advancements in the field of implicit graph processing via deep learning methods. Ultimately, our work furthers efforts in developing powerful artificial intelligence techniques capable of handling complex graph-structured data.",1
"The considerable significance of Anomaly Detection (AD) problem has recently drawn the attention of many researchers. Consequently, the number of proposed methods in this research field has been increased steadily. AD strongly correlates with the important computer vision and image processing tasks such as image/video anomaly, irregularity and sudden event detection. More recently, Deep Neural Networks (DNNs) offer a high performance set of solutions, but at the expense of a heavy computational cost. However, there is a noticeable gap between the previously proposed methods and an applicable real-word approach. Regarding the raised concerns about AD as an ongoing challenging problem, notably in images and videos, the time has come to argue over the pitfalls and prospects of methods have attempted to deal with visual AD tasks. Hereupon, in this survey we intend to conduct an in-depth investigation into the images/videos deep learning based AD methods. We also discuss current challenges and future research directions thoroughly.",0
"This paper surveys research on image/video deep anomaly detection (DA). Researchers have developed several models that use convolutional neural networks (CNNs) with Generative Adversarial Networks (GANs), autoencoders, variational autoencoders (VAEs), one-class support vector machines (SVMs), decision trees, random forest, and k-nearest neighbors (k-NN) to detect anomalies from data. We evaluate these methods based on their accuracy performance, whether they require labeled anomalies, and if they provide interpretability into why certain samples were detected as anomalous. Additionally, we analyze how different datasets affect method performance and discuss open challenges within DA research. In conclusion, our results suggest future work should explore developing hybrid systems to improve model generalization and interpretability through incorporating multiple sources of evidence such as human feedback. Our evaluation can serve practitioners seeking guidance on selecting appropriate DA approaches based on specific project requirements.",1
"Decentralized stochastic optimization methods have gained a lot of attention recently, mainly because of their cheap per iteration cost, data locality, and their communication-efficiency. In this paper we introduce a unified convergence analysis that covers a large variety of decentralized SGD methods which so far have required different intuitions, have different applications, and which have been developed separately in various communities.   Our algorithmic framework covers local SGD updates and synchronous and pairwise gossip updates on adaptive network topology. We derive universal convergence rates for smooth (convex and non-convex) problems and the rates interpolate between the heterogeneous (non-identically distributed data) and iid-data settings, recovering linear convergence rates in many special cases, for instance for over-parametrized models. Our proofs rely on weak assumptions (typically improving over prior work in several aspects) and recover (and improve) the best known complexity results for a host of important scenarios, such as for instance coorperative SGD and federated averaging (local SGD).",0
"In recent years, decentralized stochastic gradient descent (SGD) has emerged as a popular approach for distributed machine learning on networks that can change over time. However, existing theories of decentralized SGD have focused primarily on static network topologies or restricted classes of local update rules. This paper presents a unified theory of decentralized SGD that allows for changing network connectivity and general convex loss functions. We establish new convergence results under mild assumptions on the local update rules and showcase how our framework can recover previous approaches as special cases. Our findings pave the way for future research into efficient and scalable decentralized optimization algorithms that can adapt to dynamic environments.",1
"Diabetic Retinopathy (DR) is a leading cause of vision loss globally. Yet despite its prevalence, the majority of affected people lack access to the specialized ophthalmologists and equipment required for assessing their condition. This can lead to delays in the start of treatment, thereby lowering their chances for a successful outcome. Machine learning systems that automatically detect the disease in eye fundus images have been proposed as a means of facilitating access to DR severity estimates for patients in remote regions or even for complementing the human expert's diagnosis. In this paper, we propose a machine learning system for the detection of referable DR in fundus images that is based on the paradigm of multiple-instance learning. By extracting local information from image patches and combining it efficiently through an attention mechanism, our system is able to achieve high classification accuracy. Moreover, it can highlight potential image regions where DR manifests through its characteristic lesions. We evaluate our approach on publicly available retinal image datasets, in which it exhibits near state-of-the-art performance, while also producing interpretable visualizations of its predictions.",0
"This paper presents a new method for detecting diabetic retinopathy (DR) from fundus images using a multiple-instance learning approach that emphasizes interpretability. DR is one of the leading causes of blindness worldwide and early detection is crucial for preventing vision loss. Traditional computer vision methods have shown limited performance due to varying image quality and complex variations in pathological features across different stages of the disease. To address these challenges, we propose an interpretable approach that leverages both local and global contextual information obtained through spatial pyramid pooling. Our model achieves state-of-the-art results on two publicly available datasets while providing visual explanations of predictions, which can aid clinical decision making by highlighting regions of interest in the image where abnormalities occur. Overall, our proposed method provides a robust solution for automating the diagnosis of DR and has potential applications in other medical imaging domains.",1
"Early detection of skin cancers like melanoma is crucial to ensure high chances of survival for patients. Clinical application of Deep Learning (DL)-based Decision Support Systems (DSS) for skin cancer screening has the potential to improve the quality of patient care. The majority of work in the medical AI community focuses on a diagnosis setting that is mainly relevant for autonomous operation. Practical decision support should, however, go beyond plain diagnosis and provide explanations. This paper provides an overview of works towards explainable, DL-based decision support in medical applications with the example of skin cancer diagnosis from clinical, dermoscopic and histopathologic images. Analysis reveals that comparably little attention is payed to the explanation of histopathologic skin images and that current work is dominated by visual relevance maps as well as dermoscopic feature identification. We conclude that future work should focus on meeting the stakeholder's cognitive concepts, providing exhaustive explanations that combine global and local approaches and leverage diverse modalities. Moreover, the possibility to intervene and guide models in case of misbehaviour is identified as a major step towards successful deployment of AI as DL-based DSS and beyond.",0
"In recent years, deep learning has emerged as a powerful tool for medical image analysis, including applications such as detection and classification tasks. However, developing accurate and reliable models can be challenging due to differences in data quality and label inconsistencies across different datasets. This study presents a case study exploring how deep learning techniques were used to develop a decision support system for skin cancer diagnosis using dermoscopy images. We describe our approach for addressing dataset heterogeneity by utilizing transfer learning from pre-trained convolutional neural networks (CNNs) and combining multiple feature extractors in a multi-modal framework. Our results demonstrate that our method achieved promising performance on benchmark datasets and show potential for improving diagnostic accuracy and reducing inter-observer variability in clinical settings. Additionally, we discuss limitations and future directions for research in applying machine learning algorithms for computer-assisted diagnosis in medicine.",1
"The study of the visual system of the brain has attracted the attention and interest of many neuro-scientists, that derived computational models of some types of neuron that compose it. These findings inspired researchers in image processing and computer vision to deploy such models to solve problems of visual data processing. In this paper, we review approaches for image processing and computer vision, the design of which is based on neuro-scientific findings about the functions of some neurons in the visual cortex. Furthermore, we analyze the connection between the hierarchical organization of the visual system of the brain and the structure of Convolutional Networks (ConvNets). We pay particular attention to the mechanisms of inhibition of the responses of some neurons, which provide the visual system with improved stability to changing input stimuli, and discuss their implementation in image processing operators and in ConvNets.",0
"""The paper presents a new algorithm inspired by the structure and functioning of the human brain that has shown promise in improving the efficiency and accuracy of visual data processing tasks such as image recognition and object detection. Drawing on recent advances in artificial intelligence research, the proposed method leverages techniques from deep learning and neural network modeling to mimic the hierarchical organization of the cortex and other important features of biological computation. Experimental results obtained using standard benchmark datasets demonstrate significant improvements over state-of-the-art methods, paving the way towards more intelligent and adaptive machines capable of tackling complex perceptual challenges.""",1
"Deep neural networks (DNNs) have demonstrated remarkable performance for various applications, meanwhile, they are widely known to be vulnerable to the attack of adversarial perturbations. This intriguing phenomenon has attracted significant attention in machine learning and what might be more surprising to the community is the existence of universal adversarial perturbations (UAPs), i.e. a single perturbation to fool the target DNN for most images. The advantage of UAP is that it can be generated beforehand and then be applied on-the-fly during the attack. With the focus on UAP against deep classifiers, this survey summarizes the recent progress on universal adversarial attacks, discussing the challenges from both the attack and defense sides, as well as the reason for the existence of UAP. Additionally, universal attacks in a wide range of applications beyond deep classification are also covered.",0
"Artificial intelligence has become increasingly prevalent in our everyday lives, from image recognition algorithms that sort through medical images to natural language processing models that respond to customer service queries. However, these systems remain vulnerable to adversarial attacks. In this survey, we aim to provide a comprehensive overview of the current state-of-the art methods proposed to tackle universal adversarial robustness problem, as well as potential open research directions for future work. We first discuss the fundamental challenges associated with developing a system capable of generating successful attacks against any model under any input space and threat model. Next, we analyze recent advancements made towards creating such universally effective attacking mechanisms. Subsequently, we consider defenses that are specifically designed to resist a wide range of threats posed by universal attacks, before delving into evaluating their effectiveness under different scenarios. Finally, we conclude with some insights on future research directions and key takeaways from this study.",1
"Estimation of pain intensity from facial expressions captured in videos has an immense potential for health care applications. Given the challenges related to subjective variations of facial expressions, and operational capture conditions, the accuracy of state-of-the-art DL models for recognizing facial expressions may decline. Domain adaptation has been widely explored to alleviate the problem of domain shifts that typically occur between video data captured across various source and target domains. Moreover, given the laborious task of collecting and annotating videos, and subjective bias due to ambiguity among adjacent intensity levels, weakly-supervised learning is gaining attention in such applications. State-of-the-art WSL models are typically formulated as regression problems, and do not leverage the ordinal relationship among pain intensity levels, nor temporal coherence of multiple consecutive frames. This paper introduces a new DL model for weakly-supervised DA with ordinal regression that can be adapted using target domain videos with coarse labels provided on a periodic basis. The WSDA-OR model enforces ordinal relationships among intensity levels assigned to target sequences, and associates multiple relevant frames to sequence-level labels. In particular, it learns discriminant and domain-invariant feature representations by integrating multiple instance learning with deep adversarial DA, where soft Gaussian labels are used to efficiently represent the weak ordinal sequence-level labels from target domain. The proposed approach was validated using RECOLA video dataset as fully-labeled source domain data, and UNBC-McMaster shoulder pain video dataset as weakly-labeled target domain data. We have also validated WSDA-OR on BIOVID and Fatigue datasets for sequence level estimation.",0
"This study addresses the challenge of ordinal regression of pain intensity estimation using weakly labelled videos. The authors propose a deep domain adaptation approach that leverages both visual features and temporal dependencies of the data to improve the accuracy of the model. They use adversarial training to align the feature distributions across domains, ensuring that the model can generalize well to unseen instances from different patients. Experimental results demonstrate the effectiveness of their proposed method over several baseline models on two public datasets. Overall, this work presents a promising solution for addressing the problem of ordinal regression using weak labels and offers opportunities for further research in the field of medical imaging analysis.",1
"Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions; object-centric architectures make use of graph neural networks to model interactions among entities. However, pairwise interactions may not achieve global coordination or a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally specialized components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise independent specialists.",0
"This paper presents a novel approach to coordination among neural modules through the use of a shared global workspace. In traditional modular models of cognition, individual modules operate independently, communicating only through dedicated channels. However, we argue that such systems fail to capture key aspects of human cognition, including the ability to coordinate information from multiple sources. Our proposed model introduces a shared global workspace into which all active modules can freely communicate and exchange data. We show how this architecture allows for more efficient and flexible integration of information across domains, resulting in improved performance on tasks requiring complex reasoning and decision making. Through simulations of several benchmark problems, we demonstrate the effectiveness of our model compared to state-of-the-art alternatives. Overall, these results highlight the potential of a shared global workspace as a critical component of next-generation artificial intelligence systems.",1
"We consider counterfactual explanations, the problem of minimally adjusting features in a source input instance so that it is classified as a target class under a given classifier. This has become a topic of recent interest as a way to query a trained model and suggest possible actions to overturn its decision. Mathematically, the problem is formally equivalent to that of finding adversarial examples, which also has attracted significant attention recently. Most work on either counterfactual explanations or adversarial examples has focused on differentiable classifiers, such as neural nets. We focus on classification trees, both axis-aligned and oblique (having hyperplane splits). Although here the counterfactual optimization problem is nonconvex and nondifferentiable, we show that an exact solution can be computed very efficiently, even with high-dimensional feature vectors and with both continuous and categorical features, and demonstrate it in different datasets and settings. The results are particularly relevant for finance, medicine or legal applications, where interpretability and counterfactual explanations are particularly important.",0
"This paper presents efficient algorithms for computing counterfactual explanations of oblique decision trees, which are popular machine learning models used for classification and regression tasks. Our methods directly compute exact counterfactuals by traversing the tree structure based on the given example and query points. We first describe how we can generate meaningful feature interaction weights that explain the prediction behavior of the model while providing interpretability. Then, we introduce two novel algorithmic approaches for finding exact counterfactuals: one based on depth-first search and another based on dynamic programming. Both algorithms achieve optimal running time complexity O(n^2) under some mild assumptions, making them computationally feasible even for large datasets. Using comprehensive experiments, we demonstrate the effectiveness and efficiency of our proposed methodologies compared to state-of-the-art baselines. Our work contributes new insights into understanding and explaining black box models like oblique decision trees through user-friendly counterfactual queries.",1
"This paper proposes Omnidirectional Representations from Transformers (OmniNet). In OmniNet, instead of maintaining a strictly horizontal receptive field, each token is allowed to attend to all tokens in the entire network. This process can also be interpreted as a form of extreme or intensive attention mechanism that has the receptive field of the entire width and depth of the network. To this end, the omnidirectional attention is learned via a meta-learner, which is essentially another self-attention based model. In order to mitigate the computationally expensive costs of full receptive field attention, we leverage efficient self-attention models such as kernel-based (Choromanski et al.), low-rank attention (Wang et al.) and/or Big Bird (Zaheer et al.) as the meta-learner. Extensive experiments are conducted on autoregressive language modeling (LM1B, C4), Machine Translation, Long Range Arena (LRA), and Image Recognition. The experiments show that OmniNet achieves considerable improvements across these tasks, including achieving state-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena. Moreover, using omnidirectional representation in Vision Transformers leads to significant improvements on image recognition tasks on both few-shot learning and fine-tuning setups.",0
"In recent years, convolutional neural networks (CNNs) have been widely used for tasks such as image classification, object detection, and segmentation due to their ability to capture spatial features effectively. However, these methods often require large amounts of computation and memory resources, making them difficult to scale up for more complex tasks or larger datasets.  Transformer architectures were introduced as an alternative approach that can efficiently model sequential data by learning global representations through self-attention mechanisms. Despite achieving state-of-the-art results on natural language processing tasks, transformers struggle to process visual data due to their inherent limitation in capturing local contextual relationships.  To address this challenge, we propose OmniNet, a novel framework that combines the strengths of CNNs and transformers for omnidirectional feature representation. Our method consists of two stages: a lightweight encoder network based on MobileNetV2-like architecture and a decoder network based on the standard transformer architecture. During training, each input is transformed into a sequence of tokens, which are then fed into both encoders simultaneously. This enables our model to capture both local and global context while maintaining computational efficiency. We evaluate OmniNet across several benchmark datasets including ImageNet, COCO, Cityscapes, Flickr8K, and NUS Wide, demonstrating significant improvements over existing methods in terms of accuracy and speed. Our work represents a step towards bridging the gap between computer vision and natural language processing domains and paves the way for future research in multimodal applications.",1
"Deep learning models are vulnerable to adversarial examples. As a more threatening type for practical deep learning systems, physical adversarial examples have received extensive research attention in recent years. However, without exploiting the intrinsic characteristics such as model-agnostic and human-specific patterns, existing works generate weak adversarial perturbations in the physical world, which fall short of attacking across different models and show visually suspicious appearance. Motivated by the viewpoint that attention reflects the intrinsic characteristics of the recognition process, this paper proposes the Dual Attention Suppression (DAS) attack to generate visually-natural physical adversarial camouflages with strong transferability by suppressing both model and human attention. As for attacking, we generate transferable adversarial camouflages by distracting the model-shared similar attention patterns from the target to non-target regions. Meanwhile, based on the fact that human visual attention always focuses on salient items (e.g., suspicious distortions), we evade the human-specific bottom-up attention to generate visually-natural camouflages which are correlated to the scenario context. We conduct extensive experiments in both the digital and physical world for classification and detection tasks on up-to-date models (e.g., Yolo-V5) and significantly demonstrate that our method outperforms state-of-the-art methods.",0
"Title: Comprehensive Exploration of Multi-Model Active Learning Techniques in Image Classification Tasks (No paper title given) Abstract: In recent years, active learning has emerged as a promising technique that enables machine learning models to achieve high accuracy by actively selecting informative examples from large datasets for annotation. Among these techniques, multi-model active learning has gained significant attention due to its ability to leverage multiple base models during the selection process. However, there is still limited understanding on how different components of multi-model active learning methods contribute to their performance improvements. This comprehensive exploration aims to fill this gap by providing insights into the effectiveness of popular multi-model active learning methods in image classification tasks. Through extensive experiments across four benchmark datasets, we investigate three key factors affecting the performance of multi-model active learning approaches: diversity measures used for model selection, uncertainty sampling strategies, and ensemble mechanisms. Our results demonstrate that using effective diversity measures such as disagreement between models and class entropy can significantly enhance the performance of multi-model active learning algorithms. Moreover, our study reveals the critical role played by uncertainty sampling in improving the performance gain achieved via combining diverse base models under the ensemble framework. These findings provide valuable guidelines for practitioners interested in adopting multi-model active learning for their own applications.",1
"For stability and reliability of real-world applications, the robustness of DNNs in unimodal tasks has been evaluated. However, few studies consider abnormal situations that a visual question answering (VQA) model might encounter at test time after deployment in the real-world. In this study, we evaluate the robustness of state-of-the-art VQA models to five different anomalies, including worst-case scenarios, the most frequent scenarios, and the current limitation of VQA models. Different from the results in unimodal tasks, the maximum confidence of answers in VQA models cannot detect anomalous inputs, and post-training of the outputs, such as outlier exposure, is ineffective for VQA models. Thus, we propose an attention-based method, which uses confidence of reasoning between input images and questions and shows much more promising results than the previous methods in unimodal tasks. In addition, we show that a maximum entropy regularization of attention networks can significantly improve the attention-based anomaly detection of the VQA models. Thanks to the simplicity, attention-based anomaly detection and the regularization are model-agnostic methods, which can be used for various cross-modal attentions in the state-of-the-art VQA models. The results imply that cross-modal attention in VQA is important to improve not only VQA accuracy, but also the robustness to various anomalies.",0
"This work proposes regularization techniques that can effectively prevent attention networks from overfitting in visual question answering tasks. Our key idea is to constrain the weights of attention mechanisms using L2-norm constraints and spectral normalization, thus encouraging sparsity and stability during training. We show how these regularizations can significantly boost performance on popular benchmark datasets, especially under challenging settings such as few-shot learning. The proposed approach is simple yet effective and achieves state-of-the-art results without resorting to more advanced architectures or hyperparameter tuning. Importantly, our method offers explainability benefits by identifying discriminative features used by the model for detection. In summary, we present a robust and efficient solution for anomaly detection in computer vision using attention networks.",1
"Vertical federated learning (VFL) attracts increasing attention due to the emerging demands of multi-party collaborative modeling and concerns of privacy leakage. In the real VFL applications, usually only one or partial parties hold labels, which makes it challenging for all parties to collaboratively learn the model without privacy leakage. Meanwhile, most existing VFL algorithms are trapped in the synchronous computations, which leads to inefficiency in their real-world applications. To address these challenging problems, we propose a novel {\bf VF}L framework integrated with new {\bf b}ackward updating mechanism and {\bf b}ilevel asynchronous parallel architecture (VF{${\textbf{B}}^2$}), under which three new algorithms, including VF{${\textbf{B}}^2$}-SGD, -SVRG, and -SAGA, are proposed. We derive the theoretical results of the convergence rates of these three algorithms under both strongly convex and nonconvex conditions. We also prove the security of VF{${\textbf{B}}^2$} under semi-honest threat models. Extensive experiments on benchmark datasets demonstrate that our algorithms are efficient, scalable and lossless.",0
"This paper presents a secure bilevel asynchronous vertical federated learning approach with backward updating. With data privacy concerns becoming increasingly important in modern machine learning applications, we propose a novel method that addresses these issues while still achieving high performance on distributed datasets. Our framework involves two levels of optimization: one at the client level and another at the server level. By using asynchrony and vertical partitioning of the dataset across devices, our approach allows for efficient computation without compromising privacy. We also incorporate backward updating techniques to improve model accuracy and communication efficiency during training. Extensive experiments demonstrate the effectiveness of our method compared to existing approaches under different simulation settings. Overall, our work represents a significant step forward towards enabling secure and accurate large-scale machine learning in real-world scenarios.",1
"Multi-task learning (MTL) is an important subject in machine learning and artificial intelligence. Its applications to computer vision, signal processing, and speech recognition are ubiquitous. Although this subject has attracted considerable attention recently, the performance and robustness of the existing models to different tasks have not been well balanced. This article proposes an MTL model based on the architecture of the variational information bottleneck (VIB), which can provide a more effective latent representation of the input features for the downstream tasks. Extensive observations on three public data sets under adversarial attacks show that the proposed model is competitive to the state-of-the-art algorithms concerning the prediction accuracy. Experimental results suggest that combining the VIB and the task-dependent uncertainties is a very effective way to abstract valid information from the input features for accomplishing multiple tasks.",0
"This can be difficult, since you don't want to give away all the details of your work without providing enough context for readers to understand its significance. But here is one possible version:  The ""Multi-Task Variational Information Bottleneck"" paper presents a new methodology that offers significant advances over traditional approaches in several ways. Firstly, by incorporating multiple tasks into a single model, we are able to achieve better performance on each individual task than would have been possible otherwise. Secondly, our approach provides a principled framework for designing complex models that capture more powerful representations and enable better generalization. Finally, we show how these benefits extend to real-world applications where data may be limited or noisy, demonstrating the robustness and potential impact of our technique in practice. Together, these results suggest that multi-task variational learning has significant potential as a tool for accelerating progress across a range of important problems, from natural language understanding to computer vision and beyond. We believe that our contributions represent a valuable step forward in this rapidly evolving field and look forward to seeing how others build upon them in future work.",1
"Concept drift detection is a crucial task in data stream evolving environments. Most of state of the art approaches designed to tackle this problem monitor the loss of predictive models. However, this approach falls short in many real-world scenarios, where the true labels are not readily available to compute the loss. In this context, there is increasing attention to approaches that perform concept drift detection in an unsupervised manner, i.e., without access to the true labels. We propose a novel approach to unsupervised concept drift detection based on a student-teacher learning paradigm. Essentially, we create an auxiliary model (student) to mimic the behaviour of the primary model (teacher). At run-time, our approach is to use the teacher for predicting new instances and monitoring the mimicking loss of the student for concept drift detection. In a set of experiments using 19 data streams, we show that the proposed approach can detect concept drift and present a competitive behaviour relative to the state of the art approaches.",0
"This paper presents a new method called STUDD (Student-Teacher Method for Unsupervised Concept Drift Detection) which allows unsupervised detection of concept drifts in machine learning algorithms by using student models as proxies. Inspired by knowledge distillation techniques used to transfer knowledge from large pre-trained teacher models into smaller student models, we train multiple student models at different time periods during training, each representing the state of the original model at that point in time. These student models can then be used to detect changes in the distribution of incoming data over time, effectively identifying instances where the underlying pattern has shifted. Our approach uses no labeled data and requires minimal additional computational resources while still outperforming existing methods on several benchmark datasets. We provide a detailed analysis comparing our method against other popular state-of-the-art approaches, demonstrating its effectiveness in providing reliable and early warnings of shifting patterns to adaptively update machine learning systems. By enabling machines to learn incrementally and adapt to changing environments, STUDD paves the way towards creating more resilient and robust artificial intelligence systems.",1
"Action Detection is a complex task that aims to detect and classify human actions in video clips. Typically, it has been addressed by processing fine-grained features extracted from a video classification backbone. Recently, thanks to the robustness of object and people detectors, a deeper focus has been added on relationship modelling. Following this line, we propose a graph-based framework to learn high-level interactions between people and objects, in both space and time. In our formulation, spatio-temporal relationships are learned through self-attention on a multi-layer graph structure which can connect entities from consecutive clips, thus considering long-range spatial and temporal dependencies. The proposed module is backbone independent by design and does not require end-to-end training. Extensive experiments are conducted on the AVA dataset, where our model demonstrates state-of-the-art results and consistent improvements over baselines built with different backbones. Code is publicly available at https://github.com/aimagelab/STAGE_action_detection.",0
"This study presents a novel approach for video action detection based on learned graph-based representations that model spatial and temporal relationships. We design a deep neural network architecture that takes advantage of the hierarchical structure of videos and represents actions as high-level patterns. Our method extracts low-level features from frame pairs at multiple time intervals using convolutional layers and then fuses them into a unified feature representation through graph convolutions. In addition, we introduce a spatial attention mechanism that captures both short-range and long-range dependencies across space and time. Experiments conducted on three benchmark datasets demonstrate the effectiveness of our approach compared to state-of-the-art methods in terms of accuracy and efficiency.",1
"Self-supervised learning of depth has been a highly studied topic of research as it alleviates the requirement of having ground truth annotations for predicting depth. Depth is learnt as an intermediate solution to the task of view synthesis, utilising warped photometric consistency. Although it gives good results when trained using stereo data, the predicted depth is still sensitive to noise, illumination changes and specular reflections. Also, occlusion can be tackled better by learning depth from a single camera. We propose ADAA, utilising depth augmentation as depth supervision for learning accurate and robust depth. We propose a relational self-attention module that learns rich contextual features and further enhances depth results. We also optimize the auto-masking strategy across all losses by enforcing L1 regularisation over mask. Our novel progressive training strategy first learns depth at a lower resolution and then progresses to the original resolution with slight training. We utilise a ResNet18 encoder, learning features for prediction of both depth and pose. We evaluate our predicted depth on the standard KITTI driving dataset and achieve state-of-the-art results for monocular depth estimation whilst having significantly lower number of trainable parameters in our deep learning framework. We also evaluate our model on Make3D dataset showing better generalization than other methods.",0
"Self-supervised depth estimation using monocular images has gained significant attention due to its potential applications in robotics, computer vision, and autonomous vehicles. However, accurate depth estimation remains challenging due to limitations such as lack of labeled data, differences in illumination conditions, occlusions, shadows, reflections, and textureless regions. To overcome these issues, we propose a novel self-supervised method that utilizes two techniques: data augmentation (DA) and attention mechanisms. Our approach adapts the existing DA methods by designing new DA strategies tailored specifically for depth estimation tasks and applying them during training. These customized DA strategies increase the diversity and quality of generated samples, enabling better feature extraction and robustness against changes in lighting conditions. In addition, our method uses attention modules at multiple levels to focus on relevant features, suppress noise caused by incorrect predictions, and selectively adjust the contribution of different image regions. We evaluate our proposed method on several benchmark datasets and compare its performance with state-of-the-art approaches. Experimental results demonstrate the effectiveness of our approach in improving depth estimates under varying illumination and occlusion conditions. Overall, our work provides valuable insights into developing effective self-supervised learning algorithms for high-quality depth estimation from monocular imagery.",1
"\textit{Attention} computes the dependency between representations, and it encourages the model to focus on the important selective features. Attention-based models, such as Transformer and graph attention network (GAT), are widely utilized for sequential data and graph-structured data. This paper suggests a new interpretation and generalized structure of the attention in Transformer and GAT. For the attention in Transformer and GAT, we derive that the attention is a product of two parts: 1) the RBF kernel to measure the similarity of two instances and 2) the exponential of $L^{2}$ norm to compute the importance of individual instances. From this decomposition, we generalize the attention in three ways. First, we propose implicit kernel attention with an implicit kernel function instead of manual kernel selection. Second, we generalize $L^{2}$ norm as the $L^{p}$ norm. Third, we extend our attention to structured multi-head attention. Our generalized attention shows better performance on classification, translation, and regression tasks.",0
"Recent advances in natural language processing have brought attention mechanisms into focus as critical components in modeling sequential data such as text. However, existing techniques like self-attention still require explicit indication of which parts of input sequences should receive attention, leading to increased computational overhead and potential bottlenecks in processing speed. In order to address these limitations, we propose Implicit Kernel Attention (IKA), a novel approach that enables end-to-end learning of kernel functions directly from input data without the need for explicit attention vectors. By incorporating adaptive locality properties derived from kernel methods, IKA effectively captures nonlinear relationships among elements within the sequence while avoiding the scaling issues inherent in traditional attention models. We demonstrate the efficacy of our method on several benchmark datasets across different NLP tasks including machine translation, question answering, and sentiment analysis, consistently outperforming competitive baseline models. Our results indicate that implicit kernels can indeed capture complex dependencies in input data more efficiently than standard self-attention mechanisms, making them promising tools for future research in NLP.",1
"Generating human action proposals in untrimmed videos is an important yet challenging task with wide applications. Current methods often suffer from the noisy boundary locations and the inferior quality of confidence scores used for proposal retrieving. In this paper, we present BSN++, a new framework which exploits complementary boundary regressor and relation modeling for temporal proposal generation. First, we propose a novel boundary regressor based on the complementary characteristics of both starting and ending boundary classifiers. Specifically, we utilize the U-shaped architecture with nested skip connections to capture rich contexts and introduce bi-directional boundary matching mechanism to improve boundary precision. Second, to account for the proposal-proposal relations ignored in previous methods, we devise a proposal relation block to which includes two self-attention modules from the aspects of position and channel. Furthermore, we find that there inevitably exists data imbalanced problems in the positive/negative proposals and temporal durations, which harm the model performance on tail distributions. To relieve this issue, we introduce the scale-balanced re-sampling strategy. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which demonstrate that BSN++ achieves the state-of-the-art performance. Not surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet challenge leaderboard on temporal action localization task.",0
This sounds like an interesting project! Can you provide more context so I can generate an appropriate abstract?,1
"Security inspection is X-ray scanning for personal belongings in suitcases, which is significantly important for the public security but highly time-consuming for human inspectors. Fortunately, deep learning has greatly promoted the development of computer vision, offering a possible way of automatic security inspection. However, items within a luggage are randomly overlapped resulting in noisy X-ray images with heavy occlusions. Thus, traditional CNN-based models trained through common image recognition datasets fail to achieve satisfactory performance in this scenario. To address these problems, we contribute the first high-quality prohibited X-ray object detection dataset named OPIXray, which contains 8885 X-ray images from 5 categories of the widely-occurred prohibited item ``cutters''. The images are gathered from an airport and these prohibited items are annotated manually by professional inspectors, which can be used as a benchmark for model training and further facilitate future research. To better improve occluded X-ray object detection, we further propose an over-sampling de-occlusion attention network (DOAM-O), which consists of a novel de-occlusion attention module and a new over-sampling training strategy. Specifically, our de-occlusion module, namely DOAM, simultaneously leverages the different appearance information of the prohibited items; the over-sampling training strategy forces the model to put more emphasis on these hard samples consisting these items of high occlusion levels, which is more suitable for this scenario. We comprehensively evaluated DOAM-O on the OPIXray dataset, which proves that our model can stably improve the performance of the famous detection models such as SSD, YOLOv3, and FCOS, and outperform many extensively-used attention mechanisms.",0
"In recent years there has been increasing attention given to improving algorithms for detecting prohibited items in noisy x-ray images. This challenge remains largely unsolved due to numerous challenges arising from image quality issues such as high noise levels. Furthermore, object occlusion can lead to missing some objects during inspection which requires more advanced de-occluding techniques to make up for. Previous work has focused on improving detection performance through training datasets enriched by synthetic images, however, these methods lack fine tuning capabilities required to match real world environments leading to reduced accuracy in actual deployments. To address these limitations we propose using over-sampling de-occudation attention network (ODAN) to generate additional examples without need for collecting expensive labeled data and allow models to adapt their attention mechanisms for better feature extraction under variable conditions across multiple categories. Our approach was validated using publicly available large scale dataset and achieved significant improvements in recall rates compared to existing state-of-the-art approaches, while achieving competitive precision rates and outperforming previous works across all classes in occlusion cases. We believe that ODAN represents a major step forward towards generalizing prohibited item detection tasks into real-world scenarios.",1
"Motion completion is a challenging and long-discussed problem, which is of great significance in film and game applications. For different motion completion scenarios (in-betweening, in-filling, and blending), most previous methods deal with the completion problems with case-by-case designs. In this work, we propose a simple but effective method to solve multiple motion completion problems under a unified framework and achieves a new state of the art accuracy under multiple evaluation settings. Inspired by the recent great success of attention-based models, we consider the completion as a sequence to sequence prediction problem. Our method consists of two modules - a standard transformer encoder with self-attention that learns long-range dependencies of input motions, and a trainable mixture embedding module that models temporal information and discriminates key-frames. Our method can run in a non-autoregressive manner and predict multiple missing frames within a single forward propagation in real time. We finally show the effectiveness of our method in music-dance applications.",0
"Title: ""Single-Shot Motion Completion using Transformer Networks""  This research presents a novel approach to motion completion tasks that utilizes transformer networks, which have shown to be highly effective at capturing relationships between data points within natural language processing and computer vision tasks. Our proposed method leverages the attention mechanism found in these models to complete missing motions in images and video frames, allowing for accurate predictions even with limited contextual information. We evaluate our model on several benchmark datasets and demonstrate significant improvements over baseline methods, achieving state-of-the-art performance across all metrics. Overall, our work demonstrates the effectiveness of transformers for single-shot motion completion tasks, paving the way for further development of these models in related areas such as action recognition and video prediction.",1
"Mixed Sample Data Augmentation (MSDA) has received increasing attention in recent years, with many successful variants such as MixUp and CutMix. By studying the mutual information between the function learned by a VAE on the original data and on the augmented data we show that MixUp distorts learned functions in a way that CutMix does not. We further demonstrate this by showing that MixUp acts as a form of adversarial training, increasing robustness to attacks such as Deep Fool and Uniform Noise which produce examples similar to those generated by MixUp. We argue that this distortion prevents models from learning about sample specific features in the data, aiding generalisation performance. In contrast, we suggest that CutMix works more like a traditional augmentation, improving performance by preventing memorisation without distorting the data distribution. However, we argue that an MSDA which builds on CutMix to include masks of arbitrary shape, rather than just square, could further prevent memorisation whilst preserving the data distribution in the same way. To this end, we propose FMix, an MSDA that uses random binary masks obtained by applying a threshold to low frequency images sampled from Fourier space. These random masks can take on a wide range of shapes and can be generated for use with one, two, and three dimensional data. FMix improves performance over MixUp and CutMix, without an increase in training time, for a number of models across a range of data sets and problem settings, obtaining a new single model state-of-the-art result on CIFAR-10 without external data. Finally, we show that a consequence of the difference between interpolating MSDA such as MixUp and masking MSDA such as FMix is that the two can be combined to improve performance even further. Code for all experiments is provided at https://github.com/ecs-vlc/FMix .",0
"In data augmentation, mixing image samples can produce diverse outputs while reducing computational cost compared to cutout-based methods. However, existing mixers either ignore semantic consistency or require time-consuming ground truth segmentations. We address these limitations by introducing FMix, a simple yet effective approach that optimizes the blending coefficients in batch via an efficient bi-level optimization framework guided by edge maps. FMix achieves superior results over state-of-the-art methods on multiple benchmarks without requiring any additional annotations, even outperforming baselines trained on five times more parameters. For example, our method improves mean IOU from 29.7% to 48.6% on PASCAL VOC object detection, and pushes COCO object detection mAP from 24.8% to 38.0%. More interestingly, we showcase how FMix successfully transfers mixed image synthesis to downstream tasks like scene understanding on ADE20K and instance segmentation on Cityscapes. As FMix runs at only 2FPS (compared to realtime for most competitors), we further investigate the tradeoffs among different design choices and demonstrate significant gains from our edge map guidance. Finally, we provide detailed ablation studies as well as visualizations to reveal intriguing insights into howFMix balances fidelity and diversity.",1
"Ultrasound (US) is a non-invasive yet effective medical diagnostic imaging technique for the COVID-19 global pandemic. However, due to complex feature behaviors and expensive annotations of US images, it is difficult to apply Artificial Intelligence (AI) assisting approaches for lung's multi-symptom (multi-label) classification. To overcome these difficulties, we propose a novel semi-supervised Two-Stream Active Learning (TSAL) method to model complicated features and reduce labeling costs in an iterative procedure. The core component of TSAL is the multi-label learning mechanism, in which label correlations information is used to design multi-label margin (MLM) strategy and confidence validation for automatically selecting informative samples and confident labels. On this basis, a multi-symptom multi-label (MSML) classification network is proposed to learn discriminative features of lung symptoms, and a human-machine interaction is exploited to confirm the final annotations that are used to fine-tune MSML with progressively labeled data. Moreover, a novel lung US dataset named COVID19-LUSMS is built, currently containing 71 clinical patients with 6,836 images sampled from 678 videos. Experimental evaluations show that TSAL using only 20% data can achieve superior performance to the baseline and the state-of-the-art. Qualitatively, visualization of both attention map and sample distribution confirms the good consistency with the clinic knowledge.",0
"Abstract: This study proposes a semi-supervised active learning approach for improving the performance of multi-symptom classification models on COVID-19 lung ultrasound images using limited labeled data. We address the challenging task of identifying multiple symptoms such as ground glass opacities, consolidation, and pleural effusion from medical imaging data that requires specialized radiologists to interpret the findings. The proposed method leverages both labeled and unlabeled data to improve model accuracy by actively selecting instances which provide maximum information gain towards reducing uncertainty during training. Our evaluation shows significant improvement over baseline methods in terms of overall F1 score and individual symptom F1 scores while minimizing human efforts required to obtain expert annotations. Additionally, we demonstrate through qualitative analysis that the active learning framework can effectively capture different visual patterns associated with each symptom. Overall, our work presents a promising direction towards enhancing the performance of computer vision algorithms in critical applications like COVID-19 diagnosis where annotated datasets may remain scarce due to resource constraints.",1
"An important development in deep learning from the earliest MLPs has been a move towards architectures with structural inductive biases which enable the model to keep distinct sources of information and routes of processing well-separated. This structure is linked to the notion of independent mechanisms from the causality literature, in which a mechanism is able to retain the same processing as irrelevant aspects of the world are changed. For example, convnets enable separation over positions, while attention-based architectures (especially Transformers) learn which combination of positions to process dynamically. In this work we explore a way in which the Transformer architecture is deficient: it represents each position with a large monolithic hidden representation and a single set of parameters which are applied over the entire hidden representation. This potentially throws unrelated sources of information together, and limits the Transformer's ability to capture independent mechanisms. To address this, we propose Transformers with Independent Mechanisms (TIM), a new Transformer layer which divides the hidden representation and parameters into multiple mechanisms, which only exchange information through attention. Additionally, we propose a competition mechanism which encourages these mechanisms to specialize over time steps, and thus be more independent. We study TIM on a large-scale BERT model, on the Image Transformer, and on speech enhancement and find evidence for semantically meaningful specialization as well as improved performance.",0
"Transformers have been widely used in natural language processing tasks due to their strong capacity for sequence modeling and parallel computing ability. However, as transformer models become larger, they often suffer from overfitting problems and may lack robustness in complex environments. To address these issues, we propose using competitive ensembles of independent mechanisms (MiCE) with transformer networks. MiCE combines multiple weak learners into a strong learner by training each mechanism independently but encouraging diversity among them, leading to better generalization performance. We apply MiCE to several popular transformer architectures such as BERT, GPT-2 and RoBERTa and conduct experiments on benchmark datasets including GLUE, SQuAD, and WNLI. Our results show that integrating MiCE significantly improves the accuracy and stability of transformer models across different settings. Furthermore, ablation studies demonstrate the effectiveness of each component in our approach. This work provides insights into designing more effective ensemble methods for large language models, promoting robustness and better decision making.",1
"Chemical formula is an artificial language that expresses molecules as text. Neural machines that have learned chemical language can be used as a tool for inverse molecular design. Here, we propose a neural machine that creates molecules that meet some desired conditions based on a deep understanding of chemical language (generative chemical Transformer, GCT). Attention-mechanism in GCT allows a deeper understanding of molecular structures, beyond the limitations of chemical language itself that cause semantic discontinuity, by paying attention to characters sparsely. We investigate the significance of language models to inverse molecular design problems by quantitatively evaluating the quality of generated molecules. GCT generates highly realistic chemical strings that satisfy both a chemical rule and grammars of a language. Molecules parsed from generated strings simultaneously satisfy the multiple target properties and are various for a single condition set. GCT generates de novo molecules, and this is done in a short time that human experts cannot. These advances will contribute to improving the quality of human life by accelerating the process of desired material discovery.",0
"Artificial intelligence has revolutionized many fields and is now poised to disrupt chemistry by accurately predicting the behavior of molecules in response to stimuli. One method that holds great promise in this regard is generative modeling using Transformer architectures. However, these models still struggle with learning the underlying geometry of molecular systems from raw data. In this work, we propose a novel approach called Generative Chemical Transformer (GCT) which uses self-attention mechanisms to improve upon traditional strategies. We demonstrate through simulations that our GCT outperforms previous techniques on both accuracy and computational efficiency. With GCT, AI-assisted drug design, synthetic organic chemistry research, environmental monitoring, and other applications can benefit from faster development cycles and more accurate predictions. This article provides detailed explanations, experiments, code snippets, and visualizations so readers can reproduce and extend our results. By sharing our advancements openly, we aim to encourage collaboration across scientific disciplines as well as foster innovation within industry.",1
"In offline reinforcement learning, a policy learns to maximize cumulative rewards with a fixed collection of data. Towards conservative strategy, current methods choose to regularize the behavior policy or learn a lower bound of the value function. However, exorbitant conservation tends to impair the policy's generalization ability and degrade its performance, especially for the mixed datasets. In this paper, we propose the method of reducing conservativeness oriented reinforcement learning. On the one hand, the policy is trained to pay more attention to the minority samples in the static dataset to address the data imbalance problem. On the other hand, we give a tighter lower bound of value function than previous methods to discover potential optimal actions. Consequently, our proposed method is able to tackle the skewed distribution of the provided dataset and derive a value function closer to the expected value function. Experimental results demonstrate that our proposed method outperforms the state-of-the-art methods in D4RL offline reinforcement learning evaluation tasks and our own designed mixed datasets.",0
"In reducing conservatism oriented offline reinforcement learning there are several techniques that can be employed including clipping, debiasing and uncertainty estimation. Clipping involves scaling rewards and penalties so as to ensure they never exceed certain values thus reducing overestimation in Q table. Debiasing refers to subtracting a small positive value from all Q values estimates which helps reduce their magnitude thus minimizing the underestimation problem. Uncertainty estimation ensures you have more accurate estimate of q tables through Monte Carlo sampling plus some form of model based planning plus added noise to account for error. By using these methods one obtains less biased and improved results especially in cases where data is sparse.",1
"Knowledge graphs (KGs) are of great importance to many real world applications, but they generally suffer from incomplete information in the form of missing relations between entities. Knowledge graph completion (also known as relation prediction) is the task of inferring missing facts given existing ones. Most of the existing work is proposed by maximizing the likelihood of observed instance-level triples. Not much attention, however, is paid to the ontological information, such as type information of entities and relations. In this work, we propose a type-augmented relation prediction (TaRP) method, where we apply both the type information and instance-level information for relation prediction. In particular, type information and instance-level information are encoded as prior probabilities and likelihoods of relations respectively, and are combined by following Bayes' rule. Our proposed TaRP method achieves significantly better performance than state-of-the-art methods on four benchmark datasets: FB15K, FB15K-237, YAGO26K-906, and DB111K-174. In addition, we show that TaRP achieves significantly improved data efficiency. More importantly, the type information extracted from a specific dataset can generalize well to other datasets through the proposed TaRP model.",0
"Abstract: In recent years, knowledge graphs have become increasingly popular for storing structured data. However, many existing knowledge graphs still contain incomplete or missing relations, which makes querying difficult. One approach to fill these gaps has been type augmentation, where new types are inferred from existing ones based on similarity criteria like subsumption. We present a novel method called ""type-augmented relation prediction"" that combines both type inference and relation prediction into one unified framework. Our method uses graph convolutional networks (GCN) to learn complex relationships between entities and types based on their structural patterns within the graph. Our experiments show promising results in terms of predictive accuracy and efficiency compared to other state-of-the-art methods. Additionally, our method can handle multiple ontologies, making it more flexible in real-world scenarios. This work provides insights into how machine learning techniques can improve knowledge graph quality and enhance semantic computing applications.",1
"Understanding the space of probability measures on a metric space equipped with a Wasserstein distance is one of the fundamental questions in mathematical analysis. The Wasserstein metric has received a lot of attention in the machine learning community especially for its principled way of comparing distributions. In this work, we use a permutation invariant network to map samples from probability measures into a low-dimensional space such that the Euclidean distance between the encoded samples reflects the Wasserstein distance between probability measures. We show that our network can generalize to correctly compute distances between unseen densities. We also show that these networks can learn the first and the second moments of probability distributions.",0
"In recent years, permutation invariant deep learning methods have gained popularity due to their ability to model data that exhibits permutation symmetry. These methods can capture complex patterns in the data without requiring explicit input ordering, which makes them particularly well-suited for tasks such as image classification and time series analysis. However, the use of permutation invariant neural networks to learn distances or metrics has been limited by their computational complexity and scaling issues. To address these challenges, we propose the use of permutation invariant networks to learn Wasserstein metrics. This approach combines the advantages of permutation invariance with the strengths of optimal transport theory, allowing for efficient computation and accurate distance estimation. Our experimental results demonstrate the effectiveness of our method on several benchmark datasets across different domains. We conclude that permutation invariant networks can successfully learn Wasserstein metrics, providing new opportunities for using deep learning models in applications where robust metric estimation is crucial.",1
"In recommender systems (RSs), predicting the next item that a user interacts with is critical for user retention. While the last decade has seen an explosion of RSs aimed at identifying relevant items that match user preferences, there is still a range of aspects that could be considered to further improve their performance. For example, often RSs are centered around the user, who is modeled using her recent sequence of activities. Recent studies, however, have shown the effectiveness of modeling the mutual interactions between users and items using separate user and item embeddings. Building on the success of these studies, we propose a novel method called DeePRed that addresses some of their limitations. In particular, we avoid recursive and costly interactions between consecutive short-term embeddings by using long-term (stationary) embeddings as a proxy. This enable us to train DeePRed using simple mini-batches without the overhead of specialized mini-batches proposed in previous studies. Moreover, DeePRed's effectiveness comes from the aforementioned design and a multi-way attention mechanism that inspects user-item compatibility. Experiments show that DeePRed outperforms the best state-of-the-art approach by at least 14% on next item prediction task, while gaining more than an order of magnitude speedup over the best performing baselines. Although this study is mainly concerned with temporal interaction networks, we also show the power and flexibility of DeePRed by adapting it to the case of static interaction networks, substituting the short- and long-term aspects with local and global ones.",0
"This research explores the use of dynamic embeddings for interaction prediction, focusing on understanding how contextual information can improve predictive models in natural language processing tasks such as conversational agents and chatbots. The study proposes that traditional static embeddings, which represent entities and their relationships independently from each other, may limit the ability of these systems to accurately capture important interactions between entities within complex contexts. By contrast, dynamic embeddings allow interactions to be explicitly modeled by incorporating time-dependent factors into the representation process, enabling more accurate predictions based on real-world situations. Using several benchmark datasets, the results show significant improvements in performance compared to state-of-the-art methods using static embeddings, demonstrating the potential benefits of using dynamic embeddings for interactive applications.",1
"Although deep convolutional networks have been widely studied for head and neck (HN) organs at risk (OAR) segmentation, their use for routine clinical treatment planning is limited by a lack of robustness to imaging artifacts, low soft tissue contrast on CT, and the presence of abnormal anatomy. In order to address these challenges, we developed a computationally efficient nested block self-attention (NBSA) method that can be combined with any convolutional network. Our method achieves computational efficiency by performing non-local calculations within memory blocks of fixed spatial extent. Contextual dependencies are captured by passing information in a raster scan order between blocks, as well as through a second attention layer that causes bi-directional attention flow. We implemented our approach on three different networks to demonstrate feasibility. Following training using 200 cases, we performed comprehensive evaluations using conventional and clinical metrics on a separate set of 172 test scans sourced from external and internal institution datasets without any exclusion criteria. NBSA required a similar number of computations (15.7 gflops) as the most efficient criss-cross attention (CCA) method and generated significantly more accurate segmentations for brain stem (Dice of 0.89 vs. 0.86) and parotid glands (0.86 vs. 0.84) than CCA. NBSA's segmentations were less variable than multiple 3D methods, including for small organs with low soft-tissue contrast such as the submandibular glands (surface Dice of 0.90).",0
"This paper presents a new approach to robust radiotherapy planning segmentation using nested block self-attention. We propose that incorporating contextual information from surrounding tissues can improve the accuracy of tumor contouring in challenging regions such as lungs, heart, and bones. Our method builds on previous work by introducing two novel techniques: nested-block self attention and region uncertainty estimation. Firstly, we divide the image into multiple blocks which allows us to hierarchically encode local relationships within each block before capturing interdependencies between different blocks. Secondly, we estimate the uncertainty associated with each contoured pixel based on the variability across multiple expert annotations. These uncertainties are used during optimization and lead to more realistic shapes for regions where annotator agreement is low. Experimental results demonstrate improved Dice coefficients compared to state-of-the-art methods while reducing biases related to overfitting common structures such as ribs and clavicles. Overall, our method achieves better balance between sensitivity and specificity towards target volumes while addressing some limitations of traditional attention-based models and active learning pipelines.",1
"Accurately identifying different representations of the same real-world entity is an integral part of data cleaning and many methods have been proposed to accomplish it. The challenges of this entity resolution task that demand so much research attention are often rooted in the task-specificity and user-dependence of the process. Adopting deep learning techniques has the potential to lessen these challenges. In this paper, we set out to devise an entity resolution method that builds on the robustness conferred by deep autoencoders to reduce human-involvement costs. Specifically, we reduce the cost of training deep entity resolution models by performing unsupervised representation learning. This unveils a transferability property of the resulting model that can further reduce the cost of applying the approach to new datasets by means of transfer learning. Finally, we reduce the cost of labelling training data through an active learning approach that builds on the properties conferred by the use of deep autoencoders. Empirical evaluation confirms the accomplishment of our cost-reduction desideratum while achieving comparable effectiveness with state-of-the-art alternatives.",0
"In this paper, we present a novel approach to active entity resolution that achieves state-of-the-art performance while significantly reducing computational cost. Our method leverages variational inference to efficiently optimize parameters without resorting to expensive iterative optimization techniques commonly used in prior work. To evaluate our approach, we conduct extensive experiments on real-world datasets and demonstrate the effectiveness and efficiency of our method compared to several baseline methods. By successfully resolving entities within documents using fewer resources, our method has important applications in natural language processing tasks such as question answering, machine translation, and text summarization. This work represents an advancement towards more efficient use of computing resources in NLP research and development.",1
"In this work we propose a batch Bayesian optimization method for combinatorial problems on permutations, which is well suited for expensive cost functions on permutations. We introduce LAW, a new efficient batch acquisition method based on the determinantal point process, using an acquisition weighted kernel. Relying on multiple parallel evaluations, LAW accelerates the search for the optimal permutation. We provide a regret analysis for our method to gain insight in its theoretical properties. We then apply the framework to permutation problems, which have so far received little attention in the Bayesian Optimization literature, despite their practical importance. We call this method LAW2ORDER. We evaluate the method on several standard combinatorial problems involving permutations such as quadratic assignment, flowshop scheduling and the traveling salesman, as well as on a structure learning task.",0
"Title: Efficient Global Optimization of Noisy Black Box Functions Using Permutation Testing and Kernel Methods  Many real world problems involve optimizing objective functions that cannot easily be modeled mathematically due to their complexity or uncertainty. Examples include tuning hyperparameters in machine learning models and solving control problems in engineering. In such cases, global optimization techniques have proven to be effective at finding good solutions efficiently.  One popular method for global optimization is Bayesian optimization (BO), which uses probabilistic modeling and statistical inference to guide efficient exploration of promising regions of parameter space. BO has been applied successfully to many challenging problems across different domains, but still faces significant computational demands and suffers from difficulties related to handling noisy data.  To address these issues, we propose a new algorithm called batch permutation testing on acquisition weighted kernels (PTAWK) that leverages recent advances in permutation testing theory and kernel methods for BO. PTAWK improves upon previous approaches by reducing computation costs associated with evaluating candidate solutions and increasing robustness to noise in observed function values. This leads to faster convergence rates and better solution quality overall.  Experiments conducted using synthetic benchmarks demonstrate the effectiveness of our proposed approach relative to state-of-the-art alternatives. These results suggest that PTAWK can provide valuable guidance for designing more efficient algorithms capable of effectively solving complex black box optimization tasks. Future work includes adapting PTAWK to other problem classes and extending the theoretical understanding of how permutation testing interacts with BO.",1
"In the process of making a movie, directors constantly care about where the spectator will look on the screen. Shot composition, framing, camera movements or editing are tools commonly used to direct attention. In order to provide a quantitative analysis of the relationship between those tools and gaze patterns, we propose a new eye-tracking database, containing gaze pattern information on movie sequences, as well as editing annotations, and we show how state-of-the-art computational saliency techniques behave on this dataset. In this work, we expose strong links between movie editing and spectators scanpaths, and open several leads on how the knowledge of editing information could improve human visual attention modeling for cinematic content. The dataset generated and analysed during the current study is available at https://github.com/abruckert/eye_tracking_filmmaking",0
"This paper presents a new methodology for studying film editing through analysis of viewers' gaze behavior during movie watching. By using eye tracking technology, we were able to record where participants looked while they watched several different films, and then analyze how their visual attention was directed by the editorial decisions made by the filmmakers. Our findings show that certain types of shots and cuts are more likely to draw viewers' attention than others, and that the placement of actors within the frame can significantly impact where viewers look. Additionally, our results suggest that experienced editors make use of specific strategies to guide viewers' attention towards important story elements and create emotional effects. Overall, this study provides valuable insights into how filmmakers can effectively control and manipulate viewers' focus in order to convey meaning and achieve particular artistic goals.",1
"With the aim of matching a pair of instances from two different modalities, cross modality mapping has attracted growing attention in the computer vision community. Existing methods usually formulate the mapping function as the similarity measure between the pair of instance features, which are embedded to a common space. However, we observe that the relationships among the instances within a single modality (intra relations) and those between the pair of heterogeneous instances (inter relations) are insufficiently explored in previous approaches. Motivated by this, we redefine the mapping function with relational reasoning via graph modeling, and further propose a GCN-based Relational Reasoning Network (RR-Net) in which inter and intra relations are efficiently computed to universally resolve the cross modality mapping problem. Concretely, we first construct two kinds of graph, i.e., Intra Graph and Inter Graph, to respectively model intra relations and inter relations. Then RR-Net updates all the node features and edge features in an iterative manner for learning intra and inter relations simultaneously. Last, RR-Net outputs the probabilities over the edges which link a pair of heterogeneous instances to estimate the mapping results. Extensive experiments on three example tasks, i.e., image classification, social recommendation and sound recognition, clearly demonstrate the superiority and universality of our proposed model.",0
"Our approach proposes a universal model that can effectively learn cross modal mappings using relational reasoning techniques. By leveraging existing state-of-the-art models trained on individual modalities, we aim to capture relationships across different data types such as images, text, audio, and video. In our experiments, we demonstrate that our framework outperforms baseline methods and provides improved performance compared to standalone single modality systems. Additionally, we showcase several applications of the proposed method including zero shot transfer learning and multimodal retrieval tasks which highlights its utility in real world scenarios. Overall, our work represents a step towards developing more robust and versatile artificial intelligence solutions that can handle diverse datasets and provide enhanced predictions.",1
"Recently, a lot of attention has been focused on the incorporation of 3D data into face analysis and its applications. Despite providing a more accurate representation of the face, 3D facial images are more complex to acquire than 2D pictures. As a consequence, great effort has been invested in developing systems that reconstruct 3D faces from an uncalibrated 2D image. However, the 3D-from-2D face reconstruction problem is ill-posed, thus prior knowledge is needed to restrict the solutions space. In this work, we review 3D face reconstruction methods proposed in the last decade, focusing on those that only use 2D pictures captured under uncontrolled conditions. We present a classification of the proposed methods based on the technique used to add prior knowledge, considering three main strategies, namely, statistical model fitting, photometry, and deep learning, and reviewing each of them separately. In addition, given the relevance of statistical 3D facial models as prior knowledge, we explain the construction procedure and provide a list of the most popular publicly available 3D facial models. After the exhaustive study of 3D-from-2D face reconstruction approaches, we observe that the deep learning strategy is rapidly growing since the last few years, becoming the standard choice in replacement of the widespread statistical model fitting. Unlike the other two strategies, photometry-based methods have decreased in number due to the need for strong underlying assumptions that limit the quality of their reconstructions compared to statistical model fitting and deep learning methods. The review also identifies current challenges and suggests avenues for future research.",0
"This paper presents a survey of recent advances in 3D face reconstruction from uncalibrated image sources. The process of 3D face reconstruction involves creating a three-dimensional model of a human face using two-dimensional images as input. Uncalibrated images refer to photos that have not been taken under controlled conditions, such as those without specific lighting or camera settings. Despite these challenges, significant progress has been made in improving algorithms capable of accurately reconstructing 3D faces using uncalibrated images alone. In this review, we explore several state-of-the-art methods used for this purpose and evaluate their performance. We conclude by discussing potential future directions for research in this area and suggesting areas where further improvement may be necessary.",1
"Recently, temporal action localization (TAL), i.e., finding specific action segments in untrimmed videos, has attracted increasing attentions of the computer vision community. State-of-the-art solutions for TAL involves evaluating the frame-level probabilities of three action-indicating phases, i.e. starting, continuing, and ending; and then post-processing these predictions for the final localization. This paper delves deep into this mechanism, and argues that existing methods, by modeling these phases as individual classification tasks, ignored the potential temporal constraints between them. This can lead to incorrect and/or inconsistent predictions when some frames of the video input lack sufficient discriminative information. To alleviate this problem, we introduce two regularization terms to mutually regularize the learning procedure: the Intra-phase Consistency (IntraC) regularization is proposed to make the predictions verified inside each phase; and the Inter-phase Consistency (InterC) regularization is proposed to keep consistency between these phases. Jointly optimizing these two terms, the entire framework is aware of these potential constraints during an end-to-end optimization process. Experiments are performed on two popular TAL datasets, THUMOS14 and ActivityNet1.3. Our approach clearly outperforms the baseline both quantitatively and qualitatively. The proposed regularization also generalizes to other TAL methods (e.g., TSA-Net and PGCN). code: https://github.com/PeisenZhao/Bottom-Up-TAL-with-MR",0
"Title: Enhanced Temporal Action Localization through Mutual Regularization  Abstract: Temporal action localization has become increasingly important as video data becomes more prevalent due to advancements in sensor technology and accessibility. Current state-of-the art methods focus on top-down approaches that rely heavily on feature extraction techniques and often require high computational resources, which can limit their use in real-world applications. This study proposes a bottom-up approach using mutual regularization techniques to improve temporal action localization while reducing computation demand. By breaking down complex actions into simpler components and iteratively modeling them, our method allows for greater accuracy without requiring expensive hardware or intensive processing. Results from experiments using multiple datasets demonstrate improved performance compared to current top-down approaches, making this method well-suited for real-time monitoring scenarios where efficient and accurate detection is critical. Overall, our work represents a significant step forward in the development of effective and accessible temporal action recognition technologies.",1
"We propose a notation for tensors with named axes, which relieves the author, reader, and future implementers from the burden of keeping track of the order of axes and the purpose of each. It also makes it easy to extend operations on low-order tensors to higher order ones (e.g., to extend an operation on images to minibatches of images, or extend the attention mechanism to multiple attention heads). After a brief overview of our notation, we illustrate it through several examples from modern machine learning, from building blocks like attention and convolution to full models like Transformers and LeNet. Finally, we give formal definitions and describe some extensions. Our proposals build on ideas from many previous papers and software libraries. We hope that this document will encourage more authors to use named tensors, resulting in clearer papers and less bug-prone implementations.   The source code for this document can be found at https://github.com/namedtensor/notation/. We invite anyone to make comments on this proposal by submitting issues or pull requests on this repository.",0
"This research proposes a new approach to tensor notation that improves upon existing methods by providing more concise and expressive code. The proposed method builds on prior work in computer graphics programming but extends it into other areas where tensors play a central role, such as machine learning and scientific computing. The main contribution of this paper is a set of new tensor operations and data structures designed to simplify the process of working with high-dimensional arrays. These operations are based on standard mathematical conventions but have been carefully chosen to minimize redundancy and promote readability. Additionally, we introduce a novel syntax for defining tensors that allows them to be constructed quickly and easily using list comprehension. We evaluate our system through several case studies demonstrating how the proposed notation can improve the productivity of developers working on tensor applications. Our results show significant improvements over traditional approaches in terms of both speed and readability. We also provide qualitative evidence from domain experts who found our approach easier to use than current industry standards. Overall, we believe that named tensor notation represents a valuable tool for anyone working with large datasets or performing complex calculations involving multiple variables. Its adoption could lead to faster development times and improved software quality across many domains.",1
"In the field of human-robot interaction, teaching learning agents from human demonstrations via supervised learning has been widely studied and successfully applied to multiple domains such as self-driving cars and robot manipulation. However, the majority of the work on learning from human demonstrations utilizes only behavioral information from the demonstrator, i.e. what actions were taken, and ignores other useful information. In particular, eye gaze information can give valuable insight towards where the demonstrator is allocating their visual attention, and leveraging such information has the potential to improve agent performance. Previous approaches have only studied the utilization of attention in simple, synchronous environments, limiting their applicability to real-world domains. This work proposes a novel imitation learning architecture to learn concurrently from human action demonstration and eye tracking data to solve tasks where human gaze information provides important context. The proposed method is applied to a visual navigation task, in which an unmanned quadrotor is trained to search for and navigate to a target vehicle in a real-world, photorealistic simulated environment. When compared to a baseline imitation learning architecture, results show that the proposed gaze augmented imitation learning model is able to learn policies that achieve significantly higher task completion rates, with more efficient paths, while simultaneously learning to predict human visual attention. This research aims to highlight the importance of multimodal learning of visual attention information from additional human input modalities and encourages the community to adopt them when training agents from human demonstrations to perform visuomotor tasks.",0
"In order to complete tasks effectively and efficiently, robots need to learn how to navigate complex environments. One method by which they can accomplish this task is through imitating human demonstrations. This involves observing the actions taken by humans to achieve specific goals and then replicating those actions in similar circumstances. However, traditional methods of imitation learning have limitations in terms of their ability to handle multiple objectives simultaneously and adapting to changes in the environment or task requirements. As such, there is a need for developing new approaches that overcome these challenges. The paper proposes using gaze informatics as a means of improving multi-objective imitation learning from human demonstrations (MIHL). By incorporating information about where humans look while performing tasks into MIHL algorithms, robots can gain additional contextual knowledge, allowing them to make better decisions regarding which action steps are relevant for achieving different outcomes concurrently. Additionally, this approach enables robots to detect environmental changes quickly, adjust accordingly, and perform tasks more accurately than existing methods. This research focuses on evaluating whether gaze informatics significantly enhances robot performance in handling multi-objective imitation learning tasks under varying conditions and scenarios. Empirical experiments were conducted using real human participants interactively performing activities involving reaching-grasping objects while manipulating obstacles. Results showed significant improvements in overall success rates of completion when utilizing gaze information compared to non-gaze approaches. Further analysis revealed valuable insights into factors affecting success and failure cases under different situations and configurations. Overall, this study contributes t...",1
"Continual learning (CL) is a setting in which an agent has to learn from an incoming stream of data during its entire lifetime. Although major advances have been made in the field, one recurring problem which remains unsolved is that of Catastrophic Forgetting (CF). While the issue has been extensively studied empirically, little attention has been paid from a theoretical angle. In this paper, we show that the impact of CF increases as two tasks increasingly align. We introduce a measure of task similarity called the NTK overlap matrix which is at the core of CF. We analyze common projected gradient algorithms and demonstrate how they mitigate forgetting. Then, we propose a variant of Orthogonal Gradient Descent (OGD) which leverages structure of the data through Principal Component Analysis (PCA). Experiments support our theoretical findings and show how our method can help reduce CF on classical CL datasets.",0
"This paper presents a theoretical analysis of catastrophic forgetting using the neural tangent kernel (NTK) overlap matrix framework. We examine how changes to the network architecture can impact memory retention during sequential learning tasks. Our findings suggest that increasing the number of neurons per layer can mitigate catastrophic forgetting by allowing the model to maintain greater fidelity to the task sequence. Furthermore, we show that carefully controlling weight decay rates can improve memory retention without sacrificing accuracy on current tasks. These results provide new insight into the mechanisms underlying catastrophic forgetting and offer potential strategies for improving continual learning algorithms.",1
"Ship detection in remote sensing images plays a crucial role in various applications and has drawn increasing attention in recent years. However, existing multi-oriented ship detection methods are generally developed on a set of predefined rotated anchor boxes. These predefined boxes not only lead to inaccurate angle predictions but also introduce extra hyper-parameters and high computational cost. Moreover, the prior knowledge of ship size has not been fully exploited by existing methods, which hinders the improvement of their detection accuracy. Aiming at solving the above issues, in this paper, we propose a \emph{center-head point extraction based detector} (named CHPDet) to achieve arbitrary-oriented ship detection in remote sensing images. Our CHPDet formulates arbitrary-oriented ships as rotated boxes with head points which are used to determine the direction. The orientation-invariant model (OIM) is used to produce orientation-invariant feature maps. Keypoint estimation is performed to find the center of ships. Then, the size and head point of the ships are regressed. Finally, we use the target size as prior to finetune the results. Moreover, we introduce a new dataset for multi-class arbitrary-oriented ship detection in remote sensing images at a fixed ground sample distance (GSD) which is named FGSD2021. Experimental results on two ship detection datasets (i.e., FGSD2021 and HRSC2016) demonstrate that our CHPDet achieves state-of-the-art performance and can well distinguish between bow and stern. The code and dataset will be made publicly available.",0
"This paper presents an algorithm for ship detection that uses center-head point extraction as a means of orientation determination. Traditional methods for object detection often rely on feature extraction techniques, which can be limited by their reliance on predefined features. By contrast, our approach utilizes deep learning models trained on synthetic data generated from real-world imagery. Our method involves generating arbitrary orientations for objects using rendered images and then training a classifier to detect these objects within unseen images. We evaluate the performance of our algorithm against a benchmark dataset consisting of satellite images of harbors and coastlines. Results show that our approach achieves state-of-the-art accuracy in ship detection while maintaining a high degree of flexibility and generalization across different environments. In conclusion, we believe that this work represents a significant step forward in the field of computer vision and provides new possibilities for automated image analysis applications.",1
"With the broader and highly successful usage of machine learning in industry and the sciences, there has been a growing demand for Explainable AI. Interpretability and explanation methods for gaining a better understanding about the problem solving abilities and strategies of nonlinear Machine Learning, in particular, deep neural networks, are therefore receiving increased attention. In this work we aim to (1) provide a timely overview of this active emerging field, with a focus on 'post-hoc' explanations, and explain its theoretical foundations, (2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations, (3) outline best practice aspects i.e. how to best include interpretation methods into the standard usage of machine learning and (4) demonstrate successful usage of explainable AI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of machine learning.",0
"This paper provides a comprehensive review of methods used to explain deep neural networks (DNNs) and other machine learning models, as well as their applications across diverse fields such as computer vision, natural language processing, speech recognition, robotics, and decision making systems. We cover the fundamental concepts behind DNNs including artificial neurons, activation functions, backpropagation algorithm, and training objectives. In addition, we discuss several popular explanation techniques that have been proposed over the years, along with their respective strengths and limitations. These techniques fall into three categories: model interpretability approaches that aim at providing transparent understanding, model performance analysis approaches focused on measuring predictive accuracy, and feature relevance assessment approaches that explore which features contribute most significantly to predictions made by complex models. Finally, we provide case studies from real world application scenarios where these methods have been successfully implemented, illustrating their efficacy and impact in resolving challenges faced by businesses and organizations today.",1
"Carton detection is an important technique in the automatic logistics system and can be applied to many applications such as the stacking and unstacking of cartons, the unloading of cartons in the containers. However, there is no public large-scale carton dataset for the research community to train and evaluate the carton detection models up to now, which hinders the development of carton detection. In this paper, we present a large-scale carton dataset named Stacked Carton Dataset(SCD) with the goal of advancing the state-of-the-art in carton detection. Images are collected from the internet and several warehourses, and objects are labeled using per-instance segmentation for precise localization. There are totally 250,000 instance masks from 16,136 images. In addition, we design a carton detector based on RetinaNet by embedding Offset Prediction between Classification and Localization module(OPCL) and Boundary Guided Supervision module(BGS). OPCL alleviates the imbalance problem between classification and localization quality which boosts AP by 3.1% - 4.7% on SCD while BGS guides the detector to pay more attention to boundary information of cartons and decouple repeated carton textures. To demonstrate the generalization of OPCL to other datasets, we conduct extensive experiments on MS COCO and PASCAL VOC. The improvement of AP on MS COCO and PASCAL VOC is 1.8% - 2.2% and 3.4% - 4.3% respectively.",0
"The authors present SCD: Stacked Carton Dataset, a large-scale dataset consisting of images of stacked cartons in real-world scenarios. This dataset can be used to train algorithms for object detection and segmentation tasks related to box piles in warehouses and other environments. The dataset contains multiple categories of cartons with varying shapes, sizes, colors, lighting conditions, and backgrounds to challenge existing methods. In addition to providing images, the dataset includes annotations that consist of bounding boxes and pixel masks for each object in each image. To facilitate further research and development, the SCD dataset is made publicly available along with code examples on how to load and use the data.  This work addresses the limitations of current datasets by introducing more diverse and challenging scenarios such as occluded objects, cluttered scenes, reflections, shadows, and dynamic environments. By expanding the number of objects per image and increasing their variability compared to previous datasets, the SCD dataset provides a richer representation of real-life situations faced by robots working with cartons today. Evaluations show significant improvements over state-of-the-art models trained on popular benchmark datasets, demonstrating the value of this new resource. Overall, SCD holds great potential for enabling innovation in computer vision applications like robotic grasping, manipulation, and inventory management.",1
"Human action recognition as an important application of computer vision has been studied for decades. Among various approaches, skeleton-based methods recently attract increasing attention due to their robust and superior performance. However, existing skeleton-based methods ignore the potential action relationships between different persons, while the action of a person is highly likely to be impacted by another person especially in complex events. In this paper, we propose a novel group-skeleton-based human action recognition method in complex events. This method first utilizes multi-scale spatial-temporal graph convolutional networks (MS-G3Ds) to extract skeleton features from multiple persons. In addition to the traditional key point coordinates, we also input the key point speed values to the networks for better performance. Then we use multilayer perceptrons (MLPs) to embed the distance values between the reference person and other persons into the extracted features. Lastly, all the features are fed into another MS-G3D for feature fusion and classification. For avoiding class imbalance problems, the networks are trained with a focal loss. The proposed algorithm is also our solution for the Large-scale Human-centric Video Analysis in Complex Events Challenge. Results on the HiEve dataset show that our method can give superior performance compared to other state-of-the-art methods.",0
This is not possible without including the title of the paper in some capacity. If you could provide me with the full paper title I can generate the abstract from that?,1
"In this paper, we benchmark several existing graph neural network (GNN) models on different datasets for link predictions. In particular, the graph convolutional network (GCN), GraphSAGE, graph attention network (GAT) as well as variational graph auto-encoder (VGAE) are implemented dedicated to link prediction tasks, in-depth analysis are performed, and results from several different papers are replicated, also a more fair and systematic comparison are provided. Our experiments show these GNN architectures perform similarly on various benchmarks for link prediction tasks.",0
"This abstract should provide a good overview of the entire project without going into detail about methods or results (use another section for that). Focus here on setting up the scene, stating your research question, outlining how you plan to test your hypothesis, identifying any important variables involved, discussing expected contributions to the field, identifying potential applications, highlighting unique approaches you used if applicable and describing why the problem matters. ---",1
"Communication is a important factor that enables agents work cooperatively in multi-agent reinforcement learning (MARL). Most previous work uses continuous message communication whose high representational capacity comes at the expense of interpretability. Allowing agents to learn their own discrete message communication protocol emerged from a variety of domains can increase the interpretability for human designers and other agents.This paper proposes a method to generate discrete messages analogous to human languages, and achieve communication by a broadcast-and-listen mechanism based on self-attention. We show that discrete message communication has performance comparable to continuous message communication but with much a much smaller vocabulary size.Furthermore, we propose an approach that allows humans to interactively send discrete messages to agents.",0
"Artificial intelligence (AI) systems have made significant strides towards solving complex problems using deep reinforcement learning methods. However, these approaches require large amounts of environment interaction to learn policies that maximize rewards efficiently. To reduce the amount of interactions required, recent work has explored communication mechanisms between agents to facilitate coordination. In this paper, we propose a novel method called emergent discrete message communication (EDMC), which allows multiple cooperating agents to communicate effectively without requiring explicit messages. EDMC utilizes a set of continuous low-dimensional embeddings that represent the current state of each agent. These embeddings are then encoded into binary signals corresponding to discrete actions. By optimizing jointly over both policy parameters and communication signals, our approach enables efficient communication between agents while reducing the need for excessive environment interactions. Our experiments on challenging multi-agent environments demonstrate the effectiveness of EDMC, outperforming several strong baselines across different settings. Overall, our findings suggest that leveraging emergent discrete message communication can significantly improve efficiency and performance in cooperative multi-agent reinforcement learning tasks.",1
"Facial expression recognition from videos in the wild is a challenging task due to the lack of abundant labelled training data. Large DNN (deep neural network) architectures and ensemble methods have resulted in better performance, but soon reach saturation at some point due to data inadequacy. In this paper, we use a self-training method that utilizes a combination of a labelled dataset and an unlabelled dataset (Body Language Dataset - BoLD). Experimental analysis shows that training a noisy student network iteratively helps in achieving significantly better results. Additionally, our model isolates different regions of the face and processes them independently using a multi-level attention mechanism which further boosts the performance. Our results show that the proposed method achieves state-of-the-art performance on benchmark datasets CK+ and AFEW 8.0 when compared to other single models.",0
"This paper presents an approach for improving facial expression recognition models by training on noisy student datasets. We use a novel body language dataset that contains labels for both face and full-body poses, allowing us to create synthetic noise samples based on randomized pose changes. By incorporating these noise samples into our model training, we demonstrate improved performance on benchmark tests compared to standard supervised learning methods without noise augmentation. Our method can effectively regularize the model and reduce overfitting, enabling better generalization to unseen data. We evaluate the effectiveness of our approach across several popular facial expression recognition architectures and show consistent improvement in accuracy metrics. Overall, our work highlights the importance of considering noise during model training and demonstrates the benefits of using diverse datasets to improve performance.",1
"The paper approaches the task of handwritten text recognition (HTR) with attentional encoder-decoder networks trained on sequences of characters, rather than words. We experiment on lines of text from popular handwriting datasets and compare different activation functions for the attention mechanism used for aligning image pixels and target characters. We find that softmax attention focuses heavily on individual characters, while sigmoid attention focuses on multiple characters at each step of the decoding. When the sequence alignment is one-to-one, softmax attention is able to learn a more precise alignment at each step of the decoding, whereas the alignment generated by sigmoid attention is much less precise. When a linear function is used to obtain attention weights, the model predicts a character by looking at the entire sequence of characters and performs poorly because it lacks a precise alignment between the source and target. Future research may explore HTR in natural scene images, since the model is capable of transcribing handwritten text without the need for producing segmentations or bounding boxes of text in images.",0
This paper presents a novel approach to handwriting recognition based on character analysis using attention networks. The proposed method leverages convolutional neural networks (CNNs) along with recurrent layers and spatial transformer modules to process text images. The character-level transcriptions are obtained through beam search decoding. Experimental results demonstrate that our approach achieves state-of-the-art accuracy while significantly reducing computational requirements compared to existing methods. Our work paves the way for developing real-time handwriting recognition systems for smart devices and digital pen technology. -----,1
"Two apparently unrelated fields -- normalizing flows and causality -- have recently received considerable attention in the machine learning community. In this work, we highlight an intrinsic correspondence between a simple family of autoregressive normalizing flows and identifiable causal models. We exploit the fact that autoregressive flow architectures define an ordering over variables, analogous to a causal ordering, to show that they are well-suited to performing a range of causal inference tasks, ranging from causal discovery to making interventional and counterfactual predictions. First, we show that causal models derived from both affine and additive autoregressive flows with fixed orderings over variables are identifiable, i.e. the true direction of causal influence can be recovered. This provides a generalization of the additive noise model well-known in causal discovery. Second, we derive a bivariate measure of causal direction based on likelihood ratios, leveraging the fact that flow models can estimate normalized log-densities of data. Third, we demonstrate that flows naturally allow for direct evaluation of both interventional and counterfactual queries, the latter case being possible due to the invertible nature of flows. Finally, throughout a series of experiments on synthetic and real data, the proposed method is shown to outperform current approaches for causal discovery as well as making accurate interventional and counterfactual predictions.",0
"This paper presents a new approach to modeling complex high-dimensional datasets using autoregressive flows (ARF), which are continuous functions that transform one distribution into another while preserving their causal structure. ARFs have recently gained popularity due to their ability to accurately capture the dependencies within data distributions, particularly for large-scale and nonlinear problems. In addition, they can preserve the temporal relationships among variables, making them well suited for modeling time series data. We extend traditional ARF models by introducing a novel framework called causal autoregressive flows (CAF) that allows us to incorporate explicit causal knowledge into the learning process. Our method uses conditional independence restrictions induced from known causal graphs, enabling more efficient estimation and improved accuracy compared to existing methods. The proposed methodology is applied to several real-world applications including brain imaging, finance, and climate science. Results demonstrate the superior performance of our CAF model over competing approaches in terms of both quantitative metrics and visual inspections. Overall, our work represents a significant contribution towards developing powerful tools for high-dimensional inference tasks where data complexity hinders traditional analysis techniques.",1
"Classification with abstention has gained a lot of attention in recent years as it allows to incorporate human decision-makers in the process. Yet, abstention can potentially amplify disparities and lead to discriminatory predictions. The goal of this work is to build a general purpose classification algorithm, which is able to abstain from prediction, while avoiding disparate impact. We formalize this problem as risk minimization under fairness and abstention constraints for which we derive the form of the optimal classifier. Building on this result, we propose a post-processing classification algorithm, which is able to modify any off-the-shelf score-based classifier using only unlabeled sample. We establish finite sample risk, fairness, and abstention guarantees for the proposed algorithm. In particular, it is shown that fairness and abstention constraints can be achieved independently from the initial classifier as long as sufficiently many unlabeled data is available. The risk guarantee is established in terms of the quality of the initial classifier. Our post-processing scheme reduces to a sparse linear program allowing for an efficient implementation, which we provide. Finally, we validate our method empirically showing that moderate abstention rates allow to bypass the risk-fairness trade-off.",0
"This study proposes a novel approach for classification that involves abstaining from making predictions rather than making biased predictions or misclassifying data points based on sensitive features such as race, gender or ethnicity. Abstaining can prevent disparities, especially if models are applied in high-stakes decisions. However, current methods either assume full knowledge of feature importance or rely on unreliable assumptions about model uncertainty. We propose a method called Disparate Classification with Uncertainty Abstention (DISCUA) that uses a combination of model uncertainty and statistical tests for equality among groups to determine whether to make predictions or abstain. Experimental results show DISCUA outperforms state-of-the-art methods at reducing disparities while maintaining high accuracy overall. The implications of our work suggest a promising direction towards mitigating bias in machine learning applications.",1
"The pressure of ever-increasing patient demand and budget restrictions make hospital bed management a daily challenge for clinical staff. Most critical is the efficient allocation of resource-heavy Intensive Care Unit (ICU) beds to the patients who need life support. Central to solving this problem is knowing for how long the current set of ICU patients are likely to stay in the unit. In this work, we propose a new deep learning model based on the combination of temporal convolution and pointwise (1x1) convolution, to solve the length of stay prediction task on the eICU and MIMIC-IV critical care datasets. The model - which we refer to as Temporal Pointwise Convolution (TPC) - is specifically designed to mitigate common challenges with Electronic Health Records, such as skewness, irregular sampling and missing data. In doing so, we have achieved significant performance benefits of 18-68% (metric and dataset dependent) over the commonly used Long-Short Term Memory (LSTM) network, and the multi-head self-attention network known as the Transformer. By adding mortality prediction as a side-task, we can improve performance further still, resulting in a mean absolute deviation of 1.55 days (eICU) and 2.28 days (MIMIC-IV) on predicting remaining length of stay.",0
"In recent years, there has been significant interest in developing predictive models that can accurately forecast patient outcomes in the intensive care unit (ICU). One important outcome that remains challenging to predict is length of stay (LOS), which is critical for managing hospital resources and ensuring patients receive appropriate care. In this paper, we present a novel approach using temporal pointwise convolutional networks (TPCN) to predict ICU LOS. Our method leverages the strengths of both recurrent neural networks (RNNs) and conventional CNNs to capture complex patterns in time-series data while maintaining computational efficiency. We evaluate our model on two large publicly available datasets: MIMIC-III and CODAITM and demonstrate superior performance compared to state-of-the-art methods. Our findings have important implications for clinical decision making and resource allocation in the ICU setting.",1
"Time series classification problems have drawn increasing attention in the machine learning and statistical community. Closely related is the field of functional data analysis (FDA): it refers to the range of problems that deal with the analysis of data that is continuously indexed over some domain. While often employing different methods, both fields strive to answer similar questions, a common example being classification or regression problems with functional covariates. We study methods from functional data analysis, such as functional generalized additive models, as well as functionality to concatenate (functional-) feature extraction or basis representations with traditional machine learning algorithms like support vector machines or classification trees. In order to assess the methods and implementations, we run a benchmark on a wide variety of representative (time series) data sets, with in-depth analysis of empirical results, and strive to provide a reference ranking for which method(s) to use for non-expert practitioners. Additionally, we provide a software framework in R for functional data analysis for supervised learning, including machine learning and more linear approaches from statistics. This allows convenient access, and in connection with the machine-learning toolbox mlr, those methods can now also be tuned and benchmarked.",0
"This paper explores the effectiveness of functional data versus machine learning approaches for benchmarking time series classification. We compare two popular methods for representing functional data, including smoothed representations using spline functions and principal component analysis (PCA), against several prominent machine learning algorithms, including random forest and gradient boosting machines. Our results show that both PCA and spline based representations provide competitive performance compared to some of the most advanced machine learning techniques. While random forest outperforms all other models on average, we find that PCA consistently performs well across different datasets and outperforms gradient boosting machines. These results highlight the potential utility of utilizing simple, non-parametric functional data representations alongside state-of-the-art machine learning techniques in time series benchmarking. Ultimately, our study suggests the need for further research into developing more powerful functional data techniques and combining them effectively with modern machine learning algorithms to improve classification accuracy.",1
"With great progress in the development of Generative Adversarial Networks (GANs), in recent years, the quest for insights in understanding and manipulating the latent space of GAN has gained more and more attention due to its wide range of applications. While most of the researches on this task have focused on unsupervised learning method, which induces difficulties in training and limitation in results, our work approaches another direction, encoding human's prior knowledge to discover more about the hidden space of GAN. With this supervised manner, we produce promising results, demonstrated by accurate manipulation of generated images. Even though our model is more suitable for task-specific problems, we hope that its ease in implementation, preciseness, robustness, and the allowance of richer set of properties (compared to other approaches) for image manipulation can enhance the result of many current applications.",0
"""Generative adversarial networks (GANs) have recently emerged as powerful tools for generating realistic synthetic data. However, their latent space is typically left unexplored due to difficulties inherent to interpreting high-dimensional spaces. Here we present a method to interpret GAN latent spaces using supervised learning techniques. Specifically, we train linear regression models on selected features extracted from both the input images and the generated samples that map to those inputs. We demonstrate the effectiveness of our approach by analyzing two well known datasets; CelebA and MNIST.""",1
"Single image dehazing is a challenging ill-posed problem that has drawn significant attention in the last few years. Recently, convolutional neural networks have achieved great success in image dehazing. However, it is still difficult for these increasingly complex models to recover accurate details from the hazy image. In this paper, we pay attention to the feature extraction and utilization of the input image itself. To achieve this, we propose a Multi-scale Topological Network (MSTN) to fully explore the features at different scales. Meanwhile, we design a Multi-scale Feature Fusion Module (MFFM) and an Adaptive Feature Selection Module (AFSM) to achieve the selection and fusion of features at different scales, so as to achieve progressive image dehazing. This topological network provides a large number of search paths that enable the network to extract abundant image features as well as strong fault tolerance and robustness. In addition, ASFM and MFFM can adaptively select important features and ignore interference information when fusing different scale representations. Extensive experiments are conducted to demonstrate the superiority of our method compared with state-of-the-art methods.",0
"This paper presents a novel method for dehazing single images using multi-scale topological networks. Traditional haze removal techniques typically rely on filtering approaches that can lead to loss of image details or over-smoothing effects. In contrast, our approach leverages graph neural networks (GNNs) to model complex relationships between pixels in different scales, allowing for more accurate and efficient decomposition of hazy inputs. By constructing a hierarchy of graphs at multiple resolutions, we can capture localized features as well as global contextual information that may not have been captured by previous methods. Our experimental results demonstrate that our proposed method outperforms state-of-the-art algorithms in terms of both quantitative metrics and visual quality assessment, while also providing robustness against various light conditions and weather scenarios. Overall, our work represents a significant step forward in solving the challenges associated with haze removal from single images, paving the way for new applications in computer vision and image processing.",1
"Many cultures around the world believe that palm reading can be used to predict the future life of a person. Palmistry uses features of the hand such as palm lines, hand shape, or fingertip position. However, the research on palm-line detection is still scarce, many of them applied traditional image processing techniques. In most real-world scenarios, images usually are not in well-conditioned, causing these methods to severely under-perform. In this paper, we propose an algorithm to extract principle palm lines from an image of a person's hand. Our method applies deep learning networks (DNNs) to improve performance. Another challenge of this problem is the lack of training data. To deal with this issue, we handcrafted a dataset from scratch. From this dataset, we compare the performance of readily available methods with ours. Furthermore, based on the UNet segmentation neural network architecture and the knowledge of attention mechanism, we propose a highly efficient architecture to detect palm-lines. We proposed the Context Fusion Module to capture the most important context feature, which aims to improve segmentation accuracy. The experimental results show that it outperforms the other methods with the highest F1 Score about 99.42% and mIoU is 0.584 for the same dataset.",0
"This paper presents a new method for efficient palm-line segmentation using deep learning techniques. We propose a novel architecture based on the popular U-net model that incorporates context fusion modules (CFM) to improve the accuracy of the segmentation results. Our approach leverages both local features from convolutional layers and global features from downsampled feature maps in order to effectively capture the complex structure of the palms and their lines. Extensive experiments demonstrate the effectiveness of our proposed CFM strategy, achieving state-of-the-art performance on challenging datasets. Overall, we believe our work represents a significant contribution towards realizing reliable palm-line segmentation solutions suitable for real world applications such as biometric recognition systems and health monitoring devices.",1
"Convolutional neural networks have established themselves over the past years as the state of the art method for image classification, and for many datasets, they even surpass humans in categorizing images. Unfortunately, the same architectures perform much worse when they have to compare parts of an image to each other to correctly classify this image.   Until now, no well-formed theoretical argument has been presented to explain this deficiency. In this paper, we will argue that convolutional layers are of little use for such problems, since comparison tasks are global by nature, but convolutional layers are local by design. We will use this insight to reformulate a comparison task into a sorting task and use findings on sorting networks to propose a lower bound for the number of parameters a neural network needs to solve comparison tasks in a generalizable way. We will use this lower bound to argue that attention, as well as iterative/recurrent processing, is needed to prevent a combinatorial explosion.",0
"While convolutional neural networks (CNNs) have been very successful at solving computer vision tasks requiring local patterns such as object recognition and detection, their suitability for non-local pattern tasks remains unproven. In our work we argue that CNNs are ill-suited for many important classes of non-local reasoning problems because they cannot retain global contextual information without suffering from computational explosion due to the large number of parameters and computations required for every layer. We demonstrate these issues by discussing four examples: predicting protein structur e on single molecule images which requires 3D positional information; reasoning over sets of objects, exemplified in counting cars ; understanding stories involving chains of events like robberies , where recall of temporal relationships matters more than spatial arrangement; and model selection for text generation. Through analysis of these diverse example domains, we show that traditional architectures and techniques used in most recent CNN approaches can lead to severe performance degradation compared to other modalities for the same domain-specific problem. Overall, this paper argues against relying exclusively on deep learning approaches with emphasis on fully connected layers only . For non-local reasoning scenarios, hybrid models containing both CNN and traditional components or purely traditional systems should be considered instead.",1
"Nowadays underwater vision systems are being widely applied in ocean research. However, the largest portion of the ocean - the deep sea - still remains mostly unexplored. Only relatively few image sets have been taken from the deep sea due to the physical limitations caused by technical challenges and enormous costs. Deep sea images are very different from the images taken in shallow waters and this area did not get much attention from the community. The shortage of deep sea images and the corresponding ground truth data for evaluation and training is becoming a bottleneck for the development of underwater computer vision methods. Thus, this paper presents a physical model-based image simulation solution, which uses an in-air texture and depth information as inputs, to generate underwater image sequences taken by robots in deep ocean scenarios. Different from shallow water conditions, artificial illumination plays a vital role in deep sea image formation as it strongly affects the scene appearance. Our radiometric image formation model considers both attenuation and scattering effects with co-moving spotlights in the dark. By detailed analysis and evaluation of the underwater image formation model, we propose a 3D lookup table structure in combination with a novel rendering strategy to improve simulation performance. This enables us to integrate an interactive deep sea robotic vision simulation in the Unmanned Underwater Vehicles simulator. To inspire further deep sea vision research by the community, we will release the source code of our deep sea image converter to the public.",0
"In this paper we propose a new system which can simulate real time data from deep sea environments allowing researchers to develop robotics for use in these difficult conditions. Our system uses neural networks and genetic algorithms to create photorealistic images of subsea habitats that adapt based on user input. We present results of experiments conducted using our simulator, including increased efficiency in manipulation tasks and improved perception capabilities of underwater robots. Furthermore, as deep-sea exploration becomes more prevalent, our simulator has the potential to decrease costs by providing accurate representations of unexplored regions prior to physical observation. This system has applications ranging from oil and gas operations to search and rescue missions, all while minimizing risk to human life. By utilizing cutting edge technology to mimic complex environments, researchers are now able to innovate faster and safer than ever before. Keywords:deep sea simulation,real-time imaging,genetic algorithms,neural networks",1
"The issue of fairness in machine learning models has recently attracted a lot of attention as ensuring it will ensure continued confidence of the general public in the deployment of machine learning systems. We focus on mitigating the harm incurred by a biased machine learning system that offers better outputs (e.g. loans, job interviews) for certain groups than for others. We show that bias in the output can naturally be controlled in probabilistic models by introducing a latent target output. This formulation has several advantages: first, it is a unified framework for several notions of group fairness such as Demographic Parity and Equality of Opportunity; second, it is expressed as a marginalisation instead of a constrained problem; and third, it allows the encoding of our knowledge of what unbiased outputs should be. Practically, the second allows us to avoid unstable constrained optimisation procedures and to reuse off-the-shelf toolboxes. The latter translates to the ability to control the level of fairness by directly varying fairness target rates. In contrast, existing approaches rely on intermediate, arguably unintuitive, control parameters such as covariance thresholds.",0
"This is an attempt at generating some code which can take text and replace any instance of ""this"" in quotes with the appropriate pronoun such as he/she. Here is an example: if a sentence contains something like ""the president said 'this', but 'that' is incorrect"", then the output should contain the following: ""The president said ""he"", but 'that' is incorrect"".  This is challenging because there could be many different cases where it would have to correctly determine that ""this"" refers to someone specific - it might depend on context. For example ""Judge Smith asked Jim whether 'this' was true"" or ""Alicia Keys sang 'this girl', meaning her, and everyone in the audience cried."" It seems we need a model for natural language understanding, with training data consisting of pairs (input sentences, desired outputs). In other words, you would feed the model examples like those above along with their expected replacements so it can learn how to make the distinction based on context etcetera... however I am unsure about what sort of model architecture would work well here? Also, one must ensure it doesn't generate nonsense. Any ideas?",1
"Causal structure discovery from observational data is fundamental to the causal understanding of autonomous systems such as medical decision support systems, advertising campaigns and self-driving cars. This is essential to solve well-known causal decision making and prediction problems associated with those real-world applications. Recently, recursive causal discovery algorithms have gained particular attention among the research community due to their ability to provide good results by using Conditional Independent (CI) tests in smaller sub-problems. However, each of such algorithms needs a refinement function to remove undesired causal relations of the discovered graphs. Notably, with the increase of the problem size, the computation cost (i.e., the number of CI-tests) of the refinement function makes an algorithm expensive to deploy in practice. This paper proposes a generic causal structure refinement strategy that can locate the undesired relations with a small number of CI-tests, thus speeding up the algorithm for large and complex problems. We theoretically prove the correctness of our algorithm. We then empirically evaluate its performance against the state-of-the-art algorithms in terms of solution quality and completion time in synthetic and real datasets.",0
"Abstract: Recursive partition-based causality learning (RPC) has emerged as one of the most promising approaches for inferring causal structures from observational data. However, computational cost makes this method impractical on large datasets. In this work, we present acceleration techniques that significantly reduce runtime while preserving high accuracy. By preprocessing data using feature selection methods such as principal component analysis, we can prune away irrelevant features, resulting in faster RPC computations. Additionally, our novel depth-first search approach effectively eliminates many invalid orderings before they can cause excessive computation time. Our extensive experimental results demonstrate up to an order of magnitude speedup while maintaining F-score performance comparable to standard RPC without compromising model quality. These accelerations make recursive partition-based causality learning viable on larger and more complex datasets than previously possible. By exploiting these techniques, researchers and practitioners can explore broader domains and investigate deeper hypotheses into causation, ultimately facilitating discovery in fields ranging from social sciences and economics to biology and medicine. Keywords: Causality inference; Feature selection; Optimization; Computational efficiency; Graph theory; Recursive partitioning Accelerating Recursive Partition Based Causal Structure Learning: Exploiting Techniques To Enhance Performance And Broaden Applicability",1
"In physics-based cloth animation, rich folds and detailed wrinkles are achieved at the cost of expensive computational resources and huge labor tuning. Data-driven techniques make efforts to reduce the computation significantly by a database. One type of methods relies on human poses to synthesize fitted garments which cannot be applied to general cloth. Another type of methods adds details to the coarse meshes without such restrictions. However, existing works usually utilize coordinate-based representations which cannot cope with large-scale deformation, and requires dense vertex correspondences between coarse and fine meshes. Moreover, as such methods only add details, they require coarse meshes to be close to fine meshes, which can be either impossible, or require unrealistic constraints when generating fine meshes. To address these challenges, we develop a temporally and spatially as-consistent-as-possible deformation representation (named TS-ACAP) and a DeformTransformer network to learn the mapping from low-resolution meshes to detailed ones. This TS-ACAP representation is designed to ensure both spatial and temporal consistency for sequential large-scale deformations from cloth animations. With this representation, our DeformTransformer network first utilizes two mesh-based encoders to extract the coarse and fine features, respectively. To transduct the coarse features to the fine ones, we leverage the Transformer network that consists of frame-level attention mechanisms to ensure temporal coherence of the prediction. Experimental results show that our method is able to produce reliable and realistic animations in various datasets at high frame rates: 10 ~ 35 times faster than physics-based simulation, with superior detail synthesis abilities than existing methods.",0
"In recent years there has been increasing interest among computer graphics researchers in generating novel shapes which can be easily edited by artists. This has lead to many approaches for producing geometric objects and meshes, however most such systems struggle when it comes to capturing fine details and creasing found on thin sheet materials like metal, plastic, cardboard etc. Our work takes inspiration from classical shape synthesis techniques to model detailed geometry of thin shell models while guaranteeing their structural stability under applied loads. We present two new algorithms built on these principles; one that uses depth maps computed from artist illustrations or photographs as input, the other directly edits previously created models interactively through intuitive tools. We demonstrate our methods ability by applying them to challenging real world cases with quantitative evaluation showing state of the art results where we showcase the potential of deep learning based feature detection as well as exploiting edge features to capture crisp bends and folds accurately without noise. Overall our tool provides an efficient solution to produce high quality 3D content that satisfies both technical rigor and appeals to human sensibilities in design, fashion, architecture and engineering industries.",1
"Pole-like landmark has received increasing attention as a domain-invariant visual cue for visual robot self-localization across domains (e.g., seasons, times of day, weathers). However, self-localization using pole-like landmarks can be ill-posed for a passive observer, as many viewpoints may not provide any pole-like landmark view. To alleviate this problem, we consider an active observer and explore a novel ""domain-invariant"" next-best-view (NBV) planner that attains consistent performance over different domains (i.e., maintenance-free), without requiring the expensive task of training data collection and retraining. In our approach, a novel multi-encoder deep convolutional neural network enables to detect domain invariant pole-like landmarks, which are then used as the sole input to a model-free deep reinforcement learning -based domain-invariant NBV planner. Further, we develop a practical system for active self-localization using sparse invariant landmarks and dense discriminative landmarks. In experiments, we demonstrate that the proposed method is effective both in efficient landmark detection and in discriminative self-localization.",0
"An important challenge faced by autonomous robots operating in dynamic environments is maintaining accurate self-location knowledge over time. This problem becomes particularly difficult across multiple domains where sensors may behave differently due to changes in illumination, appearance, or geometry. In this work we present an approach that addresses these challenges through domain adaptation and active perception planning using non-parametric Bayesian methods. Our proposed planner models the uncertainty in sensor measurements as well as the uncertainty in map construction using non-parametric Bayesian techniques which enable us to adapt to different modalities without relying on labeled data from each domain. To further address the issue of limited sensor observations available during cross-domain transitions, we introduce an active perception planner that actively chooses promising locations for subsequent revisits based on current beliefs. We evaluate our approach experimentally using both simulated datasets as well as real world data collected from two distinct indoor domains and demonstrate improved localization accuracy compared to state of the art methods.",1
"Aided by recent advances in Deep Learning, Image Caption Generation has seen tremendous progress over the last few years. Most methods use transfer learning to extract visual information, in the form of image features, with the help of pre-trained Convolutional Neural Network models followed by transformation of the visual information using a Caption Generator module to generate the output sentences. Different methods have used different Convolutional Neural Network Architectures and, to the best of our knowledge, there is no systematic study which compares the relative efficacy of different Convolutional Neural Network architectures for extracting the visual information. In this work, we have evaluated 17 different Convolutional Neural Networks on two popular Image Caption Generation frameworks: the first based on Neural Image Caption (NIC) generation model and the second based on Soft-Attention framework. We observe that model complexity of Convolutional Neural Network, as measured by number of parameters, and the accuracy of the model on Object Recognition task does not necessarily co-relate with its efficacy on feature extraction for Image Caption Generation task.",0
"This study compares two types of Convolutional Neural Network (CNN) models commonly used for image caption generation: traditional fully convolutional networks (FCNs) and attention-based FCNs. In order to evaluate their performance on different tasks such as object detection, semantic segmentation, and scene classification, we conducted experiments using several benchmark datasets. Our results show that while both FCNs achieve good performance across all tasks, attention-based FCNs outperform traditional FCNs consistently, especially in terms of accuracy and robustness against changes in input size or resolution. These findings suggest that incorporating attention mechanisms into CNNs can significantly improve their ability to generate descriptive and relevant captions for images, making them more suitable for real-world applications such as automatic image annotation and description for visually impaired individuals. Additionally, our research provides insights into how these network structures differ in their processing of visual information, which could inform future advancements in computer vision and natural language processing. Overall, this work contributes to a better understanding of CNN architectures and their potential impact on automated image captioning.",1
"Medical time-series datasets have unique characteristics that make prediction tasks challenging. Most notably, patient trajectories often contain longitudinal variations in their input-output relationships, generally referred to as temporal conditional shift. Designing sequence models capable of adapting to such time-varying distributions remains a prevailing problem. To address this we present Model-Attentive Ensemble learning for Sequence modeling (MAES). MAES is a mixture of time-series experts which leverages an attention-based gating mechanism to specialize the experts on different sequence dynamics and adaptively weight their predictions. We demonstrate that MAES significantly out-performs popular sequence models on datasets subject to temporal shift.",0
"Model-attentive ensemble learning is a new methodology that utilizes multiple sequence models (e.g., LSTMs, GRUs) to jointly learn from data and capture different characteristics of the input sequences. By leveraging an attentional mechanism to focus on informative regions in each individual model prediction, we can combine their outputs effectively while minimizing interference. Experimental results show significant improvements over strong baselines across a variety of benchmark datasets in natural language processing tasks such as sentiment analysis, part-of-speech tagging, and machine translation. These findings demonstrate the effectiveness and potential of our proposed approach in boosting sequence model performance through efficient ensemble strategies.",1
"Deep Reinforcement Learning (Deep RL) has been receiving increasingly more attention thanks to its encouraging performance on a variety of control tasks. Yet, conventional regularization techniques in training neural networks (e.g., $L_2$ regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs. In this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find conventional regularization techniques on the policy networks can often bring large improvement, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg .",0
"This paper presents an empirical study examining the impact of regularization techniques on policy optimization for continuous control problems. We investigate different types of regularizers and their effectiveness at improving performance and stability during learning. Our results show that properly chosen regularization can significantly reduce overfitting and improve generalizability of learned policies. Furthermore, we demonstrate how these improvements translate into better overall task performance, particularly in more complex domains where exploration strategies play a critical role. By providing insights into the use of regularization for policy search algorithms, our work contributes to the broader field of reinforcement learning and offers valuable guidance for practitioners developing artificial intelligence systems.",1
"The stability and generalization of stochastic gradient-based methods provide valuable insights into understanding the algorithmic performance of machine learning models. As the main workhorse for deep learning, stochastic gradient descent has received a considerable amount of studies. Nevertheless, the community paid little attention to its decentralized variants. In this paper, we provide a novel formulation of the decentralized stochastic gradient descent. Leveraging this formulation together with (non)convex optimization theory, we establish the first stability and generalization guarantees for the decentralized stochastic gradient descent. Our theoretical results are built on top of a few common and mild assumptions and reveal that the decentralization deteriorates the stability of SGD for the first time. We verify our theoretical findings by using a variety of decentralized settings and benchmark machine learning models.",0
"The following is an example: Inference problems arise frequently in applied probability and statistics and often have complex hierarchical structures, such as those expressed by Bayesian networks with dependent observation noises (Hansen & Richardson, 2008). This article presents three classes of linearly parameterized conjugate exponential families that can model continuous observations as well as binary responses given categorical predictors subjected to missing value mechanisms under both marginal and conditional models. These distributions yield closed form posterior distributions without resorting to Markov Chain Monte Carlo methods. After reviewing some basics on the theory behind Bayesian analysis and the principles of generalized linear models and their extensions, we proceed to discuss inference issues related to binary response data impacted by missing values. We then develop our approach based on the extended inverse Gaussian distribution, introducing different sets of priors for normalizing constants through hyperparameter choices. The resulting posteriors enable analytic marginal likelihood evaluations while addressing identifiability challenges inherent in misspecified models due to censoring and truncation effects. Our simulations show good finite sample properties across several scenarios involving varied percentages of missingness, levels of censoring, and truncation intervals compared with alternative approaches. Finally, two real datasets demonstrate the computational gains achievable via these new distributions over existing methods, even allowing robustness checks against sensitivity of prior specifications. Thus, by extending the class of probabilities capable of exact posterio",1
"360-degree/omnidirectional images (OIs) have achieved remarkable attentions due to the increasing applications of virtual reality (VR). Compared to conventional 2D images, OIs can provide more immersive experience to consumers, benefitting from the higher resolution and plentiful field of views (FoVs). Moreover, observing OIs is usually in the head mounted display (HMD) without references. Therefore, an efficient blind quality assessment method, which is specifically designed for 360-degree images, is urgently desired. In this paper, motivated by the characteristics of the human visual system (HVS) and the viewing process of VR visual contents, we propose a novel and effective no-reference omnidirectional image quality assessment (NR OIQA) algorithm by Multi-Frequency Information and Local-Global Naturalness (MFILGN). Specifically, inspired by the frequency-dependent property of visual cortex, we first decompose the projected equirectangular projection (ERP) maps into wavelet subbands. Then, the entropy intensities of low and high frequency subbands are exploited to measure the multi-frequency information of OIs. Besides, except for considering the global naturalness of ERP maps, owing to the browsed FoVs, we extract the natural scene statistics features from each viewport image as the measure of local naturalness. With the proposed multi-frequency information measurement and local-global naturalness measurement, we utilize support vector regression as the final image quality regressor to train the quality evaluation model from visual quality-related features to human ratings. To our knowledge, the proposed model is the first no-reference quality assessment method for 360-degreee images that combines multi-frequency information and image naturalness. Experimental results on two publicly available OIQA databases demonstrate that our proposed MFILGN outperforms state-of-the-art approaches.",0
"This paper proposes an algorithm to assess the quality of 360 degree images using no reference materials. We use multi-frequency analysis on image pairs generated from different viewpoints as features. Our approach captures both local and global naturalness characteristics of the original images. By combining these two aspects we achieve better results than state-of-the-art methods. We test our method on several datasets showing significant improvements over previous algorithms. Finally, our method can be used in several applications including virtual reality, image compression and video streaming.",1
"Image Captioning, or the automatic generation of descriptions for images, is one of the core problems in Computer Vision and has seen considerable progress using Deep Learning Techniques. We propose to use Inception-ResNet Convolutional Neural Network as encoder to extract features from images, Hierarchical Context based Word Embeddings for word representations and a Deep Stacked Long Short Term Memory network as decoder, in addition to using Image Data Augmentation to avoid over-fitting. For data Augmentation, we use Horizontal and Vertical Flipping in addition to Perspective Transformations on the images. We evaluate our proposed methods with two image captioning frameworks- Encoder-Decoder and Soft Attention. Evaluation on widely used metrics have shown that our approach leads to considerable improvement in model performance.",0
"Automatic image caption generation has become one of the most active research areas in computer vision over recent years due to their numerous applications such as assisting visually impaired individuals, content based retrieval systems etc. In this work we propose to use deep stacked Long Short Term Memory (LSTM) networks along with contextual word embeddings learned from large text corpora like Wikipedia and Google News as well as data augmentation techniques including random resizing, flipping and rotation of images to generate natural language descriptions. We demonstrate that our model outperforms previously proposed models on publicly available benchmark datasets. Finally we conclude by discussing some future directions towards enhancing image caption quality.",1
"There is a vast body of literature related to methods for detecting changepoints (CP). However, less attention has been paid to assessing the statistical reliability of the detected CPs. In this paper, we introduce a novel method to perform statistical inference on the significance of the CPs, estimated by a Dynamic Programming (DP)-based optimal CP detection algorithm. Based on the selective inference (SI) framework, we propose an exact (non-asymptotic) approach to compute valid p-values for testing the significance of the CPs. Although it is well-known that SI has low statistical power because of over-conditioning, we address this disadvantage by introducing parametric programming techniques. Then, we propose an efficient method to conduct SI with the minimum amount of conditioning, leading to high statistical power. We conduct experiments on both synthetic and real-world datasets, through which we offer evidence that our proposed method is more powerful than existing methods, has decent performance in terms of computational efficiency, and provides good results in many practical applications.",0
"This paper presents an algorithm that uses dynamic programming to compute valid p-values for optimal changepoints based on selective inference. The proposed method addresses the issue of multiplicity adjustments in multiple testing problems where there are many potential changes across time intervals. By incorporating selective inference, the new approach offers more accurate control over type I errors while maintaining power against alternatives. Experimental results show that our method outperforms existing techniques in terms of both accuracy and computational efficiency. Our findings have important implications for applied research fields such as finance, neuroscience, and epidemiology, where detecting meaningful patterns from high-dimensional data is crucial.",1
"The Shapley value has become popular in the Explainable AI (XAI) literature, thanks, to a large extent, to a solid theoretical foundation, including four ""favourable and fair"" axioms for attribution in transferable utility games. The Shapley value is provably the only solution concept satisfying these axioms. In this paper, we introduce the Shapley value and draw attention to its recent uses as a feature selection tool. We call into question this use of the Shapley value, using simple, abstract ""toy"" counterexamples to illustrate that the axioms may work against the goals of feature selection. From this, we develop a number of insights that are then investigated in concrete simulation settings, with a variety of Shapley value formulations, including SHapley Additive exPlanations (SHAP) and Shapley Additive Global importancE (SAGE).",0
"In the field of machine learning, feature selection is an important task that involves identifying the most relevant features from a large pool of candidates to improve model performance and interpretability. One popular method used in feature selection is based on Shapley values, which provide a unique approach by considering the marginal contribution of each individual feature to the final outcome. This paper provides a comprehensive analysis of Shapley values for feature selection, discussing both their advantages and limitations.  The authors begin by introducing the concept of Shapley values and explaining how they can be applied in feature selection problems. They then present several desirable properties or ""axioms"" that any reasonable feature selection method should satisfy. These include monotonicity (the addition of a new feature cannot decrease its importance), consistency (features selected using different methods must have non-decreasing importance), and dummy axiom (dummy variables must have zero importance). The authors demonstrate that Shapley values are the only solution to these axioms that satisfies two additional criteria - efficiency and symmetry. Finally, the authors compare Shapley values against other commonly used feature selection techniques such as regularization and correlation-based filtering, highlighting their strengths and weaknesses.  Overall, this paper offers valuable insights into the merits and pitfalls of using Shapley values for feature selection. By focusing on the axioms that any reasonable feature selection method should meet, the authors provide clear guidelines for practitioners looking to implement this technique effectively in real-world applications. Despite some drawbacks associated with computational complexity, the authors argue that Shapley values offer a principled framework for feature selection that balances interpretability, accuracy, and ease of use.",1
"Recently, the advancement of 3D point clouds in deep learning has attracted intensive research in different application domains such as computer vision and robotic tasks. However, creating feature representation of robust, discriminative from unordered and irregular point clouds is challenging. In this paper, our ultimate goal is to provide a comprehensive overview of the point clouds feature representation which uses attention models. More than 75+ key contributions in the recent three years are summarized in this survey, including the 3D objective detection, 3D semantic segmentation, 3D pose estimation, point clouds completion etc. We provide a detailed characterization (1) the role of attention mechanisms, (2) the usability of attention models into different tasks, (3) the development trend of key technology.",0
"In recent years, deep learning has emerged as a powerful tool for processing point cloud data from various applications such as LiDAR systems, computer vision, robotics, and autonomous vehicles. However, due to the unique properties of point clouds, standard convolutional neural network (CNN) architectures often fail to capture high-level abstractions effectively. To overcome these limitations, attention mechanisms have been proposed as a way to weigh the importance of different points within the point cloud based on their relevance to a particular task.  This survey presents a comprehensive overview of attention models for point clouds in deep learning, covering both unstructured and structured data representations. We begin by discussing the challenges associated with point cloud processing and highlight the benefits that attention mechanisms can bring. Next, we provide a detailed analysis of state-of-the-art attention models, including those applied at different levels in the architecture and those employing various attention functions. Furthermore, we explore how attention techniques can be combined with other point cloud processing methods like normalization, registration, and feature extraction to improve performance.  The aim of this survey is twofold: firstly, to serve as a reference guide for researchers and practitioners interested in applying attention mechanisms to point cloud problems; secondly, to identify promising directions for future research in this rapidly evolving field. Our discussion concludes with a summary of key findings and open questions related to attention models for point clouds in deep learning.",1
"Meta-learning for offline reinforcement learning (OMRL) is an understudied problem with tremendous potential impact by enabling RL algorithms in many real-world applications. A popular solution to the problem is to infer task identity as augmented state using a context-based encoder, for which efficient learning of task representations remains an open challenge. In this work, we improve upon one of the SOTA OMRL algorithms, FOCAL, by incorporating intra-task attention mechanism and inter-task contrastive learning objectives for more effective task inference and learning of control. Theoretical analysis and experiments are presented to demonstrate the superior performance, efficiency and robustness of our end-to-end and model free method compared to prior algorithms across multiple meta-RL benchmarks.",0
"Abstract: In this work, we present a novel approach to offline meta-reinforcement learning (meta-RL) that combines context-based selection with attention mechanisms and contrastive learning. Our method allows agents to effectively learn from multiple task distributions simultaneously by selecting tasks based on their similarity to a given target task. We use a Transformer architecture with self-attention modules to select relevant information from previous experiences stored in the agent's memory bank, allowing efficient information retrieval and utilization across different task types. Additionally, we introduce a contrastive loss function to encourage positive pairing of similar tasks and negative pairings of dissimilar ones, further improving performance through better alignment of representations. Empirical evaluations demonstrate significant improvements over existing methods across a variety of domains and task sets, including few-shot adaptation scenarios.",1
"Graph Convolutional Networks (GCNs) have attracted more and more attentions in recent years. A typical GCN layer consists of a linear feature propagation step and a nonlinear transformation step. Recent works show that a linear GCN can achieve comparable performance to the original non-linear GCN while being much more computationally efficient. In this paper, we dissect the feature propagation steps of linear GCNs from a perspective of continuous graph diffusion, and analyze why linear GCNs fail to benefit from more propagation steps. Following that, we propose Decoupled Graph Convolution (DGC) that decouples the terminal time and the feature propagation steps, making it more flexible and capable of exploiting a very large number of feature propagation steps. Experiments demonstrate that our proposed DGC improves linear GCNs by a large margin and makes them competitive with many modern variants of non-linear GCNs.",0
"In recent years, linear graph convolutional networks (LGCNs) have gained popularity due to their ability to efficiently learn node representations on graphs. However, there remains limited understanding of how these models learn from data and diffuse signals through the network. This study aims to address this gap by providing a comprehensive analysis of LGCN diffusion processes. We focus specifically on the propagation of signals through different layers of an LGCN and the factors that influence diffusion. Our results provide insights into how LGCNs process information and make predictions, which can inform future research in this area. Overall, our work provides a deeper understanding of the inner workings of LGCNs, laying the foundation for further advancements in graph neural network architectures.",1
"This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources. We design a Dynamic Point-cloud Convolution (DConv) operator as the core component of CloudLSTMs, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points that surround different elements of the input. This operator maintains the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step -- an important aspect in spatiotemporal predictive learning. The DConv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily plugged into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. We apply our proposed architecture to two representative, practical use cases that involve point-cloud streams, i.e., mobile service traffic forecasting and air quality indicator forecasting. Our results, obtained with real-world datasets collected in diverse scenarios for each use case, show that CloudLSTM delivers accurate long-term predictions, outperforming a variety of competitor neural network models.",0
"This abstract presents a recurrent neural model called CloudLSTM that can accurately forecast spatio-temporal point clouds. With each passing day, technological advancements lead to greater amounts of point cloud data available from diverse sources like LiDAR sensors and 360Â° cameras. Unfortunately, these datasets are often high dimensional which makes them difficult to process, analyze, and visualize effectively. In order to combat this challenge, we propose a novel deep learning approach based on Long Short Term Memory (LSTM) networks that capture both temporal dependencies over time steps as well as spatial relationships between points. Our proposed architecture, CloudLSTM, leverages LSTM layers operating directly over raw coordinates of the underlying point cloud. Furthermore, CloudLSTM incorporates a Convolutional layer with dilated convolutions allowing us to learn meaningful features and drastically reduce computational complexity. We benchmark our approach against a wide variety of state-of-the art baselines including Autoencoders, Variational models, Generative Adversarial Networks, and other Recurrent architectures across three real world datasets covering city scans, aerial images, and indoor room layouts. Results show consistent improvements ranging up to +27% accuracy relative to comparisons highlighting the merits of our proposal towards accurate point stream prediction tasks. As applications surrounding point cloud processing continue to grow rapidly so must our methods to effectively handle such large scale and complex collections of geospatially relevant data.""",1
"We present a novel attention-based model for discrete event data to capture complex non-linear temporal dependence structures. We borrow the idea from the attention mechanism and incorporate it into the point processes' conditional intensity function. We further introduce a novel score function using Fourier kernel embedding, whose spectrum is represented using neural networks, which drastically differs from the traditional dot-product kernel and can capture a more complex similarity structure. We establish our approach's theoretical properties and demonstrate our approach's competitive performance compared to the state-of-the-art for synthetic and real data.",0
"In recent years, deep neural networks have proven to be very effective at solving a variety of problems across many domains. One key component of these models is their ability to learn complex representations that capture subtle patterns in data through multiple layers of processing. However, one challenge faced by researchers working on neural point processes (NPPs) has been how to extend such models to capture both global dependencies and local interactions in high dimensions. To address this problem, we propose a novel framework called ""deep Fourier kernel NPP"" (DFK-NPP), which uses a deep architecture to define a flexible class of kernels capable of learning nonlinear transformations from the input domain into a continuous latent space, where both local interactions and spatially varying spectral densities can be captured. We show experimentally that DFK-NPP performs well compared to state-of-the-art approaches on several challenging real world tasks including human motion prediction, handwritten digit classification, and image generation. Our model's performance benefits significantly from incorporating self attention modules, leading to improved interpretability and better results overall. These promising results demonstrate the effectiveness of our approach, and pave the way for future work exploring more complex applications of DFK-NPP, potentially beyond natural language processing and computer vision tasks.",1
"A heterogeneous information network (HIN) has as vertices objects of different types and as edges the relations between objects, which are also of various types. We study the problem of classifying objects in HINs. Most existing methods perform poorly when given scarce labeled objects as training sets, and methods that improve classification accuracy under such scenarios are often computationally expensive. To address these problems, we propose ConCH, a graph neural network model. ConCH formulates the classification problem as a multi-task learning problem that combines semi-supervised learning with self-supervised learning to learn from both labeled and unlabeled data. ConCH employs meta-paths, which are sequences of object types that capture semantic relationships between objects. ConCH co-derives object embeddings and context embeddings via graph convolution. It also uses the attention mechanism to fuse such embeddings. We conduct extensive experiments to evaluate the performance of ConCH against other 15 classification methods. Our results show that ConCH is an effective and efficient method for HIN classification.",0
"In recent years, heterogeneous information networks (HINs) have become increasingly prevalent due to their ability to represent complex real-world relationships among entities and concepts. Classifying nodes or edges in such networks often involves analyzing patterns across multiple types of relationships, which can lead to poor performance when using traditional machine learning techniques that treat each type of relationship independently. To address this challenge, we propose a meta-path contextualization approach that leverages the rich structure provided by meta-paths, sequences of adjacent relations connecting two nodes, to enhance classification accuracy. We demonstrate the effectiveness of our method on several benchmark datasets and show that it outperforms state-of-the-art methods that ignore the interplay between different types of relationships. Our results highlight the importance of considering the meta-path context in classification tasks within HINs and suggest promising directions for future research.",1
"Modern data acquisition routinely produce massive amounts of event sequence data in various domains, such as social media, healthcare, and financial markets. These data often exhibit complicated short-term and long-term temporal dependencies. However, most of the existing recurrent neural network based point process models fail to capture such dependencies, and yield unreliable prediction performance. To address this issue, we propose a Transformer Hawkes Process (THP) model, which leverages the self-attention mechanism to capture long-term dependencies and meanwhile enjoys computational efficiency. Numerical experiments on various datasets show that THP outperforms existing models in terms of both likelihood and event prediction accuracy by a notable margin. Moreover, THP is quite general and can incorporate additional structural knowledge. We provide a concrete example, where THP achieves improved prediction performance for learning multiple point processes when incorporating their relational information.",0
"Transformer Hawkes Process: A novel approach to processing sequential data. In recent years, deep learning has shown great promise for tackling complex pattern recognition problems. One particular architectural choice that has emerged as particularly effective at dealing with sequential time-series data is the transformer model. Here we extend traditional transformers by adding in a hawkes process mechanism - specifically, using a self attention layer conditioned on both local context and global event times. This allows us to capture more nuanced temporal dependencies in our predictions, leading to improved performance over traditional methods. In our experiments, we applied the new methodology to several benchmark datasets and achieved state-of-the-art results across all tasks tested. We believe that combining these two approaches holds significant potential in many domains such as natural language understanding, speech recognition, bioinformatics, and finance among others.",1
"Transformer is a ubiquitous model for natural language processing and has attracted wide attentions in computer vision. The attention maps are indispensable for a transformer model to encode the dependencies among input tokens. However, they are learned independently in each layer and sometimes fail to capture precise patterns. In this paper, we propose a novel and generic mechanism based on evolving attention to improve the performance of transformers. On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the attention in succeeding layers through residual connections. On the other hand, low-level and high-level attentions vary in the level of abstraction, so we adopt convolutional layers to model the evolutionary process of attention maps. The proposed evolving attention mechanism achieves significant performance improvement over various state-of-the-art models for multiple tasks, including image classification, natural language understanding and machine translation.",0
"This is a research paper that proposes a new method for computer vision tasks called ""Evolving Attention"". The motivation behind this work comes from recent advances in deep learning techniques such as residual networks, which have shown improved performance over traditional methods. However, these deep neural network architectures often require large amounts of data and computational resources, making them difficult to use on smaller scale datasets and devices with limited processing power. This work presents an approach based on attention mechanisms, which aim to selectively focus on relevant parts of an input image while performing a task. By incorporating residual connections within the attention module itself, we can improve both efficiency and accuracy compared to state-of-the-art models. Our experiments show promising results on several benchmark datasets across different computer vision tasks such as object detection, semantic segmentation, and image classification. We hope that our proposed method will serve as a starting point for future works exploring efficient and effective deep learning techniques for resource constrained environments.",1
"Adversarial machine learning has attracted a great amount of attention in recent years. In a poisoning attack, the adversary can inject a small number of specially crafted samples into the training data which make the decision boundary severely deviate and cause unexpected misclassification. Due to the great importance and popular use of support vector machines (SVM), we consider defending SVM against poisoning attacks in this paper. We study two commonly used strategies for defending: designing robust SVM algorithms and data sanitization. Though several robust SVM algorithms have been proposed before, most of them either are in lack of adversarial-resilience, or rely on strong assumptions about the data distribution or the attacker's behavior. Moreover, the research on their complexities is still quite limited. We are the first, to the best of our knowledge, to prove that even the simplest hard-margin one-class SVM with outliers problem is NP-complete, and has no fully PTAS unless P$=$NP (that means it is hard to achieve an even approximate algorithm). For the data sanitization defense, we link it to the intrinsic dimensionality of data; in particular, we provide a sampling theorem in doubling metrics for explaining the effectiveness of DBSCAN (as a density-based outlier removal method) for defending against poisoning attacks. In our empirical experiments, we compare several defenses including the DBSCAN and robust SVM methods, and investigate the influences from the intrinsic dimensionality and data density to their performances.",0
"This paper presents a novel approach to defending Support Vector Machines (SVM) from poisoning attacks. In recent years, there has been increasing concern over the vulnerability of machine learning models to such attacks, where attackers deliberately introduce malicious data into training datasets to compromise model performance. To address this issue, we propose using the Hardness and Distance-Based Spatial Clustering of Applications with Noise (DBSCAN) approaches as a means of detecting and mitigating poisoning attacks on SVMs. Our methodology involves first identifying the most influential data points in the dataset based on their hardness values, which measure how difficult it is to classify each point correctly. Next, we use DBSCAN to cluster these high-impact data points into meaningful subgroups, allowing us to identify any potential outliers that may have been added by attackers. By combining these two techniques, our proposed method can effectively defend against poisoning attacks while maintaining good overall accuracy. We evaluate the effectiveness of our approach through extensive experimental results on several real-world datasets, demonstrating its ability to successfully detect and mitigate poisoning attacks on SVMs. Overall, our work provides important new insights into protecting machine learning models from such threats and advances the state of art in this critical field.",1
"The success of deep learning methods led to significant breakthroughs in 3-D point cloud processing tasks with applications in remote sensing. Existing methods utilize convolutions that have some limitations, as they assume a uniform input distribution and cannot learn long-range dependencies. Recent works have shown that adding attention in conjunction with these methods improves performance. This raises a question: can attention layers completely replace convolutions? This paper proposes a fully attentional model - {\em Point Transformer}, for deriving a rich point cloud representation. The model's shape classification and retrieval performance are evaluated on a large-scale urban dataset - RoofN3D and a standard benchmark dataset ModelNet40. Extensive experiments are conducted to test the model's robustness to unseen point corruptions for analyzing its effectiveness on real datasets. The proposed method outperforms other state-of-the-art models in the RoofN3D dataset, gives competitive results in the ModelNet40 benchmark, and showcases high robustness to various unseen point corruptions. Furthermore, the model is highly memory and space efficient when compared to other methods.",0
"This paper presents a new method for shape classification and retrieval of 3D and ALS roof point clouds using a novel technique called the ""Point Transformer"". The Point Transformer is a deep learning architecture designed specifically for processing point cloud data. It has been shown to achieve state-of-the-art results on several benchmark datasets for tasks such as object detection, segmentation, and surface reconstruction. In this work, we extend the use of the Point Transformer to the task of shape classification and retrieval of 3D and ALS roof point clouds. Our experiments demonstrate that the proposed approach outperforms traditional methods for these tasks by a significant margin. We believe that our findings have important implications for computer vision research, particularly in the areas of scene understanding and building information modeling (BIM). Overall, this paper represents a valuable contribution to the field of 3D geometric computing, and paves the way for further advancements in point cloud analysis techniques.",1
"Most compilers for machine learning (ML) frameworks need to solve many correlated optimization problems to generate efficient machine code. Current ML compilers rely on heuristics based algorithms to solve these optimization problems one at a time. However, this approach is not only hard to maintain but often leads to sub-optimal solutions especially for newer model architectures. Existing learning based approaches in the literature are sample inefficient, tackle a single optimization problem, and do not generalize to unseen graphs making them infeasible to be deployed in practice. To address these limitations, we propose an end-to-end, transferable deep reinforcement learning method for computational graph optimization (GO), based on a scalable sequential attention mechanism over an inductive graph neural network. GO generates decisions on the entire graph rather than on each individual node autoregressively, drastically speeding up the search compared to prior methods. Moreover, we propose recurrent attention layers to jointly optimize dependent graph optimization tasks and demonstrate 33%-60% speedup on three graph optimization tasks compared to TensorFlow default optimization. On a diverse set of representative graphs consisting of up to 80,000 nodes, including Inception-v3, Transformer-XL, and WaveNet, GO achieves on average 21% improvement over human experts and 18% improvement over the prior state of the art with 15x faster convergence, on a device placement task evaluated in real systems.",0
"Recent advances in machine learning have made graph optimization increasingly important for compilers. However, current methods for optimizing graphs often require specialized knowledge and cannot easily be transferred from one domain to another. In this paper, we present a new approach to transferable graph optimization that allows for more efficient and effective compilation across multiple domains. Our method uses pre-trained models called optimizer hubs (OHub) which can perform well on different types of data without requiring specific training for each task. We demonstrate the effectiveness of our approach through experiments on various datasets and show how it outperforms existing state-of-the art approaches in terms of accuracy and efficiency. Overall, this work has significant implications for improving the quality and speed of modern machine learning systems.",1
"Deep neural networks (DNNs) are typically optimized for a specific input resolution (e.g. $224 \times 224$ px) and their adoption to inputs of higher resolution (e.g., satellite or medical images) remains challenging, as it leads to excessive computation and memory overhead, and may require substantial engineering effort (e.g., streaming). We show that multi-scale hard-attention can be an effective solution to this problem. We propose a novel architecture, TNet, which traverses an image pyramid in a top-down fashion, visiting only the most informative regions along the way. We compare our model against strong hard-attention baselines, achieving a better trade-off between resources and accuracy on ImageNet. We further verify the efficacy of our model on satellite images (fMoW dataset) of size up to $896 \times 896$ px. In addition, our hard-attention mechanism guarantees predictions with a degree of interpretability, without extra cost beyond inference. We also show that we can reduce data acquisition and annotation cost, since our model attends only to a fraction of the highest resolution content, while using only image-level labels without bounding boxes.",0
"""Hard attention mechanisms have proven to be effective in addressing the challenges associated with sequential data processing tasks such as machine translation, image generation, and speech synthesis. However, their use in large-scale image classification has been limited by computational complexity and scalability issues. In this work, we present a novel hard attention mechanism that overcomes these limitations while maintaining high accuracy on various benchmark datasets. Our approach uses dynamic weight scaling and sparse feature selection techniques to efficiently focus attention on relevant features without sacrificing performance. Extensive experiments demonstrate that our method outperforms state-of-the-art soft attention models, significantly reducing computational costs while improving model interpretability. This research paves the way for efficient and interpretable deep learning systems for scalable image classification.""  If you like, I can provide a shorter version of the Abstract: ""We propose a hard attention mechanism for scalable image classification that combines dynamic weight scaling and sparse feature selection techniques to reduce computational cost and improve model interpretability. Experimental results show that our approach achieves better accuracy than current soft attention models while significantly decreasing computational burden. This makes our method ideal for large-scale image classification applications.""",1
"Scribble-supervised semantic segmentation has gained much attention recently for its promising performance without high-quality annotations. Due to the lack of supervision, confident and consistent predictions are usually hard to obtain. Typically, people handle these problems to either adopt an auxiliary task with the well-labeled dataset or incorporate the graphical model with additional requirements on scribble annotations. Instead, this work aims to achieve semantic segmentation by scribble annotations directly without extra information and other limitations. Specifically, we propose holistic operations, including minimizing entropy and a network embedded random walk on neural representation to reduce uncertainty. Given the probabilistic transition matrix of a random walk, we further train the network with self-supervision on its neural eigenspace to impose consistency on predictions between related images. Comprehensive experiments and ablation studies verify the proposed approach, which demonstrates superiority over others; it is even comparable to some full-label supervised ones and works well when scribbles are randomly shrunk or dropped.",0
"This abstract presents scribble supervising semantic segmentation by uncertainty reduction on neural representation and self-supervision on neural eigenspace (S4Seg). Despite recent advances in fully convolutional networks (FCNs), generating high quality pixel level annotations remains prohibitively expensive. Inspired by image classification literature, we argue that training data can come from both pixels labeled with ground truth masks as well as unlabeled pixels drawn uniformly over images, which naturally handles class imbalance across object scales and encourages smoothness regularization. To improve accuracy, our S4Seg algorithm reduces segmentation uncertainty during optimization by minimizing cross entropy loss while maximizing reconstruction error along each FCN feature map dimension individually. Furthermore, we design novel modules using attention mechanisms on self-supervised spectral clustering embeddings derived from the neural eigenvalues of randomized data augmentations, reducing dependency on labeled data while enhancing generalization performance under domain shift. Extensive experiments showcase state-of-the-art results on the Pascal VOC 2012 dataset, demonstrating the effectiveness of incorporating scribbles with self-supervision into FCN frameworks. Our code is publicly available at <https://github.com/Open-Assistant/scribble_segmenter>.",1
"Recent years, many researches attempt to open the black box of deep neural networks and propose a various of theories to understand it. Among them, Information Bottleneck (IB) theory claims that there are two distinct phases consisting of fitting phase and compression phase in the course of training. This statement attracts many attentions since its success in explaining the inner behavior of feedforward neural networks. In this paper, we employ IB theory to understand the dynamic behavior of convolutional neural networks (CNNs) and investigate how the fundamental features such as convolutional layer width, kernel size, network depth, pooling layers and multi-fully connected layer have impact on the performance of CNNs. In particular, through a series of experimental analysis on benchmark of MNIST and Fashion-MNIST, we demonstrate that the compression phase is not observed in all these cases. This shows us the CNNs have a rather complicated behavior than feedforward neural networks.",0
"""The Information Bottleneck theory provides a mathematical framework for understanding how neural networks learn from input data. In particular, convolutional neural networks (CNNs) use convolution operations that are known to provide robustness against noise and translation. However, CNNs can struggle with extracting relevant features from inputs because they rely heavily on spatial pyramids. This paper presents new theoretical results which show that applying the Information Bottleneck principle directly to convolutional networks leads to substantial improvements in performance across several benchmark datasets.""  Note: This abstract is only based on the limited details provided by you. The actual content of the research paper might vary.",1
"Robust point cloud registration in real-time is an important prerequisite for many mapping and localization algorithms. Traditional methods like ICP tend to fail without good initialization, insufficient overlap or in the presence of dynamic objects. Modern deep learning based registration approaches present much better results, but suffer from a heavy run-time. We overcome these drawbacks by introducing StickyPillars, a fast, accurate and extremely robust deep middle-end 3D feature matching method on point clouds. It uses graph neural networks and performs context aggregation on sparse 3D key-points with the aid of transformer based multi-head self and cross-attention. The network output is used as the cost for an optimal transport problem whose solution yields the final matching probabilities. The system does not rely on hand crafted feature descriptors or heuristic matching strategies. We present state-of-art art accuracy results on the registration problem demonstrated on the KITTI dataset while being four times faster then leading deep methods. Furthermore, we integrate our matching system into a LiDAR odometry pipeline yielding most accurate results on the KITTI odometry dataset. Finally, we demonstrate robustness on KITTI odometry. Our method remains stable in accuracy where state-of-the-art procedures fail on frame drops and higher speeds.",0
"This work proposes StickyPillars, a novel feature matching approach for point clouds based on graph neural networks (GNN). Our method leverages the strengths of GNNs by learning robust features that capture intrinsic properties of the data and enable effective matching across various geometric deformations. We address challenges associated with prior works such as localization drift, sensitivity to initial pose estimates, and computational efficiency. Experimental results demonstrate significant improvements over state-of-the-art methods on benchmark datasets in terms of accuracy, speed, and robustness. Our approach is versatile and can be applied to different applications like dense mapping and structure from motion. Overall, our findings open up new possibilities for reliable feature matching under real-world conditions.",1
"Characterization of local minima draws much attention in theoretical studies of deep learning. In this study, we investigate the distribution of parameters in an over-parametrized finite neural network trained by ridge regularized empirical square risk minimization (RERM). We develop a new theory of ridgelet transform, a wavelet-like integral transform that provides a powerful and general framework for the theoretical study of neural networks involving not only the ReLU but general activation functions. We show that the distribution of the parameters converges to a spectrum of the ridgelet transform. This result provides a new insight into the characterization of the local minima of neural networks, and the theoretical background of an inductive bias theory based on lazy regimes. We confirm the visual resemblance between the parameter distribution trained by SGD, and the ridgelet spectrum calculated by numerical integration through numerical experiments with finite models.",0
"Ridge regression is often used as a linear modeling technique, but it suffers from high variance due to unregularized predictors. This study investigates whether ridge regression can be combined with overparametrization via deep neural networks (DNN), which have low regularization due to their wide architecture. We demonstrate that using a two-layer DNN with random initializations results in better performance than traditional ridge regression models. Our experiments show that with sufficient training data, our approach converges to the same optimal solution found by exact computation, while outperforming other machine learning models such as support vector machines, logistic regression, decision trees, and random forest. Furthermore, we show through ablation studies that overparametrization plays a key role in achieving these improvements. These findings suggest that using more parameters than necessary might indeed lead to better generalization properties when appropriate algorithms and objectives are applied. Finally, we explore different architectures, including convolutional neural networks, and show that some simple architectures might work well even with small amounts of data.",1
"Vehicle Re-identification (re-id) over surveillance camera network with non-overlapping field of view is an exciting and challenging task in intelligent transportation systems (ITS). Due to its versatile applicability in metropolitan cities, it gained significant attention. Vehicle re-id matches targeted vehicle over non-overlapping views in multiple camera network. However, it becomes more difficult due to inter-class similarity, intra-class variability, viewpoint changes, and spatio-temporal uncertainty. In order to draw a detailed picture of vehicle re-id research, this paper gives a comprehensive description of the various vehicle re-id technologies, applicability, datasets, and a brief comparison of different methodologies. Our paper specifically focuses on vision-based vehicle re-id approaches, including vehicle appearance, license plate, and spatio-temporal characteristics. In addition, we explore the main challenges as well as a variety of applications in different domains. Lastly, a detailed comparison of current state-of-the-art methods performances over VeRi-776 and VehicleID datasets is summarized with future directions. We aim to facilitate future research by reviewing the work being done on vehicle re-id till to date.",0
"In recent years there has been a growing interest in understanding how vehicles can be identified from video footage captured by cameras installed on road networks. This includes both real-time re-identification of individual vehicles as they move through traffic, as well as historical re-identification based on archived camera recordings. This comprehensive review seeks to provide a thorough overview of trends in vehicle re-identification research past, present, and future. We examine key techniques used for vehicle detection, recognition, tracking, and identification. Additionally we explore emerging applications such as surveillance, intelligent transportation systems (ITS), autonomous driving, and smart city planning. Finally, we identify important challenges remaining in this field and suggest directions for future work. With this study we aim to serve as a valuable resource for students and practitioners interested in the state-of-the art developments in vehicle re-identification technology.",1
"We consider the challenging problem of audio to animated video generation. We propose a novel method OneShotAu2AV to generate an animated video of arbitrary length using an audio clip and a single unseen image of a person as an input. The proposed method consists of two stages. In the first stage, OneShotAu2AV generates the talking-head video in the human domain given an audio and a person's image. In the second stage, the talking-head video from the human domain is converted to the animated domain. The model architecture of the first stage consists of spatially adaptive normalization based multi-level generator and multiple multilevel discriminators along with multiple adversarial and non-adversarial losses. The second stage leverages attention based normalization driven GAN architecture along with temporal predictor based recycle loss and blink loss coupled with lipsync loss, for unsupervised generation of animated video. In our approach, the input audio clip is not restricted to any specific language, which gives the method multilingual applicability. OneShotAu2AV can generate animated videos that have: (a) lip movements that are in sync with the audio, (b) natural facial expressions such as blinks and eyebrow movements, (c) head movements. Experimental evaluation demonstrates superior performance of OneShotAu2AV as compared to U-GAT-IT and RecycleGan on multiple quantitative metrics including KID(Kernel Inception Distance), Word error rate, blinks/sec",0
"This study proposes a method for generating animated videos from audio inputs alone. Using deep learning techniques such as sequence to sequence models, we show that high quality animations can be generated without any visual input data. We evaluate our approach on a large dataset of audio clips paired with corresponding video clips, demonstrating strong performance across multiple metrics including alignment accuracy, animation fidelity, and diversity of outputs. Our work has important implications for fields ranging from entertainment to education, where animated content is valuable but may face limitations due to cost or lack of availability. Additionally, our system could potentially assist individuals with visual impairments by providing a new means of accessing multimedia content through alternative sensory channels. Overall, we believe that our research represents a significant step forward in the field of audio-driven video generation and presents exciting opportunities for future applications.",1
"Virtual screening can accelerate drug discovery by identifying promising candidates for experimental evaluation. Machine learning is a powerful method for screening, as it can learn complex structure-property relationships from experimental data and make rapid predictions over virtual libraries. Molecules inherently exist as a three-dimensional ensemble and their biological action typically occurs through supramolecular recognition. However, most deep learning approaches to molecular property prediction use a 2D graph representation as input, and in some cases a single 3D conformation. Here we investigate how the 3D information of multiple conformers, traditionally known as 4D information in the cheminformatics community, can improve molecular property prediction in deep learning models. We introduce multiple deep learning models that expand upon key architectures such as ChemProp and Schnet, adding elements such as multiple-conformer inputs and conformer attention. We then benchmark the performance trade-offs of these models on 2D, 3D and 4D representations in the prediction of drug activity using a large training set of geometrically resolved molecules. The new architectures perform significantly better than 2D models, but their performance is often just as strong with a single conformer as with many. We also find that 4D deep learning models learn interpretable attention weights for each conformer.",0
"Title: Accelerating molecular modeling simulations with conformer populations Using deep neural networks and randomized solvents. Authors: [list authors]. Emails: [email protected] (corresponding author). Affiliation(s): Department(s) of Chemistry/Biochemistry/Biotechnology at Institution(s) (if all affiliations are from one institution use that name; otherwise list them separately). Abstract: In silico methods have become increasingly important tools in drug discovery due to their ability to provide rapid and cost-effective predictions of protein structures and binding affinities. One critical challenge in the field is accurately modeling protein flexibility and the impact of different environments on the resulting structure ensembles. Here we present a novel method combining deep convolutional neural networks trained on large databases of protein conformers generated using multiple solvent types and sampling techniques. We demonstrate how these learned models can rapidly generate highly diverse sets of low energy conformers suitable as initial inputs for ab initio geometry optimizations in explicit solvation calculations. Our results show significantly faster convergence rates compared to current methods when applied to a range of relevant targets from kinases to GPCRs and improved sampling across structurally diverse ligands. This work has significant implications for improving computational efficiency without sacrificing accuracy making it ideal for high throughput virtual screening applications. Keywords: molecular modeling, MD simulation, protein structure prediction, CNNSolver.org server, docking scoring functions. Introduction: The process of discovering new drugs involves identifying compounds that bind effectively to target proteins while minimizing interactions with other unintended proteins within cells. Understanding t",1
"We study a stylized dynamic assortment planning problem during a selling season of finite length $T$. At each time period, the seller offers an arriving customer an assortment of substitutable products and the customer makes the purchase among offered products according to a discrete choice model. The goal of the seller is to maximize the expected revenue, or equivalently, to minimize the worst-case expected regret. One key challenge is that utilities of products are unknown to the seller and need to be learned. Although the dynamic assortment planning problem has received increasing attention in revenue management, most existing work is based on the multinomial logit choice models (MNL). In this paper, we study the problem of dynamic assortment planning under a more general choice model -- the nested logit model, which models hierarchical choice behavior and is ``the most widely used member of the GEV (generalized extreme value) family''. By leveraging the revenue-ordered structure of the optimal assortment within each nest, we develop a novel upper confidence bound (UCB) policy with an aggregated estimation scheme. Our policy simultaneously learns customers' choice behavior and makes dynamic decisions on assortments based on the current knowledge. It achieves the accumulated regret at the order of $\tilde{O}(\sqrt{MNT})$, where $M$ is the number of nests and $N$ is the number of products in each nest. We further provide a lower bound result of $\Omega(\sqrt{MT})$, which shows the near optimality of the upper bound when $T$ is much larger than $M$ and $N$. When the number of items per nest $N$ is large, we further provide a discretization heuristic for better performance of our algorithm. Numerical results are presented to demonstrate the empirical performance of our proposed algorithms.",0
"In order to increase profits from their online retail businesses, companies need to dynamically optimize product assortments offered to individual customers based on their preferences. This requires developing predictive models that can accurately estimate consumer demand and identify optimal product combinations at each stage of the purchasing process. However, current methods may not fully capture the complex interactions among products, leading to suboptimal outcomes. We propose using nested logit models (NLMs) as an alternative approach to better characterize customer choice behavior and improve assortment selection strategies. Our methodology involves estimating the parameters of the model using transaction data obtained from company databases and conducting extensive simulations to evaluate the performance of our dynamic assortment system compared to static approaches. By incorporating realistic assumptions into the NLM framework, we demonstrate how retailers can effectively tailor their offerings to meet evolving customer needs and maximize revenue growth. Overall, our findings have important implications for eCommerce practitioners looking to enhance their operations through personalized assortment optimization techniques.",1
"Machine learning techniques used in computer-aided medical image analysis usually suffer from the domain shift problem caused by different distributions between source/reference data and target data. As a promising solution, domain adaptation has attracted considerable attention in recent years. The aim of this paper is to survey the recent advances of domain adaptation methods in medical image analysis. We first present the motivation of introducing domain adaptation techniques to tackle domain heterogeneity issues for medical image analysis. Then we provide a review of recent domain adaptation models in various medical image analysis tasks. We categorize the existing methods into shallow and deep models, and each of them is further divided into supervised, semi-supervised and unsupervised methods. We also provide a brief summary of the benchmark medical image datasets that support current domain adaptation research. This survey will enable researchers to gain a better understanding of the current status, challenges.",0
"This survey provides an overview of domain adaptation techniques used in medical image analysis applications that operate on images acquired at multiple sites, where there can be variations in data acquisition parameters (e.g., imaging protocols) as well as changes in patient demographics. These factors may result in significant differences in distribution of intensities, tissue appearances, and/or shapes across different domains, which in turn leads to degraded performance of algorithms developed using data from one site applied directly to another site. Unsupervised domain adaptation methods learn to align jointly both the feature distributions and label spaces. Alternatively, some approaches seek to optimize a unified parameter set shared across all tasks by minimizing a single risk function, whereas others propose regularization terms based on discrepancy measures, such as correlation metrics, Bhattacharyya distance, mutual information, or maximum mean discrepancy; these use the source taskâ€™s labeled data and either the target taskâ€™s partially labeled or unlabeled data. In contrast, transfer learningâ€“based supervision focuses on training several models sequentially and leveraging their learned knowledge. A small number of recent studies investigate how deep neural networks could assist humans via user interactive systems. Finally, we discuss three emerging trendsâ€”multimodality domain adaption, few-shot learning, and generative adversarial network modelsâ€”that have shown promising results but require further investigation. Overall, we provide insights into current limitations and future research directions of domain adaptation methods.",1
"Reinforcement learning (RL) with linear function approximation has received increasing attention recently. However, existing work has focused on obtaining $\sqrt{T}$-type regret bound, where $T$ is the number of interactions with the MDP. In this paper, we show that logarithmic regret is attainable under two recently proposed linear MDP assumptions provided that there exists a positive sub-optimality gap for the optimal action-value function. More specifically, under the linear MDP assumption (Jin et al. 2019), the LSVI-UCB algorithm can achieve $\tilde{O}(d^{3}H^5/\text{gap}_{\text{min}}\cdot \log(T))$ regret; and under the linear mixture MDP assumption (Ayoub et al. 2020), the UCRL-VTR algorithm can achieve $\tilde{O}(d^{2}H^5/\text{gap}_{\text{min}}\cdot \log^3(T))$ regret, where $d$ is the dimension of feature mapping, $H$ is the length of episode, $\text{gap}_{\text{min}}$ is the minimal sub-optimality gap, and $\tilde O$ hides all logarithmic terms except $\log(T)$. To the best of our knowledge, these are the first logarithmic regret bounds for RL with linear function approximation. We also establish gap-dependent lower bounds for the two linear MDP models.",0
"In recent years, there has been growing interest in developing efficient methods for reinforcement learning (RL) that can handle complex tasks with large state spaces. One promising approach is logarithmic regret for RL with linear function approximation, which provides theoretical guarantees on the performance of learning algorithms under certain conditions. This paper investigates the use of logarithmic regret as a tool for understanding the limitations and opportunities of RL with linear function approximation. We present new results on the relationship between regret bounds and policy evaluation error, showing how they relate to each other and how they depend on the problem parameters. Our findings have important implications for the design and analysis of RL algorithms, particularly those based on model-free methods such as Q-learning.",1
"Training temporal action detection in videos requires large amounts of labeled data, yet such annotation is expensive to collect. Incorporating unlabeled or weakly-labeled data to train action detection model could help reduce annotation cost. In this work, we first introduce the Semi-supervised Action Detection (SSAD) task with a mixture of labeled and unlabeled data and analyze different types of errors in the proposed SSAD baselines which are directly adapted from the semi-supervised classification task. To alleviate the main error of action incompleteness (i.e., missing parts of actions) in SSAD baselines, we further design an unsupervised foreground attention (UFA) module utilizing the ""independence"" between foreground and background motion. Then we incorporate weakly-labeled data into SSAD and propose Omni-supervised Action Detection (OSAD) with three levels of supervision. An information bottleneck (IB) suppressing the scene information in non-action frames while preserving the action information is designed to help overcome the accompanying action-context confusion problem in OSAD baselines. We extensively benchmark against the baselines for SSAD and OSAD on our created data splits in THUMOS14 and ActivityNet1.2, and demonstrate the effectiveness of the proposed UFA and IB methods. Lastly, the benefit of our full OSAD-IB model under limited annotation budgets is shown by exploring the optimal annotation strategy for labeled, unlabeled and weakly-labeled data.",0
"""Action detection is the process of identifying actions performed by actors within a video sequence. Existing methods have primarily focused on classifying actions based on their static visual features without considering their temporal dynamics. In this work, we propose a novel approach that leverages multi-level supervision to detect actions at different levels of granularity. Our method first generates action proposals from raw video frames using a temporal sliding window technique. These proposals are then fed into a neural network architecture designed to capture both short-term motion patterns and long-term contextual cues. We introduce two new types of supervision signals to guide the training: (i) actionness scores, which estimate whether an proposal contains meaningful human motion; and (ii) boundary refinement masks, which provide spatial offsets for adjusting the boundaries of high-quality proposals. By combining these complementary sources of supervision, our model achieves significant improvements over state-of-the-art approaches across several challenging benchmark datasets.""",1
"Geo-localization is a critical task in computer vision. In this work, we cast the geo-localization as a 2D image retrieval task. Current state-of-the-art methods for 2D geo-localization are not robust to locate a scene with drastic scale variations because they only exploit features from one semantic level for image representations. To address this limitation, we introduce a hierarchical attention fusion network using multi-scale features for geo-localization. We extract the hierarchical feature maps from a convolutional neural network (CNN) and organically fuse the extracted features for image representations. Our training is self-supervised using adaptive weights to control the attention of feature emphasis from each hierarchical level. Evaluation results on the image retrieval and the large-scale geo-localization benchmarks indicate that our method outperforms the existing state-of-the-art methods. Code is available here: \url{https://github.com/YanLiqi/HAF}.",0
"Abstract: This paper presents a novel approach to geolocation using hierarchical attention fusion models. In order to improve the accuracy of traditional methods that rely on single image features, we propose a two stage system that first generates multiple feature maps from different subregions within the input image, then fuses these features together using a hierarchy of attention modules. By doing so, our model can more effectively focus on relevant regions while reducing computational cost. Our experimental results show that our method outperforms previous state-of-the-art systems by achieving better mean average precision (mAP) values across several benchmark datasets including COCO and VGGish. Additionally, we demonstrate how our model performs favorably compared to other recent works under comparative evaluation metrics such as recall@K, particularly at smaller distances. Overall, our work demonstrates the effectiveness of hierarchical attention fusion for improving geolocation performance and sets a new bar for future research in this domain.",1
"Affective Computing has recently attracted the attention of the research community, due to its numerous applications in diverse areas. In this context, the emergence of video-based data allows to enrich the widely used spatial features with the inclusion of temporal information. However, such spatio-temporal modelling often results in very high-dimensional feature spaces and large volumes of data, making training difficult and time consuming. This paper addresses these shortcomings by proposing a novel model that efficiently extracts both spatial and temporal features of the data by means of its enhanced temporal modelling based on latent features. Our proposed model consists of three major networks, coined Generator, Discriminator, and Combiner, which are trained in an adversarial setting combined with curriculum learning to enable our adaptive attention modules. In our experiments, we show the effectiveness of our approach by reporting our competitive results on both the AFEW-VA and SEWA datasets, suggesting that temporal modelling improves the affect estimates both in qualitative and quantitative terms. Furthermore, we find that the inclusion of attention mechanisms leads to the highest accuracy improvements, as its weights seem to correlate well with the appearance of facial movements, both in terms of temporal localisation and intensity. Finally, we observe the sequence length of around 160\,ms to be the optimum one for temporal modelling, which is consistent with other relevant findings utilising similar lengths.",0
"This research presents a novel approach for spatio-temporal facial affect estimation using adversarial networks that combines latent features from multiple deep representations. Our method outperforms state-of-the-art methods by exploiting spatial and temporal dependencies to capture subtle differences across diverse poses, lighting conditions, expressions, and subjects. We design three types of discriminators: spatial discriminator, temporal discriminator, and combined discriminator. By incorporating these discriminators into our framework, we improve accuracy while reducing computational complexity compared to sequential concatenation. We evaluate our model on four benchmark datasets including Static Face and Expression, Discrete Emotional Intensity Labels (DEIM), EmotiW Challenge 2017, and Aff-wild2, where it achieves significant improvements over previous techniques. Our results demonstrate the effectiveness and efficiency of our proposed method for automatic affect recognition under complex real-world scenarios.",1
"We present Language-binding Object Graph Network, the first neural reasoning method with dynamic relational structures across both visual and textual domains with applications in visual question answering. Relaxing the common assumption made by current models that the object predicates pre-exist and stay static, passive to the reasoning process, we propose that these dynamic predicates expand across the domain borders to include pair-wise visual-linguistic object binding. In our method, these contextualized object links are actively found within each recurrent reasoning step without relying on external predicative priors. These dynamic structures reflect the conditional dual-domain object dependency given the evolving context of the reasoning through co-attention. Such discovered dynamic graphs facilitate multi-step knowledge combination and refinements that iteratively deduce the compact representation of the final answer. The effectiveness of this model is demonstrated on image question answering demonstrating favorable performance on major VQA datasets. Our method outperforms other methods in sophisticated question-answering tasks wherein multiple object relations are involved. The graph structure effectively assists the progress of training, and therefore the network learns efficiently compared to other reasoning models.",0
"Relational visual reasoning involves understanding relationships between objects and concepts within images. One key challenge in relational visual reasoning is that different tasks may require different types of binding between linguistic descriptions and image contents, where ""binding"" refers to establishing associations between elements from two distinct modalities (e.g., text and vision). In this work, we propose dynamic language binding as a framework for adaptively selecting appropriate binding strategies based on task requirements. Our approach integrates a set of complementary binding functions tailored to different aspects of relational information, such as spatial layout, object interactions, and semantic similarity. By dynamically switching among these functions during inference, our model can effectively adjust its behavior to match diverse query conditions while maintaining high accuracy across different visual reasoning benchmarks. Experimental results demonstrate significant improvements over strong baselines, validating the effectiveness of our proposed methodology in accommodating varying binding demands throughout each task interaction.",1
"With the goal of identifying pixel-wise salient object regions from each input image, salient object detection (SOD) has been receiving great attention in recent years. One kind of mainstream SOD methods is formed by a bottom-up feature encoding procedure and a top-down information decoding procedure. While numerous approaches have explored the bottom-up feature extraction for this task, the design on top-down flows still remains under-studied. To this end, this paper revisits the role of top-down modeling in salient object detection and designs a novel densely nested top-down flows (DNTDF)-based framework. In every stage of DNTDF, features from higher levels are read in via the progressive compression shortcut paths (PCSP). The notable characteristics of our proposed method are as follows. 1) The propagation of high-level features which usually have relatively strong semantic information is enhanced in the decoding procedure; 2) With the help of PCSP, the gradient vanishing issues caused by non-linear operations in top-down information flows can be alleviated; 3) Thanks to the full exploration of high-level features, the decoding process of our method is relatively memory efficient compared against those of existing methods. Integrating DNTDF with EfficientNet, we construct a highly light-weighted SOD model, with very low computational complexity. To demonstrate the effectiveness of the proposed model, comprehensive experiments are conducted on six widely-used benchmark datasets. The comparisons to the most state-of-the-art methods as well as the carefully-designed baseline models verify our insights on the top-down flow modeling for SOD. The code of this paper is available at https://github.com/new-stone-object/DNTD.",0
"Salient object detection (SOD) has been a popular topic in computer vision research due to its numerous applications such as video surveillance, image retrieval, and autonomous driving. In recent years, top-down flow methods have gained popularity for their effectiveness in detecting salient objects by simulating human attention mechanisms. However, existing top-down flow approaches suffer from limitations such as slow inference speed, low accuracy, and poor robustness to noise and cluttered backgrounds. This study proposes a novel approach called densely nested top-down flows (DNTF), which addresses these challenges through four key innovations: 1) hierarchical feature extraction; 2) dense connections within each layer; 3) multiple attentive fusion blocks; and 4) two cascaded flow networks. Experimental results on several benchmark datasets demonstrate that our proposed method significantly outperforms state-of-the-art SOD models, achieving new records on five publicly available datasets while maintaining real-time performance. The proposed framework holds great potential for further development and application across diverse domains where accurate detection of salient objects is critical.",1
"An effective understanding of the environment and accurate trajectory prediction of surrounding dynamic obstacles are indispensable for intelligent mobile systems (e.g. autonomous vehicles and social robots) to achieve safe and high-quality planning when they navigate in highly interactive and crowded scenarios. Due to the existence of frequent interactions and uncertainty in the scene evolution, it is desired for the prediction system to enable relational reasoning on different entities and provide a distribution of future trajectories for each agent. In this paper, we propose a generic generative neural system (called STG-DAT) for multi-agent trajectory prediction involving heterogeneous agents. The system takes a step forward to explicit interaction modeling by incorporating relational inductive biases with a dynamic graph representation and leverages both trajectory and scene context information. We also employ an efficient kinematic constraint layer applied to vehicle trajectory prediction. The constraint not only ensures physical feasibility but also enhances model performance. Moreover, the proposed prediction model can be easily adopted by multi-target tracking frameworks. The tracking accuracy proves to be improved by empirical results. The proposed system is evaluated on three public benchmark datasets for trajectory prediction, where the agents cover pedestrians, cyclists and on-road vehicles. The experimental results demonstrate that our model achieves better performance than various baseline approaches in terms of prediction and tracking accuracy.",0
"In order to model spatio-temporal interactions between multiple agents and capture their movement patterns over time, we propose a novel graph dual attention network (GDAN) architecture. Our GDAN consists of two parts: temporal dua",1
"Knowledge Graphs (KG) are gaining increasing attention in both academia and industry. Despite their diverse benefits, recent research have identified social and cultural biases embedded in the representations learned from KGs. Such biases can have detrimental consequences on different population and minority groups as applications of KG begin to intersect and interact with social spheres. This paper aims at identifying and mitigating such biases in Knowledge Graph (KG) embeddings. As a first step, we explore popularity bias -- the relationship between node popularity and link prediction accuracy. In case of node2vec graph embeddings, we find that prediction accuracy of the embedding is negatively correlated with the degree of the node. However, in case of knowledge-graph embeddings (KGE), we observe an opposite trend. As a second step, we explore gender bias in KGE, and a careful examination of popular KGE algorithms suggest that sensitive attribute like the gender of a person can be predicted from the embedding. This implies that such biases in popular KGs is captured by the structural properties of the embedding. As a preliminary solution to debiasing KGs, we introduce a novel framework to filter out the sensitive attribute information from the KG embeddings, which we call FAN (Filtering Adversarial Network). We also suggest the applicability of FAN for debiasing other network embeddings which could be explored in future work.",0
"""Learning embeddings for knowledge graphs (KGs) has been a popular task in recent years due to their ability to capture rich semantic relationships among entities. However, these models can suffer from biases present in the training data or human annotations that lead to incorrect representations. In this work, we propose adversarial learning for debiasing KG embeddings by introducing a novel regularizer that constrains the model to produce embeddings consistent across multiple perspectives.""",1
"Learning classifiers that are robust to adversarial examples has received a great deal of recent attention. A major drawback of the standard robust learning framework is the imposition of an artificial robustness radius $r$ that applies to all inputs, and ignores the fact that data may be highly heterogeneous. In this paper, we address this limitation by proposing a new framework for adaptive robustness, called neighborhood preserving robustness. We present sufficient conditions under which general non-parametric methods that can be represented as weight functions satisfy our notion of robustness, and show that both nearest neighbors and kernel classifiers satisfy these conditions in the large sample limit.",0
"In this paper we present new algorithms that enable a more robust approach to solving complex problems. These methods are based on a novel combination of traditional techniques and modern nonlinear optimization principles. We demonstrate how these approaches can significantly improve performance by considering several examples from diverse domains, including image processing, data science, finance, robotics, and control systems. Our results show clearly how our methods outperform other standard approaches by orders of magnitude. This work represents a significant breakthrough in the field and promises exciting future developments.",1
"Computational tools for forecasting yields and prices for fresh produce have been based on traditional machine learning approaches or time series modelling. We propose here an alternate approach based on deep learning algorithms for forecasting strawberry yields and prices in Santa Barbara county, California. Building the proposed forecasting model comprises three stages: first, the station-based ensemble model (ATT-CNN-LSTM-SeriesNet_Ens) with its compound deep learning components, SeriesNet with Gated Recurrent Unit (GRU) and Convolutional Neural Network LSTM with Attention layer (Att-CNN-LSTM), are trained and tested using the station-based soil temperature and moisture data of SantaBarbara as input and the corresponding strawberry yields or prices as output. Secondly, the remote sensing ensemble model (SIM_CNN-LSTM_Ens), which is an ensemble model of Convolutional NeuralNetwork LSTM (CNN-LSTM) models, is trained and tested using satellite images of the same county as input mapped to the same yields and prices as output. These two ensembles forecast strawberry yields and prices with minimal forecasting errors and highest model correlation for five weeks ahead forecasts.Finally, the forecasts of these two models are ensembled to have a final forecasted value for yields and prices by introducing a voting ensemble. Based on an aggregated performance measure (AGM), it is found that this voting ensemble not only enhances the forecasting performance by 5% compared to its best performing component model but also outperforms the Deep Learning (DL) ensemble model found in literature by 33% for forecasting yields and 21% for forecasting prices",0
"This study presents new methods for predicting strawberry yields and prices using deep learning techniques. By leveraging satellite imagery and station-based soil parameters, we developed models capable of accurately forecasting yield levels up to six months in advance. Additionally, our approach incorporates advanced feature engineering to create high-quality data that improves model accuracy. Furthermore, we demonstrate how the proposed methodology can effectively handle missing values, leading to better overall performance compared to other state-of-the-art approaches. Our results indicate that combining remote sensing data and local soil conditions provides valuable insights into future crop productivity, with potential applications across agricultural industries worldwide.",1
"Clinical practice in intensive care units (ICUs) requires early warnings when a patient's condition is about to deteriorate so that preventive measures can be undertaken. To this end, prediction algorithms have been developed that estimate the risk of mortality in ICUs. In this work, we propose a novel generative deep probabilistic model for real-time risk scoring in ICUs. Specifically, we develop an attentive deep Markov model called AttDMM. To the best of our knowledge, AttDMM is the first ICU prediction model that jointly learns both long-term disease dynamics (via attention) and different disease states in health trajectory (via a latent variable model). Our evaluations were based on an established baseline dataset (MIMIC-III) with 53,423 ICU stays. The results confirm that compared to state-of-the-art baselines, our AttDMM was superior: AttDMM achieved an area under the receiver operating characteristic curve (AUROC) of 0.876, which yielded an improvement over the state-of-the-art method by 2.2%. In addition, the risk score from the AttDMM provided warnings several hours earlier. Thereby, our model shows a path towards identifying patients at risk so that health practitioners can intervene early and save patient lives.",0
"In this study, we present AttDMM (Attentive Deep Markov Model), which addresses these limitations by introducing attention mechanisms into DMMs and utilizing clinical expertise through interpretable features derived from EHR data. Using ICU patient data, we train and evaluate our model on risk scoring tasks relevant to critical care decisions. We achieve competitive results compared to state-of-the-art models such as GAMENet while offering explainability and interpretability through feature attribution techniques. Our method has the potential to enhance decision support systems, increase transparency in machine learning applications, and reduce unintended consequences stemming from overreliance on opaque black-box predictions. We believe that deepening understanding of how ML models function can lead towards better integration in healthcare systems and improve overall outcomes. Our work represents a step forward in realizing the benefits of artificial intelligence at point-of-care decisions without compromising patient safety or the reliance on evidence-based medicine practices.",1
"We present lambda layers -- an alternative framework to self-attention -- for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Lambda layers capture such interactions by transforming available contexts into linear functions, termed lambdas, and applying these linear functions to each input separately. Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs such as images. The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, COCO object detection and COCO instance segmentation, while being more computationally efficient. Additionally, we design LambdaResNets, a family of hybrid architectures across different scales, that considerably improves the speed-accuracy tradeoff of image classification models. LambdaResNets reach excellent accuracies on ImageNet while being 3.2 - 4.4x faster than the popular EfficientNets on modern machine learning accelerators. When training with an additional 130M pseudo-labeled images, LambdaResNets achieve up to a 9.5x speed-up over the corresponding EfficientNet checkpoints.",0
"In recent years, deep learning models such as graph neural networks (GNN) have achieved state-of-the art performance in numerous tasks involving node representation learning on graphs, including social network analysis, chemical compound prediction, and natural language processing on knowledge graphs. One key advantage of these models lies in their ability to capture long-range interactions among nodes by iteratively aggregating messages from neighboring nodes before computing node representations. However, while attention mechanisms can further enhance the capacity of GNNs by selectively incorporating relevant features into message propagation, they often come at the cost of increased computational complexity and memory usage. To address this tradeoff, we propose LambdaNetworks, a novel approach that enables modeling of arbitrary soft interactions between nodes without relying on explicit attention mechanisms. Our method is based on a variational autoencoder framework with learnable latent spaces, allowing us to introduce richer dependencies between data points through nonlinear transformations of the input variables. We apply our method to two benchmark datasets commonly used for evaluating GNN performance - Cora and Citeseer - and demonstrate significantly improved accuracy compared to several popular baseline methods. Overall, LambdaNetworks offers a promising alternative for efficient yet expressive modeling of complex relationships on graphs.",1
"In this work we propose Pathfinder Discovery Networks (PDNs), a method for jointly learning a message passing graph over a multiplex network with a downstream semi-supervised model. PDNs inductively learn an aggregated weight for each edge, optimized to produce the best outcome for the downstream learning task. PDNs are a generalization of attention mechanisms on graphs which allow flexible construction of similarity functions between nodes, edge convolutions, and cheap multiscale mixing layers. We show that PDNs overcome weaknesses of existing methods for graph attention (e.g. Graph Attention Networks), such as the diminishing weight problem. Our experimental results demonstrate competitive predictive performance on academic node classification tasks. Additional results from a challenging suite of node classification experiments show how PDNs can learn a wider class of functions than existing baselines. We analyze the relative computational complexity of PDNs, and show that PDN runtime is not considerably higher than static-graph models. Finally, we discuss how PDNs can be used to construct an easily interpretable attention mechanism that allows users to understand information propagation in the graph.",0
"In recent years, there has been significant interest in developing novel deep learning architectures that can capture complex relationships between input variables. One such approach involves using graph neural networks (GNNs), which have shown promising results in applications such as node classification, link prediction, and graph generation tasks. However, GNNs often struggle with overfitting due to their tendency to memorize patterns in data rather than learning underlying principles.  In order to address these limitations, we propose a new framework called Pathfinder Discovery Networks (PDN) that combines traditional message passing techniques used in GNNs with path-based reasoning. PDNs leverage both shortest path distances and random walk based diffusion probabilities to guide message passing across a graph. This allows them to learn meaningful representations by extracting important relationships from the topology itself while simultaneously utilizing global structure learned through random walks.  Experimental results on several benchmark datasets demonstrate that our proposed method outperforms state-of-the-art methods in terms of accuracy and stability. We believe that PDNs offer a powerful toolkit for working with graphs, and they could potentially lead to even more advanced models capable of solving challenges that existing frameworks struggle with today. Overall, the research presented in this paper offers valuable insights into designing effective algorithms for handling complex network structures.",1
"Multi-label classification (MLC) is a generalization of standard classification where multiple labels may be assigned to a given sample. In the real world, it is more common to deal with noisy datasets than clean datasets, given how modern datasets are labeled by a large group of annotators on crowdsourcing platforms, but little attention has been given to evaluating multi-label classifiers with noisy labels. Exploiting label correlations now becomes a standard component of a multi-label classifier to achieve competitive performance. However, this component makes the classifier more prone to poor generalization - it overfits labels as well as label dependencies. We identify three common real-world label noise scenarios and show how previous approaches per-form poorly with noisy labels. To address this issue, we present a Context-Based Multi-LabelClassifier (CbMLC) that effectively handles noisy labels when learning label dependencies, without requiring additional supervision. We compare CbMLC against other domain-specific state-of-the-art models on a variety of datasets, under both the clean and the noisy settings. We show CbMLC yields substantial improvements over the previous methods in most cases.",0
"Title: Evaluating Multi-Label Classification Models under Real-World Conditions  The evaluation of multi-label classification models is important but challenging due to the presence of noise and imprecision in the labels. In practice, label datasets may contain errors such as incorrect or missing annotations that can affect model performance. This work proposes a framework for evaluating multi-label classifiers considering noisy labels by introducing random perturbations into the training and testing sets to simulate real-world conditions. We evaluate five state-of-the-art multi-label learning methods using several evaluation metrics including accuracy, precision, recall, F1 score, and confusion matrix based measures. Results show that all evaluated models exhibit significant decreases in performance with increased levels of label noise, demonstrating the importance of considering noisy labels during evaluation. Our proposed approach provides insight into how different models behave under varying degrees of uncertainty and offers guidance on selecting appropriate multi-label classifiers for specific applications.",1
"Visual place recognition (VPR) is the problem of recognising a previously visited location using visual information. Many attempts to improve the performance of VPR methods have been made in the literature. One approach that has received attention recently is the multi-process fusion where different VPR methods run in parallel and their outputs are combined in an effort to achieve better performance. The multi-process fusion, however, does not have a well-defined criterion for selecting and combining different VPR methods from a wide range of available options. To the best of our knowledge, this paper investigates the complementarity of state-of-the-art VPR methods systematically for the first time and identifies those combinations which can result in better performance. The paper presents a well-defined framework which acts as a sanity check to find the complementarity between two techniques by utilising a McNemar's test-like approach. The framework allows estimation of upper and lower complementarity bounds for the VPR techniques to be combined, along with an estimate of maximum VPR performance that may be achieved. Based on this framework, results are presented for eight state-of-the-art VPR methods on ten widely-used VPR datasets showing the potential of different combinations of techniques for achieving better performance.",0
"In recent years, visual place recognition (VPR) has become increasingly important in fields such as robotics, autonomous vehicles, and augmented reality systems. VPR algorithms aim to identify whether two images depict the same location, regardless of changes in illumination, weather conditions, or time of day. Despite significant progress in this field, there remains a challenge in achieving robustness under these variations while maintaining high accuracy. This paper presents a method that maximizes complementarity between different image representations to improve VPR performance. We demonstrate through experiments on public datasets that our approach outperforms state-of-the-art methods, achieving higher accuracy and robustness under challenging conditions. Our findings have important implications for a wide range of applications that rely on accurate and reliable VPR.",1
"Visual perception of the objects in a 3D environment is a key to successful performance in autonomous driving and simultaneous localization and mapping (SLAM). In this paper, we present a real time approach for estimating the distances to multiple objects in a scene using only a single-shot image. Given a 2D Bounding Box (BBox) and object parameters, a 3D distance to the object can be calculated directly using 3D reprojection; however, such methods are prone to significant errors because an error from the 2D detection can be amplified in 3D. In addition, it is also challenging to apply such methods to a real-time system due to the computational burden. In the case of the traditional multi-object detection methods, %they mostly pay attention to existing works have been developed for specific tasks such as object segmentation or 2D BBox regression. These methods introduce the concept of anchor BBox for elaborate 2D BBox estimation, and predictors are specialized and trained for specific 2D BBoxes. In order to estimate the distances to the 3D objects from a single 2D image, we introduce the notion of \textit{anchor distance} based on an object's location and propose a method that applies the anchor distance to the multi-object detector structure. We let the predictors catch the distance prior using anchor distance and train the network based on the distance. The predictors can be characterized to the objects located in a specific distance range. By propagating the distance prior using a distance anchor to the predictors, it is feasible to perform the precise distance estimation and real-time execution simultaneously. The proposed method achieves about 30 FPS speed, and shows the lowest RMSE compared to the existing methods.",0
"Incorporate some key terms related to computer vision like anchor distance, multi-object, 3D distances and single shot. Use descriptive language that captures the essence of your work:  This work presents a novel approach to estimating 3D distances between objects using only a single RGB image. We propose the use of anchor distance as a metric for measuring 3D distances, which involves selecting multiple pairs of corresponding points between two images and computing their Euclidean distances to determine depth relationships. Our method leverages recent advancements in deep learning techniques such as feature extraction networks and optimization algorithms to estimate distances with high accuracy even in situations where traditional methods fail. By demonstrating promising results on challenging benchmark datasets, we aim to contribute to the growing field of computer vision and enable applications in areas such as autonomous navigation, robotics, and augmented reality.  The ability to accurately measure distances between objects is essential in many fields, including autonomous navigation, robotics, and augmented reality. Traditional approaches to obtaining 3D distance measurements can often require specialized hardware, sensor configurations, and may suffer from limited performance in complex environments. To address these limitations, we propose a new approach based on multi-object anchor distance estimation from a single 2D image. This method builds upon recent advances in deep learning by utilizing feature extraction networks and optimization algorithms to achieve high levels of accuracy even under difficult imaging conditions. Through extensive evaluation on several benchmark datasets, our proposed algorithm has consistently demonstrated improved robustness and flexibility compared to conventional methods. As such, our research represents an important contribution to the broader field of computer vision and highlights the potential applications of enhanced 3D object detection capabilities in real-world settings.",1
"Precipitation nowcasting is an important task for weather forecasting. Many recent works aim to predict the high rainfall events more accurately with the help of deep learning techniques, but such events are relatively rare. The rarity is often addressed by formulations that re-weight the rare events. Somehow such a formulation carries a side effect of making ""blurry"" predictions in low rainfall regions and cannot convince meteorologists to trust its practical usability. We fix the trust issue by introducing a discriminator that encourages the prediction model to generate realistic rain-maps without sacrificing predictive accuracy. Furthermore, we extend the nowcasting time frame from one hour to three hours to further address the needs from meteorologists. The extension is based on consecutive attentions across different hours. We propose a new deep learning model for precipitation nowcasting that includes both the discrimination and attention techniques. The model is examined on a newly-built benchmark dataset that contains both radar data and actual rain data. The benchmark, which will be publicly released, not only establishes the superiority of the proposed model, but also is expected to encourage future research on precipitation nowcasting.",0
"Despite recent advances in numerical weather prediction (NWP) models, precipitation nowcasting remains challenging due to the high variability of weather conditions over short time scales. This study presents two approaches that improve upon traditional methods by leveraging temporal attention mechanisms and rain map discrimination techniques. Our first approach introduces consecutive attention layers within recurrent neural networks (RNNs), allowing them to selectively focus on relevant input features based on their past experiences. We test our model using radar observations from three different locations across Germany, achieving statistically significant improvements in both accuracy and precision compared to state-of-the-art NWP models. In addition, we propose a second method that integrates auxiliary datasets such as digital surface maps into our model architecture via adversarial training. This ensures that the RNN learns to differentiate between true and false rainfall signals while maintaining meteorologically meaningful interpretations of its outputs. Results indicate consistent performance gains in terms of spatial resolution and quantitative accuracy. These findings support the effectiveness of incorporating advanced machine learning techniques into atmospheric sciences research to enhance precipitation nowcasting capabilities. By harnessing the power of deep learning algorithms, scientists can generate more reliable predictions at finer spatiotemporal granularities, ultimately benefiting society through better forecasting abilities during extreme weather events.",1
"Advertising channels have evolved from conventional print media, billboards and radio advertising to online digital advertising (ad), where the users are exposed to a sequence of ad campaigns via social networks, display ads, search etc. While advertisers revisit the design of ad campaigns to concurrently serve the requirements emerging out of new ad channels, it is also critical for advertisers to estimate the contribution from touch-points (view, clicks, converts) on different channels, based on the sequence of customer actions. This process of contribution measurement is often referred to as multi-touch attribution (MTA). In this work, we propose CAMTA, a novel deep recurrent neural network architecture which is a casual attribution mechanism for user-personalised MTA in the context of observational data. CAMTA minimizes the selection bias in channel assignment across time-steps and touchpoints. Furthermore, it utilizes the users' pre-conversion actions in a principled way in order to predict pre-channel attribution. To quantitatively benchmark the proposed MTA model, we employ the real world Criteo dataset and demonstrate the superior performance of CAMTA with respect to prediction accuracy as compared to several baselines. In addition, we provide results for budget allocation and user-behaviour modelling on the predicted channel attribution.",0
"Title your work ""Abstract for Paper"" and sign off as per usual requirements (e.g. MLA). Include keywords you would use if submitting to arXiv, e.g.: Advertising technology, Artificial Intelligence, Marketing, Attribution modelling, etc.)",1
"We present EgoACO, a deep neural architecture for video action recognition that learns to pool action-context-object descriptors from frame level features by leveraging the verb-noun structure of action labels in egocentric video datasets. The core component of EgoACO is class activation pooling (CAP), a differentiable pooling operation that combines ideas from bilinear pooling for fine-grained recognition and from feature learning for discriminative localization. CAP uses self-attention with a dictionary of learnable weights to pool from the most relevant feature regions. Through CAP, EgoACO learns to decode object and scene context descriptors from video frame features. For temporal modeling in EgoACO, we design a recurrent version of class activation pooling termed Long Short-Term Attention (LSTA). LSTA extends convolutional gated LSTM with built-in spatial attention and a re-designed output gate. Action, object and context descriptors are fused by a multi-head prediction that accounts for the inter-dependencies between noun-verb-action structured labels in egocentric video datasets. EgoACO features built-in visual explanations, helping learning and interpretation. Results on the two largest egocentric action recognition datasets currently available, EPIC-KITCHENS and EGTEA, show that by explicitly decoding action-context-object descriptors, EgoACO achieves state-of-the-art recognition performance.",0
"This paper proposes a novel method for recognizing actions performed on objects in first-person video using attention dictionaries. Firstly, we introduce a convolutional neural network architecture that can extract features from egocentric videos by using a dilated convulation layer designed to capture contextual information at multiple scales. Then, we use these extracted feature maps as input to our action recognition model, which consists of two main components: an attention dictionary and a classifier. Our attention dictionary learns to attend selectively to different regions of the image where discriminative visual evidence resides for each action class, while the classifier uses these attended representations along with their corresponding spatial weights to make predictions. We demonstrate that our method significantly outperforms state-of-the-art methods on three popular benchmark datasets, achieving new records in performance. Additionally, we conduct ablation studies to investigate the contributions of individual components and provide insights into future improvements. Overall, this work presents a promising approach to addressing the challenges associated with recognizing actions performed on objects in egocentric video.",1
"In recent years, physiological signal based authentication has shown great promises,for its inherent robustness against forgery. Electrocardiogram (ECG) signal, being the most widely studied biosignal, has also received the highest level of attention in this regard. It has been proven with numerous studies that by analyzing ECG signals from different persons, it is possible to identify them, with acceptable accuracy. In this work, we present, EDITH, a deep learning-based framework for ECG biometrics authentication system. Moreover, we hypothesize and demonstrate that Siamese architectures can be used over typical distance metrics for improved performance. We have evaluated EDITH using 4 commonly used datasets and outperformed the prior works using less number of beats. EDITH performs competitively using just a single heartbeat (96-99.75% accuracy) and can be further enhanced by fusing multiple beats (100% accuracy from 3 to 6 beats). Furthermore, the proposed Siamese architecture manages to reduce the identity verification Equal Error Rate (EER) to 1.29%. A limited case study of EDITH with real-world experimental data also suggests its potential as a practical authentication system.",0
"This paper presents a new approach to ECG-based individual authentication using deep learning techniques. We propose a system called EDITH (Electrocardiogram-based Deep Learning for Identity Authentication) that utilizes convolutional neural networks to extract unique features from individuals' ECG signals, enabling highly accurate identification of authorized users. Our method outperforms traditional methods in terms of accuracy, robustness, and ease of use. In addition, our system can work under different conditions such as varying heart rates, body movements, and environmental noise. Finally, we evaluate the security of the proposed method against potential attacks like replay attacks and man-in-the-middle attacks. Our results demonstrate the effectiveness and reliability of EDITH in protecting sensitive information and preventing unauthorized access. Overall, our research paves the way for more secure, efficient, and accessible ECG-based authentication systems.",1
"The recent success of graph neural networks has significantly boosted molecular property prediction, advancing activities such as drug discovery. The existing deep neural network methods usually require large training dataset for each property, impairing their performances in cases (especially for new molecular properties) with a limited amount of experimental data, which are common in real situations. To this end, we propose Meta-MGNN, a novel model for few-shot molecular property prediction. Meta-MGNN applies molecular graph neural network to learn molecular representation and builds a meta-learning framework for model optimization. To exploit unlabeled molecular information and address task heterogeneity of different molecular properties, Meta-MGNN further incorporates molecular structure, attribute based self-supervised modules and self-attentive task weights into the former framework, strengthening the whole learning model. Extensive experiments on two public multi-property datasets demonstrate that Meta-MGNN outperforms a variety of state-of-the-art methods.",0
"This paper presents a novel approach for molecular property prediction using few-shot graph learning methods. In traditional machine learning approaches, large amounts of data are required to accurately predict properties such as solubility and reactivity. However, acquiring labeled training data can be expensive and time consuming, limiting the scalability of these models. To address this challenge, we propose a method that leverages existing knowledge from closely related molecules to make predictions on new, unseen compounds. Our method utilizes a neural network architecture designed specifically for few-shot learning tasks and allows us to rapidly adapt our model to new chemical environments. We evaluate our approach using two benchmark datasets and demonstrate significant improvements over state-of-the art baselines across both metrics. Overall, our work demonstrates the effectiveness of few-shot graph learning methods for molecular property prediction, opening up exciting possibilities for applications in drug discovery, materials design, and other fields where rapid, accurate predictions are critical.",1
"Exploiting available medical records to train high performance computer-aided diagnosis (CAD) models via the semi-supervised learning (SSL) setting is emerging to tackle the prohibitively high labor costs involved in large-scale medical image annotations. Despite the extensive attentions received on SSL, previous methods failed to 1) account for the low disease prevalence in medical records and 2) utilize the image-level diagnosis indicated from the medical records. Both issues are unique to SSL for CAD models. In this work, we propose a new knowledge distillation method that effectively exploits large-scale image-level labels extracted from the medical records, augmented with limited expert annotated region-level labels, to train a rib and clavicle fracture CAD model for chest X-ray (CXR). Our method leverages the teacher-student model paradigm and features a novel adaptive asymmetric label sharpening (AALS) algorithm to address the label imbalance problem that specially exists in medical domain. Our approach is extensively evaluated on all CXR (N = 65,845) from the trauma registry of anonymous hospital over a period of 9 years (2008-2016), on the most common rib and clavicle fractures. The experiment results demonstrate that our method achieves the state-of-the-art fracture detection performance, i.e., an area under receiver operating characteristic curve (AUROC) of 0.9318 and a free-response receiver operating characteristic (FROC) score of 0.8914 on the rib fractures, significantly outperforming previous approaches by an AUROC gap of 1.63% and an FROC improvement by 3.74%. Consistent performance gains are also observed for clavicle fracture detection.",0
"This paper presents a method for semi-supervised fracture detection in chest x-rays using knowledge distillation with adaptive asymmetric label sharpening (AALS). The proposed approach leverages deep learning techniques to improve the accuracy and robustness of fracture detection on limited labeled data by utilizing unlabeled images to regularize model training. Our key contribution is the introduction of AALS as an effective means of generating high quality pseudo labels from teacher models trained on labeled datasets to guide self-training iterations towards more challenging examples that yield better student model performance. Experimental results demonstrate significant improvements over strong baseline methods across multiple metrics, including radiologist agreement studies, further supporting the effectiveness of our framework for improving patient care outcomes in clinical practice. Overall, we believe this work paves the way for future research into developing novel semi-supervised learning approaches for medical image analysis applications where large annotated datasets may be difficult to obtain.",1
"Many real-world applications involve data from multiple modalities and thus exhibit the view heterogeneity. For example, user modeling on social media might leverage both the topology of the underlying social network and the content of the users' posts; in the medical domain, multiple views could be X-ray images taken at different poses. To date, various techniques have been proposed to achieve promising results, such as canonical correlation analysis based methods, etc. In the meanwhile, it is critical for decision-makers to be able to understand the prediction results from these methods. For example, given the diagnostic result that a model provided based on the X-ray images of a patient at different poses, the doctor needs to know why the model made such a prediction. However, state-of-the-art techniques usually suffer from the inability to utilize the complementary information of each view and to explain the predictions in an interpretable manner.   To address these issues, in this paper, we propose a deep co-attention network for multi-view subspace learning, which aims to extract both the common information and the complementary information in an adversarial setting and provide robust interpretations behind the prediction to the end-users via the co-attention mechanism. In particular, it uses a novel cross reconstruction loss and leverages the label information to guide the construction of the latent representation by incorporating the classifier into our model. This improves the quality of latent representation and accelerates the convergence speed. Finally, we develop an efficient iterative algorithm to find the optimal encoders and discriminator, which are evaluated extensively on synthetic and real-world data sets. We also conduct a case study to demonstrate how the proposed method robustly interprets the predictions on an image data set.",0
"Title: Deep Co-Attention Network for Multi-View Subspace Learning  Abstract: This research proposes a novel deep learning architecture that addresses the challenge of multi-view subspace learning by introducing the concept of co-attention mechanism. The proposed model utilizes multiple views (either as input features or latent representation) and learns their interactions through attention mechanisms. We first propose a deep co-attention network structure, where each view can attend to different aspects of other views, resulting in more effective feature interaction. Secondly, we introduce a new method called channel scattering convolutions which effectively aggregates channel dependencies within the same view while preserving relative positions among channels. We evaluate our approach on several benchmark datasets including MNIST, CIFAR-10, NUS-WIDE, COCO, and PMFGANs and achieve state-of-the-art results across all tasks. Our contribution showcases significant improvements over existing methods for handling multiple views. Overall, our work advances the current understanding of multi-view representation learning and has important implications for real-world applications such as computer vision and natural language processing.",1
"Today, the use of social networking data has attracted a lot of academic and commercial attention in predicting the stock market. In most studies in this area, the sentiment analysis of the content of user posts on social networks is used to predict market fluctuations. Predicting stock marketing is challenging because of the variables involved. In the short run, the market behaves like a voting machine, but in the long run, it acts like a weighing machine. The purpose of this study is to predict EUR/USD stock behavior using Capsule Network on finance texts and Candlestick images. One of the most important features of Capsule Network is the maintenance of features in a vector, which also takes into account the space between features. The proposed model, TI-Capsule (Text and Image information based Capsule Neural Network), is trained with both the text and image information simultaneously. Extensive experiments carried on the collected dataset have demonstrated the effectiveness of TI-Capsule in solving the stock exchange prediction problem with 91% accuracy.",0
"Title: Improving Stock Exchange Predictions using Capsule Networks  Artificial Intelligence (AI) has been applied across multiple domains ranging from computer vision to natural language processing. One area where AI can make significant impact is stock exchange predictions. This study presents a novel approach towards improving stock exchange predictions by utilizing capsule networks.  Capsule networks have recently gained attention due to their ability to capture more hierarchical relationships within images compared to traditional convolutional neural networks (CNN). However, their application in time-series prediction tasks such as stock exchange forecasting has remained relatively unexplored. In this work, we propose TI-capsule, which integrates temporal information into capsule networks by extending the capsule architecture to allow for dynamic routing over both spatial and temporal dimensions.  The performance of our proposed model was evaluated on several benchmark datasets for stock exchange prediction, including closing price and moving averages. Experimental results show that our method outperforms state-of-the-art models, achieving higher accuracy in predicting future stock prices. Further analysis shows that our method captures better temporal patterns in financial data, indicating the effectiveness of incorporating temporal dynamics into the capsule network framework.  This research demonstrates that capsule networks are capable of producing high quality predictions in the domain of finance, opening up new possibilities for exploring alternative architectures beyond CNNs. Our work provides valuable insights into how these models may be further improved through the integration of temporal information, laying the groundwork for future research directions in AI-driven stock market prediction.",1
"Predicting saliency in videos is a challenging problem due to complex modeling of interactions between spatial and temporal information, especially when ever-changing, dynamic nature of videos is considered. Recently, researchers have proposed large-scale datasets and models that take advantage of deep learning as a way to understand what's important for video saliency. These approaches, however, learn to combine spatial and temporal features in a static manner and do not adapt themselves much to the changes in the video content. In this paper, we introduce Gated Fusion Network for dynamic saliency (GFSalNet), the first deep saliency model capable of making predictions in a dynamic way via gated fusion mechanism. Moreover, our model also exploits spatial and channel-wise attention within a multi-scale architecture that further allows for highly accurate predictions. We evaluate the proposed approach on a number of datasets, and our experimental analysis demonstrates that it outperforms or is highly competitive with the state of the art. Importantly, we show that it has a good generalization ability, and moreover, exploits temporal information more effectively via its adaptive fusion scheme.",0
"In this study, we present a gated fusion network (GFN) architecture designed specifically for dynamic saliency prediction tasks such as action recognition. Our model utilizes both temporal and spatial information in order to predict the most important regions of interest in each frame. We propose using gate mechanisms at different scales to selectively fuse features from multiple branches, allowing our model to adaptively focus on relevant details during runtime. Extensive experiments on two large datasets show that our approach outperforms several state-of-the-art methods by significant margins while maintaining real-time performance. Our findings demonstrate that GFN effectively captures complex patterns in video data and can serve as a powerful tool for other computer vision applications where high accuracy and efficiency are crucial requirements.",1
"While Transformer architectures have show remarkable success, they are bound to the computation of all pairwise interactions of input element and thus suffer from limited scalability. Recent work has been successful by avoiding the computation of the complete attention matrix, yet leads to problems down the line. The absence of an explicit attention matrix makes the inclusion of inductive biases relying on relative interactions between elements more challenging. An extremely powerful inductive bias is translational equivariance, which has been conjectured to be responsible for much of the success of Convolutional Neural Networks on image recognition tasks. In this work we show how translational equivariance can be implemented in efficient Transformers based on kernelizable attention - Performers. Our experiments highlight that the devised approach significantly improves robustness of Performers to shifts of input images compared to their naive application. This represents an important step on the path of replacing Convolutional Neural Networks with more expressive Transformer architectures and will help to improve sample efficiency and robustness in this realm.",0
"In this technical paper, we examine the concept of translational equivariance as applied to kernelizable attention, a technique used in deep learning to model dependencies among different parts of data sequences. We discuss the challenges faced by existing models in achieving perfect translational equivariance and propose several improvements that can be made to address these issues. Our proposed method allows for better control over how attention weights are learned, leading to improved performance on various tasks such as image classification, machine translation, and question answering. Through rigorous experimentation, we demonstrate the effectiveness of our approach compared to state-of-the-art methods. Overall, our work provides insights into ways to enhance current deep learning techniques through more precise handling of translational equivariance in kernelizable attention models.",1
"Traffic forecasting is important for the success of intelligent transportation systems. Deep learning models, including convolution neural networks and recurrent neural networks, have been extensively applied in traffic forecasting problems to model spatial and temporal dependencies. In recent years, to model the graph structures in transportation systems as well as contextual information, graph neural networks have been introduced and have achieved state-of-the-art performance in a series of traffic forecasting problems. In this survey, we review the rapidly growing body of research using different graph neural networks, e.g. graph convolutional and graph attention networks, in various traffic forecasting problems, e.g. road traffic flow and speed forecasting, passenger flow forecasting in urban rail transit systems, and demand forecasting in ride-hailing platforms. We also present a comprehensive list of open data and source resources for each problem and identify future research directions. To the best of our knowledge, this paper is the first comprehensive survey that explores the application of graph neural networks for traffic forecasting problems. We have also created a public GitHub repository where the latest papers, open data, and source resources will be updated.",0
"Title: Graph Neural Networks for Traffic Forecasting: A Comprehensive Review  This comprehensive review surveys the state-of-the-art developments on graph neural networks (GNN) for traffic forecasting applications. GNNs have emerged as powerful tools for processing spatially explicit data such as graphs, allowing researchers to model complex relationships between nodes and edges. In recent years, there has been growing interest in using GNNs for transportation systems due to their ability to capture both structured and unstructured spatiotemporal patterns in large datasets.  The paper presents an overview of GNN architectures commonly used in traffic forecasting tasks such as urban network traffic volume prediction, incident detection and speed estimation. We provide a systematic analysis of the latest advancements, challenges faced by existing models, and future directions for improving accuracy in real-time traffic forecasting through GNN techniques.  In conclusion, our review emphasizes that harnessing the power of GNNs effectively requires appropriate integration of external features such as weather conditions and public events, handling missing or incomplete data in real-world settings, and incorporating domain knowledge into the training process. Our work can serve as a reference guide for those wishing to explore novel approaches based on GNNs for tackling critical issues related to traffic forecasting and management.",1
"Since its discovery in 2013, the phenomenon of adversarial examples has attracted a growing amount of attention from the machine learning community. A deeper understanding of the problem could lead to a better comprehension of how information is processed and encoded in neural networks and, more in general, could help to solve the issue of interpretability in machine learning. Our idea to increase adversarial resilience starts with the observation that artificial neurons can be divided in two broad categories: AND-like neurons and OR-like neurons. Intuitively, the former are characterised by a relatively low number of combinations of input values which trigger neuron activation, while for the latter the opposite is true. Our hypothesis is that the presence in a network of a sufficiently high number of OR-like neurons could lead to classification ""brittleness"" and increase the network's susceptibility to adversarial attacks. After constructing an operational definition of a neuron AND-like behaviour, we proceed to introduce several measures to increase the proportion of AND-like neurons in the network: L1 norm weight normalisation; application of an input filter; comparison between the neuron output's distribution obtained when the network is fed with the actual data set and the distribution obtained when the network is fed with a randomised version of the former called ""scrambled data set"". Tests performed on the MNIST data set hint that the proposed measures could represent an interesting direction to explore.",0
"Adversarial attacks have become increasingly prevalent as deep learning models continue to gain popularity across industries. To strengthen model resilience against such attacks, researchers often seek to improve the architectural components of these systems. One important component is the activation function used within each individual neuron, which plays a crucial role in shaping the output distribution and ultimately affects the overall model performance. In this work, we explore the impact of using different types of activation functionsâ€”sigmoidal vs. rectified linear units (ReLU)â€”within each artificial neuron on their robustness against adversarial examples. Our experiments show that sigmoidal activation functions lead to better adversarial resistance compared to ReLUs; however, they come at the cost of reduced accuracy on clean inputs. By analyzing these results, we provide insights into how the choice of activation functions can affect both the adversarial robustness and the clean accuracy of neural networks. This study contributes to our understanding of how seemingly small changes within artificial neurons could potentially enhance their security while maintaining satisfactory levels of task performance.",1
"Electric Vehicle (EV) has become a preferable choice in the modern transportation system due to its environmental and energy sustainability. However, in many large cities, EV drivers often fail to find the proper spots for charging, because of the limited charging infrastructures and the spatiotemporally unbalanced charging demands. Indeed, the recent emergence of deep reinforcement learning provides great potential to improve the charging experience from various aspects over a long-term horizon. In this paper, we propose a framework, named Multi-Agent Spatio-Temporal Reinforcement Learning (Master), for intelligently recommending public accessible charging stations by jointly considering various long-term spatiotemporal factors. Specifically, by regarding each charging station as an individual agent, we formulate this problem as a multi-objective multi-agent reinforcement learning task. We first develop a multi-agent actor-critic framework with the centralized attentive critic to coordinate the recommendation between geo-distributed agents. Moreover, to quantify the influence of future potential charging competition, we introduce a delayed access strategy to exploit the knowledge of future charging competition during training. After that, to effectively optimize multiple learning objectives, we extend the centralized attentive critic to multi-critics and develop a dynamic gradient re-weighting strategy to adaptively guide the optimization direction. Finally, extensive experiments on two real-world datasets demonstrate that Master achieves the best comprehensive performance compared with nine baseline approaches.",0
"In recent years, electric vehicles (EVs) have gained popularity due to their environmental benefits and cost savings compared to traditional gasoline cars. However, one major challenge facing EV adoption is limited driving range and difficulty finding charging stations during travel. To address these challenges, we propose an intelligent EV charging recommendation system based on multi-agent reinforcement learning (MARL). Our approach uses several MARL algorithms such as QMIX, COMA, and MADDPG, which allow multiple agents to learn from each other's experiences and make better decisions collectively. These agents can optimize both individual vehicle performance and network-level energy efficiency by collaboratively recommending appropriate charging actions at different timescales. For instance, at the daily scale, the agent may suggest slower charging while parked at home overnight where cheaper tariffs typically apply; whereas at the hourly scale, fast charging sessions during peak renewable power generation periods could minimize grid infrastructure costs. Overall, our research shows that the proposed intelligent EV charging recommendation system using MARL significantly improves average driving ranges and reduces battery degradation while providing flexible options for drivers based on their preferences and needs. By increasing trust in EV technology, our solution could stimulate further deployment of electric mobility and reduce carbon emissions from transport sector worldwide.",1
"Graph neural networks (GNNs) have received massive attention in the field of machine learning on graphs. Inspired by the success of neural networks, a line of research has been conducted to train GNNs to deal with various tasks, such as node classification, graph classification, and link prediction. In this work, our task of interest is graph classification. Several GNN models have been proposed and shown great accuracy in this task. However, the question is whether usual training methods fully realize the capacity of the GNN models.   In this work, we propose a two-stage training framework based on triplet loss. In the first stage, GNN is trained to map each graph to a Euclidean-space vector so that graphs of the same class are close while those of different classes are mapped far apart. Once graphs are well-separated based on labels, a classifier is trained to distinguish between different classes. This method is generic in the sense that it is compatible with any GNN model. By adapting five GNN models to our method, we demonstrate the consistent improvement in accuracy and utilization of each GNN's allocated capacity over the original training method of each model up to 5.4\% points in 12 datasets.",0
This paper presents a novel two-stage training approach for graph neural networks (GNN) which improves their performance on challenging benchmark datasets for graph classification tasks.,1
"Image generation from a single image using generative adversarial networks is quite interesting due to the realism of generated images. However, recent approaches need improvement for such realistic and diverse image generation, when the global context of the image is important such as in face, animal, and architectural image generation. This is mainly due to the use of fewer convolutional layers for mainly capturing the patch statistics and, thereby, not being able to capture global statistics very well. We solve this problem by using attention blocks at selected scales and feeding a random Gaussian blurred image to the discriminator for training. Our results are visually better than the state-of-the-art particularly in generating images that require global context. The diversity of our image generation, measured using the average standard deviation of pixels, is also better.",0
"In recent years, generative models have shown great progress in generating high quality images from textual descriptions or random noise inputs (e.g., DALL-E, Midjourney). However, most existing methods generate globally coherent images without allowing controllability on global structures or scenes, which limits their use cases in real applications such as image editing, VR/AR, etc. To tackle these challenges, we propose a novel method called DisGen that can generate diverse single images with controllable global structure via self-attention modules. Our approach starts by using a pretrained generator model that produces initial outputs, then iteratively refines them with self-attention mechanisms guided by user constraints expressed as natural language queries. This allows users to steer the generation process towards desired directions, while keeping structural consistency guaranteed by attention mechanisms across different regions or objects within a scene. Experimental results demonstrate our methodâ€™s effectiveness at producing highly varied yet coherent outputs with fine details or accurate semantic layouts, compared against other state-of-the-art baselines. The source code, trained models, and detailed user manual will all be publicly available upon submission acceptance. As future work, extensions to video generation could leverage our current framework with simple temporal dynamics, though more advanced techniques may potentially improve longer sequence generation further.",1
"Attribution methods calculate attributions that visually explain the predictions of deep neural networks (DNNs) by highlighting important parts of the input features. In particular, gradient-based attribution (GBA) methods are widely used because they can be easily implemented through automatic differentiation. In this study, we use the attributions that filter out irrelevant parts of the input features and then verify the effectiveness of this approach by measuring the classification accuracy of a pre-trained DNN. This is achieved by calculating and applying an \textit{attribution mask} to the input features and subsequently introducing the masked features to the DNN, for which the mask is designed to recursively focus attention on the parts of the input related to the target label. The accuracy is enhanced under a certain condition, i.e., \textit{no implicit bias}, which can be derived based on our theoretical insight into compressing the DNN into a single-layer neural network. We also provide Gradient\,*\,Sign-of-Input (GxSI) to obtain the attribution mask that further improves the accuracy. As an example, on CIFAR-10 that is modified using the attribution mask obtained from GxSI, we achieve the accuracy ranging from 99.8\% to 99.9\% without additional training.",0
"This research proposal investigates the performance implications of input feature filtering as a means to optimize deep neural networks (DNNs). With growing datasets becoming increasingly unwieldy due to unneeded features, finding ways to streamline model size and improve computational efficiency has become essential in maintaining real-time operations. As such, our study focuses on how attributing attention weights allows us to identify important features by recursively applying them during training time via feature masking. Our method enables more efficient feature selection without compromising accuracy across multiple benchmark datasets and architecturesâ€”ultimately demonstrating its effectiveness as a generalizable solution capable of scaling well into modern DNN design paradigms.",1
"Training deep reinforcement learning agents on environments with multiple levels / scenes from the same task, has become essential for many applications aiming to achieve generalization and domain transfer from simulation to the real world. While such a strategy is helpful with generalization, the use of multiple scenes significantly increases the variance of samples collected for policy gradient computations. Current methods, effectively continue to view this collection of scenes as a single Markov decision process (MDP), and thus learn a scene-generic value function V(s). However, we argue that the sample variance for a multi-scene environment is best minimized by treating each scene as a distinct MDP, and then learning a joint value function V(s,M) dependent on both state s and MDP M. We further demonstrate that the true joint value function for a multi-scene environment, follows a multi-modal distribution which is not captured by traditional CNN / LSTM based critic networks. To this end, we propose a dynamic value estimation (DVE) technique, which approximates the true joint value function through a sparse attention mechanism over multiple value function hypothesis / modes. The resulting agent not only shows significant improvements in the final reward score across a range of OpenAI ProcGen environments, but also exhibits enhanced navigation efficiency and provides an implicit mechanism for unsupervised state-space skill decomposition.",0
"One approach to reinforcement learning is through dynamic value estimation (DVE), which involves estimating a temporal difference (TD) target using Monte Carlo methods. While traditional DVE uses fixed attention over discrete actions, we propose sparse attentional guiding that focuses on small subsets of states as well as continuous action vectors. We show improvements across multiple RL benchmarks, including MuJoCo locomotion tasks. By reducing computational complexity while improving sample efficiency, our method provides more scalability towards real-world applications.",1
"Knowledge graph embedding methods learn embeddings of entities and relations in a low dimensional space which can be used for various downstream machine learning tasks such as link prediction and entity matching. Various graph convolutional network methods have been proposed which use different types of information to learn the features of entities and relations. However, these methods assign the same weight (importance) to the neighbors when aggregating the information, ignoring the role of different relations with the neighboring entities. To this end, we propose a relation-aware graph attention model that leverages relation information to compute different weights to the neighboring nodes for learning embeddings of entities and relations. We evaluate our proposed approach on link prediction and entity matching tasks. Our experimental results on link prediction on three datasets (one proprietary and two public) and results on unsupervised entity matching on one proprietary dataset demonstrate the effectiveness of the relation-aware attention.",0
"Here is my attempt: The use of graph convolutional networks (GCN) has become increasingly popular as a method to analyze complex data sets by leveraging their underlying structure. In recent years, knowledge graphs have emerged as a powerful tool for organizing and storing large amounts of information from diverse sources. While GCNs have been applied successfully to many types of graph data, they can struggle when working with multi-relational data like that found in knowledge graphs due to their limited ability to handle different relational patterns. This paper presents an approach called ""Knowledge Graph Embedding using Graph Convolutional Networks with Relation-aware Attention"" which addresses these limitations by introducing relation-aware attention mechanisms into GCNs. By doing so, we enable our model to selectively focus on specific relationships within the graph while aggregating information from neighboring nodes. We evaluate our approach through extensive experiments across several benchmark datasets commonly used for evaluating methods designed to embed knowledge graphs such as FB15k, WN18RR and YAGO2. Our results demonstrate that incorporating attention significantly enhances node representations generated via GCNs, resulting in improved performance compared to state-of-the-art baselines. Our research contributes towards advancing the field of knowledge graph embedding and demonstrates the effectiveness of combining traditional techniques in computational linguistics with deep learning models.",1
"This paper describes an end-to-end solution for the relationship prediction task in heterogeneous, multi-relational graphs. We particularly address two building blocks in the pipeline, namely heterogeneous graph representation learning and negative sampling. Existing message passing-based graph neural networks use edges either for graph traversal and/or selection of message encoding functions. Ignoring the edge semantics could have severe repercussions on the quality of embeddings, especially when dealing with two nodes having multiple relations. Furthermore, the expressivity of the learned representation depends on the quality of negative samples used during training. Although existing hard negative sampling techniques can identify challenging negative relationships for optimization, new techniques are required to control false negatives during training as false negatives could corrupt the learning process. To address these issues, first, we propose RelGNN -- a message passing-based heterogeneous graph attention model. In particular, RelGNN generates the states of different relations and leverages them along with the node states to weigh the messages. RelGNN also adopts a self-attention mechanism to balance the importance of attribute features and topological features for generating the final entity embeddings. Second, we introduce a parameter-free negative sampling technique -- adaptive self-adversarial (ASA) negative sampling. ASA reduces the false-negative rate by leveraging positive relationships to effectively guide the identification of true negative samples. Our experimental evaluation demonstrates that RelGNN optimized by ASA for relationship prediction improves state-of-the-art performance across established benchmarks as well as on a real industrial dataset.",0
"Abstract:  In recent years, graph neural networks (GNNs) have shown great success in handling relational data such as social networks, knowledge graphs, and biological molecules. However, GNNs often suffer from performance degradation due to oversmoothing, where the embedding becomes uniform across nodes. To address this issue, we propose a relation-aware graph attention model with adaptive self-adversarial training (RaAT). Our RaAT model incorporates inter-node relationship awareness into graph attention by explicitly considering both intra-cluster relations and cross-cluster relations. We develop an adaptive self-adversarial training framework that adjusts perturbations based on node embeddings and applies them during training to enhance robustness. Experimental results show that our RaAT model outperforms state-of-the-art baselines on four benchmark datasets for node classification and link prediction tasks.",1
"Real-time image captioning, along with adequate precision, is the main challenge of this research field. The present work, Multiple Transformers for Self-Attention Mechanism (MTSM), utilizes multiple transformers to address these problems. The proposed algorithm, MTSM, acquires region proposals using a transformer detector (DETR). Consequently, MTSM achieves the self-attention mechanism by transferring these region proposals and their visual and geometrical features through another transformer and learns the objects' local and global interconnections. The qualitative and quantitative results of the proposed algorithm, MTSM, are shown on the MSCOCO dataset.",0
"Recent advances in computer vision have led to significant improvements in image caption generation through deep learning models like transformer networks. However, most existing approaches still struggle with accurately capturing relationships between visual concepts and textual descriptions. In this paper, we propose using multiple transformer networks for self-attention mechanisms in order to address this challenge. Our approach combines attention mechanisms from both ViT (Vision Transformer) and LSTM (Long Short Term Memory) models, enabling better understanding of complex visual features. Furthermore, our method employs pretraining on large datasets and fine-tuning for improved accuracy. Experimental results show that our model outperforms several state-of-the-art methods across various evaluation metrics, demonstrating its effectiveness in generating accurate and descriptive image captions. Overall, our work represents a step forward in improving artificial intelligence systems' ability to generate meaningful natural language descriptions of images.",1
"Joint damage in Rheumatoid Arthritis (RA) is assessed by manually inspecting and grading radiographs of hands and feet. This is a tedious task which requires trained experts whose subjective assessment leads to low inter-rater agreement. An algorithm which can automatically predict the joint level damage in hands and feet can help optimize this process, which will eventually aid the doctors in better patient care and research. In this paper, we propose a two-staged approach which amalgamates object detection and convolution neural networks with attention which can efficiently and accurately predict the overall and joint level narrowing and erosion from patients radiographs. This approach has been evaluated on hands and feet radiographs of patients suffering from RA and has achieved a weighted root mean squared error (RMSE) of 1.358 and 1.404 in predicting joint level narrowing and erosion Sharp van der Heijde (SvH) scores which is 31% and 19% improvement with respect to the baseline SvH scores, respectively. The proposed approach achieved a weighted absolute error of 1.456 in predicting the overall damage in hands and feet radiographs for the patients which is a 79% improvement as compared to the baseline. Our method also provides an inherent capability to provide explanations for model predictions using attention weights, which is essential given the black box nature of deep learning models. The proposed approach was developed during the RA2 Dream Challenge hosted by Dream Challenges and secured 4th and 8th position in predicting overall and joint level narrowing and erosion SvH scores from radiographs.",0
"Articular cartilage damage can lead to joint degeneration, causing chronic pain and disability. Accurate assessment of cartilage status is essential for early diagnosis and effective management of arthritis. Current clinical tools rely on subjective visual inspection by expert radiologists, which results in high intra-observer variability and low sensitivity. This study proposes a novel deep learning model called ""DeepRA"" that uses convolutional neural networks (CNN) with attention mechanisms to predict articular cartilage damage from knee radiographs. By leveraging large-scale datasets of annotated radiographs, DeepRA was trained to identify and localize lesions in the tibiofemoral compartments. Experimental results show promising accuracy for binary (74%) and overall Osteoarthritis Knee Score (OKS) prediction compared to existing methods. In conclusion, DeepRA presents significant potential as a reliable decision support tool for assisted diagnoses, ultimately facilitating early interventions and improved patient outcomes. Further work remains to address generalizability across imaging centers and scanner types. Future research could explore the integration of clinical data and domain knowledge to enhance performance.",1
"It is a long-term goal to transfer biological processing principles as well as the power of human recognition into machine vision and engineering systems. One of such principles is visual attention, a smart human concept which focuses processing on a part of a scene. In this contribution, we utilize attention to improve the automatic detection of defect patterns for wafers within the domain of semiconductor manufacturing. Previous works in the domain have often utilized classical machine learning approaches such as KNNs, SVMs, or MLPs, while a few have already used modern approaches like deep neural networks (DNNs). However, one problem in the domain is that the faults are often very small and have to be detected within a larger size of the chip or even the wafer. Therefore, small structures in the size of pixels have to be detected in a vast amount of image data. One interesting principle of the human brain for solving this problem is visual attention. Hence, we employ here a biologically plausible model of visual attention for automatic visual inspection. We propose a hybrid system of visual attention and a deep neural network. As demonstrated, our system achieves among other decisive advantages an improvement in accuracy from 81% to 92%, and an increase in accuracy for detecting faults from 67% to 88%. Hence, the error rates are reduced from 19% to 8%, and notably from 33% to 12% for detecting a fault in a chip. These results show that attention can greatly improve the performance of visual inspection systems. Furthermore, we conduct a broad evaluation, identifying specific advantages of the biological attention model in this application, and benchmarks standard deep learning approaches as an alternative with and without attention.   This work is an extended arXiv version of the original conference article published in ""IECON 2020"", which has been extended regarding visual attention.",0
"In recent years there has been increasing interest in developing automated visual fault detection systems that can identify defects on manufactured parts quickly, accurately, and objectively. Existing approaches rely heavily on deep learning techniques to learn discriminative features from large datasets of labeled examples, but these methods often struggle with limited annotated data or domain shift, which makes their performance unstable and error-prone. This paper proposes a novel approach to improving automated visual fault detection by combining a biologically plausible model of visual attention with deep learning. Our method uses top-down visual attention to select relevant regions of interest (ROIs) based on high-level task goals, such as detecting cracks in welded joints. These ROIs are then fed into a convolutional neural network (CNN) trained using transfer learning and fine-tuning with small amounts of target dataset annotations. We evaluate our method on two challenging benchmark datasets and demonstrate significant improvements over state-of-the-art baselines under different settings. By incorporating principles from human vision and leveraging limited supervision, our work shows promising results towards robust and adaptive automated visual inspection in real-world scenarios.",1
"Recent GAN-based (Generative adversarial networks) inpainting methods show remarkable improvements and generate plausible images using multi-stage networks or Contextual Attention Modules (CAM). However, these techniques increase the model complexity limiting their application in low-resource environments. Furthermore, they fail in generating high-resolution images with realistic texture details due to the GAN stability problem. Motivated by these observations, we propose a multi-GAN architecture improving both the performance and rendering efficiency. Our training schema optimizes the parameters of four progressive efficient generators and discriminators in an end-to-end manner. Filling in low-resolution images is less challenging for GANs due to the small dimensional space. Meanwhile, it guides higher resolution generators to learn the global structure consistency of the image. To constrain the inpainting task and ensure fine-grained textures, we adopt an LBP-based loss function to minimize the difference between the generated and the ground truth textures. We conduct our experiments on Places2 and CelebHQ datasets. Qualitative and quantitative results show that the proposed method not only performs favorably against state-of-the-art algorithms but also speeds up the inference time.",0
"Image inpainting involves filling in missing regions of an image with realistic details that match the surrounding context. In recent years, generative adversarial networks (GANs) have become popular tools for image inpainting due to their ability to generate high-quality synthetic data. However, existing GAN-based approaches often suffer from limitations such as lack of textural fidelity, difficulty in handling large missing regions, and computational inefficiency.  This paper proposes a novel framework called efficient texture-aware multi-GAN (MixupGAN) for effective image inpainting. Our approach addresses two key challenges: texture synthesis and shape completion. We achieve this by training multiple discriminators and incorporating texture loss functions to capture fine details. Furthermore, we introduce a mix-and-match strategy between different generator architectures tailored to handle large missing regions efficiently.  Our experimental results demonstrate significant improvements over state-of-the-art methods across various metrics including visual quality, structural similarity index measure (SSIM), mean absolute error (MAE), and peak signal-to-noise ratio (PSNR). For example, our model outperforms previous methods by up to 24% on SSIM scores while requiring less time and memory during inference. Moreover, we provide a comprehensive analysis of ablation studies and qualitative evaluations to showcase the effectiveness of each component in our proposed framework.  In summary, MixupGAN offers a flexible yet powerful solution for image inpainting by balancing texture accuracy and computational efficiency. Our method has promising applications in areas such as image restoration, virtual reality, and augmented reality where seamless content generation is crucial.",1
"Adversarial examples are inputs intentionally perturbed with the aim of forcing a machine learning model to produce a wrong prediction, while the changes are not easily detectable by a human. Although this topic has been intensively studied in the image domain, classification tasks in the audio domain have received less attention. In this paper we address the existence of universal perturbations for speech command classification. We provide evidence that universal attacks can be generated for speech command classification tasks, which are able to generalize across different models to a significant extent. Additionally, a novel analytical framework is proposed for the evaluation of universal perturbations under different levels of universality, demonstrating that the feasibility of generating effective perturbations decreases as the universality level increases. Finally, we propose a more detailed and rigorous framework to measure the amount of distortion introduced by the perturbations, demonstrating that the methods employed by convention are not realistic in audio-based problems.",0
"This paper explores the vulnerability of speech recognition systems to universal adversarial examples (UAEs), which are crafted inputs designed to fool models into making incorrect predictions across multiple input domains. We first present a thorough analysis on UAEs generated against state-of-the-art speech command classifiers, demonstrating that they can effectively transfer attacks across different audio distributions and architectures. Furthermore, we investigate several defense mechanisms to mitigate the impact of these UAEs but find them largely ineffective under more general attack scenarios. Our results highlight the importance of understanding robustness against such attacks, particularly given the increasing adoption of voice control technologies in safety-critical applications. By providing insightful evaluations of UAEs across different settings, our work contributes towards achieving more secure machine learning models in real-world deployments.",1
"Advances in image-based dietary assessment methods have allowed nutrition professionals and researchers to improve the accuracy of dietary assessment, where images of food consumed are captured using smartphones or wearable devices. These images are then analyzed using computer vision methods to estimate energy and nutrition content of the foods. Food image segmentation, which determines the regions in an image where foods are located, plays an important role in this process. Current methods are data dependent, thus cannot generalize well for different food types. To address this problem, we propose a class-agnostic food image segmentation method. Our method uses a pair of eating scene images, one before start eating and one after eating is completed. Using information from both the before and after eating images, we can segment food images by finding the salient missing objects without any prior information about the food class. We model a paradigm of top down saliency which guides the attention of the human visual system (HVS) based on a task to find the salient missing objects in a pair of images. Our method is validated on food images collected from a dietary study which showed promising results.",0
"Title: Saliency-Aware Class-Agnostic Food Image Segmentation  Abstract: In food image segmentation, accurately separating objects from backgrounds is crucial for downstream applications such as recipe suggestion systems and autonomous dining assistants. However, existing methods have often relied on prior knowledge of object classes, which can be limiting in real-world scenarios where we may encounter unseen objects. To address these challenges, we propose a saliency-aware class-agnostic approach that segments food images without requiring explicit class labels. Our method leverages recent advances in deep learning architectures and combines them with novel techniques that explicitly model inter-region dependencies. We show through extensive experiments that our method outperforms state-of-the-art approaches across a variety of metrics while requiring fewer training resources. Furthermore, our framework provides robustness against changes in object appearance caused by factors such as lighting conditions, viewpoints, and occlusions. These results demonstrate the effectiveness of our saliency-aware approach for high-quality segmentations in cluttered, complex scenes such as those encountered in online food images, paving the way towards more advanced vision tasks based on accurate region understanding.",1
"Weakly supervised learning has drawn considerable attention recently to reduce the expensive time and labor consumption of labeling massive data. In this paper, we investigate a novel weakly supervised learning problem of learning from similarity-confidence (Sconf) data, where we aim to learn an effective binary classifier from only unlabeled data pairs equipped with confidence that illustrates their degree of similarity (two examples are similar if they belong to the same class). To solve this problem, we propose an unbiased estimator of the classification risk that can be calculated from only Sconf data and show that the estimation error bound achieves the optimal convergence rate. To alleviate potential overfitting when flexible models are used, we further employ a risk correction scheme on the proposed risk estimator. Experimental results demonstrate the effectiveness of the proposed methods.",0
"In recent years, machine learning has become increasingly important for understanding complex phenomena such as human behavior and decision making. One common approach to developing machine learning models involves using data that capture both similarity (e.g., how similar two objects are) and confidence (i.e., certainty) dimensions. This allows researchers to model uncertainty and ambiguity in their predictions, which can improve the accuracy and generalizability of their results.  However, working with these types of datasets presents unique challenges due to their special structure and properties. For example, traditional approaches may struggle to properly account for the correlations between the different elements within each data point. To overcome these limitations, we propose a novel method that uses probabilistic latent variable models, specifically Latent Dirichlet Allocation (LDA), to learn from similarity-confidence data. LDA offers several advantages over other methods because it can simultaneously identify hidden patterns and relationships in large and high-dimensional datasets while providing interpretable results.  Our experimental evaluations demonstrate the effectiveness of our proposed method by comparing its performance against state-of-the-art techniques on simulated and real datasets from diverse domains, including finance, text classification, and psychology. Results show that our method outperforms existing approaches by achieving higher prediction accuracies and better interpretability of the learned models. Overall, this work advances our understanding of how to effectively leverage similarity-confidence data to develop more accurate and reliable machine learning models across multiple disciplines.",1
"Face Presentation Attack Detection (PAD) has drawn increasing attentions to secure the face recognition systems that are widely used in many applications. Conventional face anti-spoofing methods have been proposed, assuming that testing is from the same domain used for training, and so cannot generalize well on unseen attack scenarios. The trained models tend to overfit to the acquisition sensors and attack types available in the training data. In light of this, we propose an end-to-end learning framework based on Domain Adaptation (DA) to improve PAD generalization capability. Labeled source-domain samples are used to train the feature extractor and classifier via cross-entropy loss, while unsupervised data from the target domain are utilized in adversarial DA approach causing the model to learn domain-invariant features. Using DA alone in face PAD fails to adapt well to target domain that is acquired in different conditions with different devices and attack types than the source domain. And so, in order to keep the intrinsic properties of the target domain, deep clustering of target samples is performed. Training and deep clustering are performed end-to-end, and experiments performed on several public benchmark datasets validate that our proposed Deep Clustering guided Unsupervised Domain Adaptation (DCDA) can learn more generalized information compared with the state-of-the-art classification error on the target domain.",0
"In recent years, there has been growing concern over the ability of face presentation attacks (FPAs), such as printed photos or digital images on smartphones, to fool automatic facial recognition systems (FR). These FR systems rely heavily on deep learning algorithms trained on massive amounts of labeled data from one domain to perform well in their respective evaluation datasets. However, these models tend to suffer significant performance drop when tested under different environmental conditions that differ from the training setups (e.g., lighting, pose, makeup). To address this issue, adversarial unsupervised domain adaptation (UDA) techniques have emerged as potential solutions by transferring knowledge learned from source domains to target domains without access to annotations in the latter. In this work, we present a novel UDA approach guided by clustering for FPA detection that leverages deep features obtained via unsupervised feature learning methods. Our method clusters both genuine and spoofed samples into shared latent subspaces across multiple datasets and adapts the model's weights using adversarial losses imposed by the generated synthetic examples. We comprehensively evaluate our approach on several benchmark datasets against state-of-the-art methods and demonstrate consistent improvements in detecting FPAs under challenging scenarios. Additionally, we showcase our findings through a detailed ablation study and visualization analyses to provide insights into the behavior of our proposed system. Overall, our results highlight the effectiveness of integrating deep clustering guidance with adversarial UDA for enhancing robustness of face presentation attack detection frameworks against various sources of variability",1
"Stochastic gradient Langevin dynamics (SGLD) has gained the attention of optimization researchers due to its global optimization properties. This paper proves an improved convergence property to local minimizers of nonconvex objective functions using SGLD accelerated by variance reductions. Moreover, we prove an ergodicity property of the SGLD scheme, which gives insights on its potential to find global minimizers of nonconvex objectives.",0
"This is a research study that proposes an improved optimization method called Stochastic Gradient Langevin Dynamics (SGLD) which incorporates variance reduction techniques to enhance sampling efficiency and accuracy. By combining SGLD and covariance matrix scaling methods such as MinRes, L-BFGS, and RAS, we aim to reduce noise and bias in samples generated by SGLD while improving its robustness and scalability. Our proposed approach allows us to effectively solve highdimensional problems in machine learning applications such as deep generative models. We evaluate our method on several benchmark datasets and show that it outperforms existing SGLD variants and other popular samplers in terms of speed and quality of sampled distributions. Finally, we discuss potential future directions for enhancing our method through adaptive scaling approaches, preconditioning, and Bayesian uncertainty quantification. Overall, our work contributes to the development of efficient and accurate Markov Chain Monte Carlo algorithms suitable for modern computing architectures and big data sets common in scientific simulations.",1
"In this work, we present a hybrid CTC/Attention model based on a ResNet-18 and Convolution-augmented transformer (Conformer), that can be trained in an end-to-end manner. In particular, the audio and visual encoders learn to extract features directly from raw pixels and audio waveforms, respectively, which are then fed to conformers and then fusion takes place via a Multi-Layer Perceptron (MLP). The model learns to recognise characters using a combination of CTC and an attention mechanism. We show that end-to-end training, instead of using pre-computed visual features which is common in the literature, the use of a conformer, instead of a recurrent network, and the use of a transformer-based language model, significantly improve the performance of our model. We present results on the largest publicly available datasets for sentence-level speech recognition, Lip Reading Sentences 2 (LRS2) and Lip Reading Sentences 3 (LRS3), respectively. The results show that our proposed models raise the state-of-the-art performance by a large margin in audio-only, visual-only, and audio-visual experiments.",0
"This paper presents a novel approach to end-to-end audio-visual speech recognition that utilizes conformer networks, which have recently been shown to achieve state-of-the-art performance on several tasks. Our method combines both visual and auditory features using an attention mechanism, allowing the model to focus on the most relevant information from each modality. We demonstrate the effectiveness of our approach through experiments on two challenging datasets, achieving improved results compared to previous methods. Additionally, we investigate different designs for conformer networks and find that incorporating dilated causal convolutions can further improve performance. Overall, our work shows that end-to-end audio-visual speech recognition is feasible and can outperform traditional approaches based on separate audio and video models. By integrating multiple modalities into one system, our method has potential applications in real-world scenarios such as sign language understanding, multimodal machine translation, and human-machine interaction.",1
"Current state-of-the-art self-supervised learning methods for graph neural networks (GNNs) are based on contrastive learning. As such, they heavily depend on the construction of augmentations and negative examples. For example, on the standard PPI benchmark, increasing the number of negative pairs improves performance, thereby requiring computation and memory cost quadratic in the number of nodes to achieve peak performance. Inspired by BYOL, a recently introduced method for self-supervised learning that does not require negative pairs, we present Bootstrapped Graph Latents, BGRL, a self-supervised graph representation method that gets rid of this potentially quadratic bottleneck. BGRL outperforms or matches the previous unsupervised state-of-the-art results on several established benchmark datasets. Moreover, it enables the effective usage of graph attentional (GAT) encoders, allowing us to further improve the state of the art. In particular on the PPI dataset, using GAT as an encoder we achieve state-of-the-art 70.49% Micro-F1, using the linear evaluation protocol. On all other datasets under consideration, our model is competitive with the equivalent supervised GNN results, often exceeding them.",0
"This should serve as an introduction to anyone new to graph representation learning that explains some basic concepts such as graphs, machine learning, deep learning, neural networks, backpropagation, etc. At the end, provide contact details if you have published your research in academic journals so interested readers can reach out to learn more about them. Some key terms related to Graph Neural Networks (GNNs) may include node embedding, feature propagation, message passing, edge convolution, attention mechanisms, subgraph sampling.",1
"Weather nowcasting consists of predicting meteorological components in the short term at high spatial resolutions. Due to its influence in many human activities, accurate nowcasting has recently gained plenty of attention. In this paper, we treat the nowcasting problem as an image-to-image translation problem using satellite imagery. We introduce Broad-UNet, a novel architecture based on the core UNet model, to efficiently address this problem. In particular, the proposed Broad-UNet is equipped with asymmetric parallel convolutions as well as Atrous Spatial Pyramid Pooling (ASPP) module. In this way, The the Broad-UNet model learns more complex patterns by combining multi-scale features while using fewer parameters than the core UNet model. The proposed model is applied on two different nowcasting tasks, i.e. precipitation maps and cloud cover nowcasting. The obtained numerical results show that the introduced Broad-UNet model performs more accurate predictions compared to the other examined architectures.",0
"This paper describes a deep learning architecture called Broad-UNet that was developed specifically for use in weather forecasting applications known as ""nowcasts."" By leveraging multi-scale features extracted from meteorological satellite imagery data and incorporating novel attention mechanisms, Broad-Unet achieves state-of-the-art accuracy on various nowcasting tasks such as rain detection and snow line estimation. In addition, we provide detailed ablation studies and visualizations to validate our approach. Our findings indicate that Broad-Unets have strong potential for improving real-time decision making based on satellite observations in atmospheric science.",1
"Earth observation offers new insight into anthropogenic changes to nature, and how these changes are effecting (and are effected by) the built environment and the real economy. With the global availability of medium-resolution (10-30m) synthetic aperture radar (SAR) Sentinel-1 and multispectral Sentinel-2 imagery, machine learning can be employed to offer these insights at scale, unbiased to the reporting of companies and countries. In this paper, I introduce DeepSentinel, a data pipeline and experimentation framework for producing general-purpose semantic embeddings of paired Sentinel-1 and Sentinel-2 imagery. I document the development of an extensible corpus of labelled and unlabelled imagery for the purposes of sensor fusion research. With this new dataset I develop a set of experiments applying popular self-supervision methods and encoder architectures to a land cover classification problem. Tile2vec spatial encoding with a self-attention enabled ResNet model outperforms deeper ResNet variants as well as pretraining with variational autoencoding and contrastive loss. All supporting and derived data and code are made publicly available.",0
"This paper presents two contributions towards enabling effective development, testing, and validation of Earth observation (EO) applications that integrate multiple data sources and sensors. Firstly, we introduce the DeepSentinel corpus â€“ a large collection of multi-temporal Sentinel-1 and Sentinel-2 images annotated with meaningful labels that describe various geophysical features observable from EO data. This extensive resource addresses several challenges currently faced by researchers developing machine learning algorithms for remote sensing tasks, such as limited accessibility to suitable datasets. Secondly, we propose a generic approach called ""sensor fusion semantics"" to jointly embed heterogeneous feature sets extracted from different sensors into continuous vector spaces based on self-supervised graph convolution networks. We demonstrate how this unified embedding can enable improved performance over baseline methods on selected interoperability use cases across Sentinel-1/2 and Landsat-8 systems, thus offering potential benefits in terms of more accurate land cover classification and time series analysis. By combining these contributions within the DeepSentinel framework, we aim at fostering innovation while ensuring more efficient application development through streamlined validation against relevant benchmarks. In conclusion, our work establishes a valuable foundation for future advancements in the field of multispectral EO data processing and utilization.",1
"Physiological computing uses human physiological data as system inputs in real time. It includes, or significantly overlaps with, brain-computer interfaces, affective computing, adaptive automation, health informatics, and physiological signal based biometrics. Physiological computing increases the communication bandwidth from the user to the computer, but is also subject to various types of adversarial attacks, in which the attacker deliberately manipulates the training and/or test examples to hijack the machine learning algorithm output, leading to possibly user confusion, frustration, injury, or even death. However, the vulnerability of physiological computing systems has not been paid enough attention to, and there does not exist a comprehensive review on adversarial attacks to it. This paper fills this gap, by providing a systematic review on the main research areas of physiological computing, different types of adversarial attacks and their applications to physiological computing, and the corresponding defense strategies. We hope this review will attract more research interests on the vulnerability of physiological computing systems, and more importantly, defense strategies to make them more secure.",0
"This paper performs a systematic review of recent advances in physiological computing adversarial attacks and defenses. After performing keyword searches through academic databases such as PubMed Central, we identified 24 publications that were deemed relevant based on their focus on physiological computing attacks/defenses, rather than other types of biomedical engineering applications. We then extracted data from these papers regarding the nature of the attacks, the techniques used by attackers to evade detection, and the methods used by defense systems to thwart malicious actors. Our results demonstrate a range of vulnerabilities in current physiological computing systems and highlight promising directions for future research aimed at improving security in this rapidly evolving field. We hope that our work can serve as a resource for practitioners and policymakers interested in ensuring the safe and effective use of physiological computing technology.",1
"Federated Learning (FL) creates an ecosystem for multiple agents to collaborate on building models with data privacy consideration. The method for contribution measurement of each agent in the FL system is critical for fair credits allocation but few are proposed. In this paper, we develop a real-time contribution measurement method FedCM that is simple but powerful. The method defines the impact of each agent, comprehensively considers the current round and the previous round to obtain the contribution rate of each agent with attention aggregation. Moreover, FedCM updates contribution every round, which enable it to perform in real-time. Real-time is not considered by the existing approaches, but it is critical for FL systems to allocate computing power, communication resources, etc. Compared to the state-of-the-art method, the experimental results show that FedCM is more sensitive to data quantity and data quality under the premise of real-time. Furthermore, we developed federated learning open-source software based on FedCM. The software has been applied to identify COVID-19 based on medical images.",0
"In recent years, there has been significant interest in developing methods that can accurately measure each participantâ€™s contribution to federated learning (FL) models. Such contributions reflect how much effort is put into model training by individual devices such as smartphones and servers. Accurate contribution measurement is essential since it enables fairness in allocating rewards and penalties among participants in FL systems. This study proposes FedCM, a real-time method to calculate individual contributions at different levels during the entire learning process without burdening users or increasing communication overheads. Compared to existing approaches, FedCM offers several advantages, including efficiency, accuracy, robustness against malicious attacks, user privacy preservation, and support for multiple FL settings, particularly hybrid architectures combining centralized server and decentralized peer-to-peer components. By incorporating both first-order and second-order information while adaptively calibrating individual weights online, FedCM effectively balances bias reduction and variance minimization. This innovative solution fosters trustworthy participation incentives in emerging large-scale distributed machine learning applications.",1
"In neuroscience, the functional magnetic resonance imaging (fMRI) is a vital tool to non-invasively access brain activity. Using fMRI, the functional connectivity (FC) between brain regions can be inferred, which has contributed to a number of findings of the fundamental properties of the brain. As an important clinical application of FC, clustering of subjects based on FC recently draws much attention, which can potentially reveal important heterogeneity in subjects such as subtypes of psychiatric disorders. In particular, a multiple-view clustering method is a powerful analytical tool, which identifies clustering patterns of subjects depending on their FC in specific brain areas. However, when one applies an existing multiple-view clustering method to fMRI data, there is a need to simplify the data structure, independently dealing with elements in a FC matrix, i.e., vectorizing a correlation matrix. Such a simplification may distort the clustering results. To overcome this problem, we propose a novel multiple-view clustering method based on Wishart mixture models, which preserves the correlation matrix structure without vectorization. The uniqueness of this method is that the multiple-view clustering of subjects is based on particular networks of nodes (or regions of interest, ROIs), optimized in a data-driven manner. Hence, it can identify multiple underlying pairs of associations between a subject cluster solution and a ROI sub-network. The key assumption of the method is independence among sub-networks, which is effectively addressed by whitening correlation matrices. We applied the proposed method to synthetic and fMRI data, demonstrating the usefulness and power of the proposed method.",0
"This paper presents a novel approach for analyzing fMRI data by applying multiple-view clustering techniques to identify both subject clusters and sub-networks within the human brain. We propose to use linear combinations of views as features that preserve subjectwise variability and encode intersubject similarity measures. This method enables us to address scalability issues when working with highdimensional datasets, while allowing for the identification of meaningful clusters from different imaging modalities. Our results show that these methods can achieve better separation of individual subjects compared to traditional approaches relying on feature vector normalizations, thus providing more robust measures of subject variability. Furthermore, our analysis reveals that we can capture complex and nonlinear patterns of cortical interactions underlying human cognition, opening up exciting new possibilities for understanding healthy brain function and exploring disease mechanisms. Ultimately, this work contributes to advancing the state-of-the-art in brain decoding and personalized medicine research. Keywords: multi-modal feature learning, brain sub-networks, subject clustering, matrix factorization, deep neural networks.",1
"Often, what is termed algorithmic bias in machine learning will be due to historic bias in the training data. But sometimes the bias may be introduced (or at least exacerbated) by the algorithm itself. The ways in which algorithms can actually accentuate bias has not received a lot of attention with researchers focusing directly on methods to eliminate bias - no matter the source. In this paper we report on initial research to understand the factors that contribute to bias in classification algorithms. We believe this is important because underestimation bias is inextricably tied to regularization, i.e. measures to address overfitting can accentuate bias.",0
"Underestimation bias occurs when the predictive power of a machine learning model is overlooked or undervalued during training, leading to suboptimal performance on unseen data. This phenomenon has been observed across different domains and can result in decreased accuracy, precision, recall, and F1 score. In our study, we explore two common causes of underestimation bias: model simplicity and regularization. We use both simulated and real-world datasets to demonstrate how these factors contribute to underfitting and reduced generalization ability. Our results show that models with high complexity but low capacity tend to suffer from severe underestimation bias due to their propensity to memorize patterns in the training set rather than capturing underlying relationships. Regularization techniques such as L2 norms have limited effectiveness in addressing underestimation bias while L1 regularization provides better protection against overfitting at minimal cost to the modelâ€™s fitness. Lastly, early stopping may provide marginal improvements to the overall performance but its utility should be considered carefully depending on the dataset characteristics. Overall, understanding the behavior of underestimation bias in machine learning systems requires careful consideration of several model selection criteria including bias, variance, sensitivity to initial conditions, computational resources, and model interpretability.",1
"Deep reinforcement learning (RL) algorithms are powerful tools for solving visuomotor decision tasks. However, the trained models are often difficult to interpret, because they are represented as end-to-end deep neural networks. In this paper, we shed light on the inner workings of such trained models by analyzing the pixels that they attend to during task execution, and comparing them with the pixels attended to by humans executing the same tasks. To this end, we investigate the following two questions that, to the best of our knowledge, have not been previously studied. 1) How similar are the visual features learned by RL agents and humans when performing the same task? and, 2) How do similarities and differences in these learned features explain RL agents' performance on these tasks? Specifically, we compare the saliency maps of RL agents against visual attention models of human experts when learning to play Atari games. Further, we analyze how hyperparameters of the deep RL algorithm affect the learned features and saliency maps of the trained agents. The insights provided by our results have the potential to inform novel algorithms for the purpose of closing the performance gap between human experts and deep RL agents.",0
"Deep reinforcement learning (DRL) has emerged as a powerful tool for solving sequential decision making problems across diverse application domains. However, while DRL algorithms have been applied successfully in many areas, their performance remains subject to debate due to differences between machine and human attention processes. In particular, humans rely heavily on visual attention mechanisms to selectively process relevant sensory input, whereas current DRL methods lack a similar mechanism to guide exploration. This creates challenges that can limit the effectiveness of these models in complex environments with limited visibility. To address this gap, we propose using deep neural networks that learn to predict human fixations to improve the efficiency and robustness of DRL policies. We evaluate our approach through comprehensive experiments across multiple tasks, demonstrating significant improvements compared to state-of-the-art RL methods without explicit attention modelling. Our findings highlight the potential benefits of incorporating human-like attentional behaviour into DRL models and provide new insights for future research at the intersection of cognitive science and artificial intelligence.",1
"Reinforcement learning is about learning agent models that make the best sequential decisions in unknown environments. In an unknown environment, the agent needs to explore the environment while exploiting the collected information, which usually forms a sophisticated problem to solve. Derivative-free optimization, meanwhile, is capable of solving sophisticated problems. It commonly uses a sampling-and-updating framework to iteratively improve the solution, where exploration and exploitation are also needed to be well balanced. Therefore, derivative-free optimization deals with a similar core issue as reinforcement learning, and has been introduced in reinforcement learning approaches, under the names of learning classifier systems and neuroevolution/evolutionary reinforcement learning. Although such methods have been developed for decades, recently, derivative-free reinforcement learning exhibits attracting increasing attention. However, recent survey on this topic is still lacking. In this article, we summarize methods of derivative-free reinforcement learning to date, and organize the methods in aspects including parameter updating, model selection, exploration, and parallel/distributed methods. Moreover, we discuss some current limitations and possible future directions, hoping that this article could bring more attentions to this topic and serve as a catalyst for developing novel and efficient approaches.",0
"This review provides an overview of derivative-free reinforcement learning (RL) algorithms. In standard RL, the agent updates its policy by taking gradients of expected returns with respect to parameters. However, some applications require smooth policies, which precludes gradient estimates. Moreover, model misspecification can cause these methods to diverge, even if a perfect model were used. These challenges have led researchers to study alternative methods that avoid derivatives. We survey recent progress on DFRL, which includes both model-based and model-free approaches. These methods typically use optimization techniques such as stochastic approximation or simulated annealing. Some have shown promising results across several domains and compared favorably against their derivative counterparts, particularly when exploration and model uncertainty must be considered jointly. Furthermore, DFRL may provide benefits beyond stability and generalization performance, including better interpretability and scalability. Finally, we discuss open questions and future directions for DFRL research.",1
"Attention is a powerful component of modern neural networks across a wide variety of domains. In this paper, we seek to quantify the regularity (i.e. the amount of smoothness) of the attention operation. To accomplish this goal, we propose a new mathematical framework that uses measure theory and integral operators to model attention. We show that this framework is consistent with the usual definition, and that it captures the essential properties of attention. Then we use this framework to prove that, on compact domains, the attention operation is Lipschitz continuous and provide an estimate of its Lipschitz constant. Additionally, by focusing on a specific type of attention, we extend these Lipschitz continuity results to non-compact domains. We also discuss the effects regularity can have on NLP models, and applications to invertible and infinitely-deep networks.",0
"""On the Regularity of Attention"" investigates the concept of attention from both philosophical and psychological perspectives. Attention has been studied extensively within cognitive psychology as a fundamental aspect of human perception, thought, and action. However, recent advances in machine learning have suggested new ways of understanding attention that challenge traditional views in philosophy and psychology. This paper argues that attention can be understood as a regular process that operates according to predictable patterns and principles. By examining the neural mechanisms underlying attention and their relation to conscious awareness, we can gain insight into the nature of human experience and the workings of the mind more generally. Ultimately, the goal of this study is to develop a comprehensive theory of attention that integrates findings across disciplines and provides new insights into the complex processes involved in human behavior.",1
"Transfer learning from ImageNet is the go-to approach when applying deep learning to medical images. The approach is either to fine-tune a pre-trained model or use it as a feature extractor. Most modern architecture contain batch normalisation layers, and fine-tuning a model with such layers requires taking a few precautions as they consist of trainable and non-trainable weights and have two operating modes: training and inference. Attention is primarily given to the non-trainable weights used during inference, as they are the primary source of unexpected behaviour or degradation in performance during transfer learning. It is typically recommended to fine-tune the model with the batch normalisation layers kept in inference mode during both training and inference. In this paper, we pay closer attention instead to the trainable weights of the batch normalisation layers, and we explore their expressive influence in the context of transfer learning. We find that only fine-tuning the trainable weights (scale and centre) of the batch normalisation layers leads to similar performance as to fine-tuning all of the weights, with the added benefit of faster convergence. We demonstrate this on a variety of seven publicly available medical imaging datasets, using four different model architectures.",0
"In this work, we investigate the impact of partial transfusion on the expressiveness of batch normalization (BN) parameters during pre-training for transfer learning tasks. We show that using partially transferred BN layers can significantly improve fine-grained control over feature representation without sacrificing overall performance. Our approach utilizes adaptive batch renormalization, which allows us to modulate the scale and shift parameters based on task specificity. This enables more flexible optimization and faster convergence during fine-tuning, leading to better results across a range of vision benchmarks. Importantly, our method outperforms existing alternatives while maintaining model efficiency. Overall, these findings provide new insights into the role of BN in deep neural networks and open up opportunities for further research into the design of customized architectures for challenging problems in computer vision.",1
"The problem of missing data, usually absent incurated and competition-standard datasets, is an unfortunate reality for most machine learning models used in industry applications. Recent work has focused on understanding the nature and the negative effects of such phenomena, while devising solutions for optimal imputation of the missing data, using both discriminative and generative approaches. We propose a novel mechanism based on multi-head attention which can be applied effortlessly in any model and achieves better downstream performance without the introduction of the full dataset in any part of the modeling pipeline. Our method inductively models patterns of missingness in the input data in order to increase the performance of the downstream task. Finally, after evaluating our method against baselines for a number of datasets, we found performance gains that tend to be larger in scenarios of high missingness.",0
"Title: ""Multihead Attention Imputation Networks""  Abstract: In modern data analysis tasks such as image processing or natural language understanding, missing values can significantly impact the performance of machine learning models. Traditional methods for dealing with missing values involve replacing them with either mean imputations or more advanced statistical models like Gaussian mixture models. However, these approaches often fail to capture complex relationships among multiple features that are present in real world datasets. Our proposed method Multihead Attention Imputation Network (MAIN) addresses this issue by leveraging self attention mechanisms from transformers to attend to different parts of input data while simultaneously considering context provided by other modalities. We evaluate our approach on both synthetic and benchmark datasets showing improvement over baseline imputation techniques. Additionally we showcase the use case on a real-world dataset where the application of our model outperforms current state-of-the art methods by a significant margin. Overall, our work demonstrates the effectiveness of using multi-head attention networks for efficient handling of missing data points.",1
"Bone age assessment is challenging in clinical practice due to the complicated bone age assessment process. Current automatic bone age assessment methods were designed with rare consideration of the diagnostic logistics and thus may yield certain uninterpretable hidden states and outputs. Consequently, doctors can find it hard to cooperate with such models harmoniously because it is difficult to check the correctness of the model predictions. In this work, we propose a new graph-based deep learning framework for bone age assessment with hand radiographs, called Doctor Imitator (DI). The architecture of DI is designed to learn the diagnostic logistics of doctors using the scoring methods (e.g., the Tanner-Whitehouse method) for bone age assessment. Specifically, the convolutions of DI capture the local features of the anatomical regions of interest (ROIs) on hand radiographs and predict the ROI scores by our proposed Anatomy-based Group Convolution, summing up for bone age prediction. Besides, we develop a novel Dual Graph-based Attention module to compute patient-specific attention for ROI features and context attention for ROI scores. As far as we know, DI is the first automatic bone age assessment framework following the scoring methods without fully supervised hand radiographs. Experiments on hand radiographs with only bone age supervision verify that DI can achieve excellent performance with sparse parameters and provide more interpretability.",0
"In order to accurately determine bone age assessment using hand radiographs, our proposed framework utilizes graph convolutional neural networks (graph CNN) combined with dual attention mechanisms for feature extraction from hand radiograph images. By applying graph CNNs on bones segmented by automatic methods, we can enhance local connectivity through nonlinear relationships between skeletons. This approach is based upon a thorough review and understanding of current state-of-the-art techniques in bone age estimation, including those that use machine learning and deep learning models. We evaluated the effectiveness of our framework on 2 large datasets comprised of over 8,768 hand X-ray images and corresponding ages ranging from 4 years old to adult. Our results demonstrate improved performance compared to existing methods across both quantitative and qualitative measures, demonstrating the utility and effectiveness of our proposed approach.",1
"We propose a novel neural network architecture, called autoencoder-constrained graph convolutional network, to solve node classification task on graph domains. As suggested by its name, the core of this model is a convolutional network operating directly on graphs, whose hidden layers are constrained by an autoencoder. Comparing with vanilla graph convolutional networks, the autoencoder step is added to reduce the information loss brought by Laplacian smoothing. We consider applying our model on both homogeneous graphs and heterogeneous graphs. For homogeneous graphs, the autoencoder approximates to the adjacency matrix of the input graph by taking hidden layer representations as encoder and another one-layer graph convolutional network as decoder. For heterogeneous graphs, since there are multiple adjacency matrices corresponding to different types of edges, the autoencoder approximates to the feature matrix of the input graph instead, and changes the encoder to a particularly designed multi-channel pre-processing network with two layers. In both cases, the error occurred in the autoencoder approximation goes to the penalty term in the loss function. In extensive experiments on citation networks and other heterogeneous graphs, we demonstrate that adding autoencoder constraints significantly improves the performance of graph convolutional networks. Further, we notice that our technique can be applied on graph attention network to improve the performance as well. This reveals the wide applicability of the proposed autoencoder technique.",0
"In recent years, graph convolutional networks (GCNs) have emerged as powerful tools for modeling irregularly structured data such as graphs. However, their performance can suffer from overfitting due to the limited expressive power of the linear layers used by GCNs. To address this issue, we propose a novel architecture called AEGCN that integrates autoencoder constraints into the design of GCNs. This allows us to regularize the learned representation, improve generalization ability and reduce the risk of overfitting. By leveraging the power of both GCNs and autoencoders, our approach achieves state-of-the-art results on several benchmark datasets across different domains while requiring fewer parameters compared to previous methods. We demonstrate the effectiveness of our proposed method through extensive experiments, ablation studies, visualizations, and comparisons against baseline models. Our findings suggest that AEGCN provides a promising framework for advancing research at the intersection of graph deep learning and unsupervised feature learning.",1
"Granger causal modeling is an emerging topic that can uncover Granger causal relationship behind multivariate time series data. In many real-world systems, it is common to encounter a large amount of multivariate time series data collected from different individuals with sharing commonalities. However, there are ongoing concerns regarding Granger causality's applicability in such large scale complex scenarios, presenting both challenges and opportunities for Granger causal structure reconstruction. Existing methods usually train a distinct model for each individual, suffering from inefficiency and over-fitting issues. To bridge this gap, we propose an Inductive GRanger cAusal modeling (InGRA) framework for inductive Granger causality learning and common causal structure detection on multivariate time series, which exploits the shared commonalities underlying the different individuals. In particular, we train one global model for individuals with different Granger causal structures through a novel attention mechanism, called prototypical Granger causal attention. The model can detect common causal structures for different individuals and infer Granger causal structures for newly arrived individuals. Extensive experiments, as well as an online A/B test on an E-commercial advertising platform, demonstrate the superior performances of InGRA.",0
"Abstract: This research introduces inductive Granger causality analysis as a tool for modeling multivariate time series data. In contrast to traditional Granger causality testing, which relies on restrictive assumptions such as linearity and stationarity, our approach allows for more flexible models that can capture complex interactions between variables. We demonstrate the effectiveness of our method through simulation studies and apply it to real-world datasets from finance and meteorology. Our results show that inductive Granger causality modeling can provide valuable insights into the underlying relationships among multiple time series variables.",1
"We propose a novel framework for value function factorization in multi-agent deep reinforcement learning (MARL) using graph neural networks (GNNs). In particular, we consider the team of agents as the set of nodes of a complete directed graph, whose edge weights are governed by an attention mechanism. Building upon this underlying graph, we introduce a mixing GNN module, which is responsible for i) factorizing the team state-action value function into individual per-agent observation-action value functions, and ii) explicit credit assignment to each agent in terms of fractions of the global team reward. Our approach, which we call GraphMIX, follows the centralized training and decentralized execution paradigm, enabling the agents to make their decisions independently once training is completed. We show the superiority of GraphMIX as compared to the state-of-the-art on several scenarios in the StarCraft II multi-agent challenge (SMAC) benchmark. We further demonstrate how GraphMIX can be used in conjunction with a recent hierarchical MARL architecture to both improve the agents' performance and enable fine-tuning them on mismatched test scenarios with higher numbers of agents and/or actions.",0
"Graph convolutional value decomposition (GraphCVaDe) is a promising approach that leverages graph neural networks to decompose joint actions into local contributions from individual agents in multiagent reinforcement learning (MARL). However, applying GraphCVaDe to realworld MARL problems can be challenging due to computational constraints and uncertainties associated with partial observability. In this work, we propose novel techniques to address these limitations and expand the scope of GraphCVaDe applications by providing efficient implementations of Graph CVaDe algorithms using deep learning libraries and uncertainty quantification measures. Our results demonstrate that our proposed methods improve the accuracy and efficiency of Graph CVaDe under various levels of observability, making it suitable for larger and more complex MARL scenarios. Overall, our work offers new insights into understanding the mechanisms underlying coordination among autonomous entities operating in uncertain environments, opening up exciting possibilities for future research directions in both artificial intelligence and cognitive science.",1
"This paper develops a novel encoder-decoder deep network architecture which exploits the several contextual frames of 2D+t sequential images in a sliding window centered at current frame to segment 2D vessel masks from the current frame. The architecture is equipped with temporal-spatial feature extraction in encoder stage, feature fusion in skip connection layers and channel attention mechanism in decoder stage. In the encoder stage, a series of 3D convolutional layers are employed to hierarchically extract temporal-spatial features. Skip connection layers subsequently fuse the temporal-spatial feature maps and deliver them to the corresponding decoder stages. To efficiently discriminate vessel features from the complex and noisy backgrounds in the XCA images, the decoder stage effectively utilizes channel attention blocks to refine the intermediate feature maps from skip connection layers for subsequently decoding the refined features in 2D ways to produce the segmented vessel masks. Furthermore, Dice loss function is implemented to train the proposed deep network in order to tackle the class imbalance problem in the XCA data due to the wide distribution of complex background artifacts. Extensive experiments by comparing our method with other state-of-the-art algorithms demonstrate the proposed method's superior performance over other methods in terms of the quantitative metrics and visual validation. The source codes are at https://github.com/Binjie-Qin/SVS-net",0
"In this research paper, we propose a novel approach for sequential vessel segmentation using a deep learning model called Channel Attention Network (CAN). The proposed method addresses the limitations of existing methods by leveraging channel attention mechanisms that adaptively focus on discriminative features from different channels. We design a tailored architecture for medical image segmentation tasks that consists of multiple stages of CAN modules. Each stage progressively refines the initial output until we obtain accurate results. Furthermore, our framework ensures efficient utilization of computational resources owing to the use of mini-batch gradient descent optimization at each stage. We evaluated our method on three publicly available datasets: DRIVE, STARE, and HRFSI, where our system achieved state-of-the art performance compared to other well-established approaches. Our study demonstrates that the proposed framework has significant potential for improving the accuracy and efficiency of vessel segmentation in clinical settings. In conclusion, our work provides new insights into applying deep learning techniques for medical image analysis problems while addressing practical challenges facing todayâ€™s healthcare systems.",1
"Value-based methods of multi-agent reinforcement learning (MARL), especially the value decomposition methods, have been demonstrated on a range of challenging cooperative tasks. However, current methods pay little attention to the interaction between agents, which is essential to teamwork in games or real life. This limits the efficiency of value-based MARL algorithms in the two aspects: collaborative exploration and value function estimation. In this paper, we propose a novel cooperative MARL algorithm named as interactive actor-critic~(IAC), which models the interaction of agents from the perspectives of policy and value function. On the policy side, a multi-agent joint stochastic policy is introduced by adopting a collaborative exploration module, which is trained by maximizing the entropy-regularized expected return. On the value side, we use the shared attention mechanism to estimate the value function of each agent, which takes the impact of the teammates into consideration. At the implementation level, we extend the value decomposition methods to continuous control tasks and evaluate IAC on benchmark tasks including classic control and multi-agent particle environments. Experimental results indicate that our method outperforms the state-of-the-art approaches and achieves better performance in terms of cooperation.",0
"In multi-agent reinforcement learning (MARL), modeling the interaction between agents can lead to more effective collaboration among them. This study focuses on developing algorithms that capture such interactions effectively. We propose a method based on Graph Convolutional Networks (GCN) which takes into account interdependencies between variables affecting agent behavior in multi-agent environments, allowing agents to learn from each otherâ€™s experiences and improve their overall performance. Our approach models the underlying social relationships within the environment through edge weights on graphs and captures how these relationships change over time. Experimental results demonstrate significant improvement using our proposed algorithm compared to conventional centralized training methods without considering agent-to-agent interaction. Overall, our research shows that GCN based algorithms have great potential in addressing issues related to credit assignment and scalability in MARL domains involving complex and uncertain interactions between agents.",1
"The transduction of sequence has been mostly done by recurrent networks, which are computationally demanding and often underestimate uncertainty severely. We propose a computationally efficient attention-based network combined with the Gaussian process regression to generate real-valued sequence, which we call the Attentive-GP. The proposed model not only improves the training efficiency by dispensing recurrence and convolutions but also learns the factorized generative distribution with Bayesian representation. However, the presence of the GP precludes the commonly used mini-batch approach to the training of the attention network. Therefore, we develop a block-wise training algorithm to allow mini-batch training of the network while the GP is trained using full-batch, resulting in a scalable training method. The algorithm has been proved to converge and shows comparable, if not better, quality of the found solution. As the algorithm does not assume any specific network architecture, it can be used with a wide range of hybrid models such as neural networks with kernel machine layers in the scarcity of resources for computation and memory.",0
"In this work we consider the task of generating realistic time-series data that can capture complex dependencies and patterns found in real world systems. We propose the use of attentive Gaussian Processes (AGP), which combine the flexibility of nonparametric models like GPs with the ability to focus attention on specific parts of the input space, as an effective method for this purpose. Our model learns from existing data to generate synthetic time-series that exhibit similar statistical properties while capturing new unseen behaviors that can occur over time. Through extensive evaluation using multiple benchmark datasets, we demonstrate the superior performance of AGP over alternative methods such as ARIMA and naÃ¯ve GP approaches, achieving competitive accuracy in both predicting future values and capturing underlying trends. The effectiveness of our approach makes it particularly well suited for situations where traditional linear models fail to adequately represent complex system dynamics. Overall, our results highlight the promise of attentiveGPs as a powerful tool for probablistic time-series prediction.",1
"Deep learning applied to weather forecasting has started gaining popularity because of the progress achieved by data-driven models. The present paper compares two different deep learning architectures to perform weather prediction on daily data gathered from 18 cities across Europe and spanned over a period of 15 years. We propose the Deep Attention Unistream Multistream (DAUM) networks that investigate different types of input representations (i.e. tensorial unistream vs. multistream ) as well as the incorporation of the attention mechanism. In particular, we show that adding a self-attention block within the models increases the overall forecasting performance. Furthermore, visualization techniques such as occlusion analysis and score maximization are used to give an additional insight on the most important features and cities for predicting a particular target feature of target cities.",0
"This is a machine learning approach that uses deep multi-stations weather forecasts generated by convolutional neural network (CNN) models to provide explanations for forecast errors. We use novel architectures based on residual connections, batch normalization, dropout regularization, LeakyReLUs and gradient clipping to mitigate underfitting and overfitting issues. Experiments show that our model significantly outperforms other state-of-the-art methods while providing interpretable results through visual attention maps and conceptual interpretability analysis. Our contributions are threefold: we propose a novel deep CNN architecture tailored towards spatiotemporal weather data; demonstrate experimentally that our model performs better than current state-of-the art approaches in terms of both accuracy and explainability; and introduce two new techniques to improve interpretability without sacrificing predictive performance. The goal of this work is to develop deep neural models capable of producing accurate predictions as well as explaining their rationale behind those predictions. This problem setting is motivated by applications where accurate predictions are critical but understanding how these predictions were arrived at remains equally important. In order to achieve this goal we design Recurrent Neural Networks augmented by Convolutions (RNCNs). RNCNs combine temporal convolutions with self attention mechanisms which enable them to capture short range dependencies within a given time sequence as well as longer range dependencies between different time steps. To make these models practically applicable we further optimize them using knowledge distillation and late fusion from other strong baselines. Through extensive experiments on several benchmark datasets, we demonstrate significant improvements in prediction accuracy as well as better calibration compared to all competitive baseline models including traditional ones like Linear Regression etc. Further more detailed ablation studies reveal insights into the significance of each component of our proposed model, highlighting the importance of careful selection of appropriate components to build high performing Explainable Models",1
"We address the problem of learning on sets of features, motivated by the need of performing pooling operations in long biological sequences of varying sizes, with long-range dependencies, and possibly few labeled data. To address this challenging task, we introduce a parametrized representation of fixed size, which embeds and then aggregates elements from a given input set according to the optimal transport plan between the set and a trainable reference. Our approach scales to large datasets and allows end-to-end training of the reference, while also providing a simple unsupervised learning mechanism with small computational cost. Our aggregation technique admits two useful interpretations: it may be seen as a mechanism related to attention layers in neural networks, or it may be seen as a scalable surrogate of a classical optimal transport-based kernel. We experimentally demonstrate the effectiveness of our approach on biological sequences, achieving state-of-the-art results for protein fold recognition and detection of chromatin profiles tasks, and, as a proof of concept, we show promising results for processing natural language sequences. We provide an open-source implementation of our embedding that can be used alone or as a module in larger learning models at https://github.com/claying/OTK.",0
"This work proposes a novel embedding method that optimizes transport distances between input features to learn a low dimensional representation that encodes meaningful relationships between samples. By leveraging optimal transport theory, we design a trainable energy function that regularizes feature aggregation while preserving global context. Our approach differs from traditional attention mechanisms by learning a continuous nonlinear mapping directly in input space, without introducing additional parameters or computational overhead. We evaluate our model on several benchmark datasets across different domains, demonstrating improved performance over alternative methods, including state-of-the-art attention models. Overall, our contributions provide a new perspective on how optimal transport can improve feature embeddings, with potential applications in areas such as image generation, computer vision, natural language processing, and machine learning more broadly.",1
"We consider the problem of referring segmentation in images and videos with natural language. Given an input image (or video) and a referring expression, the goal is to segment the entity referred by the expression in the image or video. In this paper, we propose a cross-modal self-attention (CMSA) module to utilize fine details of individual words and the input image or video, which effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the visual input. We further propose a gated multi-level fusion (GMLF) module to selectively integrate self-attentive cross-modal features corresponding to different levels of visual features. This module controls the feature fusion of information flow of features at different levels with high-level and low-level semantic information related to different attentive words. Besides, we introduce cross-frame self-attention (CFSA) module to effectively integrate temporal information in consecutive frames which extends our method in the case of referring segmentation in videos. Experiments on benchmark datasets of four referring image datasets and two actor and action video segmentation datasets consistently demonstrate that our proposed approach outperforms existing state-of-the-art methods.",0
"In computer vision tasks, image segmentation involves partitioning an input image into multiple segments representing distinct objects present within the scene. Video segmentation performs similar task on images in videos. Image and video segmentation methods use object detection as intermediate step, which firstly detects locations of all objects in the scenes and then predicts masks for each detected object individually or groupwise in a postprocessing step, where groundtruth object masks are provided along with the bounding boxes. Recent advancements in these techniques have leveraged deep learning architectures that directly learn from pixelwise semantic predictions given raw pixels, resulting in improved accuracy without requiring explicit detector outputs or external supervision such as COCO/PASCAL annotations. In this work we introduce two novel models for fully convolutional semantic segmentation: Pixel2pixels (PixelCNN) based model, called Pixel2seg and another self attention mechanism over spatial features extracted by ConvNet. Our approach can potentially perform better than previous state-of-the-art methods while using lower capacity backbones like MobileNetV2 due to its strong capacity in terms of feature representation ability across modalities. We show results on challenging VOC Pascal dataset and demonstrate performance superiority against other baselines in most metrics except FLOPS, clearly indicating future potential improvements via increased computational power and model scale. This work could serve as starting point towards unifying segmenters with newer object detectors like YOLO since it uses box supervisions only during training. Leveraging recent advances in fully convolutional networks has enabled significant improvement in high level visual understanding tasks such as image and video se",1
"Assessing advertisements, specifically on the basis of user preferences and ad quality, is crucial to the marketing industry. Although recent studies have attempted to use deep neural networks for this purpose, these studies have not utilized image-related auxiliary attributes, which include embedded text frequently found in ad images. We, therefore, investigated the influence of these attributes on ad image preferences. First, we analyzed large-scale real-world ad log data and, based on our findings, proposed a novel multi-step modality fusion network (M2FN) that determines advertising images likely to appeal to user preferences. Our method utilizes auxiliary attributes through multiple steps in the network, which include conditional batch normalization-based low-level fusion and attention-based high-level fusion. We verified M2FN on the AVA dataset, which is widely used for aesthetic image assessment, and then demonstrated that M2FN can achieve state-of-the-art performance in preference prediction using a real-world ad dataset with rich auxiliary attributes.",0
"This research presents a novel method for image assessment that combines multiple modalities into a single metric for evaluating advertising images. The proposed approach, called ""M2FN,"" uses a multi-step process to fuse different types of features extracted from each image into one unified score. This score can then be used by advertisers and marketers to quickly and accurately evaluate the effectiveness of their ads on consumers. By incorporating both global and local features, as well as color and texture information, the M2FN model outperforms other state-of-the-art methods for predicting human preferences towards advertisements across three datasets with varying domains. Overall, our work provides insights into how multimodal fusion techniques can enhance image evaluation tasks and improve decision making in real-world applications such as online marketing campaigns.",1
"Unsupervised image-to-image translation is used to transform images from a source domain to generate images in a target domain without using source-target image pairs. Promising results have been obtained for this problem in an adversarial setting using two independent GANs and attention mechanisms. We propose a new method that uses a single shared discriminator between the two GANs, which improves the overall efficacy. We assess the qualitative and quantitative results on image transfiguration, a cross-domain translation task, in a setting where the target domain shares similar semantics to the source domain. Our results indicate that even without adding attention mechanisms, our method performs at par with attention-based methods and generates images of comparable quality.",0
"In the rapidly advancing field of computer vision, image translation remains a challenging task as it requires translating images from one domain into another while preserving their content. To overcome the limitations faced by existing unsupervised cross-domain image-to-image translation methods that rely on cycle consistency loss alone, we propose a novel approach using shared discriminator. By doing so, our method can successfully learn a mapping function that effectively transfers images from multiple source domains to a single target domain without any explicit supervision. We evaluated our proposed method on several benchmark datasets and achieved state-of-the-art results across all tasks. Our study highlights the effectiveness of utilizing a shared discriminator in learning unsupervised image-to-image translation, thus paving the way towards advanced applications such as medical imaging analysis, autonomous vehicles, and virtual reality experiences.",1
"Koopman spectral analysis has attracted attention for nonlinear dynamical systems since we can analyze nonlinear dynamics with a linear regime by embedding data into a Koopman space by a nonlinear function. For the analysis, we need to find appropriate embedding functions. Although several neural network-based methods have been proposed for learning embedding functions, existing methods require long time-series for training neural networks. This limitation prohibits performing Koopman spectral analysis in applications where only short time-series are available. In this paper, we propose a meta-learning method for estimating embedding functions from unseen short time-series by exploiting knowledge learned from related but different time-series. With the proposed method, a representation of a given short time-series is obtained by a bidirectional LSTM for extracting its properties. The embedding function of the short time-series is modeled by a neural network that depends on the time-series representation. By sharing the LSTM and neural networks across multiple time-series, we can learn common knowledge from different time-series while modeling time-series-specific embedding functions with the time-series representation. Our model is trained such that the expected test prediction error is minimized with the episodic training framework. We experimentally demonstrate that the proposed method achieves better performance in terms of eigenvalue estimation and future prediction than existing methods.",0
This paper presents a novel approach to meta-learning for Koopman spectral analysis using short time-series data. We propose a method that leverages multiple short time-series datasets from related but diverse systems to improve the accuracy and generalization performance of the resulting spectral models. Our framework extends traditional transfer learning by incorporating domain knowledge through system parameters and physical constraints. An optimization problem is formulated to jointly learn the optimal weights for all tasks while ensuring that the resulting models satisfy these additional domain-specific conditions. Experimental results on a variety of dynamical systems show significant improvements over baseline methods and demonstrate the effectiveness of our proposed approach.,1
"A practical large scale product recognition system suffers from the phenomenon of long-tailed imbalanced training data under the E-commercial circumstance at Alibaba. Besides product images at Alibaba, plenty of image related side information (e.g. title, tags) reveal rich semantic information about images. Prior works mainly focus on addressing the long tail problem in visual perspective only, but lack of consideration of leveraging the side information. In this paper, we present a novel side information based large scale visual recognition co-training~(SICoT) system to deal with the long tail problem by leveraging the image related side information. In the proposed co-training system, we firstly introduce a bilinear word attention module aiming to construct a semantic embedding over the noisy side information. A visual feature and semantic embedding co-training scheme is then designed to transfer knowledge from classes with abundant training data (head classes) to classes with few training data (tail classes) in an end-to-end fashion. Extensive experiments on four challenging large scale datasets, whose numbers of classes range from one thousand to one million, demonstrate the scalable effectiveness of the proposed SICoT system in alleviating the long tail problem. In the visual search platform Pailitao\footnote{http://www.pailitao.com} at Alibaba, we settle a practical large scale product recognition application driven by the proposed SICoT system, and achieve a significant gain of unique visitor~(UV) conversion rate.",0
"At Alibaba Group Holding Limited (NYSE: BABA), we have developed a large scale product recognition system that can accurately identify items across different contexts using deep learning techniques. Our system is designed to handle millions of images and text queries per day from users on our popular e-commerce platforms, such as Taobao Marketplace, Tmall, and AliExpress. With our state-of-the-art model architecture and optimization methods, we achieved significant improvement over existing models on both accuracy and efficiency metrics. In this technical report, we describe the details of our proposed solution including data preparation, feature extraction, model training and deployment pipeline, evaluation methods and performance benchmark results against competitive baselines. Overall, we showcase how our product recognition system delivers unprecedented quality for high demand use cases in the real world setting like product matching, search query refinement, recommendation generation and user experience enhancement. We believe that our work provides valuable insights into large-scale image recognition research and applications, which has implications for many other industries beyond retail and e-commerce.",1
"Autonomous robotic systems and self driving cars rely on accurate perception of their surroundings as the safety of the passengers and pedestrians is the top priority. Semantic segmentation is one the essential components of environmental perception that provides semantic information of the scene. Recently, several methods have been introduced for 3D LiDAR semantic segmentation. While, they can lead to improved performance, they are either afflicted by high computational complexity, therefore are inefficient, or lack fine details of smaller instances. To alleviate this problem, we propose AF2-S3Net, an end-to-end encoder-decoder CNN network for 3D LiDAR semantic segmentation. We present a novel multi-branch attentive feature fusion module in the encoder and a unique adaptive feature selection module with feature map re-weighting in the decoder. Our AF2-S3Net fuses the voxel based learning and point-based learning into a single framework to effectively process the large 3D scene. Our experimental results show that the proposed method outperforms the state-of-the-art approaches on the large-scale SemanticKITTI benchmark, ranking 1st on the competitive public leaderboard competition upon publication.",0
"Title: ""Attentive Feature Fusion and Adaptive Feature Selection for Improved Sparse Semantic Segmentation""  This research presents a new approach for sparse semantic segmentation using deep learning techniques. Our method, named AF2-S3Net, utilizes both attentive feature fusion and adaptive feature selection to improve the accuracy and efficiency of semantic segmentation in images.  The proposed model consists of two branches - one for attentively fusing features from different layers and another for selecting discriminative features dynamically. These components work together to learn informative representations that capture important information while eliminating irrelevant details.  Experimental results on benchmark datasets demonstrate that our framework outperforms state-of-the-art methods by achieving higher accuracies with fewer parameters. Furthermore, we provide an ablation study to evaluate the effectiveness of each component in our network design.  In summary, the contributions of this work lie in developing a novel architecture for efficient and accurate semantic segmentation via attentive feature fusion and dynamic feature selection. Future directions may explore expanding our framework to other computer vision tasks or incorporating additional attention mechanisms to further enhance performance.",1
"Energy disaggregation, known in the literature as Non-Intrusive Load Monitoring (NILM), is the task of inferring the power demand of the individual appliances given the aggregate power demand recorded by a single smart meter which monitors multiple appliances. In this paper, we propose a deep neural network that combines a regression subnetwork with a classification subnetwork for solving the NILM problem. Specifically, we improve the generalization capability of the overall architecture by including an encoder-decoder with a tailored attention mechanism in the regression subnetwork. The attention mechanism is inspired by the temporal attention that has been successfully applied in neural machine translation, text summarization, and speech recognition. The experiments conducted on two publicly available datasets--REDD and UK-DALE--show that our proposed deep neural network outperforms the state-of-the-art in all the considered experimental conditions. We also show that modeling attention translates into the network's ability to correctly detect the turning on or off an appliance and to locate signal sections with high power consumption, which are of extreme interest in the field of energy disaggregation.",0
"This research explores the use of deep learning techniques to improve non-intrusive load disaggregation (NILD) methods. NILD involves breaking down aggregate energy consumption data into individual appliance level usage patterns without requiring additional sensors. Traditional NILD approaches have limitations, such as difficulty handling noisy data and varying operating conditions, which can lead to reduced accuracy in estimation. To address these issues, we propose using an attention-based deep neural network model that adapts to changes in operational states over time by selectively focusing on relevant features in each input sequence. Our approach achieves state-of-the-art performance compared to other NILD techniques across different benchmark datasets, demonstrating the effectiveness of incorporating deep learning models into NILD applications. By improving upon traditional algorithms, our work has potential impacts in several areas including energy management systems, demand response programs, and home automation settings. Further studies could explore how advanced machine learning techniques may contribute to intelligent decision making for optimized energy efficiency.",1
"Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the u-shaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization.   We argue that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information. TransUNet achieves superior performances to various competing methods on different medical applications including multi-organ segmentation and cardiac segmentation. Code and models are available at https://github.com/Beckschen/TransUNet.",0
"Title: TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation  Abstract: Deep learning models have shown great promise in medical image segmentation due to their ability to learn features from large datasets. However, these models typically require strong encoders to extract relevant information from input images. In this study, we propose using transformers as encoders for medical image segmentation tasks. Our proposed model, called TransUNet, utilizes a UNet architecture with a transformer encoder for feature extraction. We evaluate our approach on three popular medical imaging benchmarks (MICCAI Brain Tumor Segmentation Challenge, ISIC Chest X-ray, and Multi-organ Segmentation in CT) and show that TransUNet outperforms traditional convolutional networks across all metrics while achieving real-time inference speed (<2 seconds per frame). Additionally, our method is more efficient in terms of parameters and data requirements compared to other state-of-the-art methods. These results demonstrate that transformers can serve as effective encoders for deep learning applications in medical image segmentation, making them a promising direction for future research in healthcare AI.",1
"Navigation inside a closed area with no GPS-signal accessibility is a highly challenging task. In order to tackle this problem, recently the imaging-based methods have grabbed the attention of many researchers. These methods either extract the features (e.g. using SIFT, or SOSNet) and map the descriptive ones to the camera position and rotation information, or deploy an end-to-end system that directly estimates this information out of RGB images, similar to PoseNet. While the former methods suffer from heavy computational burden during the test process, the latter suffers from lack of accuracy and robustness against environmental changes and object movements. However, end-to-end systems are quite fast during the test and inference and are pretty qualified for real-world applications, even though their training phase could be longer than the former ones. In this paper, a novel multi-modal end-to-end system for large-scale indoor positioning has been proposed, namely APS (Alpha Positioning System), which integrates a Pix2Pix GAN network to reconstruct the point cloud pair of the input query image, with a deep CNN network in order to robustly estimate the position and rotation information of the camera. For this integration, the existing datasets have the shortcoming of paired RGB/point cloud images for indoor environments. Therefore, we created a new dataset to handle this situation. By implementing the proposed APS system, we could achieve a highly accurate camera positioning with a precision level of less than a centimeter.",0
"This paper presents APS (A Large-scale multi modal indoor camera positioning system), a new method for accurately locating cameras within buildings using a combination of WiFi signals and visual features from cameras themselves. The authors note that while GPS works well outdoors, it struggles to provide accurate location data inside due to signal interference from large structures such as walls. By contrast, WiFi signals can still propagate relatively freely through buildings, allowing them to serve as a basis for localization. In addition, by analyzing images taken with a camera itself - e.g., by looking at distinctive shapes or features in a room - one can supplement the location provided by WiFi alone, thus improving accuracy further. The paper includes experimental results demonstrating the effectiveness of their approach on several hundred million images. They conclude by discussing potential applications, including robotics and AR/VR technologies. Overall, APS shows great promise as a low cost means for determining accurate locations anywhere within a building, opening up many possibilities for innovative uses.",1
"Offline reinforcement learning (RL) aims at learning a good policy from a batch of collected data, without extra interactions with the environment during training. However, current offline RL benchmarks commonly have a large reality gap, because they involve large datasets collected by highly exploratory policies, and the trained policy is directly evaluated in the environment. In real-world situations, running a highly exploratory policy is prohibited to ensure system safety, the data is commonly very limited, and a trained policy should be well validated before deployment. In this paper, we present a near real-world offline RL benchmark, named NeoRL, which contains datasets from various domains with controlled sizes, and extra test datasets for policy validation. We evaluate existing offline RL algorithms on NeoRL and argue that the performance of a policy should also be compared with the deterministic version of the behavior policy, instead of the dataset reward. The empirical results demonstrate that the tested offline RL algorithms become less competitive to the deterministic policy on many datasets, and the offline policy evaluation hardly helps. The NeoRL suit can be found at http://polixir.ai/research/neorl. We hope this work will shed some light on future research and draw more attention when deploying RL in real-world systems.",0
"In recent years, deep reinforcement learning (DRL) has achieved significant successes across a wide range of domains, including video games, robotics, and even real-world systems such as traffic management. However, most DRL algorithms require vast amounts of training data collected on-policy using real-time interactions with the environment, which can be costly, time-consuming, and potentially harmful in some cases. To address these limitations, offline RL was introduced, which focuses on training agents using pre-collected datasets without further access to the environment. Despite promising results in certain benchmarks, existing offline RL approaches have been criticized for their limited applicability to more complex problems that involve partial observability, stochasticity, delayed rewards, and other features commonly found in real-world systems. This work introduces neuroevolution for offline RL (NeoRL), a novel approach that combines offline IL with neuroevolution, aiming at developing near real-world benchmarks for evaluating offline RL algorithms. Through comprehensive experiments on three well-known benchmark environments with increasing complexity - Mujoco locomotion tasks, Minecraft and VizDoom game levels - our method shows promising results, outperforming state-of-the-art baselines by large margins while reducing computational costs significantly. Our research suggests that NeoRL offers great potential in bridging the gap between simulated worlds and reality towards deploying effective solutions based on deep RL techniques into critical applications such as autonomous robots or control systems.",1
"Graph Neural Networks (GNNs) have recently received significant research attention due to their superior performance on a variety of graph-related learning tasks. Most of the current works focus on either static or dynamic graph settings, addressing a single particular task, e.g., node/graph classification, link prediction. In this work, we investigate the question: can GNNs be applied to continuously learning a sequence of tasks? Towards that, we explore the Continual Graph Learning (CGL) paradigm and present the Experience Replay based framework ER-GNN for CGL to alleviate the catastrophic forgetting problem in existing GNNs. ER-GNN stores knowledge from previous tasks as experiences and replays them when learning new tasks to mitigate the catastrophic forgetting issue. We propose three experience node selection strategies: mean of feature, coverage maximization, and influence maximization, to guide the process of selecting experience nodes. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our ER-GNN and shed light on the incremental graph (non-Euclidean) structure learning.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful tools for modeling complex systems on graphs such as social networks, traffic flow patterns, protein structures, and more. However, training GNN models is challenged by catastrophic forgetting: when new tasks are learned, previous knowledge can rapidly decay due to capacity limitations of the network. This issue severely hampers their ability to continuously adapt to changing environments, making it difficult to apply them to real-world situations where data arrives incrementally over time. To address this problem, we propose a novel method that uses experience replay to mitigate catastrophic forgetting in GNNs. By randomly sampling mini-batches from previously seen examples, our approach allows the model to learn new tasks while retaining important features from past experiences. We demonstrate through extensive experiments that our proposed method significantly improves accuracy and stability across different datasets and architectures compared to existing state-of-the-art methods for addressing catastrophic forgetting. Our results showcase the potential impact of using experience replay techniques to improve performance in applications involving dynamic graphs. As such, our work provides valuable insights into enabling perpetual learning scenarios in which deep learning models adapt to evolving environments without suffering severe forgetfulness.",1
"Saliency detection based on the complementary information from RGB images and depth maps has recently gained great popularity. In this paper, we propose Complementary Attention and Adaptive Integration Network (CAAI-Net), a novel RGB-D saliency detection model that integrates complementary attention based feature concentration and adaptive cross-modal feature fusion into a unified framework for accurate saliency detection. Specifically, we propose a context-aware complementary attention (CCA) module, which consists of a feature interaction component, a complementary attention component, and a global-context component. The CCA module first utilizes the feature interaction component to extract rich local context features. The resulting features are then fed into the complementary attention component, which employs the complementary attention generated from adjacent levels to guide the attention at the current layer so that the mutual background disturbances are suppressed and the network focuses more on the areas with salient objects. Finally, we utilize a specially-designed adaptive feature integration (AFI) module, which sufficiently considers the low-quality issue of depth maps, to aggregate the RGB and depth features in an adaptive manner. Extensive experiments on six challenging benchmark datasets demonstrate that CAAI-Net is an effective saliency detection model and outperforms nine state-of-the-art models in terms of four widely-used metrics. In addition, extensive ablation studies confirm the effectiveness of the proposed CCA and AFI modules.",0
"This paper presents a novel approach for accurate RGB-D saliency detection using complementary attention mechanisms and adaptive integration techniques. We introduce a three-stream convolutional network architecture that takes advantage of both depth and color features of the input image while simultaneously attending to task-relevant regions in each stream. Our key contributions can be summarized as follows:  * we propose two types of attention modules â€“ global contextual attention (GCA) and local channel-wise attention (LCA) - that operate on different levels of abstraction to improve performance. * we introduce adaptive integration mechanisms that learn to fuse depth and color features in a dynamic manner based on the underlying visual content and complexity. * extensive experimental evaluation demonstrates state-of-the art results across several benchmark datasets. The proposed method outperforms existing approaches by a significant margin in terms of effectiveness (measured via mean average precision), speed, memory efficiency, and scalability.  This work represents a major advance towards real-time high-quality saliency detection systems applicable to robotics, AR/VR, gaming, multimedia retrieval, among other domains where human-computer interaction plays a critical role. With our frameworkâ€™s accuracy, stability, and versatility we contribute important advances toward reliable computer vision tools empowered by modern hardware architectures.  The remainder of the paper proceeds as follows. Section II reviews relevant literature from the areas of saliency detection, deep learning, feature fusion, and visual attention. In section III, we detail the technical description of our proposed approach; followed by experiments and analysis provided in sections IV, V, VI, VII; concluding discussion and future works appear i",1
"Recently, Zero-shot Sketch-based Image Retrieval (ZS-SBIR) has attracted the attention of the computer vision community due to it's real-world applications, and the more realistic and challenging setting than found in SBIR. ZS-SBIR inherits the main challenges of multiple computer vision problems including content-based Image Retrieval (CBIR), zero-shot learning and domain adaptation. The majority of previous studies using deep neural networks have achieved improved results through either projecting sketch and images into a common low-dimensional space or transferring knowledge from seen to unseen classes. However, those approaches are trained with complex frameworks composed of multiple deep convolutional neural networks (CNNs) and are dependent on category-level word labels. This increases the requirements on training resources and datasets. In comparison, we propose a simple and efficient framework that does not require high computational training resources, and can be trained on datasets without semantic categorical labels. Furthermore, at training and inference stages our method only uses a single CNN. In this work, a pre-trained ImageNet CNN (e.g., ResNet50) is fine-tuned with three proposed learning objects: domain-aware quadruplet loss, semantic classification loss, and semantic knowledge preservation loss. The domain-aware quadruplet and semantic classification losses are introduced to learn discriminative, semantic and domain invariant features through considering ZS-SBIR as object detection and verification problem. ...",0
"This paper presents an efficient framework for zero-shot sketch-based image retrieval (SBIR), which can effectively search images from large databases using only rough sketches drawn by users as input queries without any examples from target domains during training. We use state-of-the-art deep learning models pretrained on large datasets to learn robust visual representations that encode both shape and texture information. Our method enables efficient querying and filtering operations based on shape similarity while preserving relevant textural details via feature augmentation techniques. Experimental results demonstrate the effectiveness of our approach on multiple benchmark SBIR datasets. In conclusion, we propose an innovative SBIR system capable of retrieving high quality results using zero-shot transfer learning methods across diverse domains.",1
"Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games). In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).",0
"This paper presents a new algorithm called UPDeT (Universal Multi-Agent Reinforcement Learning via Policy Decoupling with Transformers) for training effective policies in multi-agent reinforcement learning problems. UPDeT uses policy decoupling and transformer networks to learn individual agent policies that can then be combined into a single shared policy for use in cooperative settings. We show through experiments on both traditional benchmarks as well as more complex scenarios that involve coordination and communication among agents, that UPDeT outperforms several state-of-the-art methods. Our results suggest that the key benefits of UPDeT come from the combination of efficient exploration across multiple agents using value decomposition and action decoupling, along with parallelization over distributed GPUs during inference. These advantages make UPDeT a promising approach for solving large-scale, real-world multi-agent applications where scalability and coordination are critical factors.",1
"In 2D image processing, some attempts decompose images into high and low frequency components for describing edge and smooth parts respectively. Similarly, the contour and flat area of 3D objects, such as the boundary and seat area of a chair, describe different but also complementary geometries. However, such investigation is lost in previous deep networks that understand point clouds by directly treating all points or local patches equally. To solve this problem, we propose Geometry-Disentangled Attention Network (GDANet). GDANet introduces Geometry-Disentangle Module to dynamically disentangle point clouds into the contour and flat part of 3D objects, respectively denoted by sharp and gentle variation components. Then GDANet exploits Sharp-Gentle Complementary Attention Module that regards the features from sharp and gentle variation components as two holistic representations, and pays different attentions to them while fusing them respectively with original point cloud features. In this way, our method captures and refines the holistic and complementary 3D geometric semantics from two distinct disentangled components to supplement the local information. Extensive experiments on 3D object classification and segmentation benchmarks demonstrate that GDANet achieves the state-of-the-arts with fewer parameters. Code is released on https://github.com/mutianxu/GDANet.",0
"Title: ""Geometric Disentanglement of Shape, Appearance, and Relational Structure for Learning Rich 3D Scene Analysis"" Abstract: This paper presents a method for learning rich and interpretable representations of complex, real-world 3D scenes from raw point cloud data using the concepts of geometry disentanglement. We introduce three complementary disentangled representations which capture shape, appearance, and relational structure of objects within the scene, respectively. Our proposed approach jointly optimizes these representations via adversarial training to enhance their quality while preserving interpretability. Experiments demonstrate that our model can effectively learn robust geometric features across multiple datasets and tasks, including object classification, segmentation, pose estimation, and generative design. Results show significant improvement over previous state-of-the-art methods, suggesting the potential value of geometrically disentangling key aspects of 3D scenes in computer vision research.",1
"While Semi-supervised learning has gained much attention in computer vision on image data, yet limited research exists on its applicability in the time series domain. In this work, we investigate the transferability of state-of-the-art deep semi-supervised models from image to time series classification. We discuss the necessary model adaptations, in particular an appropriate model backbone architecture and the use of tailored data augmentation strategies. Based on these adaptations, we explore the potential of deep semi-supervised learning in the context of time series classification by evaluating our methods on large public time series classification problems with varying amounts of labelled samples. We perform extensive comparisons under a decidedly realistic and appropriate evaluation scheme with a unified reimplementation of all algorithms considered, which is yet lacking in the field. We find that these transferred semi-supervised models show significant performance gains over strong supervised, semi-supervised and self-supervised alternatives, especially for scenarios with very few labelled samples.",0
"Abstract: Improving accuracy on time series classification problems using machine learning models can become challenging due to limited labeled data availability. To address this issue, semi-supervised learning methods have been proposed which utilize unlabeled data along with a small amount of labeled data for better performance. In addition, deep neural networks (DNNs) have achieved state-of-the-art results on various benchmark datasets, including time series classification tasks. However, existing semi-supervised DNN methods often rely heavily on artificial augmentation techniques, such as adding noise or shifting the timing of the data points, which may not always be suitable for time series data. This paper presents a novel method for time series classification that combines the strengths of both deep learning and graph-based semi-supervised learning approaches. We propose a new algorithm called DeepSemiTime, which leverages domain knowledge through graph construction based on shifted local linear embeddings of the time series data. Our approach is effective even with very few labeled examples, significantly outperforming previous state-of-the-art methods across multiple real-world time series classification benchmark datasets. Keywords: semi-supervised learning, deep learning, time series, classification.",1
"Dynamic graphs are rife with higher-order interactions, such as co-authorship relationships and protein-protein interactions in biological networks, that naturally arise between more than two nodes at once. In spite of the ubiquitous presence of such higher-order interactions, limited attention has been paid to the higher-order counterpart of the popular pairwise link prediction problem. Existing higher-order structure prediction methods are mostly based on heuristic feature extraction procedures, which work well in practice but lack theoretical guarantees. Such heuristics are primarily focused on predicting links in a static snapshot of the graph. Moreover, these heuristic-based methods fail to effectively utilize and benefit from the knowledge of latent substructures already present within the higher-order structures. In this paper, we overcome these obstacles by capturing higher-order interactions succinctly as \textit{simplices}, model their neighborhood by face-vectors, and develop a nonparametric kernel estimator for simplices that views the evolving graph from the perspective of a time process (i.e., a sequence of graph snapshots). Our method substantially outperforms several baseline higher-order prediction methods. As a theoretical achievement, we prove the consistency and asymptotic normality in terms of the Wasserstein distance of our estimator using Stein's method.",0
"This paper presents a new approach for understanding higher-order structures in evolving graphs using kernel estimation techniques based on simplicial complexes. In traditional graph analysis, only pairwise connections are considered, but many real world networks have higher-order structures that can provide valuable insights into their dynamics. To capture these higher-order interactions, we propose the use of the nerve construction from algebraic topology to build a simplicial complex representation of the graph, which provides a natural framework for estimating kernels over graphs with arbitrary dimension. Our approach allows us to compare data points across different scales, allowing us to detect patterns and relationships that would otherwise go unnoticed. We demonstrate our method on two case studies, one involving protein-protein interaction networks and another looking at social influence networks. Results show that our approach outperforms state-of-the-art methods for capturing important features in each network, emphasizing the effectiveness of our proposed technique. Overall, this work makes a significant contribution towards better understanding high-dimensional dynamic systems through graph theory and topological techniques.",1
"The virtual try-on task is so attractive that it has drawn considerable attention in the field of computer vision. However, presenting the three-dimensional (3D) physical characteristic (e.g., pleat and shadow) based on a 2D image is very challenging. Although there have been several previous studies on 2D-based virtual try-on work, most 1) required user-specified target poses that are not user-friendly and may not be the best for the target clothing, and 2) failed to address some problematic cases, including facial details, clothing wrinkles and body occlusions. To address these two challenges, in this paper, we propose an innovative template-free try-on image synthesis (TF-TIS) network. The TF-TIS first synthesizes the target pose according to the user-specified in-shop clothing. Afterward, given an in-shop clothing image, a user image, and a synthesized pose, we propose a novel model for synthesizing a human try-on image with the target clothing in the best fitting pose. The qualitative and quantitative experiments both indicate that the proposed TF-TIS outperforms the state-of-the-art methods, especially for difficult cases.",0
"Include important keywords such as semantic image synthesis, try-on simulation, optimization, and generative adversarial networks (GANs). Provide a brief overview of the problem addressed by the research, how the proposed approach differs from existing methods, and highlight key contributions. Mention potential applications in areas like e-commerce, healthcare, education, etc. Finally, end with a statement encouraging further investigation into this exciting research area.",1
"Data acquisition forms the primary step in all empirical research. The availability of data directly impacts the quality and extent of conclusions and insights. In particular, larger and more detailed datasets provide convincing answers even to complex research questions. The main problem is that 'large and detailed' usually implies 'costly and difficult', especially when the data medium is paper and books. Human operators and manual transcription have been the traditional approach for collecting historical data. We instead advocate the use of modern machine learning techniques to automate the digitisation process. We give an overview of the potential for applying machine digitisation for data collection through two illustrative applications. The first demonstrates that unsupervised layout classification applied to raw scans of nurse journals can be used to construct a treatment indicator. Moreover, it allows an assessment of assignment compliance. The second application uses attention-based neural networks for handwritten text recognition in order to transcribe age and birth and death dates from a large collection of Danish death certificates. We describe each step in the digitisation pipeline and provide implementation insights.",0
"Document digitization refers to the process of converting physical documents into digital form so that they can be stored and accessed electronically. This has become increasingly important as more businesses seek to go paperless and improve their efficiency by automating their document management systems. Machine learning (ML) has emerged as one of the most promising technologies for improving the accuracy and speed of document digitization processes. In this article, we present an overview of some of the key applications of ML in document digitization and discuss how these techniques have been used to overcome challenges associated with traditional approaches. We begin by describing traditional methods used in document scanning and highlighting some of the limitations of these methods. Then, we discuss how ML algorithms such as optical character recognition (OCR), object detection, image classification, feature extraction, etc., can be applied to improve document digitization accuracy. Finally, we provide case studies demonstrating real-world examples of how organizations are using ML for document digitization tasks. Overall, our research suggests that there is significant potential for leveraging ML technologies to enhance document digitization processes across industries, leading to increased productivity, reduced costs, and improved accessibility.",1
"Graph attention networks (GATs) have been recognized as powerful tools for learning in graph structured data. However, how to enable the attention mechanisms in GATs to smoothly consider both structural and feature information is still very challenging. In this paper, we propose Graph Joint Attention Networks (JATs) to address the aforementioned challenge. Different from previous attention-based graph neural networks (GNNs), JATs adopt novel joint attention mechanisms which can automatically determine the relative significance between node features and structural coefficients learned from graph topology, when computing the attention scores. Therefore, representations concerning more structural properties can be inferred by JATs. Besides, we theoretically analyze the expressive power of JATs and further propose an improved strategy for the joint attention mechanisms that enables JATs to reach the upper bound of expressive power which every message-passing GNN can ultimately achieve, i.e., 1-WL test. JATs can thereby be seen as most powerful message-passing GNNs. The proposed neural architecture has been extensively tested on widely used benchmarking datasets, and has been compared with state-of-the-art GNNs for various downstream predictive tasks. Experimental results show that JATs achieve state-of-the-art performance on all the testing datasets.",0
"Graph convolutional networks (GCNs) have been proven effective on many tasks involving graph structured data such as social network analysis and drug discovery applications. However, these models assume that each node only attends to its local neighborhood during the learning process, which can limit their ability to capture global context. In this work we propose a new model called Graph Joint Attention Networks (GJAN), which extends standard GCNs by incorporating joint attention mechanisms at multiple scales. Our method allows nodes to attend to other regions of the graph based on their degree of similarity or importance, enabling the capture of more complex relationships between entities. We evaluate our approach on several benchmark datasets commonly used for GCN evaluation including Citeseer, Cora, Pubmed, and Cornell Movie Review Data. Experimental results demonstrate that our proposed model outperforms state-of-the art GCN baselines significantly. Additionally, ablation studies show that incorporation of joint attention modules indeed improves performance across different scales. Lastly, visualizations provide insights into the working mechanism of our approach by highlighting how attention weights vary depending on input graphs and hyperparameters settings. Overall, our findings suggest that GJAN could serve as a powerful tool in the area of graph representation learning, paving the road for exciting future research directions in areas ranging from natural language processing to knowledge graph embedding and beyond. This research provides a first step towards building artificial intelligence systems capable of handling rich interactions among components in a systematic fashion.",1
"Semantic segmentation for aerial platforms has been one of the fundamental scene understanding task for the earth observation. Most of the semantic segmentation research focused on scenes captured in nadir view, in which objects have relatively smaller scale variation compared with scenes captured in oblique view. The huge scale variation of objects in oblique images limits the performance of deep neural networks (DNN) that process images in a single scale fashion. In order to tackle the scale variation issue, in this paper, we propose the novel bidirectional multi-scale attention networks, which fuse features from multiple scales bidirectionally for more adaptive and effective feature extraction. The experiments are conducted on the UAVid2020 dataset and have shown the effectiveness of our method. Our model achieved the state-of-the-art (SOTA) result with a mean intersection over union (mIoU) score of 70.80%.",0
"This paper presents a method for semantic segmentation of oblique images captured by unmanned aerial vehicles (UAVs). Using multi-scale attention networks with both encoder-decoder and U-Net architectures, we propose a new approach called bidirectional multi-scale attention network (BMA) that improves performance on challenging oblique UAV imagery. Our results show significant improvements over state-of-the art methods across several metrics including mean intersection-over-union (mIOU), average precision (AP), pixel accuracy (PA), and f1 score. We further demonstrate the effectiveness of our method through qualitative analysis. By incorporating attention mechanisms into a bi-directional framework, BMA captures detailed contextual features from both the coarse and fine scales, making it particularly suited for semantically segmenting complex scenes present in oblique UAV imagery. Overall, our work provides a valuable contribution towards addressing one of the key limitations of current UAV image processing pipelines - accurate object detection and semantic segmentation of complex, real world scenes.",1
"As 3D point cloud analysis has received increasing attention, the insufficient scale of point cloud datasets and the weak generalization ability of networks become prominent. In this paper, we propose a simple and effective augmentation method for the point cloud data, named PointCutMix, to alleviate those problems. It finds the optimal assignment between two point clouds and generates new training data by replacing the points in one sample with their optimal assigned pairs. Two replacement strategies are proposed to adapt to the accuracy or robustness requirement for different tasks, one of which is to randomly select all replacing points while the other one is to select k nearest neighbors of a single random point. Both strategies consistently and significantly improve the performance of various models on point cloud classification problems. By introducing the saliency maps to guide the selection of replacing points, the performance further improves. Moreover, PointCutMix is validated to enhance the model robustness against the point attack. It is worth noting that when using as a defense method, our method outperforms the state-of-the-art defense algorithms. The code is available at:https://github.com/cuge1995/PointCutMix",0
"In recent years point clouds have gained increasing popularity as a data structure used in computer vision tasks such as object recognition, semantic segmentation and scene understanding. This has been mainly driven by the availability of new sensors capturing real world scenes at high resolutions and with fine details like LIDAR and RGBD cameras. While deep learning methods like convolutional neural networks (CNN) and more recently transformer based architectures have proven successful on these datasets, they often struggle to generalize well due to their large capacity, leading to overfitting issues. We propose PointCutMix, a novel regularizer that addresses this problem by applying Mixup-style augmentations directly to the input points themselves, rather than features extracted from them after processing through a network. By doing so we encourage models to be robust against input perturbations and focus on underlying geometric patterns instead of superficial characteristics like texture or color. Additionally our method can be combined easily with other techniques such as traditional feature space mixups. Experiments show consistent improvement across multiple backbones trained on five benchmarks, including the recent ScanNet dataset. Our source code and pretrained models are available online.",1
"Predicting motion of surrounding agents is critical to real-world applications of tactical path planning for autonomous driving. Due to the complex temporal dependencies and social interactions of agents, on-line trajectory prediction is a challenging task. With the development of attention mechanism in recent years, transformer model has been applied in natural language sequence processing first and then image processing. In this paper, we present a Spatial-Channel Transformer Network for trajectory prediction with attention functions. Instead of RNN models, we employ transformer model to capture the spatial-temporal features of agents. A channel-wise module is inserted to measure the social interaction between agents. We find that the Spatial-Channel Transformer Network achieves promising results on real-world trajectory prediction datasets on the traffic scenes.",0
"In this study we develop a novel approach that uses transformers specifically tuned for processing spatio-temporal data from traffic videos, achieving state-of-the art performance in the task of trajectory prediction. Our method leverages two key innovations: firstly, we propose using spatial attention modules within our spatio-temporal transformer network, which allows us to dynamically selectively weight different parts of input frames based on their relevance to predicting future trajectories. Secondly, we introduce channel attention modules to learn more efficient feature representations tailored specifically to capturing interactions between objects over time. We evaluate our model against several strong baselines on three challenging datasets for tracking pedestrians or vehicles. Our results show consistent improvements across all benchmarks, demonstrating the superior effectiveness of our design choices for accurate path forecasting in complex dynamic scenarios.",1
"Knowledge distillation extracts general knowledge from a pre-trained teacher network and provides guidance to a target student network. Most studies manually tie intermediate features of the teacher and student, and transfer knowledge through pre-defined links. However, manual selection often constructs ineffective links that limit the improvement from the distillation. There has been an attempt to address the problem, but it is still challenging to identify effective links under practical scenarios. In this paper, we introduce an effective and efficient feature distillation method utilizing all the feature levels of the teacher without manually selecting the links. Specifically, our method utilizes an attention-based meta-network that learns relative similarities between features, and applies identified similarities to control distillation intensities of all possible pairs. As a result, our method determines competent links more efficiently than the previous approach and provides better performance on model compression and transfer learning tasks. Further qualitative analyses and ablative studies describe how our method contributes to better distillation. The implementation code is available at github.com/clovaai/attention-feature-distillation.",0
"This paper presents a novel methodology for knowledge distillation, using attention mechanisms to match features from teacher networks to student networks. We introduce the concept of ""attentive feature matching,"" which allows us to focus on specific parts of each network and ensure that only relevant features are used in training the student model. Our approach has several advantages over previous methods. Firstly, our use of attentional weights ensures that we can effectively balance the different contributions made by different regions of the teacher network during training. Secondly, our method allows for efficient communication between teacher and student models even if they have different architectures. Finally, our method results in significantly improved performance compared to prior art approaches. Through extensive experimentation and evaluation, we demonstrate the effectiveness of our proposed method across multiple benchmark datasets. Overall, this work represents a significant step forward in understanding how to effectively transfer knowledge from large pretrained models to smaller ones, addressing some of the key challenges faced in deep learning today.",1
"Predicting (1) when the next hospital admission occurs and (2) what will happen in the next admission about a patient by mining electronic health record (EHR) data can provide granular readmission predictions to assist clinical decision making. Recurrent neural network (RNN) and point process models are usually employed in modelling temporal sequential data. Simple RNN models assume that sequences of hospital visits follow strict causal dependencies between consecutive visits. However, in the real-world, a patient may have multiple co-existing chronic medical conditions, i.e., multimorbidity, which results in a cascade of visits where a non-immediate historical visit can be most influential to the next visit. Although a point process (e.g., Hawkes process) is able to model a cascade temporal relationship, it strongly relies on a prior generative process assumption. We propose a novel model, MEDCAS, to address these challenges. MEDCAS combines the strengths of RNN-based models and point processes by integrating point processes in modelling visit types and time gaps into an attention-based sequence-to-sequence learning model, which is able to capture the temporal cascade relationships. To supplement the patients with short visit sequences, a structural modelling technique with graph-based methods is used to construct the markers of the point process in MEDCAS. Extensive experiments on three real-world EHR datasets have been performed and the results demonstrate that \texttt{MEDCAS} outperforms state-of-the-art models in both tasks.",0
"In summary, we develop two new models of Electronic Health Record (EHR) data: one based on temporal cascades and another based on structural features. We evaluate their performance in predicting readmissions as well as compare them against previous state-of-the-art approaches which use static patient representations. Both novel methods lead to significant improvements over past techniques while outperforming benchmarks across multiple metrics including area under the curve, precision/recall curves, and accuracy. Additionally, our findings demonstrate that considering temporal aspects of patientsâ€™ health states during prediction can further improve overall performance. Through these contributions, this work provides important insights into developing more accurate predictive algorithms using longitudinal EHR records.",1
"This paper is a study of reinforcement learning (RL) as an optimal-control strategy for control of nonlinear valves. It is evaluated against the PID (proportional-integral-derivative) strategy, using a unified framework. RL is an autonomous learning mechanism that learns by interacting with its environment. It is gaining increasing attention in the world of control systems as a means of building optimal-controllers for challenging dynamic and nonlinear processes. Published RL research often uses open-source tools (Python and OpenAI Gym environments). We use MATLAB's recently launched (R2019a) Reinforcement Learning Toolbox to develop the valve controller; trained using the DDPG (Deep Deterministic Policy-Gradient) algorithm and Simulink to simulate the nonlinear valve and create the experimental test-bench for evaluation. Simulink allows industrial engineers to quickly adapt and experiment with other systems of their choice. Results indicate that the RL controller is extremely good at tracking the signal with speed and produces a lower error with respect to the reference signal. The PID, however, is better at disturbance rejection and hence provides a longer life for the valves. Successful machine learning involves tuning many hyperparameters requiring significant investment of time and efforts. We introduce ""Graded Learning"" as a simplified, application oriented adaptation of the more formal and algorithmic ""Curriculum for Reinforcement Learning"". It is shown via experiments that it helps converge the learning task of complex non-linear real world systems. Finally, experiential learnings gained from this research are corroborated against published research.",0
"This paper presents an approach to controlling valves using reinforcement learning (RL). Traditional control methods often rely on explicit models of systems and carefully designed feedback controllers that require significant engineering effort and are limited by their conservative assumptions. In contrast, RL enables agents to learn control policies directly from trial and error, allowing them to adapt to complex and uncertain environments.  In particular, we consider two important factors affecting valve control: reliability and efficiency. We use deep RL algorithms to learn optimal control policies balancing these conflicting objectives. Our framework can handle real-world constraints such as valve dynamics and actuator limits, enabling more effective and practical solutions.  Experiments conducted on benchmark testbeds demonstrate the effectiveness of our approach compared to both classical model-based control strategies and other state-of-the-art RL methods. Furthermore, we showcase how learned policies generalize well across different operating conditions. These results suggest promising opportunities for applying RL to industrial valve management and related challenges in process automation.",1
"Semantic segmentation of remotely sensed images plays a crucial role in precision agriculture, environmental protection, and economic assessment. In recent years, substantial fine-resolution remote sensing images are available for semantic segmentation. However, due to the complicated information caused by the increased spatial resolution, state-of-the-art deep learning algorithms normally utilize complex network architectures for segmentation, which usually incurs high computational complexity. Specifically, the high-caliber performance of the convolutional neural network (CNN) heavily relies on fine-grained spatial details (fine resolution) and sufficient contextual information (large receptive fields), both of which trigger high computational costs. This crucially impedes their practicability and availability in real-world scenarios that require real-time processing. In this paper, we propose an Attentive Bilateral Contextual Network (ABCNet), a convolutional neural network (CNN) with double branches, with prominently lower computational consumptions compared to the cutting-edge algorithms, while maintaining a competitive accuracy. Code is available at https://github.com/lironui/ABCNet.",0
"This paper presents a novel deep learning architecture called ABCNet which uses attentive bilateral contextual reasoning modules (ABCRMs) for efficient semantic segmentation of fine-resolution remote sensing images (RSIs). RSIs have become increasingly important as they provide critical information such as land use/cover classification, object detection, and mapping change analysis over large geographic areas at high spatial resolution. Due to their large size and complexity, processing these images becomes challenging, requiring efficient algorithms that can capture rich features while balancing computational requirements, storage capacity, and memory usage during runtime. Our proposed approach addresses these challenges by using lightweight CRMs integrated into a U-shape network topology tailored towards segmenting coarse-level objects. We evaluate our model on several benchmark datasets containing aerial and satellite imagery with up to 7cm ground sampling distance (GSD), achieving superior performance compared to state-of-the-art models under varying conditions. Additionally, we provide comprehensive ablation studies demonstrating the importance of each component in our network design. Overall, our work provides evidence that ABCNet represents an effective alternative for addressing real-world applications involving high-resolution RSI analysis.",1
"Deep learning (DL) has gained much attention and become increasingly popular in modern data science. Computer scientists led the way in developing deep learning techniques, so the ideas and perspectives can seem alien to statisticians. Nonetheless, it is important that statisticians become involved -- many of our students need this expertise for their careers. In this paper, developed as part of a program on DL held at the Statistical and Applied Mathematical Sciences Institute, we address this culture gap and provide tips on how to teach deep learning to statistics graduate students. After some background, we list ways in which DL and statistical perspectives differ, provide a recommended syllabus that evolved from teaching two iterations of a DL graduate course, offer examples of suggested homework assignments, give an annotated list of teaching resources, and discuss DL in the context of two research areas.",0
"Incorporate at least five keywords related to machine learning: linear models, neural networks, statistical modeling, deep learning, optimization algorithms A statistician teaches deep learning can refer to both a statistican teaching the subject matter of deep learning (for example a teacher) as well as a novice learner doing deep learning by themselves but using statistics methods they are familiar with already. As such, there could be many different papers that fit into that category so there may need to be more context before an abstract can be written. Would you like some examples? Or perhaps you would prefer me to come up with a specific type of paper?",1
"This paper considers a video caption generating network referred to as Semantic Grouping Network (SGN) that attempts (1) to group video frames with discriminating word phrases of partially decoded caption and then (2) to decode those semantically aligned groups in predicting the next word. As consecutive frames are not likely to provide unique information, prior methods have focused on discarding or merging repetitive information based only on the input video. The SGN learns an algorithm to capture the most discriminating word phrases of the partially decoded caption and a mapping that associates each phrase to the relevant video frames - establishing this mapping allows semantically related frames to be clustered, which reduces redundancy. In contrast to the prior methods, the continuous feedback from decoded words enables the SGN to dynamically update the video representation that adapts to the partially decoded caption. Furthermore, a contrastive attention loss is proposed to facilitate accurate alignment between a word phrase and video frames without manual annotations. The SGN achieves state-of-the-art performances by outperforming runner-up methods by a margin of 2.1%p and 2.4%p in a CIDEr-D score on MSVD and MSR-VTT datasets, respectively. Extensive experiments demonstrate the effectiveness and interpretability of the SGN.",0
"This is an abstract of a research paper that presents a new approach to automatic video captioning using deep learning techniques. The proposed method uses a semantic grouping network (SGN) to generate descriptive text summaries of videos based on their visual content. The SGN consists of multiple layers of convolutional neural networks (CNNs) that learn to group together visually similar frames into meaningful segments. These segments can then be used as the basis for generating natural language descriptions of the video. The authors evaluate their method on several large datasets of YouTube videos and demonstrate significant improvements over state-of-the-art video captioning methods. Overall, the work represents an important contribution to the field of computer vision and has potential applications in a variety of domains including accessibility services for the visually impaired, automated video indexing, and multimedia retrieval systems.",1
"The use of machine learning to develop intelligent software tools for interpretation of radiology images has gained widespread attention in recent years. The development, deployment, and eventual adoption of these models in clinical practice, however, remains fraught with challenges. In this paper, we propose a list of key considerations that machine learning researchers must recognize and address to make their models accurate, robust, and usable in practice. Namely, we discuss: insufficient training data, decentralized datasets, high cost of annotations, ambiguous ground truth, imbalance in class representation, asymmetric misclassification costs, relevant performance metrics, generalization of models to unseen datasets, model decay, adversarial attacks, explainability, fairness and bias, and clinical validation. We describe each consideration and identify techniques to address it. Although these techniques have been discussed in prior research literature, by freshly examining them in the context of medical imaging and compiling them in the form of a laundry list, we hope to make them more accessible to researchers, software developers, radiologists, and other stakeholders.",0
"As machine learning continues to gain traction in radiology practice, there are several key technology considerations that must be taken into account during development and deployment of these models. Firstly, the quality and quantity of data used to train these models is crucial as it directly affects their accuracy and generalization ability. Secondly, attention should be paid towards algorithm transparency and interpretability so that clinicians can better trust and utilize model output. Additionally, software infrastructure needs to be scalable, modular, and user friendly so that teams can efficiently develop, test, and integrate models into existing workflows. Lastly, careful consideration must be given to regulatory and ethical concerns, such as ensuring patient privacy and compliance with relevant regulations. By addressing these technology considerations, we can improve the effectiveness and adoption of machine learning models within clinical radiology practice.",1
"Visual attention in Visual Question Answering (VQA) targets at locating the right image regions regarding the answer prediction. However, recent studies have pointed out that the highlighted image regions from the visual attention are often irrelevant to the given question and answer, leading to model confusion for correct visual reasoning. To tackle this problem, existing methods mostly resort to aligning the visual attention weights with human attentions. Nevertheless, gathering such human data is laborious and expensive, making it burdensome to adapt well-developed models across datasets. To address this issue, in this paper, we devise a novel visual attention regularization approach, namely AttReg, for better visual grounding in VQA. Specifically, AttReg firstly identifies the image regions which are essential for question answering yet unexpectedly ignored (i.e., assigned with low attention weights) by the backbone model. And then a mask-guided learning scheme is leveraged to regularize the visual attention to focus more on these ignored key regions. The proposed method is very flexible and model-agnostic, which can be integrated into most visual attention-based VQA models and require no human attention supervision. Extensive experiments over three benchmark datasets, i.e., VQA-CP v2, VQA-CP v1, and VQA v2, have been conducted to evaluate the effectiveness of AttReg. As a by-product, when incorporating AttReg into the strong baseline LMH, our approach can achieve a new state-of-the-art accuracy of 59.92% with an absolute performance gain of 6.93% on the VQA-CP v2 benchmark dataset. In addition to the effectiveness validation, we recognize that the faithfulness of the visual attention in VQA has not been well explored in literature. In the light of this, we propose to empirically validate such property of visual attention and compare it with the prevalent gradient-based approaches.",0
"This is a research paper that presents a novel method for image classification called ""Answer Questions with Right Image Regions"" (AQR). The goal of AQR is to address two limitations of existing methods: overreliance on object detection algorithms and lack of interpretability. To solve these problems, we propose a visual attention regularization approach based on question answering. Our model can selectively focus on important regions from given images and answer questions accordingly, thus improving both accuracy and interpretation. We evaluate our proposed method using benchmark datasets and show significant improvements compared to other state-of-the-art approaches. Overall, our work represents a step forward towards more effective and interpretable computer vision systems.",1
"The purpose of few-shot recognition is to recognize novel categories with a limited number of labeled examples in each class. To encourage learning from a supplementary view, recent approaches have introduced auxiliary semantic modalities into effective metric-learning frameworks that aim to learn a feature similarity between training samples (support set) and test samples (query set). However, these approaches only augment the representations of samples with available semantics while ignoring the query set, which loses the potential for the improvement and may lead to a shift between the modalities combination and the pure-visual representation. In this paper, we devise an attributes-guided attention module (AGAM) to utilize human-annotated attributes and learn more discriminative features. This plug-and-play module enables visual contents and corresponding attributes to collectively focus on important channels and regions for the support set. And the feature selection is also achieved for query set with only visual information while the attributes are not available. Therefore, representations from both sets are improved in a fine-grained manner. Moreover, an attention alignment mechanism is proposed to distill knowledge from the guidance of attributes to the pure-visual branch for samples without attributes. Extensive experiments and analysis show that our proposed module can significantly improve simple metric-based approaches to achieve state-of-the-art performance on different datasets and settings.",0
"In summary, our goal was to create a method that would combine attributes guided learning and pure visual attention alignment to improve few shot recognition. To achieve this, we trained a convolutional neural network (CNN) using an attribute classification loss function, as well as a traditional cross entropy loss function. We then applied reinforcement learning techniques to adjust parameters like weights and biases in order to increase accuracy. Our experimental results showed that our approach outperformed previous state of the art methods in several benchmark datasets such as Omniglot and miniImagenet, demonstrating the effectiveness of combining both approaches. Additionally, we conducted ablation studies which provided insights into how each component contributed to overall performance. Overall, this work presents a novel approach for improving few-shot recognition tasks.",1
"One of the major limitations of deep learning models is that they face catastrophic forgetting in an incremental learning scenario. There have been several approaches proposed to tackle the problem of incremental learning. Most of these methods are based on knowledge distillation and do not adequately utilize the information provided by older task models, such as uncertainty estimation in predictions. The predictive uncertainty provides the distributional information can be applied to mitigate catastrophic forgetting in a deep learning framework. In the proposed work, we consider a Bayesian formulation to obtain the data and model uncertainties. We also incorporate self-attention framework to address the incremental learning problem. We define distillation losses in terms of aleatoric uncertainty and self-attention. In the proposed work, we investigate different ablation analyses on these losses. Furthermore, we are able to obtain better results in terms of accuracy on standard benchmarks.",0
"As we strive for artificial intelligence (AI) systems that can perform a wide range of complex tasks across many different domains, catastrophic forgetting has emerged as a key challenge. This phenomenon occurs when an AI system learns new information but then rapidly forgets previously learned knowledge in order to accommodate the new data. To address catastrophic forgetting, researchers have developed mitigation strategies such as regularization techniques like Elastic Weight Consolidation (EWC), which add constraints to prevent large updates during learning so that important previous knowledge remains intact. However, these approaches focus mainly on reducing the impact of new training examples on old ones, leading to suboptimal performance if uncertainty is high in older tasks. In this study, we investigate methods that explicitly manage uncertainty caused by both past experiences and new data, allowing us to better cope with catastrophic forgetting. Our findings show that incorporating uncertainty into regularization helps maintain stability over time and provides more effective learning under challenging conditions. Overall, our work sheds light on the importance of managing uncertainty in developing more robust AI systems that can learn from diverse data sources without suffering from significant decline in overall performance.",1
"Multi-Instance Learning(MIL) aims to learn the mapping between a bag of instances and the bag-level label. Therefore, the relationships among instances are very important for learning the mapping. In this paper, we propose an MIL algorithm based on a graph built by structural relationship among instances within a bag. Then, Graph Convolutional Network(GCN) and the graph-attention mechanism are used to learn bag-embedding. In the task of medical image classification, our GCN-based MIL algorithm makes full use of the structural relationships among patches(instances) in an original image space domain, and experimental results verify that our method is more suitable for handling medical high-resolution images. We also verify experimentally that the proposed method achieves better results than previous methods on five bechmark MIL datasets and four medical image datasets.",0
"Title: ""Utilizing Structural Relationships Among Instances for Improved Multi-instance Learning""  Multiple instance learning (MIL) is a challenging task that involves predicting the labels of bags containing multiple instances based on their weakly labeled training examples. In traditional MIL methods, each bag is treated as an independent instance, which can lead to suboptimal results due to lack of exploitation of underlying structural relationships among instances. To address this limitation, we propose a new approach utilizing structural relationship among instances for improved multi-instance learning. Our method leverages graph theory to model these interdependencies, allowing us to better capture contextual information and improve prediction accuracy. Experimental evaluations demonstrate significant improvements over state-of-the-art MIL approaches across diverse application domains. This research opens up exciting opportunities for further exploration into the use of graph theory for enhanced representation and analysis of complex data structures in machine learning and computer vision applications.",1
"Super-resolution is a fundamental problem in computer vision which aims to overcome the spatial limitation of camera sensors. While significant progress has been made in single image super-resolution, most algorithms only perform well on synthetic data, which limits their applications in real scenarios. In this paper, we study the problem of real-scene single image super-resolution to bridge the gap between synthetic data and real captured images. We focus on two issues of existing super-resolution algorithms: lack of realistic training data and insufficient utilization of visual information obtained from cameras. To address the first issue, we propose a method to generate more realistic training data by mimicking the imaging process of digital cameras. For the second issue, we develop a two-branch convolutional neural network to exploit the radiance information originally-recorded in raw images. In addition, we propose a dense channel-attention block for better image restoration as well as a learning-based guided filter network for effective color correction. Our model is able to generalize to different cameras without deliberately training on images from specific camera types. Extensive experiments demonstrate that the proposed algorithm can recover fine details and clear structures, and achieve high-quality results for single image super-resolution in real scenes.",0
"This paper proposes a novel approach called Exploiting Raw Images for Real-Scene Super-Resolution (ERR), which leverages raw image data directly to improve real-scene super-resolution performance without requiring any additional training. By utilizing the unique characteristics of raw images, such as high dynamic range and precise color representation, our method achieves state-of-the-art results on several benchmark datasets while maintaining a simple architecture. We demonstrate that our technique outperforms previous methods by exploiting raw images in all parts of the SR pipeline, including feature extraction and reconstruction. Additionally, we provide detailed ablation studies to analyze the contribution of each component in our system. Our work advances the field of single image super-resolution by showing the benefits of using raw images in real scenes, paving the way for future research in this area.",1
"Deep Convolutional Neural Networks (DCNNs) are capable of obtaining powerful image representations, which have attracted great attentions in image recognition. However, they are limited in modeling orientation transformation by the internal mechanism. In this paper, we develop Orientation Convolution Networks (OCNs) for image recognition based on the proposed Landmark Gabor Filters (LGFs) that the robustness of the learned representation against changed of orientation can be enhanced. By modulating the convolutional filter with LGFs, OCNs can be compatible with any existing deep learning networks. LGFs act as a Gabor filter bank achieved by selecting $ p $ $ \left( \ll n\right) $ representative Gabor filters as andmarks and express the original Gabor filters as sparse linear combinations of these landmarks. Specifically, based on a matrix factorization framework, a flexible integration for the local and the global structure of original Gabor filters by sparsity and low-rank constraints is utilized. With the propogation of the low-rank structure, the corresponding sparsity for representation of original Gabor filter bank can be significantly promoted. Experimental results over several benchmarks demonstrate that our method is less sensitive to the orientation and produce higher performance both in accuracy and cost, compared with the existing state-of-art methods. Besides, our OCNs have few parameters to learn and can significantly reduce the complexity of training network.",0
"In recent years, convolutional neural networks (CNN) have proven to be highly effective in image recognition tasks. However, one major limitation of CNNs is their reliance on predefined kernels which lack adaptability. To address this issue, we propose orientation convolutional networks (OCN), a novel architecture that replaces fixed kernel sizes with learnable orientations. This allows OCNs to capture more complex features from images, resulting in improved performance across multiple benchmark datasets. We evaluate our approach using standard metrics such as accuracy, precision, recall, and F1 score, showing consistent improvement over traditional CNN architectures. Our results demonstrate that OCNs can effectively capture spatial context and achieve state-of-the-art performance in image recognition tasks. By introducing learnable orientations into the kernel design, we open up new possibilities in deep learning research and offer a promising direction for future work.",1
"As hashing becomes an increasingly appealing technique for large-scale image retrieval, multi-label hashing is also attracting more attention for the ability to exploit multi-level semantic contents. In this paper, we propose a novel deep hashing method for scalable multi-label image search. Unlike existing approaches with conventional objectives such as contrast and triplet losses, we employ a rank list, rather than pairs or triplets, to provide sufficient global supervision information for all the samples. Specifically, a new rank-consistency objective is applied to align the similarity orders from two spaces, the original space and the hamming space. A powerful loss function is designed to penalize the samples whose semantic similarity and hamming distance are mismatched in two spaces. Besides, a multi-label softmax cross-entropy loss is presented to enhance the discriminative power with a concise formulation of the derivative function. In order to manipulate the neighborhood structure of the samples with different labels, we design a multi-label clustering loss to cluster the hashing vectors of the samples with the same labels by reducing the distances between the samples and their multiple corresponding class centers. The state-of-the-art experimental results achieved on three public multi-label datasets, MIRFLICKR-25K, IAPRTC12 and NUS-WIDE, demonstrate the effectiveness of the proposed method.",0
"This research presents a novel approach to multi-label image search using deep hashing techniques. Our method, called rank-consistency deep hashing (RCDH), combines the power of convolutional neural networks and hash tables to achieve efficient retrieval of images from large datasets while maintaining high accuracy. By optimizing thehashing processusing a rank-based loss function that aligns the similarity between features extracted by CNNs and their corresponding distances in Hamming space, we achieve superior performance compared to state-of-the-art methods. In addition, ourmethod offers scalabilityby reducing computational complexity and memory requirements through deep hashing, making it well-suited for real-time applications such as content-based image retrieval systems. Experimental results on several benchmark data sets demonstrate the effectiveness and efficiency of RCDH for multi-label image search tasks.",1
"Automatic estimation of pain intensity from facial expressions in videos has an immense potential in health care applications. However, domain adaptation (DA) is needed to alleviate the problem of domain shifts that typically occurs between video data captured in source and target do-mains. Given the laborious task of collecting and annotating videos, and the subjective bias due to ambiguity among adjacent intensity levels, weakly-supervised learning (WSL)is gaining attention in such applications. Yet, most state-of-the-art WSL models are typically formulated as regression problems, and do not leverage the ordinal relation between intensity levels, nor the temporal coherence of multiple consecutive frames. This paper introduces a new deep learn-ing model for weakly-supervised DA with ordinal regression(WSDA-OR), where videos in target domain have coarse la-bels provided on a periodic basis. The WSDA-OR model enforces ordinal relationships among the intensity levels as-signed to the target sequences, and associates multiple relevant frames to sequence-level labels (instead of a single frame). In particular, it learns discriminant and domain-invariant feature representations by integrating multiple in-stance learning with deep adversarial DA, where soft Gaussian labels are used to efficiently represent the weak ordinal sequence-level labels from the target domain. The proposed approach was validated on the RECOLA video dataset as fully-labeled source domain, and UNBC-McMaster video data as weakly-labeled target domain. We have also validated WSDA-OR on BIOVID and Fatigue (private) datasets for sequence level estimation. Experimental results indicate that our approach can provide a significant improvement over the state-of-the-art models, allowing to achieve a greater localization accuracy.",0
"This work presents a novel deep learning approach to estimate pain intensity from weakly-labeled videos using ordinal regression. The proposed method leverages deep convolutional neural networks (CNNs) to learn feature representations that capture subtle changes in facial expressions, body language, and speech patterns associated with pain intensities. Unlike traditional approaches relying on manually engineered features or fully supervised labels, our model utilizes minimal annotations (i.e., only label distribution), resulting in improved scalability while maintaining high estimation accuracy across different datasets. We evaluate the efficacy of our technique through extensive experiments, demonstrating its effectiveness compared to state-of-the-art methods under varying conditions. Our contributions can potentially enhance automated pain assessment tools for healthcare applications, enabling more accurate monitoring, diagnosis, and treatment of pain management.",1
"Convolutional Neural Networks (CNNs) have successfully been used to classify diabetic retinopathy (DR) fundus images in recent times. However, deeper representations in CNNs may capture higher-level semantics at the expense of spatial resolution. To make predictions usable for ophthalmologists, we use a post-attention technique called Gradient-weighted Class Activation Mapping (Grad-CAM) on the penultimate layer of deep learning models to produce coarse localisation maps on DR fundus images. This is to help identify discriminative regions in the images, consequently providing evidence for ophthalmologists to make a diagnosis and potentially save lives by early diagnosis. Specifically, this study uses pre-trained weights from four state-of-the-art deep learning models to produce and compare localisation maps of DR fundus images. The models used include VGG16, ResNet50, InceptionV3, and InceptionResNetV2. We find that InceptionV3 achieves the best performance with a test classification accuracy of 96.07%, and localise lesions better and faster than the other models.",0
"This paper presents a novel approach to localising lesions in diabetic retinopathy (DR) using deep learning techniques. DR is one of the leading causes of blindness worldwide and early detection through regular eye examinations is crucial for preventing vision loss. Current methods for detecting DR rely on manual grading by experts, which can be time-consuming and inconsistent. Our proposed method uses a convolutional neural network (CNN) to automatically identify lesions in fundus images, which can then be used to alert patients and healthcare providers of potential issues before they become severe. We evaluate our model on a large dataset consisting of thousands of images from different populations across multiple countries. Results show that our method outperforms existing state-of-the-art algorithms in terms of accuracy, sensitivity, specificity, precision, recall, and F1 score. In addition, we conduct ablation studies to investigate the contributions of each component in our system and demonstrate the efficacy of each part in improving overall performance. Overall, our work represents a significant step towards automating the process of identifying and monitoring DR lesions, allowing health care professionals to focus their attention where it is most needed.",1
"The Q-learning algorithm is known to be affected by the maximization bias, i.e. the systematic overestimation of action values, an important issue that has recently received renewed attention. Double Q-learning has been proposed as an efficient algorithm to mitigate this bias. However, this comes at the price of an underestimation of action values, in addition to increased memory requirements and a slower convergence. In this paper, we introduce a new way to address the maximization bias in the form of a ""self-correcting algorithm"" for approximating the maximum of an expected value. Our method balances the overestimation of the single estimator used in conventional Q-learning and the underestimation of the double estimator used in Double Q-learning. Applying this strategy to Q-learning results in Self-correcting Q-learning. We show theoretically that this new algorithm enjoys the same convergence guarantees as Q-learning while being more accurate. Empirically, it performs better than Double Q-learning in domains with rewards of high variance, and it even attains faster convergence than Q-learning in domains with rewards of zero or low variance. These advantages transfer to a Deep Q Network implementation that we call Self-correcting DQN and which outperforms regular DQN and Double DQN on several tasks in the Atari 2600 domain.",0
"Avoid introducing any hypotheses you will test in the body of your paper. Instead concentrate on generalizing from existing work and promising novel approaches that are central to your research question(s). Finally, make no errors! The study proposed herein seeks to explore self-correction as a means for improving the effectiveness of reinforcement learning algorithms like Q-learning. By employing controlled experiments designed to rigorously test our predictions, we aim to shed light on whether incorporating self-correction into these methods can lead to improved performance across diverse environments and task domains. In particular, we highlight several key elements of self-correction which we believe hold great promise for boosting the reliability of these models: their ability to detect and correct mistakes; to minimize overfitting via selective forgetting mechanisms; and their capacity to balance exploration versus exploitation tradeoffs. Ultimately, our goal is to provide guidance to machine learning practitioners seeking to achieve optimal results through better model selection choices, tuning techniques, and algorithmic refinements. We expect our findings to open up new horizons for advancing artificial intelligence through the integration of adaptive learning principles inspired by human cognitive processes. With this foundation established, the next step involves designing realistic scenarios where these algorithms might be applied effectively within complex systems such as autonomous robots or intelligent assistants, leading towards ever more capable forms of artificial life.",1
"Student dropout prediction provides an opportunity to improve student engagement, which maximizes the overall effectiveness of learning experiences. However, researches on student dropout were mainly conducted on school dropout or course dropout, and study session dropout in a mobile learning environment has not been considered thoroughly. In this paper, we investigate the study session dropout prediction problem in a mobile learning environment. First, we define the concept of the study session, study session dropout and study session dropout prediction task in a mobile learning environment. Based on the definitions, we propose a novel Transformer based model for predicting study session dropout, DAS: Deep Attentive Study Session Dropout Prediction in Mobile Learning Environment. DAS has an encoder-decoder structure which is composed of stacked multi-head attention and point-wise feed-forward networks. The deep attentive computations in DAS are capable of capturing complex relations among dynamic student interactions. To the best of our knowledge, this is the first attempt to investigate study session dropout in a mobile learning environment. Empirical evaluations on a large-scale dataset show that DAS achieves the best performance with a significant improvement in area under the receiver operating characteristic curve compared to baseline models.",0
"This research explores the challenge of predicting student dropouts from attentive study sessions on mobile learning platforms. The rise of digital education has led to increased accessibility but also presents new challenges such as attention span limitations caused by frequent interruptions. Predicting which students might disengage before completing their learning tasks can assist instructors and designers optimize content delivery and user engagement mechanisms. We have analyzed data captured through our mobile app designed to provide video lectures for high school STEM subjects and targeted at low socioeconomic background populations. Our work focuses on building models capable of accurately classifying whether a session will end prematurely due to dropout or natural completion. To overcome technical difficulties we developed deep neural networks that learn relationships among contextual features including: duration since login; type of subject currently viewed (Maths/Science); amount of viewed material and last interaction time. By using cross validation methods to compare multiple models constructed from different subsets of features we established optimal configurations yielding performance metrics better than chance. Further experiments validated the robustness of these results under varied settings and demonstrated competitive accuracy against recent state of the art approaches in educational dropout prediction literature. Despite success attained, there still exist areas requiring improvement e.g., inclusion of individualized factors like prior GPA scores. Future works aim towards incorporation of more fine-grained personalized details into our models leading to even better precision rates. Ultimately our findings could guide developers to reconsider how educational materials should be delivered especially within developing regions where retention issues persist.",1
"Convolutional Neural Networks (CNNs) model long-range dependencies by deeply stacking convolution operations with small window sizes, which makes the optimizations difficult. This paper presents region-based non-local (RNL) operations as a family of self-attention mechanisms, which can directly capture long-range dependencies without using a deep stack of local operations. Given an intermediate feature map, our method recalibrates the feature at a position by aggregating the information from the neighboring regions of all positions. By combining a channel attention module with the proposed RNL, we design an attention chain, which can be integrated into the off-the-shelf CNNs for end-to-end training. We evaluate our method on two video classification benchmarks. The experimental results of our method outperform other attention mechanisms, and we achieve state-of-the-art performance on the Something-Something V1 dataset.",0
"This paper presents a novel approach for video classification that utilizes region-based non-local operations. We propose using self-attention mechanisms to identify regions of interest within each frame, which can then be used as anchors for computing relationships between frames. These inter-frame relationships are crucial for capturing temporal dependencies and dynamics that are essential for accurate video understanding. Our method enables efficient feature fusion between regions across different frames without resorting to expensive spatial attention mechanisms. Experiments on multiple challenging benchmarks demonstrate that our model significantly outperforms previous state-of-the-art methods for video recognition tasks while achieving better computational efficiency.",1
"Unsupervised health monitoring has gained much attention in the last decade as the most practical real-time structural health monitoring (SHM) approach. Among the proposed unsupervised techniques in the literature, there are still obstacles to robust and real-time health monitoring. These barriers include loss of information from dimensionality reduction in feature extraction steps, case-dependency of those steps, lack of a dynamic clustering, and detection results' sensitivity to user-defined parameters. This study introduces an unsupervised real-time SHM method with a mixture of low- and high-dimensional features without a case-dependent extraction scheme. Both features are used to train multi-ensembles of Generative Adversarial Networks (GAN) and one-class joint Gaussian distribution models (1-CG). A novelty detection system of limit-state functions based on GAN and 1-CG models' detection scores is constructed. The Resistance of those limit-state functions (detection thresholds) is tuned to user-defined parameters with the GAN-generated data objects by employing the Monte Carlo histogram sampling through a reliability-based analysis. The tuning makes the method robust to user-defined parameters, which is crucial as there is no rule for selecting those parameters in a real-time SHM. The proposed novelty detection framework is applied to two standard SHM datasets to illustrate its generalizability: Yellow Frame (twenty damage classes) and Z24 Bridge (fifteen damage classes). All different damage categories are identified with low sensitivity to the initial choice of user-defined parameters with both introduced dynamic and static baseline approaches with few or no false alarms.",0
"This paper presents a system for unsupervised real-time structural health monitoring using generative adversarial networks (GANs) and one-class joint Gaussian distributions. The proposed approach uses multiple ensembles of these models to increase reliability and accuracy in detecting anomalous behavior in sensor data. Our method leverages the strengths of each model type while mitigating their limitations through ensemble learning. By combining these techniques, we achieve improved performance over traditional methods that rely on single classifiers. Experimental results demonstrate the effectiveness of our approach in identifying and predicting potential damage events in large infrastructure systems such as bridges. Overall, the presented framework provides a powerful tool for asset managers, engineers, and decision makers seeking to enhance safety and longevity of critical infrastructure assets.",1
"We propose a robust in-time predictor for in-hospital COVID-19 patient's probability of requiring mechanical ventilation. A challenge in the risk prediction for COVID-19 patients lies in the great variability and irregular sampling of patient's vitals and labs observed in the clinical setting. Existing methods have strong limitations in handling time-dependent features' complex dynamics, either oversimplifying temporal data with summary statistics that lose information or over-engineering features that lead to less robust outcomes. We propose a novel in-time risk trajectory predictive model to handle the irregular sampling rate in the data, which follows the dynamics of risk of performing mechanical ventilation for individual patients. The model incorporates the Multi-task Gaussian Process using observed values to learn the posterior joint multi-variant conditional probability and infer the missing values on a unified time grid. The temporal imputed data is fed into a multi-objective self-attention network for the prediction task. A novel positional encoding layer is proposed and added to the network for producing in-time predictions. The positional layer outputs a risk score at each user-defined time point during the entire hospital stay of an inpatient. We frame the prediction task into a multi-objective learning framework, and the risk scores at all time points are optimized altogether, which adds robustness and consistency to the risk score trajectory prediction. Our experimental evaluation on a large database with nationwide in-hospital patients with COVID-19 also demonstrates that it improved the state-of-the-art performance in terms of AUC (Area Under the receiver operating characteristic Curve) and AUPRC (Area Under the Precision-Recall Curve) performance metrics, especially at early times after hospital admission.",0
"Abstract: This paper presents a real-time prediction model that utilizes multi-task learning with a multi-objective self-attention network to predict key clinical parameters related to mechanical ventilation in patients diagnosed with COVID-19. Our approach combines both task specific and global feature attention mechanisms which allows for accurate predictions across multiple tasks while still allowing our model to capture important interdependencies among them. Experiments conducted on two large publicly available datasets demonstrate improved performance compared to existing approaches, resulting in better overall accuracy (up to 4% improvement) and lower error rates.",1
"Researchers have compared machine learning (ML) classifiers and discrete choice models (DCMs) in predicting travel behavior, but the generalizability of the findings is limited by the specifics of data, contexts, and authors' expertise. This study seeks to provide a generalizable empirical benchmark by comparing hundreds of ML and DCM classifiers in a highly structured manner. The experiments evaluate both prediction accuracy and computational cost by spanning four hyper-dimensions, including 105 ML and DCM classifiers from 12 model families, 3 datasets, 3 sample sizes, and 3 outputs. This experimental design leads to an immense number of 6,970 experiments, which are corroborated with a meta dataset of 136 experiment points from 35 previous studies. This study is hitherto the most comprehensive and almost exhaustive comparison of the classifiers for travel behavioral prediction. We found that the ensemble methods and deep neural networks achieve the highest predictive performance, but at a relatively high computational cost. Random forests are the most computationally efficient, balancing between prediction and computation. While discrete choice models offer accuracy with only 3-4 percentage points lower than the top ML classifiers, they have much longer computational time and become computationally impossible with large sample size, high input dimensions, or simulation-based estimation. The relative ranking of the ML and DCM classifiers is highly stable, while the absolute values of the prediction accuracy and computational time have large variations. Overall, this paper suggests using deep neural networks, model ensembles, and random forests as baseline models for future travel behavior prediction. For choice modeling, the DCM community should switch more attention from fitting models to improving computational efficiency, so that the DCMs can be widely adopted in the big data context.",0
"This article provides an empirical comparison between hundreds of machine learning algorithms and discrete choice models for predicting travel behaviour. By evaluating their performance on publicly available data sets and simulated scenarios, we can better understand which method works best under certain conditions, identify areas where current approaches could improve, and provide guidance for practitioners selecting an approach. Despite different strengths and weaknesses among methods, our findings suggest that no single algorithm consistently outperforms others across all cases. We also discuss how our results relate to existing literature, highlighting the importance of carefully considering both model assumptions and context when making predictions. In summary, this research offers valuable insights into predicting travel behaviour through state-of-the-art techniques and contributes to ongoing efforts towards sustainability in transportation planning.",1
"In recent years, extending variational autoencoder's framework to learn disentangled representations has received much attention. We address this problem by proposing a framework capable of disentangling class-related and class-independent factors of variation in data. Our framework employs an attention mechanism in its latent space in order to improve the process of extracting class-related factors from data. We also deal with the multimodality of data distribution by utilizing mixture models as learnable prior distributions, as well as incorporating the Bhattacharyya coefficient in the objective function to prevent highly overlapping mixtures. Our model's encoder is further trained in a semi-supervised manner, with a small fraction of labeled data, to improve representations' interpretability. Experiments show that our framework disentangles class-related and class-independent factors of variation and learns interpretable features. Moreover, we demonstrate our model's performance with quantitative and qualitative results on various datasets.",0
"This semi supervised disentanglement approach can enable discovering class related factors that lead to variation on target variables while keeping class independent factors fixed .It is done by applying a VAE(Variational Autoencoder) which maps samples from high dimensional inputs to lower dimensions latent representation Z . Samples are projected onto two separate one dimensional subspaces corresponding to class dependent variations U_c ,U_nc along with a global mean gamma .The class dependent variations are used to predict labels and class independent part uses the information shared between different classes as side information .This method has been tested on several benchmark datasets like omniglot and MNIST where low data augmentation was performed where even if more labeled training samples were added there was no significant improvement seen but rather performance degraded and in all such cases our method outperformed and in some cases even better than fully supervised baseline methods on heldout test accuracy as well as qualitatively visualizing discovered representations for both discriminative task like digit classification and generative tasks like generating new images resembling originals generated through interpolation in the learned embedding spaces .",1
"Conditional Variational Auto Encoders (VAE) are gathering significant attention as an Explainable Artificial Intelligence (XAI) tool. The codes in the latent space provide a theoretically sound way to produce counterfactuals, i.e. alterations resulting from an intervention on a targeted semantic feature. To be applied on real images more complex models are needed, such as Hierarchical CVAE. This comes with a challenge as the naive conditioning is no longer effective. In this paper we show how relaxing the effect of the posterior leads to successful counterfactuals and we introduce VAEX an Hierarchical VAE designed for this approach that can visually audit a classifier in applications.",0
"This paper proposes a novel approach using hierarchical variational autoencoders (VAEs) to generate visual counterfactual examples. VAEs have been shown to effectively model data distributions and their variations by learning underlying latent representations that capture important patterns in the data. In contrast, traditional approaches based on generative models such as GANs suffer from mode collapse and lack stability during training, making them less effective at generating high quality samples. Our method utilizes two levels of hierarchy within the VAE framework: a top-level encoder network that generates hypothetical scenarios given natural language descriptions, and a bottom-level decoder network responsible for producing plausible images corresponding to these hypotheses. We demonstrate experimentally that our approach outperforms previous state-of-the-art methods across multiple benchmark datasets and leads to higher fidelity predictions while requiring fewer parameters. Additionally, we showcase several applications of our system including image completion tasks and video prediction problems, providing strong evidence for the broad utility of our technique. Overall, we believe our work represents an exciting step forward towards building more capable machine intelligence systems able to reason and generalize over complex domains.",1
"Numerical weather forecasting on high-resolution physical models consume hours of computations on supercomputers. Application of deep learning and machine learning methods in forecasting revealed new solutions in this area. In this paper, we forecast high-resolution numeric weather data using both input weather data and observations by providing a novel deep learning architecture. We formulate the problem as spatio-temporal prediction. Our model is composed of Convolutional Long-short Term Memory, and Convolutional Neural Network units with encoder-decoder structure. We enhance the short-long term performance and interpretability with an attention and a context matcher mechanism. We perform experiments on high-scale, real-life, benchmark numerical weather dataset, ERA5 hourly data on pressure levels, and forecast the temperature. The results show significant improvements in capturing both spatial and temporal correlations with attention matrices focusing on different parts of the input series. Our model obtains the best validation and the best test score among the baseline models, including ConvLSTM forecasting network and U-Net. We provide qualitative and quantitative results and show that our model forecasts 10 time steps with 3 hour frequency with an average of 2 degrees error. Our code and the data are publicly available.",0
"This should discuss any important results obtained by the authors. The use of deep learning methods such as convolutional neural networks (CNN) has greatly improved weather forecasting accuracy in recent years. However, due to the complexity of spatio-temporal relationships involved in weather patterns, traditional CNN models have limited performance in capturing these correlations accurately. To address this issue, this study proposes a novel approach that utilizes attention mechanisms within convolutional LSTM frameworks. Our method shows promising results in terms of improving the accuracy and stability of spatial temporal predictions. By applying an encoder-decoder architecture, our model can capture both short term dynamics and spatial relationships between grid points. Our experimental analysis demonstrates significant improvements over previous state-of-the-art approaches, including increased accuracy and robustness under extreme weather conditions. Furthermore, we provide an extensive ablation study to demonstrate the effectiveness of each component in our proposed framework. Overall, our contributions highlight the potential benefits of incorporating attention mechanisms into spatio-temporal weather forecasting models, paving the way for future research opportunities.",1
"In the face recognition application scenario, we need to process facial images captured in various conditions, such as at night by near-infrared (NIR) surveillance cameras. The illumination difference between NIR and visible-light (VIS) causes a domain gap between facial images, and the variations in pose and emotion also make facial matching more difficult. Heterogeneous face recognition (HFR) has difficulties in domain discrepancy, and many studies have focused on extracting domain-invariant features, such as facial part relational information. However, when pose variation occurs, the facial component position changes, and a different part relation is extracted. In this paper, we propose a part relation attention module that crops facial parts obtained through a semantic mask and performs relational modeling using each of these representative features. Furthermore, we suggest component adaptive triplet loss function using adaptive weights for each part to reduce the intra-class identity regardless of the domain as well as pose. Finally, our method exhibits a performance improvement in the CASIA NIR-VIS 2.0 and achieves superior result in the BUAA-VisNir with large pose and emotion variations.",0
"Face recognition technology has advanced significantly over recent years due to advances in computer vision techniques such as deep learning. One challenge facing current face recognition systems is their limited performance under different lighting conditions. In particular, near infrared (NIR) illumination can result in poor recognition accuracy since traditional RGB cameras are less sensitive to NIR wavelengths. To address this issue, we propose a novel approach that utilizes a part adaptive and relation attention mechanism to transform NIR facial features into visible spectrum ones while preserving discriminative details. Our method leverages a pre-trained convolutional neural network to learn effective feature representations from both NIR and VIS domains. An attention mechanism then selectively focuses on key facial regions to enhance robustness against variations in pose and expression. Additionally, our method models inter-relations between different facial parts using graph convolutional networks to capture complex dependencies across different regions. Experiments conducted on two benchmark datasets show substantial improvements over state-of-the-art methods, validating the effectiveness of our proposed approach.",1
"Many problems in computer vision require dealing with sparse, unordered data in the form of point clouds. Permutation-equivariant networks have become a popular solution-they operate on individual data points with simple perceptrons and extract contextual information with global pooling. This can be achieved with a simple normalization of the feature maps, a global operation that is unaffected by the order. In this paper, we propose Attentive Context Normalization (ACN), a simple yet effective technique to build permutation-equivariant networks robust to outliers. Specifically, we show how to normalize the feature maps with weights that are estimated within the network, excluding outliers from this normalization. We use this mechanism to leverage two types of attention: local and global-by combining them, our method is able to find the essential data points in high-dimensional space to solve a given task. We demonstrate through extensive experiments that our approach, which we call Attentive Context Networks (ACNe), provides a significant leap in performance compared to the state-of-the-art on camera pose estimation, robust fitting, and point cloud classification under noise and outliers. Source code: https://github.com/vcg-uvic/acne.",0
"In this work, we present ACNe (Attentive Context Normalization), a novel normalization technique that enables robust permutation equivariance in deep learning models. We show how the use of attention mechanisms can effectively reduce the impact of input order on model predictions by allowing the model to focus on meaningful features. Our approach enhances existing normalization techniques such as batch renormalization and layer normalization. Experimental results demonstrate the effectiveness of our method across multiple benchmark datasets including image classification, object detection, and semantic segmentation tasks.",1
"Knowledge tracing, the act of modeling a student's knowledge through learning activities, is an extensively studied problem in the field of computer-aided education. Although models with attention mechanism have outperformed traditional approaches such as Bayesian knowledge tracing and collaborative filtering, they share two limitations. Firstly, the models rely on shallow attention layers and fail to capture complex relations among exercises and responses over time. Secondly, different combinations of queries, keys and values for the self-attention layer for knowledge tracing were not extensively explored. Usual practice of using exercises and interactions (exercise-response pairs) as queries and keys/values respectively lacks empirical support. In this paper, we propose a novel Transformer based model for knowledge tracing, SAINT: Separated Self-AttentIve Neural Knowledge Tracing. SAINT has an encoder-decoder structure where exercise and response embedding sequence separately enter the encoder and the decoder respectively, which allows to stack attention layers multiple times. To the best of our knowledge, this is the first work to suggest an encoder-decoder model for knowledge tracing that applies deep self-attentive layers to exercises and responses separately. The empirical evaluations on a large-scale knowledge tracing dataset show that SAINT achieves the state-of-the-art performance in knowledge tracing with the improvement of AUC by 1.8% compared to the current state-of-the-art models.",0
"This paper proposes new ways to compute queries, keys, and values that are well suited for knowledge tracing, which helps students improve their learning by tracking their progress on different topics over time. We begin by introducing some background on how current methods fail to address key challenges in query computation. Then we present our novel approach, including details on how to effectively capture the context of a studentâ€™s learning activities, as well as how to accurately represent concepts, skills, and competencies. Our method offers more flexibility than existing techniques while still allowing for efficient implementation. Finally, we evaluate our method using real data from three large datasets and compare it against state-of-the-art approaches, demonstrating improved accuracy and effectiveness. Overall, this work represents an important step forward towards better understanding and improving knowledge tracing, with potential applications across education research and practice.",1
"False positive is one of the most serious problems brought by agnostic domain shift in domain adaptive pedestrian detection. However, it is impossible to label each box in countless target domains. Therefore, it yields our attention to suppress false positive in each target domain in an unsupervised way. In this paper, we model an object detection task into a ranking task among positive and negative boxes innovatively, and thus transform a false positive suppression problem into a box re-ranking problem elegantly, which makes it feasible to solve without manual annotation. An attached problem during box re-ranking appears that no labeled validation data is available for cherrypicking. Considering we aim to keep the detection of true positive unchanged, we propose box number alignment, a self-supervised evaluation metric, to prevent the optimized model from capacity degeneration. Extensive experiments conducted on cross-domain pedestrian detection datasets have demonstrated the effectiveness of our proposed framework. Furthermore, the extension to two general unsupervised domain adaptive object detection benchmarks also supports our superiority to other state-of-the-arts.",0
"This paper presents a novel approach for unsupervised domain adaptive pedestrian detection using box re-ranking. By leveraging the power of deep learning techniques, we aim to significantly reduce false positive detections while maintaining high recall rates across multiple domains. Our proposed method consists of two stages - (i) initial object proposal generation using the popular YOLOv7 framework, followed by (ii) unsupervised suppression based on geometric constraints imposed on bounding boxes. Experimental results obtained from diverse datasets demonstrate that our method outperforms state-of-the-art approaches, achieving significant reduction in false positives without sacrificing accuracy. Overall, our work paves the way towards efficient real-world deployment of autonomous systems capable of operating under variable road conditions.",1
"Recently, zero-shot learning (ZSL) emerged as an exciting topic and attracted a lot of attention. ZSL aims to classify unseen classes by transferring the knowledge from seen classes to unseen classes based on the class description. Despite showing promising performance, ZSL approaches assume that the training samples from all seen classes are available during the training, which is practically not feasible. To address this issue, we propose a more generalized and practical setup for ZSL, i.e., continual ZSL (CZSL), where classes arrive sequentially in the form of a task and it actively learns from the changing environment by leveraging the past experience. Further, to enhance the reliability, we develop CZSL for a single head continual learning setting where task identity is revealed during the training process but not during the testing. To avoid catastrophic forgetting and intransigence, we use knowledge distillation and storing and replay the few samples from previous tasks using a small episodic memory. We develop baselines and evaluate generalized CZSL on five ZSL benchmark datasets for two different settings of continual learning: with and without class incremental. Moreover, CZSL is developed for two types of variational autoencoders, which generates two types of features for classification: (i) generated features at output space and (ii) generated discriminative features at the latent space. The experimental results clearly indicate the single head CZSL is more generalizable and suitable for practical applications.",0
"Zero-shot learning has been gaining interest as a means to enable machines to learn new concepts without any explicit supervision. However, current zero-shot approaches have limitations such as lack of generalization across domains or tasks and high computational cost. This paper proposes generalized continual zero-shot learning (CZSL), which addresses these issues by incorporating meta-learning and multi-task learning techniques. Experiments show that our approach outperforms existing methods on various datasets while requiring less computation time, demonstrating improved generalization abilities. Our findings provide valuable insights into the development of efficient zero-shot learning models applicable to real-world scenarios.",1
"Cycling is a promising sustainable mode for commuting and leisure in cities, however, the fear of getting hit or fall reduces its wide expansion as a commuting mode. In this paper, we introduce a novel method called CyclingNet for detecting cycling near misses from video streams generated by a mounted frontal camera on a bike regardless of the camera position, the conditions of the built, the visual conditions and without any restrictions on the riding behaviour. CyclingNet is a deep computer vision model based on convolutional structure embedded with self-attention bidirectional long-short term memory (LSTM) blocks that aim to understand near misses from both sequential images of scenes and their optical flows. The model is trained on scenes of both safe rides and near misses. After 42 hours of training on a single GPU, the model shows high accuracy on the training, testing and validation sets. The model is intended to be used for generating information that can draw significant conclusions regarding cycling behaviour in cities and elsewhere, which could help planners and policy-makers to better understand the requirement of safety measures when designing infrastructure or drawing policies. As for future work, the model can be pipelined with other state-of-the-art classifiers and object detectors simultaneously to understand the causality of near misses based on factors related to interactions of road-users, the built and the natural environments.",0
"This research presents CyclingNet, a novel method for detecting close calls involving cyclists using computer vision techniques based on convolutional neural networks (CNNs). CyclingNet processes large volumes of unstructured data captured by cameras mounted on bicycles, roadside sensors, dashcams, and other sources. Our approach allows us to identify potentially hazardous incidents that may have gone unnoticed due to the sheer volume of raw footage available. We train our model on thousands of annotated frames collected from different cities across Europe and North America, where cycling infrastructure varies widely, thus making our solution applicable worldwide. In conclusion, our work contributes significantly towards enhancing cycling safety through better understanding of near misses, which can guide future planning and policy decisions.",1
"Change detection (CD) is an important problem in remote sensing, especially in disaster time for urban management. Most existing traditional methods for change detection are categorized based on pixel or objects. Object-based models are preferred to pixel-based methods for handling very high-resolution remote sensing (VHR RS) images. Such methods can benefit from the ongoing research on deep learning. In this paper, a fully automatic change-detection algorithm on VHR RS images is proposed that deploys Fully Convolutional Siamese Concatenate networks (FC-Siam-Conc). The proposed method uses preprocessing and an attention gate layer to improve accuracy. Gaussian attention (GA) as a soft visual attention mechanism is used for preprocessing. GA helps the network to handle feature maps like biological visual systems. Since the GA parameters cannot be adjusted during network training, an attention gate layer is introduced to play the role of GA with parameters that can be tuned among other network parameters. Experimental results obtained on Onera Satellite Change Detection (OSCD) and RIVER-CD datasets confirm the superiority of the proposed architecture over the state-of-the-art algorithms.",0
"Title: ""Urban change detection using fully convolutional siamese concatenate networks with attention"" Abstract: This paper presents a novel approach for urban change detection using deep learning techniques. We propose a fully convolutional siamese concatenate network architecture that utilizes multi-scale feature extraction and nonlinear similarity measurement to accurately detect changes in urban areas over time. Our method incorporates an attention mechanism to selectively focus on relevant features and improve model interpretability. To evaluate our proposed method, we conduct experiments on two public datasets and compare our results against state-of-the-art approaches. Results demonstrate that our approach outperforms existing methods in terms of accuracy and robustness while providing detailed pixelwise maps of change regions. This study has important implications for applications such as environmental monitoring, infrastructure management, and disaster response planning.",1
"3D object detection is receiving increasing attention from both industry and academia thanks to its wide applications in various fields. In this paper, we propose the Point-Voxel Region based Convolution Neural Networks (PV-RCNNs) for accurate 3D detection from point clouds. First, we propose a novel 3D object detector, PV-RCNN-v1, which employs the voxel-to-keypoint scene encoding and keypoint-to-grid RoI feature abstraction two novel steps. These two steps deeply incorporate both 3D voxel CNN and PointNet-based set abstraction for learning discriminative point-cloud features. Second, we propose a more advanced framework, PV-RCNN-v2, for more efficient and accurate 3D detection. It consists of two major improvements, where the first one is the sectorized proposal-centric strategy for efficiently producing more representative and uniformly distributed keypoints, and the second one is the VectorPool aggregation to replace set abstraction for better aggregating local point-cloud features with much less resource consumption. With these two major modifications, our PV-RCNN-v2 runs more than twice as fast as the v1 version while still achieving better performance on the large-scale Waymo Open Dataset with 150m * 150m detection range. Extensive experiments demonstrate that our proposed PV-RCNNs significantly outperform previous state-of-the-art 3D detection methods on both the Waymo Open Dataset and the highly-competitive KITTI benchmark.",0
"This paper presents PV-RCNN++, a point cloud object detection algorithm that utilizes point-voxel feature abstraction with local vector representation (PV-RCNN++) to improve accuracy and efficiency compared to traditional approaches. By using voxels to discretize point clouds into volumetric grids and applying convolutional neural networks on these grids, our method can effectively capture spatial context while reducing computational cost. Additionally, we propose a novel aggregation strategy called local vector representation which enables more efficient encoding of local features within each grid cell. Our evaluation shows significant improvement over state-of-the-art methods, achieving better performance at reduced inference time across multiple benchmark datasets. These results demonstrate the effectiveness of our approach in addressing key challenges faced by current point cloud processing algorithms.",1
"Finding good correspondences is a critical prerequisite in many feature based tasks. Given a putative correspondence set of an image pair, we propose a neural network which finds correct correspondences by a binary-class classifier and estimates relative pose through classified correspondences. First, we analyze that due to the imbalance in the number of correct and wrong correspondences, the loss function has a great impact on the classification results. Thus, we propose a new Guided Loss that can directly use evaluation criterion (Fn-measure) as guidance to dynamically adjust the objective function during training. We theoretically prove that the perfect negative correlation between the Guided Loss and Fn-measure, so that the network is always trained towards the direction of increasing Fn-measure to maximize it. We then propose a hybrid attention block to extract feature, which integrates the Bayesian attentive context normalization (BACN) and channel-wise attention (CA). BACN can mine the prior information to better exploit global context and CA can capture complex channel context to enhance the channel awareness of the network. Finally, based on our Guided Loss and hybrid attention block, a cascade network is designed to gradually optimize the result for more superior performance. Experiments have shown that our network achieves the state-of-the-art performance on benchmark datasets. Our code will be available in https://github.com/wenbingtao/GLHA.",0
"Here is an example of how you could write such an abstract: ""This paper proposes a new method called Cascade Network with Guided Loss and Hybrid Attention (CNwGLAH) that effectively finds good correspondences by leveraging both hard and soft attention mechanisms as well as guiding losses to better align feature distributions in deep neural networks. Traditional methods have relied on heuristics or hand engineering features which can lead to suboptimal results. Instead, CNwGLAH learns to find good correspondences automatically through end-to-end learning, achieving state-of-the-art performance on several benchmark datasets.""",1
"For the sake of recognizing and classifying textile defects, deep learning-based methods have been proposed and achieved remarkable success in single-label textile images. However, detecting multi-label defects in a textile image remains challenging due to the coexistence of multiple defects and small-size defects. To address these challenges, a multi-level, multi-attentional deep learning network (MLMA-Net) is proposed and built to 1) increase the feature representation ability to detect small-size defects; 2) generate a discriminative representation that maximizes the capability of attending the defect status, which leverages higher-resolution feature maps for multiple defects. Moreover, a multi-label object detection dataset (DHU-ML1000) in textile defect images is built to verify the performance of the proposed model. The results demonstrate that the network extracts more distinctive features and has better performance than the state-of-the-art approaches on the real-world industrial dataset.",0
"In this paper we propose a novel deep neural network architecture that outperforms all published results on multi-label object detection benchmarks for texture images by utilizing our unique attention scheme which focuses both at different levels (local/global) as well as multiple attentions per level based on their semantic features. Our approach provides better localization accuracy while maintaining competitive classification performance compared to other state-of-the-art approaches. We demonstrate significant improvements over previous methods through extensive ablation studies which justify the importance of each component of our framework such as pyramid feature extraction, channel-wise local attention maps generation, and two stage training strategy. Furthermore, we evaluate the effectiveness of the proposed model on three diverse datasets including real industrial settings thus proving its generalizability. Finally, future work sections concludes this study along with discussion of potential use cases for object detection in non cooperative settings where capturing quality images can become very challenging thus making this technology suitable for large scale adoption.",1
"Fine-grained visual classification is a challenging task that recognizes the sub-classes belonging to the same meta-class. Large inter-class similarity and intra-class variance is the main challenge of this task. Most exiting methods try to solve this problem by designing complex model structures to explore more minute and discriminative regions. In this paper, we argue that mining multi-regional multi-grained features is precisely the key to this task. Specifically, we introduce a new loss function, termed top-down spatial attention loss (TDSA-Loss), which contains a multi-stage channel constrained module and a top-down spatial attention module. The multi-stage channel constrained module aims to make the feature channels in different stages category-aligned. Meanwhile, the top-down spatial attention module uses the attention map generated by high-level aligned feature channels to make middle-level aligned feature channels to focus on particular regions. Finally, we can obtain multiple discriminative regions on high-level feature channels and obtain multiple more minute regions within these discriminative regions on middle-level feature channels. In summary, we obtain multi-regional multi-grained features. Experimental results over four widely used fine-grained image classification datasets demonstrate the effectiveness of the proposed method. Ablative studies further show the superiority of two modules in the proposed method. Codes are available at: https://github.com/dongliangchang/Top-Down-Spatial-Attention-Loss.",0
"This paper presents an approach that simultaneously learns multi-regional multi-scale features from fine-grained image classification tasks, which significantly improves performance compared to state-of-the-art methods. Our method uses a network consisting of multiple branches with dilated convolutions, which allows us to capture hierarchical representations at different spatial resolutions and scales within each region. Additionally, our approach leverages attention mechanisms to focus on discriminative regions and automatically identify informative parts of the input images. Extensive experiments on three challenging benchmark datasets demonstrate significant improvements over previous works. Finally, we show that by using our learned features as inputs into SVM classifiers, we achieve further improved results, highlighting the effectiveness of our approach. Overall, our work provides new insights into the design of deep networks for fine-grained visual recognition problems, paving the way towards more accurate solutions in real-world applications.",1
"Although CNNs are widely considered as the state-of-the-art models in various applications of image analysis, one of the main challenges still open is the training of a CNN on high resolution images. Different strategies have been proposed involving either a rescaling of the image or an individual processing of parts of the image. Such strategies cannot be applied to images, such as gigapixel histopathological images, for which a high reduction in resolution inherently effects a loss of discriminative information, and in respect of which the analysis of single parts of the image suffers from a lack of global information or implies a high workload in terms of annotating the training images in such a way as to select significant parts. We propose a method for the analysis of gigapixel histopathological images solely by using weak image-level labels. In particular, two analysis tasks are taken into account: a binary classification and a prediction of the tumor proliferation score. Our method is based on a CNN structure consisting of a compressing path and a learning path. In the compressing path, the gigapixel image is packed into a grid-based feature map by using a residual network devoted to the feature extraction of each patch into which the image has been divided. In the learning path, attention modules are applied to the grid-based feature map, taking into account spatial correlations of neighboring patch features to find regions of interest, which are then used for the final whole slide analysis. Our method integrates both global and local information, is flexible with regard to the size of the input images and only requires weak image-level labels. Comparisons with different methods of the state-of-the-art on two well known datasets, Camelyon16 and TUPAC16, have been made to confirm the validity of the proposed model.",0
"This abstract describes a method for analyzing gigapixel histopathological images using attention-based neural networks (ABNN). ABNN was selected due to its ability to process large image data sets effectively while still producing high quality results. The performance of the ABNN system was evaluated on a set of test images that were labeled by expert pathologists. Results showed that ABNN outperformed other state-of-the-art methods in terms of accuracy and speed. Additionally, ABNN was able to detect subtle features in the images that were missed by human annotators. Overall, the use of ABNN shows promise as a tool for improving the diagnostic accuracy of digital pathology systems.",1
"Attention mechanisms, which enable a neural network to accurately focus on all the relevant elements of the input, have become an essential component to improve the performance of deep neural networks. There are mainly two attention mechanisms widely used in computer vision studies, \textit{spatial attention} and \textit{channel attention}, which aim to capture the pixel-level pairwise relationship and channel dependency, respectively. Although fusing them together may achieve better performance than their individual implementations, it will inevitably increase the computational overhead. In this paper, we propose an efficient Shuffle Attention (SA) module to address this issue, which adopts Shuffle Units to combine two types of attention mechanisms effectively. Specifically, SA first groups channel dimensions into multiple sub-features before processing them in parallel. Then, for each sub-feature, SA utilizes a Shuffle Unit to depict feature dependencies in both spatial and channel dimensions. After that, all sub-features are aggregated and a ""channel shuffle"" operator is adopted to enable information communication between different sub-features. The proposed SA module is efficient yet effective, e.g., the parameters and computations of SA against the backbone ResNet50 are 300 vs. 25.56M and 2.76e-3 GFLOPs vs. 4.12 GFLOPs, respectively, and the performance boost is more than 1.34% in terms of Top-1 accuracy. Extensive experimental results on common-used benchmarks, including ImageNet-1k for classification, MS COCO for object detection, and instance segmentation, demonstrate that the proposed SA outperforms the current SOTA methods significantly by achieving higher accuracy while having lower model complexity. The code and models are available at https://github.com/wofmanaf/SA-Net.",0
"In this work we present Shuffle Attention (SA), a novel module that can be integrated within any deep convolutional neural network (CNN) architecture to improve feature learning and image classification accuracy. Our method adopts and modifies recent ideas on attention mechanisms to selectively combine the outputs from multiple stages of processing within a CNN. Unlike most existing approaches which simply concatenate features, our scheme dynamically shuffles them before applying a lightweight attention head, enabling better capturing of dependencies between different regions in an input. We apply the SA module at key points inside popular architectures such as ResNets and DenseNets, demonstrating improved performance across several benchmark datasets including CIFAR-10/100, ImageNet, Pascal VOC2007, COCO, and SVHN.",1
"Despite data augmentation being a de facto technique for boosting the performance of deep neural networks, little attention has been paid to developing augmentation strategies for generative adversarial networks (GANs). To this end, we introduce a novel augmentation scheme designed specifically for GAN-based semantic image synthesis models. We propose to randomly warp object shapes in the semantic label maps used as an input to the generator. The local shape discrepancies between the warped and non-warped label maps and images enable the GAN to learn better the structural and geometric details of the scene and thus to improve the quality of generated images. While benchmarking the augmented GAN models against their vanilla counterparts, we discover that the quantification metrics reported in the previous semantic image synthesis studies are strongly biased towards specific semantic classes as they are derived via an external pre-trained segmentation network. We therefore propose to improve the established semantic image synthesis evaluation scheme by analyzing separately the performance of generated images on the biased and unbiased classes for the given segmentation network. Finally, we show strong quantitative and qualitative improvements obtained with our augmentation scheme, on both class splits, using state-of-the-art semantic image synthesis models across three different datasets. On average across COCO-Stuff, ADE20K and Cityscapes datasets, the augmented models outperform their vanilla counterparts by ~3 mIoU and ~10 FID points.",0
"In this study we investigate ways to improve augmentations techniques in order to generate higher quality synthetic images for computer vision tasks such as object recognition. We explore different techniques including data augmentation, image generation, model interpretation, adversarial training methods and transfer learning, focusing on their effectiveness at improving task performance. Our results show that using these augmentation methods can produce significant improvements over state of the art methods in terms of both accuracy and interpretability. These improvements have important implications for future research into artificial intelligence systems, particularly those involved in the creation of new and innovative technologies based on generative models. By providing high quality images which accurately represent objects and scenes, our approach paves the way towards more advanced and reliable image analysis algorithms. Ultimately, this has exciting applications in fields like healthcare, security and self driving vehicles where reliable imaging and accurate recognition is crucial.",1
"Rising concern for the societal implications of artificial intelligence systems has inspired demands for greater transparency and accountability. However the datasets which empower machine learning are often used, shared and re-used with little visibility into the processes of deliberation which led to their creation. Which stakeholder groups had their perspectives included when the dataset was conceived? Which domain experts were consulted regarding how to model subgroups and other phenomena? How were questions of representational biases measured and addressed? Who labeled the data? In this paper, we introduce a rigorous framework for dataset development transparency which supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields a set of documents that facilitate improved communication and decision-making, as well as drawing attention the value and necessity of careful data work. The proposed framework is intended to contribute to closing the accountability gap in artificial intelligence systems, by making visible the often overlooked work that goes into dataset creation.",0
"Title should be something like ""Accounting for Human Errors in Machine Learning""",1
"Forecasting spatio-temporal correlated time series of sensor values is crucial in urban applications, such as air pollution alert, biking resource management, and intelligent transportation systems. While recent advances exploit graph neural networks (GNN) to better learn spatial and temporal dependencies between sensors, they cannot model time-evolving spatio-temporal correlation (STC) between sensors, and require pre-defined graphs, which are neither always available nor totally reliable, and target at only a specific type of sensor data at one time. Moreover, since the form of time-series fluctuation is varied across sensors, a model needs to learn fluctuation modulation. To tackle these issues, in this work, we propose a novel GNN-based model, Attention-adjusted Graph Spatio-Temporal Network (AGSTN). In AGSTN, multi-graph convolution with sequential learning is developed to learn time-evolving STC. Fluctuation modulation is realized by a proposed attention adjustment mechanism. Experiments on three sensor data, air quality, bike demand, and traffic flow, exhibit that AGSTN outperforms the state-of-the-art methods.",0
"In recent years, there has been increasing interest in forecasting short-term sensor values in urban areas using machine learning techniques. One key challenge in these settings is that data may be unpredictably collected at different times in some places than others, leading to complex spatiotemporal patterns that cannot easily be captured by traditional models. This paper presents AGSTN, a novel model designed specifically for handling such situations. AGSTN learns attention-adjusted graph spatiotemporal networks, effectively capturing how important features vary across both space and time even if they were missed during training. We demonstrate AGSTN on two challenging real-world datasets from transportation sensors, showing marked improvements over existing methods including popular deep learning approaches like LSTM and Transformer-based architectures as well as state-of-the-art baselines. These results suggest AGSTN can serve as a powerful tool for future work in urban sensor value prediction.",1
"In this paper, we propose a novel SpatioTemporal convolutional Dense Network (STDNet) to address the video-based crowd counting problem, which contains the decomposition of 3D convolution and the 3D spatiotemporal dilated dense convolution to alleviate the rapid growth of the model size caused by the Conv3D layer. Moreover, since the dilated convolution extracts the multiscale features, we combine the dilated convolution with the channel attention block to enhance the feature representations. Due to the error that occurs from the difficulty of labeling crowds, especially for videos, imprecise or standard-inconsistent labels may lead to poor convergence for the model. To address this issue, we further propose a new patch-wise regression loss (PRL) to improve the original pixel-wise loss. Experimental results on three video-based benchmarks, i.e., the UCSD, Mall and WorldExpo'10 datasets, show that STDNet outperforms both image- and video-based state-of-the-art methods. The source codes are released at \url{https://github.com/STDNet/STDNet}.",0
"This paper presents a novel approach to crowd estimation using video data. Our method uses spatiotemporal dilated convolutions to capture global contextual features from input videos and uncertain matching techniques to estimate the density of crowds in each frame. We propose that combining these two approaches allows for more accurate predictions while minimizing computational cost compared to traditional methods. Extensive experiments on three benchmark datasets demonstrate that our proposed method outperforms state-of-the-art algorithms by achieving higher accuracy and efficiency, making it a promising tool for real-world applications such as traffic management and emergency response planning.",1
"Deep learning (DL) has demonstrated its powerful capabilities in the field of image inpainting, which could produce visually plausible results. Meanwhile, the malicious use of advanced image inpainting tools (e.g. removing key objects to report fake news) has led to increasing threats to the reliability of image data. To fight against the inpainting forgeries, in this work, we propose a novel end-to-end Generalizable Image Inpainting Detection Network (GIID-Net), to detect the inpainted regions at pixel accuracy. The proposed GIID-Net consists of three sub-blocks: the enhancement block, the extraction block and the decision block. Specifically, the enhancement block aims to enhance the inpainting traces by using hierarchically combined special layers. The extraction block, automatically designed by Neural Architecture Search (NAS) algorithm, is targeted to extract features for the actual inpainting detection tasks. In order to further optimize the extracted latent features, we integrate global and local attention modules in the decision block, where the global attention reduces the intra-class differences by measuring the similarity of global features, while the local attention strengthens the consistency of local features. Furthermore, we thoroughly study the generalizability of our GIID-Net, and find that different training data could result in vastly different generalization capability. Extensive experimental results are presented to validate the superiority of the proposed GIID-Net, compared with the state-of-the-art competitors. Our results would suggest that common artifacts are shared across diverse image inpainting methods. Finally, we build a public inpainting dataset of 10K image pairs for the future research in this area.",0
"Artificial intelligence has made tremendous advances in image manipulation techniques such as image generation, editing, and super resolution. However, detecting tampered images remains challenging due to their realism. To address this issue, we propose GIID-Net (Generalizable Image Inpainting Detection via Neural Architecture Search and Attention), which leverages neural architecture search and attention mechanisms to detect image inpainting artifacts from three types of input sources: image pairs, compressed images, and visual features extracted directly from unpaired images. Our method outperforms state-of-the-art detection methods by significant margins on multiple benchmark datasets. We show that our model generalizes well across different domains, including face, object, and scene images.",1
"Gliomas are one of the most frequent brain tumors and are classified into high grade and low grade gliomas. The segmentation of various regions such as tumor core, enhancing tumor etc. plays an important role in determining severity and prognosis. Here, we have developed a multi-threshold model based on attention U-Net for identification of various regions of the tumor in magnetic resonance imaging (MRI). We propose a multi-path segmentation and built three separate models for the different regions of interest. The proposed model achieved mean Dice Coefficient of 0.59, 0.72, and 0.61 for enhancing tumor, whole tumor and tumor core respectively on the training dataset. The same model gave mean Dice Coefficient of 0.57, 0.73, and 0.61 on the validation dataset and 0.59, 0.72, and 0.57 on the test dataset.",0
"In summary text: The main finding in our study was that combining attention modules from both encoder and decoder improved segmentation accuracy by reducing false positives compared to state-of-the art models such as V-net. Furthermore, using multiple thresholds helped reduce overfitting by fine tuning model hyperparameters. We believe this could have important implications in clinical practice and future brain tumor research. Keywords: multi-threshold attention u-net, multimodal brain tumor segmentation, magnetic resonance imaging (MRI), deep learning algorithms, convolutional neural networks (CNNs). Please note I am looking for something brief and punchy.  This manuscript presents a novel approach to solving one of healthcareâ€™s most pressing challenges; accurate automated detection and separation of brain tumors from MRI images. Our team developed a multi-threshold attention unit which improves upon existing methods used in current deep-learning approaches like V-net. This method uses three different thresholds at inference time which helps ensure robustness while making predictions. Additionally, we showcased that utilizing both high resolution patch information and global image features provide more contextual cues to guide the network resulting in superior results than those achieved without these features. Extensive experiments on two separate datasets demonstrated significant improvements including better Dice scores, sensitivity and specificity metrics, leading us to conclude that this new method has promising potential to improve patient outcomes through early cancer diagnosis allowing for prompt treatment and improved survival rates.",1
"Deep hashing methods have been shown to be the most efficient approximate nearest neighbor search techniques for large-scale image retrieval. However, existing deep hashing methods have a poor small-sample ranking performance for case-based medical image retrieval. The top-ranked images in the returned query results may be as a different class than the query image. This ranking problem is caused by classification, regions of interest (ROI), and small-sample information loss in the hashing space. To address the ranking problem, we propose an end-to-end framework, called Attention-based Triplet Hashing (ATH) network, to learn low-dimensional hash codes that preserve the classification, ROI, and small-sample information. We embed a spatial-attention module into the network structure of our ATH to focus on ROI information. The spatial-attention module aggregates the spatial information of feature maps by utilizing max-pooling, element-wise maximum, and element-wise mean operations jointly along the channel axis. The triplet cross-entropy loss can help to map the classification information of images and similarity between images into the hash codes. Extensive experiments on two case-based medical datasets demonstrate that our proposed ATH can further improve the retrieval performance compared to the state-of-the-art deep hashing methods and boost the ranking performance for small samples. Compared to the other loss methods, the triplet cross-entropy loss can enhance the classification performance and hash code-discriminability",0
"Recent advances in deep learning have led to significant improvements in image retrieval tasks such as medical imaging, where accurate diagnosis depends on the ability to quickly locate similar images from a large database. One popular approach has been to use triplet loss functions which minimize the distance between positive (similar) pairs while maximizing it for negative (dissimilar) ones. This paper presents a novel architecture called Deep Triplet Hashing Networks (DTHNs), which combines deep neural networks with hash coding techniques for efficient case-based retrieval. DTHNs consists of multiple subnetworks that progressively learn to extract features at different levels of abstraction, followed by a shared binary classification layer that generates compact hash codes based on the learned feature representations. Experiments on two public datasets demonstrate that DTHNs outperform several state-of-the-art methods for case-based medical image retrieval, making it a promising technique for faster and more accurate diagnosis.",1
"Recent extensive numerical experiments in high scale machine learning have allowed to uncover a quite counterintuitive phase transition, as a function of the ratio between the sample size and the number of parameters in the model. As the number of parameters $p$ approaches the sample size $n$, the generalisation error (a.k.a. testing error) increases, but in many cases, it starts decreasing again past the threshold $p=n$. This surprising phenomenon, brought to the theoretical community attention in \cite{belkin2019reconciling}, has been thoroughly investigated lately, more specifically for simpler models than deep neural networks, such as the linear model when the parameter is taken to be the minimum norm solution to the least-square problem, mostly in the asymptotic regime when $p$ and $n$ tend to $+\infty$; see e.g. \cite{hastie2019surprises}. In the present paper, we propose a finite sample analysis of non-linear models of \textit{ridge} type, where we investigate the \textit{overparametrised regime} of the double descent phenomenon for both the \textit{estimation problem} and the \textit{prediction} problem. Our results provide a precise analysis of the distance of the best estimator from the true parameter as well as a generalisation bound which complements recent works of \cite{bartlett2020benign} and \cite{chinot2020benign}. Our analysis is based on efficient but elementary tools closely related to the continuous Newton method \cite{neuberger2007continuous}.",0
"This paper analyzes the occurrence of benign overfitting using finite samples for ridge regression models. Overfitting can occur when a model fits training data well but poorly predicts new data. Benign overfitting describes cases where overfit models generalize better than underfit ones. We investigate whether benign overfitting occurs consistently across different types of error metrics (e.g., mean squared error versus cross entropy) and regularization strengths, as well as how varying these factors affect the frequency of occurrence. Our findings suggest that benign overfitting does happen frequently, but only under certain conditions - namely, stronger convexity, larger sample sizes, and weaker regularization levels. These results have important implications for applied machine learning practitioners, who must consider both model performance measures and sample size limitations when choosing among competing models. Furthermore, our research reinforces the importance of evaluating generalization ability through careful testing, rather than solely relying on training metrics like accuracy. By understanding the nuances of overfitting behavior, we can improve our abilities to select accurate models in practice.",1
"Continual learning aims to provide intelligent agents capable of learning multiple tasks sequentially with neural networks. One of its main challenging, catastrophic forgetting, is caused by the neural networks non-optimal ability to learn in non-stationary distributions. In most settings of the current approaches, the agent starts from randomly initialized parameters and is optimized to master the current task regardless of the usefulness of the learned representation for future tasks. Moreover, each of the future tasks uses all the previously learned knowledge although parts of this knowledge might not be helpful for its learning. These cause interference among tasks, especially when the data of previous tasks is not accessible. In this paper, we propose a new method, named Self-Attention Meta-Learner (SAM), which learns a prior knowledge for continual learning that permits learning a sequence of tasks, while avoiding catastrophic forgetting. SAM incorporates an attention mechanism that learns to select the particular relevant representation for each future task. Each task builds a specific representation branch on top of the selected knowledge, avoiding the interference between tasks. We evaluate the proposed method on the Split CIFAR-10/100 and Split MNIST benchmarks in the task agnostic inference. We empirically show that we can achieve a better performance than several state-of-the-art methods for continual learning by building on the top of selected representation learned by SAM. We also show the role of the meta-attention mechanism in boosting informative features corresponding to the input data and identifying the correct target in the task agnostic inference. Finally, we demonstrate that popular existing continual learning methods gain a performance boost when they adopt SAM as a starting point.",0
"In recent years, there has been increasing interest in developing artificial intelligence (AI) models that can continually learn new tasks without forgetting previously acquired knowledge. This problem is known as ""continual learning"" and represents one of the major challenges facing the field of machine learning today. To address this challenge, we propose a novel approach based on self-attention meta-learning. Our method uses a small number of gradient updates during training to adapt the model to each new task, while minimizing forgetting of previous knowledge. We evaluate our proposed algorithm using several benchmark datasets and demonstrate its effectiveness compared to state-of-the-art methods. Overall, our findings suggest that self-attention meta-learning provides a promising solution for tackling continual learning problems in AI.",1
"Multi-label zero-shot learning strives to classify images into multiple unseen categories for which no data is available during training. The test samples can additionally contain seen categories in the generalized variant. Existing approaches rely on learning either shared or label-specific attention from the seen classes. Nevertheless, computing reliable attention maps for unseen classes during inference in a multi-label setting is still a challenge. In contrast, state-of-the-art single-label generative adversarial network (GAN) based approaches learn to directly synthesize the class-specific visual features from the corresponding class attribute embeddings. However, synthesizing multi-label features from GANs is still unexplored in the context of zero-shot setting. In this work, we introduce different fusion approaches at the attribute-level, feature-level and cross-level (across attribute and feature-levels) for synthesizing multi-label features from their corresponding multi-label class embedding. To the best of our knowledge, our work is the first to tackle the problem of multi-label feature synthesis in the (generalized) zero-shot setting. Comprehensive experiments are performed on three zero-shot image classification benchmarks: NUS-WIDE, Open Images and MS COCO. Our cross-level fusion-based generative approach outperforms the state-of-the-art on all three datasets. Furthermore, we show the generalization capabilities of our fusion approach in the zero-shot detection task on MS COCO, achieving favorable performance against existing methods. The source code is available at https://github.com/akshitac8/Generative_MLZSL.",0
"A new method has been developed that allows machine learning models to automatically generate labels for unseen data points. This ""Generative Multi-label Zero-shot Learning"" (GMLZSL) approach uses deep neural networks to learn semantic representations of both seen and unseen classes, enabling accurate label prediction without requiring any labeled examples from those classes. Our experiments demonstrate promising results on several benchmark datasets, outperforming state-of-the-art methods for zero-shot multi-label classification while significantly reducing computational costs. These findings have important implications for tasks such as image classification, natural language processing, and robotics, where annotating large amounts of training data can be time-consuming and expensive. Overall, GMLZSL offers a powerful tool for improving the accuracy and efficiency of artificial intelligence applications.",1
"Learning quickly and continually is still an ambitious task for neural networks. Indeed, many real-world applications do not reflect the learning setting where neural networks shine, as data are usually few, mostly unlabelled and come as a stream. To narrow this gap, we introduce FUSION - Few-shot UnSupervIsed cONtinual learning - a novel strategy which aims to deal with neural networks that ""learn in the wild"", simulating a real distribution and flow of unbalanced tasks. We equip FUSION with MEML - Meta-Example Meta-Learning - a new module that simultaneously alleviates catastrophic forgetting and favours the generalisation and future learning of new tasks. To encourage features reuse during the meta-optimisation, our model exploits a single inner loop per task, taking advantage of an aggregated representation achieved through the use of a self-attention mechanism. To further enhance the generalisation capability of MEML, we extend it by adopting a technique that creates various augmented tasks and optimises over the hardest. Experimental results on few-shot learning benchmarks show that our model exceeds the other baselines in both FUSION and fully supervised case. We also explore how it behaves in standard continual learning consistently outperforming state-of-the-art approaches.",0
"In order to solve problems efficiently and effectively, intelligent agents often need to quickly acquire new skills and knowledge that can generalise well beyond their training data. However, relying solely on handcrafted features can limit such generalisation across domains and tasks. Therefore, we propose a method to learn these features automatically from limited feedback examples using meta-learning techniques. Specifically, our algorithm takes advantage of unlabelled example pairs which share a common attribute (such as object class), but vary substantially in terms of appearance and context. By learning to extract relevant features for discriminating between these contrastive examples, our approach facilitates efficient adaptation to novel situations where accurate predictions require more generic representations. Experimental evaluation shows that our method significantly outperforms standard continual learning methods when applied to challenging benchmark datasets for image classification under partial observability. These findings demonstrate the effectiveness of leveraging meta-examples for learning robust feature embeddings amenable to transfer learning across diverse scenarios encountered online.",1
"Fashion is the way we present ourselves to the world and has become one of the world's largest industries. Fashion, mainly conveyed by vision, has thus attracted much attention from computer vision researchers in recent years. Given the rapid development, this paper provides a comprehensive survey of more than 200 major fashion-related works covering four main aspects for enabling intelligent fashion: (1) Fashion detection includes landmark detection, fashion parsing, and item retrieval, (2) Fashion analysis contains attribute recognition, style learning, and popularity prediction, (3) Fashion synthesis involves style transfer, pose transformation, and physical simulation, and (4) Fashion recommendation comprises fashion compatibility, outfit matching, and hairstyle suggestion. For each task, the benchmark datasets and the evaluation protocols are summarized. Furthermore, we highlight promising directions for future research.",0
"In recent years, there has been increasing interest in applying computer vision techniques to fashion related tasks such as garment recognition, outfit recommendation, and image generation. This survey aims to provide an overview of the current state-of-the-art methods used in this field, discussing their strengths, limitations, and potential future developments. We begin by examining existing works on clothing item classification and segmentation, which represent fundamental challenges faced in many fashion applications involving images. Next, we explore approaches to generating new styles using generative adversarial networks (GANs) that have recently emerged as powerful tools capable of synthesizing high quality imagery. Finally, we investigate advanced applications combining different deep learning models tailored specifically for fashion problems. Our review highlights open issues and identifies promising directions for future research at the intersection of these two fields. Overall, our findings indicate substantial progress toward unlocking the full potential of computers as collaborators and creative partners in the world of fashion.",1
"Matching two different sets of items, called heterogeneous set-to-set matching problem, has recently received attention as a promising problem. The difficulties are to extract features to match a correct pair of different sets and also preserve two types of exchangeability required for set-to-set matching: the pair of sets, as well as the items in each set, should be exchangeable. In this study, we propose a novel deep learning architecture to address the abovementioned difficulties and also an efficient training framework for set-to-set matching. We evaluate the methods through experiments based on two industrial applications: fashion set recommendation and group re-identification. In these experiments, we show that the proposed method provides significant improvements and results compared with the state-of-the-art methods, thereby validating our architecture for the heterogeneous set matching problem.",0
"Deep neural networks (DNNs) have emerged as one of the most powerful tools for tackling complex tasks such as image recognition, natural language processing, and even board games like Go. However, there remain several challenges that limit their performance and versatility, including lack of interpretability and difficulty in handling uncertainty and variability. In this work, we propose a novel approach to DNN design called exchangeable deep neural networks (xDNNs), which address these limitations by introducing a new form of parameter sharing and permutation equivariance. We demonstrate how xDNNs can outperform standard DNN architectures on difficult benchmark datasets for set-to-set matching problems, where each example consists of multiple inputs related via sets rather than individual pairs. Our analysis shows that the improved performance is due to increased robustness to noise and variations in input data, enabling the models to generalize better across different problem domains. Additionally, our framework enables efficient training of large-scale systems through distributed computing while retaining the benefits of permutation symmetry and scalability. These findings contribute towards more advanced artificial intelligence capable of dealing with real-world complexity and ambiguity, and could pave the way for applications ranging from medical diagnosis to financial forecasting. By explicitly modeling exchangeability, we take a step towards making machine learning methods truly interpretable and reliable.",1
"Graph Neural Networks (GNNs) have received considerable attention on graph-structured data learning for a wide variety of tasks. The well-designed propagation mechanism which has been demonstrated effective is the most fundamental part of GNNs. Although most of GNNs basically follow a message passing manner, litter effort has been made to discover and analyze their essential relations. In this paper, we establish a surprising connection between different propagation mechanisms with a unified optimization problem, showing that despite the proliferation of various GNNs, in fact, their proposed propagation mechanisms are the optimal solution optimizing a feature fitting function over a wide class of graph kernels with a graph regularization term. Our proposed unified optimization framework, summarizing the commonalities between several of the most representative GNNs, not only provides a macroscopic view on surveying the relations between different GNNs, but also further opens up new opportunities for flexibly designing new GNNs. With the proposed framework, we discover that existing works usually utilize naive graph convolutional kernels for feature fitting function, and we further develop two novel objective functions considering adjustable graph kernels showing low-pass or high-pass filtering capabilities respectively. Moreover, we provide the convergence proofs and expressive power comparisons for the proposed models. Extensive experiments on benchmark datasets clearly show that the proposed GNNs not only outperform the state-of-the-art methods but also have good ability to alleviate over-smoothing, and further verify the feasibility for designing GNNs with our unified optimization framework.",0
"Graph neural networks (GNNs) have achieved great success in tasks ranging from node classification to graph-level regression. However, interpreting GNN models remains challenging as they consist of multiple nonlinear operations that preclude straightforward analysis similar to traditional neural networks. This article introduces a unified optimization framework to jointly interpret and improve existing GNN architectures by incorporating sparsity constraints and edge dropout regularizations in both training and inference stages. We propose a novel objective function that bridges the gap between model accuracy and interpretability via maximizing confidence scores and minimizing uncertainty measures under these regularizations. Extensive experiments across real-world benchmark datasets demonstrate that our approach significantly improves performance without losing interpretability over several strong GNN baselines. Furthermore, we perform comprehensive analyses on GCN-based systems under different settings, providing new insights into interpretable GNNs and paving the way for future research in the field. Keywords: graph neural network, optimization framework, sparsity constraint, edge dropout regularization, interpretation, GCN-based system",1
"With the gradual maturity of 5G technology,autonomous driving technology has attracted moreand more attention among the research commu-nity. Autonomous driving vehicles rely on the co-operation of artificial intelligence, visual comput-ing, radar, monitoring equipment and GPS, whichenables computers to operate motor vehicles auto-matically and safely without human interference.However, the large-scale dataset for training andsystem evaluation is still a hot potato in the devel-opment of robust perception models. In this paper,we present the NEOLIX dataset and its applica-tions in the autonomous driving area. Our datasetincludes about 30,000 frames with point cloud la-bels, and more than 600k 3D bounding boxes withannotations. The data collection covers multipleregions, and various driving conditions, includingday, night, dawn, dusk and sunny day. In orderto label this complete dataset, we developed vari-ous tools and algorithms specified for each task tospeed up the labelling process. It is expected thatour dataset and related algorithms can support andmotivate researchers for the further developmentof autonomous driving in the field of computer vi-sion.",0
"This research paper presents the design, development, and evaluation of the NEOLIX open dataset for autonomous driving. The goal behind creating this dataset was to provide researchers and developers with high quality, diverse, and challenging data sets for training and testing their self-driving systems. To achieve this, we collected over four terabytes of real world traffic videos from multiple countries, featuring different weather conditions and lighting scenarios, road types, objects, and vehicles. We then applied advanced computer vision algorithms to label all relevant objects in each video frame, resulting in more than 47 million bounding boxes labeled across multiple classes, including pedestrians, bicycles, cars, buses, motorcycles, trucks, emergency vehicles, and many other categories. Our work has been validated by comparing our results against ground truth annotations created using Amazon Mechanical Turk, demonstrating a high degree of accuracy (>96%) in most cases. Additionally, we present a detailed analysis of various aspects of our datasets, such as object distribution, localization, occlusion, and motion patterns, which can provide valuable insights for improving the performance of autonomous driving systems. Finally, we discuss potential future directions for enhancing and expanding the NEOLIX dataset, including crowdsourced annotation approaches, semantic segmentation, and integration with other sources of sensor data such as lidar point clouds and radar signals. Overall, the release of the NEOLIX dataset represents a significant step forward towards promoting innovations in auton",1
"In this paper, we consider the image captioning task from a new sequence-to-sequence prediction perspective and propose CaPtion TransformeR (CPTR) which takes the sequentialized raw images as the input to Transformer. Compared to the ""CNN+Transformer"" design paradigm, our model can model global context at every encoder layer from the beginning and is totally convolution-free. Extensive experiments demonstrate the effectiveness of the proposed model and we surpass the conventional ""CNN+Transformer"" methods on the MSCOCO dataset. Besides, we provide detailed visualizations of the self-attention between patches in the encoder and the ""words-to-patches"" attention in the decoder thanks to the full Transformer architecture.",0
Image caption generation using deep learning has gained popularity due to the ability of neural networks to automatically generate descriptions for images. One such model that has shown promising results in image captioning is the full transformer network (CPTR). This work presents a detailed analysis of the performance and design choices made in implementing the full transformer architecture in image captioning tasks. Experimental results on multiple datasets show that our implementation outperforms other state-of-the-art models in terms of both quantitative metrics and human evaluations. Our contributions can provide valuable insights for researchers looking to improve their own approaches to image captioning.,1
"While knowledge distillation (transfer) has been attracting attentions from the research community, the recent development in the fields has heightened the need for reproducible studies and highly generalized frameworks to lower barriers to such high-quality, reproducible deep learning research. Several researchers voluntarily published frameworks used in their knowledge distillation studies to help other interested researchers reproduce their original work. Such frameworks, however, are usually neither well generalized nor maintained, thus researchers are still required to write a lot of code to refactor/build on the frameworks for introducing new methods, models, datasets and designing experiments. In this paper, we present our developed open-source framework built on PyTorch and dedicated for knowledge distillation studies. The framework is designed to enable users to design experiments by declarative PyYAML configuration files, and helps researchers complete the recently proposed ML Code Completeness Checklist. Using the developed framework, we demonstrate its various efficient training strategies, and implement a variety of knowledge distillation methods. We also reproduce some of their original experimental results on the ImageNet and COCO datasets presented at major machine learning conferences such as ICLR, NeurIPS, CVPR and ECCV, including recent state-of-the-art methods. All the source code, configurations, log files and trained model weights are publicly available at https://github.com/yoshitomo-matsubara/torchdistill .",0
"Torchdistill is a modular, configuration-driven framework designed specifically for knowledge distillation. Unlike existing frameworks which use pretrained models as black boxes, Torchdistill allows users to explore different components of knowledge distillation by breaking down the process into smaller modules that can be customized to meet their specific needs. Additionally, Torchdistill comes with several built-in configurations based on state-of-the-art techniques in knowledge distillation research, making it easy for users to select an appropriate configuration without having to manually implement each component themselves. Our experiments show that Torchdistill significantly improves model performance over baseline methods and outperforms other open source alternatives. We anticipate that Torchdistill will be widely used by both practitioners and researchers due to its ease of use, flexibility, and high accuracy.",1
"Fluorescence microscopy images contain several channels, each indicating a marker staining the sample. Since many different marker combinations are utilized in practice, it has been challenging to apply deep learning based segmentation models, which expect a predefined channel combination for all training samples as well as at inference for future application. Recent work circumvents this problem using a modality attention approach to be effective across any possible marker combination. However, for combinations that do not exist in a labeled training dataset, one cannot have any estimation of potential segmentation quality if that combination is encountered during inference. Without this, not only one lacks quality assurance but one also does not know where to put any additional imaging and labeling effort. We herein propose a method to estimate segmentation quality on unlabeled images by (i) estimating both aleatoric and epistemic uncertainties of convolutional neural networks for image segmentation, and (ii) training a Random Forest model for the interpretation of uncertainty features via regression to their corresponding segmentation metrics. Additionally, we demonstrate that including these uncertainty measures during training can provide an improvement on segmentation performance.",0
"One potential option would read: In the realm of deep learning segmentation, uncertainty estimation is becoming increasingly important due to the growing popularity of machine learning algorithms in many scientific fields, including fluorescence microscopy image analysis. By integrating Bayesian methods into traditional computer vision techniques, researches have been able achieve state-of-the-art results in various tasks such as object detection, semantic scene understanding, and medical imaging applications like retinal vessel segmentation and lung nodule detection. However, despite the success of these approaches, there still exists significant challenges in accurately predicting pixel-wise uncertainties from these models, especially when encountering missing markers that are crucial for fine-grained prediction of object boundaries. Consequently, several recent works focused on applying ensemble strategies that leverage multiple base models trained with different random initializations and weight decay regularization hyperparameters. Nonetheless, the efficacy of current ensembling frameworks relies heavily on the quality of individual models within them, potentially resulting in less accurate uncertainty estimates if any member model makes erroneous predictions. Thus, alternative solutions should be investigated to improve robustness under ambiguous situations. This study proposes an innovative approach based on adversarial training to tackle the issue of missing markers during training in uncertain deep learning segmentation for fluorescence microscopy images. Our method generates synthetic copies of the original dataset containing artificial missing marker examples and uses them together with real data to form an augmented training set. Afterwards, we train two neural networks, one for prediction and another for estimati",1
"Knowledge distillation (KD), as an efficient and effective model compression technique, has been receiving considerable attention in deep learning. The key to its success is to transfer knowledge from a large teacher network to a small student one. However, most of the existing knowledge distillation methods consider only one type of knowledge learned from either instance features or instance relations via a specific distillation strategy in teacher-student learning. There are few works that explore the idea of transferring different types of knowledge with different distillation strategies in a unified framework. Moreover, the frequently used offline distillation suffers from a limited learning capacity due to the fixed teacher-student architecture. In this paper we propose a collaborative teacher-student learning via multiple knowledge transfer (CTSL-MKT) that prompts both self-learning and collaborative learning. It allows multiple students learn knowledge from both individual instances and instance relations in a collaborative way. While learning from themselves with self-distillation, they can also guide each other via online distillation. The experiments and ablation studies on four image datasets demonstrate that the proposed CTSL-MKT significantly outperforms the state-of-the-art KD methods.",0
"This study examines collaborative teacher-student learning through multiple knowledge transfer (MKT). MKT involves students teaching their peers new concepts and principles while applying them to real-world situations. In addition, teachers serve as facilitators and provide guidance during group activities. Our research aimed to explore how such interactions affect studentsâ€™ understanding of academic content. We found that participating in MKT resulted in increased student engagement, motivation, and performance. Furthermore, our findings showed that students who were involved in the transfer process had greater retention and application of learned material than those who received traditional lectures alone. These results suggest that incorporating MKT into classroom practices can enhance both collaboration and individual learning outcomes. Therefore, educational practitioners should consider implementing similar strategies to foster positive learning environments.",1
"This paper proposes an efficient video summarization framework that will give a gist of the entire video in a few key-frames or video skims. Existing video summarization frameworks are based on algorithms that utilize computer vision low-level feature extraction or high-level domain level extraction. However, being the ultimate user of the summarized video, humans remain the most neglected aspect. Therefore, the proposed paper considers human's role in summarization and introduces human visual attention-based summarization techniques. To understand human attention behavior, we have designed and performed experiments with human participants using electroencephalogram (EEG) and eye-tracking technology. The EEG and eye-tracking data obtained from the experimentation are processed simultaneously and used to segment frames containing useful information from a considerable video volume. Thus, the frame segmentation primarily relies on the cognitive judgments of human beings. Using our approach, a video is summarized by 96.5% while maintaining higher precision and high recall factors. The comparison with the state-of-the-art techniques demonstrates that the proposed approach yields ceiling-level performance with reduced computational cost in summarising the videos.",0
"In recent years, video summarization has gained significant attention as a means to effectively condense large amounts of visual content while preserving important information. This paper presents an efficient framework that utilizes both electroencephalography (EEG) signals and eye-tracking data to achieve robust video summarization. By analyzing brain activity and visual focus from viewers, our method can identify salient moments within videos that would otherwise be overlooked by traditional approaches. Our experiments demonstrate the effectiveness of our approach, which significantly outperforms existing techniques in terms of accuracy and efficiency. We believe that our findings have the potential to revolutionize the field of video summarization, ultimately enabling users to extract valuable insights from vast multimedia archives with ease.",1
"Ultrasound tongue imaging is widely used for speech production research, and it has attracted increasing attention as its potential applications seem to be evident in many different fields, such as the visual biofeedback tool for second language acquisition and silent speech interface. Unlike previous studies, here we explore the feasibility of age estimation using the ultrasound tongue image of the speakers. Motivated by the success of deep learning, this paper leverages deep learning on this task. We train a deep convolutional neural network model on the UltraSuite dataset. The deep model achieves mean absolute error (MAE) of 2.03 for the data from typically developing children, while MAE is 4.87 for the data from the children with speech sound disorders, which suggest that age estimation using ultrasound is more challenging for the children with speech sound disorder. The developed method can be used a tool to evaluate the performance of speech therapy sessions. It is also worthwhile to notice that, although we leverage the ultrasound tongue imaging for our study, the proposed methods may also be extended to other imaging modalities (e.g. MRI) to assist the studies on speech production.",0
"This paper presents a novel approach to age estimation using convolutional neural networks (CNNs) on b-mode ultrasound tongue images. We developed a deep learning model that takes as input a cropped image patch of the tongue and outputs an estimated age value in years. Our method leverages recent advances in CNN architectures and fine-tunes a pretrained model on our dataset of ultrasound tongue images from healthy subjects across different age groups. We validated our model against human annotators through several experiments, achieving promising results with high accuracy, precision, recall, F1 score and low error rates. Our work demonstrates that automatic age estimation based on ultrasound tongue imaging has potential applications in clinical practice and may complement traditional methods such as medical history taking and physical examination. Overall, we believe that this study represents a first step towards enhancing our understanding of how machine learning can support decision making in medicine.",1
"Deep neural networks (DNNs) are known vulnerable to backdoor attacks, a training time attack that injects a trigger pattern into a small proportion of training data so as to control the model's prediction at the test time. Backdoor attacks are notably dangerous since they do not affect the model's performance on clean examples, yet can fool the model to make incorrect prediction whenever the trigger pattern appears during testing. In this paper, we propose a novel defense framework Neural Attention Distillation (NAD) to erase backdoor triggers from backdoored DNNs. NAD utilizes a teacher network to guide the finetuning of the backdoored student network on a small clean subset of data such that the intermediate-layer attention of the student network aligns with that of the teacher network. The teacher network can be obtained by an independent finetuning process on the same clean subset. We empirically show, against 6 state-of-the-art backdoor attacks, NAD can effectively erase the backdoor triggers using only 5\% clean training data without causing obvious performance degradation on clean examples. Code is available in https://github.com/bboylyg/NAD.",0
"This paper presents a novel approach to mitigating backdoor attacks in deep neural networks through attention distillation. We show that by minimizing interference from irrelevant features in the training process, we can significantly reduce the impact of malicious triggers without compromising overall model performance. Our method involves identifying key components responsible for trigger detection using attentional analysis and selectively erasing their contribution during fine-tuning. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach in mitigating backdoor vulnerabilities while maintaining high classification accuracy. Furthermore, we provide insights into how attention patterns can facilitate understanding of model behavior and guide future research in adversarial robustness.",1
"Artificial Intelligence is everywhere today. But unfortunately, Agriculture has not been able to get that much attention from Artificial Intelligence (AI). A lack of automation persists in the agriculture industry. For over many years, farmers and crop field owners have been facing a problem of trespassing of wild animals for which no feasible solution has been provided. Installing a fence or barrier like structure is neither feasible nor efficient due to the large areas covered by the fields. Also, if the landowner can afford to build a wall or barrier, government policies for building walls are often very irksome. The paper intends to give a simple intelligible solution to the problem with Automated Crop Field Surveillance using Computer Vision. The solution will significantly reduce the cost of crops destroyed annually and completely automate the security of the field.",0
"This paper presents an automated method for surveilling crop fields using computer vision techniques. By deploying camera systems in the field, the system can capture images of the crops and use image processing algorithms to analyze them for a variety of purposes, including plant health monitoring, yield estimation, and detection of pests and diseases. The proposed method is able to accurately identify different types of plants and classify their growth stages, as well as detect and track anomalies such as missing plants, weeds, or damaged leaves. Furthermore, the system can provide real-time alerts for urgent issues that require human intervention. Overall, our approach provides a cost-effective solution for large-scale crop monitoring, offering several benefits over manual methods. Our experiments demonstrate high accuracy and robustness under varying light conditions and environmental factors. Future work will focus on integrating sensor data from other sources, expanding the range of monitored crop parameters, and improving prediction models for better decision making.",1
"Distributed data analysis without revealing the individual data has recently attracted significant attention in several applications. A collaborative data analysis through sharing dimensionality reduced representations of data has been proposed as a non-model sharing-type federated learning. This paper analyzes the accuracy and privacy evaluations of this novel framework. In the accuracy analysis, we provided sufficient conditions for the equivalence of the collaborative data analysis and the centralized analysis with dimensionality reduction. In the privacy analysis, we proved that collaborative users' private datasets are protected with a double privacy layer against insider and external attacking scenarios.",0
"This research paper presents evaluations of collaborative data analysis methods in terms of accuracy and privacy considerations. With the increasing availability of large datasets, collaborative data analysis has become an important tool for extracting valuable insights from these data collections. However, such analysis raises questions regarding the tradeoffs between data sharing and individual privacy concerns. To address these issues, we present empirical studies comparing various approaches to collaborative data analysis based on their ability to protect user privacy while maintaining high levels of accuracy. Our results show that there exists significant variation in both privacy protection and accuracy across different methods, depending on factors such as dataset characteristics and attack models. We provide recommendations for choosing effective collaboration strategies given specific scenarios and highlight opportunities for future research directions in this area. Overall, our work provides crucial insights into achieving optimal balance between accuracy and privacy in modern data analysis practices.",1
"This paper studies video inpainting detection, which localizes an inpainted region in a video both spatially and temporally. In particular, we introduce VIDNet, Video Inpainting Detection Network, which contains a two-stream encoder-decoder architecture with attention module. To reveal artifacts encoded in compression, VIDNet additionally takes in Error Level Analysis frames to augment RGB frames, producing multimodal features at different levels with an encoder. Exploring spatial and temporal relationships, these features are further decoded by a Convolutional LSTM to predict masks of inpainted regions. In addition, when detecting whether a pixel is inpainted or not, we present a quad-directional local attention module that borrows information from its surrounding pixels from four directions. Extensive experiments are conducted to validate our approach. We demonstrate, among other things, that VIDNet not only outperforms by clear margins alternative inpainting detection methods but also generalizes well on novel videos that are unseen during training.",0
"This paper presents a novel deep learning approach for detecting video frames that have been retouched or modified using image inpainting techniques. Image inpainting involves filling in missing regions of an image with synthesized content that matches the surrounding context. While these techniques can produce high quality results, they may also be used maliciously to manipulate videos in order to deceive viewers or evade detection by content moderation systems. Our proposed method leverages advances in computer vision and deep learning to automatically identify video frames that contain evidence of inpainting manipulation. We trained our model on a large dataset of original and tampered video frames, and evaluated its performance through rigorous experiments. Results show that our system achieves high accuracy in identifying inpainted frames while maintaining low false positive rates. Overall, this work represents a significant step towards addressing the growing problem of digital media manipulation.",1
"In this work, we propose a new solution to 3D human pose estimation in videos. Instead of directly regressing the 3D joint locations, we draw inspiration from the human skeleton anatomy and decompose the task into bone direction prediction and bone length prediction, from which the 3D joint locations can be completely derived. Our motivation is the fact that the bone lengths of a human skeleton remain consistent across time. This promotes us to develop effective techniques to utilize global information across all the frames in a video for high-accuracy bone length prediction. Moreover, for the bone direction prediction network, we propose a fully-convolutional propagating architecture with long skip connections. Essentially, it predicts the directions of different bones hierarchically without using any time-consuming memory units e.g. LSTM). A novel joint shift loss is further introduced to bridge the training of the bone length and bone direction prediction networks. Finally, we employ an implicit attention mechanism to feed the 2D keypoint visibility scores into the model as extra guidance, which significantly mitigates the depth ambiguity in many challenging poses. Our full model outperforms the previous best results on Human3.6M and MPI-INF-3DHP datasets, where comprehensive evaluation validates the effectiveness of our model.",0
"Abstract: This paper presents a novel method for estimating human pose from monocular RGB images using deep learning models. We propose a bone-based pose decomposition that represents poses as combinations of fundamental motions at joints connected by virtual bones. To perform 3D pose estimation, we first predict pseudo ground truth depth maps for each RGB image using another network trained separately on synthetic data. By enforcing depth predictions only along bone directions using L2 regularization, our network can learn to generate coherent body part motion estimates in 3D space. As a result, our model achieves state-of-the-art results on multiple challenging benchmark datasets while running in realtime.",1
"Stereo matching is an important problem in computer vision which has drawn tremendous research attention for decades. Recent years, data-driven methods with convolutional neural networks (CNNs) are continuously pushing stereo matching to new heights. However, data-driven methods require large amount of training data, which is not an easy task for real stereo data due to the annotation difficulties of per-pixel ground-truth disparity. Though synthetic dataset is proposed to fill the gaps of large data demand, the fine-tuning on real dataset is still needed due to the domain variances between synthetic data and real data. In this paper, we found that in synthetic datasets, close-to-real-scene texture rendering is a key factor to boost up stereo matching performance, while close-to-real-scene 3D modeling is less important. We then propose semi-synthetic, an effective and fast way to synthesize large amount of data with close-to-real-scene texture to minimize the gap between synthetic data and real data. Extensive experiments demonstrate that models trained with our proposed semi-synthetic datasets achieve significantly better performance than with general synthetic datasets, especially on real data benchmarks with limited training data. With further fine-tuning on the real dataset, we also achieve SOTA performance on Middlebury and competitive results on KITTI and ETH3D datasets.",0
"Stereo matching is a fundamental problem in computer vision that involves estimating depth maps from a pair of images taken by two synchronized cameras at different viewpoints. This task has numerous applications including 3D reconstruction, robotics, autonomous driving, and augmented reality. However, training accurate stereo models requires large amounts of data, which can be time consuming and expensive to collect and annotate manually. To address this issue, we propose a novel approach called semi-synthesis for generating synthetic training sets quickly and efficiently. Our method leverages real image content while maintaining high levels of diversity and quality, making it suitable for developing deep learning algorithms for stereo matching tasks. In our experiments, we demonstrate that using our generated dataset results in higher accuracy compared to other state-of-the-art approaches. We believe that semi-synthesis offers a promising direction towards creating more powerful stereo matchers with less human effort.",1
"Objectives: To evaluate the consequences of the framing of machine learning risk prediction models. We evaluate how framing affects model performance and model learning in four different approaches previously applied in published artificial-intelligence (AI) models.   Setting and participants: We analysed structured secondary healthcare data from 221,283 citizens from four Danish municipalities who were 18 years of age or older.   Results: The four models had similar population level performance (a mean area under the receiver operating characteristic curve of 0.73 to 0.82), in contrast to the mean average precision, which varied greatly from 0.007 to 0.385. Correspondingly, the percentage of missing values also varied between framing approaches. The on-clinical-demand framing, which involved samples for each time the clinicians made an early warning score assessment, showed the lowest percentage of missing values among the vital sign parameters, and this model was also able to learn more temporal dependencies than the others. The Shapley additive explanations demonstrated opposing interpretations of SpO2 in the prediction of sepsis as a consequence of differentially framed models.   Conclusions: The profound consequences of framing mandate attention from clinicians and AI developers, as the understanding and reporting of framing are pivotal to the successful development and clinical implementation of future AI technology. Model framing must reflect the expected clinical environment. The importance of proper problem framing is by no means exclusive to sepsis prediction and applies to most clinical risk prediction models.",0
"""The framing of machine learning risk prediction models can have significant consequences on their accuracy and clinical impact. In this study, we evaluated the performance of sepsis risk prediction models using data from general wards and found that the choice of input variables and model architecture greatly affected the results. By comparing different types of algorithms and feature sets, we identified key factors that influenced model performance such as model complexity, regularization methods, and balanced evaluation metrics. We demonstrated how these factors could lead to overfitting or underfitting if not properly addressed during the development process. Our findings highlight the importance of careful consideration and validation of risk prediction models before they are adopted into clinical practice.""",1
"The outpainting results produced by existing approaches are often too random to meet users' requirement. In this work, we take the image outpainting one step forward by allowing users to harvest personal custom outpainting results using sketches as the guidance. To this end, we propose an encoder-decoder based network to conduct sketch-guided outpainting, where two alignment modules are adopted to impose the generated content to be realistic and consistent with the provided sketches. First, we apply a holistic alignment module to make the synthesized part be similar to the real one from the global view. Second, we reversely produce the sketches from the synthesized part and encourage them be consistent with the ground-truth ones using a sketch alignment module. In this way, the learned generator will be imposed to pay more attention to fine details and be sensitive to the guiding sketches. To our knowledge, this work is the first attempt to explore the challenging yet meaningful conditional scenery image outpainting. We conduct extensive experiments on two collected benchmarks to qualitatively and quantitatively validate the effectiveness of our approach compared with the other state-of-the-art generative models.",0
"Title: Generative Image Outpainting with User Guidance via Sketches  Abstract:  This paper presents a novel approach for generative image outpainting that utilizes user guidance through sketch input. Traditional methods for image outpainting aim to extend an existing image beyond its boundaries by synthesizing new content that seamlessly blends with the original image. However, these methods often struggle to capture the desired context and may produce unexpected results, requiring manual intervention and correction from users. To address this issue, we propose a method that integrates user feedback into the image generation process by allowing them to provide rough sketch inputs to guide the outpainting algorithm towards desirable outcomes. Our approach uses these sketches as a form of prior knowledge to generate more coherent and structured outpainted images. We demonstrate the effectiveness of our method using qualitative evaluations and quantitative comparisons against state-of-the-art image outpainting techniques. The proposed framework offers a significant improvement over current approaches while providing greater control to users throughout the outpainting process. Overall, this work highlights the potential benefits of incorporating user interaction in image synthesis tasks, enabling more flexible and personalized outcomes.",1
"A number of recent approaches have been proposed for pruning neural network parameters at initialization with the goal of reducing the size and computational burden of models while minimally affecting their training dynamics and generalization performance. While each of these approaches have some amount of well-founded motivation, a rigorous analysis of the effect of these pruning methods on network training dynamics and their formal relationship to each other has thus far received little attention. Leveraging recent theoretical approximations provided by the Neural Tangent Kernel, we unify a number of popular approaches for pruning at initialization under a single path-centric framework. We introduce the Path Kernel as the data-independent factor in a decomposition of the Neural Tangent Kernel and show the global structure of the Path Kernel can be computed efficiently. This Path Kernel decomposition separates the architectural effects from the data-dependent effects within the Neural Tangent Kernel, providing a means to predict the convergence dynamics of a network from its architecture alone. We analyze the use of this structure in approximating training and generalization performance of networks in the absence of data across a number of initialization pruning approaches. Observing the relationship between input data and paths and the relationship between the Path Kernel and its natural norm, we additionally propose two augmentations of the SynFlow algorithm for pruning at initialization.",0
"Pruning during initialization improves performance by reducing model parameters and computational cost without losing accuracy. This method leverages paths created after layer expansion of a neural network, which provides better control over how the optimization process affects connections between neurons. By focusing on these paths, we can effectively prune connections that have little impact on prediction output while retaining important ones. Our approach achieves results comparable to previous methods such as random connectivity-based regularization techniques but requires fewer hyperparameters to tune. With our path-wise pruning strategy, researchers can more easily explore different architectures without sacrificing accuracy. In summary, we propose a new path-centric perspective for initializing networks that leads to effective pruning and efficient models.",1
"Electronic health record (EHR) data is sparse and irregular as it is recorded at irregular time intervals, and different clinical variables are measured at each observation point. In this work, we propose a multi-view features integration learning from irregular multivariate time series data by self-attention mechanism in an imputation-free manner. Specifically, we devise a novel multi-integration attention module (MIAM) to extract complex information inherent in irregular time series data. In particular, we explicitly learn the relationships among the observed values, missing indicators, and time interval between the consecutive observations, simultaneously. The rationale behind our approach is the use of human knowledge such as what to measure and when to measure in different situations, which are indirectly represented in the data. In addition, we build an attention-based decoder as a missing value imputer that helps empower the representation learning of the inter-relations among multi-view observations for the prediction task, which operates at the training phase only. We validated the effectiveness of our method over the public MIMIC-III and PhysioNet challenge 2012 datasets by comparing with and outperforming the state-of-the-art methods for in-hospital mortality prediction.",0
"Incorporate the following keywords: multi-view integration, clinical time series, irregular sampling, imputation, variational autoencoder (VAE). Time series data arising from healthcare applications present distinct challenges owing to their complex nature and variability over time. With the increasing use of digital technologies for patient care, we often encounter high-dimensional physiological signals that capture different aspects of patientsâ€™ conditions at regular intervals. However, such recordings can suffer from missing values due to equipment failure or other reasons, compromising the accuracy of any analysis based on them. While traditional methods have employed simple interpolation techniques like linear regression to fill in the gaps, these approaches can result in unsatisfactory estimates that deviate significantly from reality. To address this problem, we develop a novel framework termed Variational Autoencoder Guided Imputation Network (VAIGuideNet) capable of handling multiple views associated with each patient simultaneously. Specifically, our method uses a VAE along with other conditionals encoding features extracted from different modalities available for each subject. In combination with appropriate weight initialization and variational inference constraints, VAIGuideNet excels at generating realistic samples while ensuring stability across diverse populations. Our experimental results using clinically derived datasets demonstrate improved performance compared against baseline methods and ablation studies quantifying benefits arising from our network design choices.",1
"Neural architecture search (NAS) has attracted a lot of attention and has been illustrated to bring tangible benefits in a large number of applications in the past few years. Architecture topology and architecture size have been regarded as two of the most important aspects for the performance of deep learning models and the community has spawned lots of searching algorithms for both aspects of the neural architectures. However, the performance gain from these searching algorithms is achieved under different search spaces and training setups. This makes the overall performance of the algorithms to some extent incomparable and the improvement from a sub-module of the searching model unclear. In this paper, we propose NATS-Bench, a unified benchmark on searching for both topology and size, for (almost) any up-to-date NAS algorithm. NATS-Bench includes the search space of 15,625 neural cell candidates for architecture topology and 32,768 for architecture size on three datasets. We analyze the validity of our benchmark in terms of various criteria and performance comparison of all candidates in the search space. We also show the versatility of NATS-Bench by benchmarking 13 recent state-of-the-art NAS algorithms on it. All logs and diagnostic information trained using the same setup for each candidate are provided. This facilitates a much larger community of researchers to focus on developing better NAS algorithms in a more comparable and computationally cost friendly environment. All codes are publicly available at: https://xuanyidong.com/assets/projects/NATS-Bench.",0
"NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size Abstract:  NAS (Neural Architecture Search) has become increasingly important as deep learning models have grown larger and more complex, making handcrafted architectures less efficient and effective. As a result, there is a need for tools that can automate the search process for finding optimal architecture topologies and sizes. In this work we present NATS-Bench, a benchmark suite that evaluates several prominent NAS algorithms across multiple datasets and metrics, providing insights into their performance characteristics and strengths/weaknesses. Our experiments show that different algorithms excel at different tasks and dataset types, suggesting a need for algorithm selection based on the specific problem domain. We believe our findings provide valuable guidance for users looking to employ NAS techniques effectively in practice. Overall, NATS-Bench offers a comprehensive evaluation framework for assessing key players in the field and informing future research directions.",1
"Hierarchical Latent Attribute Models (HLAMs) are a family of discrete latent variable models that are attracting increasing attention in educational, psychological, and behavioral sciences. The key ingredients of an HLAM include a binary structural matrix and a directed acyclic graph specifying hierarchical constraints on the configurations of latent attributes. These components encode practitioners' design information and carry important scientific meanings. Despite the popularity of HLAMs, the fundamental identifiability issue remains unaddressed. The existence of the attribute hierarchy graph leads to degenerate parameter space, and the potentially unknown structural matrix further complicates the identifiability problem. This paper addresses this issue of identifying the latent structure and model parameters underlying an HLAM. We develop sufficient and necessary identifiability conditions. These results directly and sharply characterize the different impacts on identifiability cast by different attribute types in the graph. The proposed conditions not only provide insights into diagnostic test designs under the attribute hierarchy, but also serve as tools to assess the validity of an estimated HLAM.",0
"In general latent attribute models (e.g., Thurstone, Likert) assume that individual's abilities can be represented as unobservable continuous traits which linearly contribute to observable variables, where each variable corresponds to one specific ability. While these models have been popular in educational and psychological testing, they often lack flexibility in handling complex structures such as multidimensionality, local dependence etc.. To address these issues, we propose using hierarchical latent attribute model (HLAM), which extends traditional latent attribute models by allowing higher level attributes to influence lower level ones while preserving the interpretations at both levels. We show that HLAM possesses better statistical efficiency than some commonly used nonlinear IRT models. Moreover, under mild conditions we prove identifiablility of HLAM up to label swapping and scaling. Simulation studies support our theoretical findings and suggest good performance of HLAM compared to other models. Our work contributes to literature on test equating/linking methods in psychometrics since the identifiability result ensures existence and uniqueness of true scores under different testing situations and test forms.",1
"Image-text matching is an interesting and fascinating task in modern AI research. Despite the evolution of deep-learning-based image and text processing systems, multi-modal matching remains a challenging problem. In this work, we consider the problem of accurate image-text matching for the task of multi-modal large-scale information retrieval. State-of-the-art results in image-text matching are achieved by inter-playing image and text features from the two different processing pipelines, usually using mutual attention mechanisms. However, this invalidates any chance to extract separate visual and textual features needed for later indexing steps in large-scale retrieval systems. In this regard, we introduce the Transformer Encoder Reasoning Network (TERN), an architecture built upon one of the modern relationship-aware self-attentive architectures, the Transformer Encoder (TE). This architecture is able to separately reason on the two different modalities and to enforce a final common abstract concept space by sharing the weights of the deeper transformer layers. Thanks to this design, the implemented network is able to produce compact and very rich visual and textual features available for the successive indexing step. Experiments are conducted on the MS-COCO dataset, and we evaluate the results using a discounted cumulative gain metric with relevance computed exploiting caption similarities, in order to assess possibly non-exact but relevant search results. We demonstrate that on this metric we are able to achieve state-of-the-art results in the image retrieval task. Our code is freely available at https://github.com/mesnico/TERN.",0
"Title: Transformer Reasoning Network for Image-Text Matching and Retrieval  Abstract: Advancements in artificial intelligence (AI) have led to significant improvements in image-text matching and retrieval tasks. These tasks involve finding similar images based on their textual descriptions and vice versa. In recent years, deep learning techniques such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been used extensively for these purposes. However, these models suffer from drawbacks like limited reasoning capabilities, difficulty in modeling complex relationships, and slow inference speeds. To address these issues, we propose a novel architecture called Transformer Reasoning Network (TRN). Our proposed model uses self-attention mechanisms inspired by transformer architectures that enable parallel processing of input data, enabling efficient encoding of both image and text features. We further introduce an iterative reasoning module to incorporate logical reasoning into our TRN model, allowing better understanding of relational concepts. Experimental results on popular benchmark datasets show that our method outperforms state-of-the-art approaches in terms of accuracy and efficiency. This work demonstrates the effectiveness of combining visual and textual representations with reasoning abilities in achieving high-quality solutions for challenging image-text matching problems.",1
"Time series classification(TSC) has always been an important and challenging research task. With the wide application of deep learning, more and more researchers use deep learning models to solve TSC problems. Since time series always contains a lot of noise, which has a negative impact on network training, people usually filter the original data before training the network. The existing schemes are to treat the filtering and training as two stages, and the design of the filter requires expert experience, which increases the design difficulty of the algorithm and is not universal. We note that the essence of filtering is to filter out the insignificant frequency components and highlight the important ones, which is similar to the attention mechanism. In this paper, we propose an attention mechanism that acts on spectrum (SAM). The network can assign appropriate weights to each frequency component to achieve adaptive filtering. We use L1 regularization to further enhance the frequency screening capability of SAM. We also propose a segmented-SAM (SSAM) to avoid the loss of time domain information caused by using the spectrum of the whole sequence. In which, a tumbling window is introduced to segment the original data. Then SAM is applied to each segment to generate new features. We propose a heuristic strategy to search for the appropriate number of segments. Experimental results show that SSAM can produce better feature representations, make the network converge faster, and improve the robustness and classification accuracy.",0
"This paper presents a novel approach for time series classification using attention mechanisms that operate on different frequency bands or spectral components of the data. Existing methods for time series classification often rely on aggregating temporal features into high-level representations, but these approaches can struggle to capture important patterns and variations across different frequencies. In contrast, our proposed method uses attention to selectively focus on relevant regions of the spectrum for each instance in the dataset. By doing so, we aim to improve the representation learning process for time series classification tasks while addressing some of the challenges associated with traditional methods. Our experiments show that our approach outperforms several state-of-the-art baseline models across multiple datasets, demonstrating the effectiveness of the proposed spectrum attention mechanism for time series classification. We believe that this work has significant implications for real-world applications such as anomaly detection, predictive maintenance, and control systems where accurate time series prediction plays a crucial role. Overall, our research advances the field by introducing a new perspective on attention mechanisms for time series analysis, paving the way for future innovations in this domain.",1
"The emergence of communication systems between agents which learn to play referential signalling games with realistic images has attracted a lot of attention recently. The majority of work has focused on using fixed, pretrained image feature extraction networks which potentially bias the information the agents learn to communicate. In this work, we consider a signalling game setting in which a `sender' agent must communicate the information about an image to a `receiver' who must select the correct image from many distractors. We investigate the effect of the feature extractor's weights and of the task being solved on the visual semantics learned by the models. We first demonstrate to what extent the use of pretrained feature extraction networks inductively bias the visual semantics conveyed by emergent communication channel and quantify the visual semantics that are induced.   We then go on to explore ways in which inductive biases can be introduced to encourage the emergence of semantically meaningful communication without the need for any form of supervised pretraining of the visual feature extractor. We impose various augmentations to the input images and additional tasks in the game with the aim to induce visual representations which capture conceptual properties of images. Through our experiments, we demonstrate that communication systems which capture visual semantics can be learned in a completely self-supervised manner by playing the right types of game. Our work bridges a gap between emergent communication research and self-supervised feature learning.",0
"Humans communicate using multiple channels including vision and language. Recently, computer algorithms have been trained on large collections of images and text. However, most state-of-the-art systems remain limited in their ability to interpret visual scenes or reason over them. In our work, we take one step towards building more versatile agents by introducing two new methods that enable machines to better comprehend image content and reason over the possibilities they observe. Our contributions can be summarized as follows: (a) We introduce Communication Games, which offer a flexible way to integrate natural language into deep learning models without relying on special annotations such as bounding boxes, segmentations, or human rationales. By having humans and machine cooperate in simple mini-games, our method gains access to valuable supervision signals at essentially zero additional annotation cost. Importantly, these tasks generalize across different architectures and modalities, making them applicable beyond the specifics of the ImageNet paradigm; (b) As another contribution, we leverage novel insights from cognitive science on how humans might choose actions strategically depending on the game rules to improve model performance significantly on several benchmark datasets, even outperforming fully supervised models and other strong baselines. While promising results have previously been reported when applying reinforcement learning directly to individual policies for vision problems, we believe our approach offers a scalable framework applicable to many scenarios without requiring substantial customization per task at hand. Overall, our findings show that integrating natural language can greatly enhance computer visions capabilities both quantitatively and qualitatively without needing expensive modifications to existing models or data collection processes.",1
"The method of deep learning has achieved excellent results in improving the performance of robotic grasping detection. However, the deep learning methods used in general object detection are not suitable for robotic grasping detection. Current modern object detectors are difficult to strike a balance between high accuracy and fast inference speed. In this paper, we present an efficient and robust fully convolutional neural network model to perform robotic grasping pose estimation from an n-channel input image of the real grasping scene. The proposed network is a lightweight generative architecture for grasping detection in one stage. Specifically, a grasping representation based on Gaussian kernel is introduced to encode training samples, which embodies the principle of maximum central point grasping confidence. Meanwhile, to extract multi-scale information and enhance the feature discriminability, a receptive field block (RFB) is assembled to the bottleneck of our grasping detection architecture. Besides, pixel attention and channel attention are combined to automatically learn to focus on fusing context information of varying shapes and sizes by suppressing the noise feature and highlighting the grasping object feature. Extensive experiments on two public grasping datasets, Cornell and Jacquard demonstrate the state-of-the-art performance of our method in balancing accuracy and inference speed. The network is an order of magnitude smaller than other excellent algorithms while achieving better performance with an accuracy of 98.9$\%$ and 95.6$\%$ on the Cornell and Jacquard datasets, respectively.",0
"This research presents a lightweight convolutional neural network (CNN) framework that uses grasping representation based on Gaussian functions to detect object poses in robotic grasping applications. The proposed method leverages low-resolution depth maps obtained from RGB-D sensors and reduces computational complexity by using small kernels, which makes it suitable for real-time implementations. Experiments performed on public datasets demonstrate that our approach achieves comparable accuracy with state-of-the-art methods while significantly reducing computational requirements. Furthermore, we showcase how our method can be integrated into an interactive robotic grasping system and evaluated through extensive experiments under different conditions. Our findings suggest that the presented CNN framework provides accurate grasp detection in real-world settings without sacrificing efficiency and deployability, opening up opportunities for enhancing robotic manipulation capabilities. In summary, this work contributes to advancing computer vision-based robotics research, focusing on efficient solutions for high-dimensional sensor data processing and intelligent decision making for robotic systems.",1
"Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.",0
"Although transformer networks have achieved great success in natural language processing tasks, they suffer from several limitations that hinder their performance. One major limitation is that transformers lack explicit memory mechanisms that can retain relevant contextual information across different parts of the model. To address these limitations, we propose incorporating feedback memory into the transformer architecture. Our proposed method, called ""FeedbackTransformer"", adds a simple but effective mechanism that maintains and updates a small memory bank during training. This memory bank acts as external feedback to enhance the self-attention operation by allowing the model to selectively attend to previous hidden states within each layer, enabling better handling of long-term dependencies. In our experiments on six challenging NLP benchmark datasets, the proposed approach outperforms strong baseline models, demonstrating the effectiveness of integrating feedback memory into the transformer architecture. These results provide new insights into the design of memory-augmented neural architectures that could improve generalization ability while retaining interpretability.",1
"A key functionality of emerging connected autonomous systems such as smart transportation systems, smart cities, and the industrial Internet-of-Things, is the ability to process and learn from data collected at different physical locations. This is increasingly attracting attention under the terms of distributed learning and federated learning. However, in this setup data transfer takes place over communication resources that are shared among many users and tasks or subject to capacity constraints. This paper examines algorithms for efficiently allocating resources to linear regression tasks by exploiting the informativeness of the data. The algorithms developed enable adaptive scheduling of learning tasks with reliable performance guarantees.",0
"In recent years, machine learning has become increasingly important across multiple fields and industries. As a result, there has been growing interest in distributed computing paradigms that enable efficient processing of large datasets on networked machines. One key challenge in these systems is effective scheduling of tasks across nodes. This paper presents an adaptive task scheduling algorithm designed specifically for distributed machine learning workflows. Our approach dynamically adjusts resource allocations based on real-time performance metrics, enabling faster convergence rates and reduced completion times without sacrificing accuracy. Extensive simulations demonstrate the effectiveness of our method, which outperforms static allocation schemes under varying workload conditions. We discuss potential applications in domains such as scientific research, big data analysis, and online services, highlighting the broad impact of our contributions. Overall, this work contributes new insights into designing scalable and high-performing distributed computing platforms well suited for modern AI tasks.",1
"Domain adaptation has received a lot of attention in recent years, and many algorithms have been proposed with impressive progress. However, it is still not fully explored concerning the joint probability distribution (P(X, Y)) distance for this problem, since its empirical estimation derived from the maximum mean discrepancy (joint maximum mean discrepancy, JMMD) will involve complex tensor-product operator that is hard to manipulate. To solve this issue, this paper theoretically derives a unified form of JMMD that is easy to optimize, and proves that the marginal, class conditional and weighted class conditional probability distribution distances are our special cases with different label kernels, among which the weighted class conditional one not only can realize feature alignment across domains in the category level, but also deal with imbalance dataset using the class prior probabilities. From the revealed unified JMMD, we illustrate that JMMD degrades the feature-label dependence (discriminability) that benefits to classification, and it is sensitive to the label distribution shift when the label kernel is the weighted class conditional one. Therefore, we leverage Hilbert Schmidt independence criterion and propose a novel MMD matrix to promote the dependence, and devise a novel label kernel that is robust to label distribution shift. Finally, we conduct extensive experiments on several cross-domain datasets to demonstrate the validity and effectiveness of the revealed theoretical results.",0
"This paper presents a new method that allows models trained on one domain to make accurate predictions on images from another related but different domain. We accomplish this by minimizing a novel joint loss function which combines a maximum mean discrepancy (MMD) term and a classification error term. Our approach adapts pretrained deep convolutional networks for object detection tasks, outperforming previous adaptation methods such as feature normalization or adversarial training. In addition to extensive quantitative evaluation, we demonstrate how our unified framework leads to visually better adapted features on real world scenarios. Our code is available at https://github.com/facebookresearch/unjoint-mmd.",1
"To enable a deep learning-based system to be used in the medical domain as a computer-aided diagnosis system, it is essential to not only classify diseases but also present the locations of the diseases. However, collecting instance-level annotations for various thoracic diseases is expensive. Therefore, weakly supervised localization methods have been proposed that use only image-level annotation. While the previous methods presented the disease location as the most discriminative part for classification, this causes a deep network to localize wrong areas for indistinguishable X-ray images. To solve this issue, we propose a spatial attention method using disease masks that describe the areas where diseases mainly occur. We then apply the spatial attention to find the precise disease area by highlighting the highest probability of disease occurrence. Meanwhile, the various sizes, rotations and noise in chest X-ray images make generating the disease masks challenging. To reduce the variation among images, we employ an alignment module to transform an input X-ray image into a generalized image. Through extensive experiments on the NIH-Chest X-ray dataset with eight kinds of diseases, we show that the proposed method results in superior localization performances compared to state-of-the-art methods.",0
"Automatic localization of disease from medical images requires accurate detection of relevant image regions while avoiding irrelevant ones, which can be challenging due to varying clinical presentations and imaging appearances. In this work, we propose a weakly supervised method that leverages annotations at the image level rather than pixel-level segmentation masks for locating diseases in thoracic computed tomography (CT) scans. Our approach involves learning a mapping function that takes as input a CT scan and produces a probability map indicating the presence of a specific disease. This mapping function utilizes a pre-trained convolutional neural network architecture fine-tuned on expert-annotated abnormality maps corresponding to a variety of lung diseases such as pneumonia, pleural effusion, and ground glass opacities. To ensure generalizability across different disease patterns and image acquisitions, our model utilizes adversarial training to learn an interpretable representation of the disease distribution in CT scans. Experimental evaluations demonstrate that our approach significantly outperforms prior methods based on full supervision or self-supervision using image-level labels alone. These findings suggest that weakly supervised models guided by coarse annotation sources can effectively tackle fine-grained visual tasks, even those involving subtle differences in appearance and texture among different pathological conditions. Further extensions could enable wider adoption of AI technologies in healthcare settings through more efficient use of human labeling resources.",1
"Graph Neural Networks (GNNs) have recently caught great attention and achieved significant progress in graph-level applications. In this paper, we propose a framework for graph neural networks with multiresolution Haar-like wavelets, or MathNet, with interrelated convolution and pooling strategies. The underlying method takes graphs in different structures as input and assembles consistent graph representations for readout layers, which then accomplishes label prediction. To achieve this, the multiresolution graph representations are first constructed and fed into graph convolutional layers for processing. The hierarchical graph pooling layers are then involved to downsample graph resolution while simultaneously remove redundancy within graph signals. The whole workflow could be formed with a multi-level graph analysis, which not only helps embed the intrinsic topological information of each graph into the GNN, but also supports fast computation of forward and adjoint graph transforms. We show by extensive experiments that the proposed framework obtains notable accuracy gains on graph classification and regression tasks with performance stability. The proposed MathNet outperforms various existing GNN models, especially on big data sets.",0
"Multiresolution analysis (MRA) has been successfully applied to image processing tasks such as compression and feature extraction by using wavelet transforms. Recently, graph theory has gained popularity for representing complex systems due to their ability to capture both local and global structure through nodes and edges. However, applying traditional MRA techniques directly on graphs is challenging because most graphs lack any inherent notion of scale or resolution. In this paper, we propose a novel method called ""MathNet"" that incorporates Haar-like wavelets into a multiscale framework tailored specifically for graphs. Our approach achieves this by leveraging the concept of bandlimited functions over finite domains which allows us to define a discrete Haar-like basis function defined on graphs. We demonstrate how our proposed method can learn rich representations by performing experiments on several benchmark datasets commonly used in graph representation learning. These results show significant improvements compared to baseline methods and existing state-of-the-art approaches. Furthermore, we provide qualitative insights into the features extracted from our learned representations and discuss potential applications of MathNet in areas such as semi-supervised learning and anomaly detection in graph data. Overall, our work shows the promise of employing graph signal processing tools based on mathematical morphology to achieve better performance in graph representation learning tasks.",1
"Deep learning-based methods have achieved remarkable success in image restoration and enhancement, but are they still competitive when there is a lack of paired training data? As one such example, this paper explores the low-light image enhancement problem, where in practice it is extremely challenging to simultaneously take a low-light and a normal-light photo of the same visual scene. We propose a highly effective unsupervised generative adversarial network, dubbed EnlightenGAN, that can be trained without low/normal-light image pairs, yet proves to generalize very well on various real-world test images. Instead of supervising the learning using ground truth data, we propose to regularize the unpaired training using the information extracted from the input itself, and benchmark a series of innovations for the low-light image enhancement problem, including a global-local discriminator structure, a self-regularized perceptual loss fusion, and attention mechanism. Through extensive experiments, our proposed approach outperforms recent methods under a variety of metrics in terms of visual quality and subjective user study. Thanks to the great flexibility brought by unpaired training, EnlightenGAN is demonstrated to be easily adaptable to enhancing real-world images from various domains. The code is available at \url{https://github.com/yueruchen/EnlightenGAN}",0
"This paper presents a new generative adversarial network (GAN) approach for enhancing low light images using paired supervision. By leveraging recent advances in unpaired image-to-image translation methods, our method can learn how to enhance images from one domain to another without needing explicit pixel correspondence training data. We evaluate our model on several datasets, showing that it outperforms state-of-the art methods across multiple metrics. In addition, we demonstrate qualitatively that our model preserves important visual details while generating more visually pleasing results than other methods. Our work shows promising results towards realizing high quality low light enhancements in a variety of applications such as surveillance and automotive imagery.",1
"Visual object tracking, which is representing a major interest in image processing field, has facilitated numerous real world applications. Among them, equipping unmanned aerial vehicle (UAV) with real time robust visual trackers for all day aerial maneuver, is currently attracting incremental attention and has remarkably broadened the scope of applications of object tracking. However, prior tracking methods have merely focused on robust tracking in the well-illuminated scenes, while ignoring trackers' capabilities to be deployed in the dark. In darkness, the conditions can be more complex and harsh, easily posing inferior robust tracking or even tracking failure. To this end, this work proposed a novel discriminative correlation filter based tracker with illumination adaptive and anti dark capability, namely ADTrack. ADTrack firstly exploits image illuminance information to enable adaptability of the model to the given light condition. Then, by virtue of an efficient and effective image enhancer, ADTrack carries out image pretreatment, where a target aware mask is generated. Benefiting from the mask, ADTrack aims to solve a dual regression problem where dual filters, i.e., the context filter and target focused filter, are trained with mutual constraint. Thus ADTrack is able to maintain continuously favorable performance in all-day conditions. Besides, this work also constructed one UAV nighttime tracking benchmark UAVDark135, comprising of more than 125k manually annotated frames, which is also very first UAV nighttime tracking benchmark. Exhaustive experiments are extended on authoritative daytime benchmarks, i.e., UAV123 10fps, DTB70, and the newly built dark benchmark UAVDark135, which have validated the superiority of ADTrack in both bright and dark conditions on a single CPU.",0
"This could be done by describing the problem and proposed solution in concise detail while highlighting significant contributions and insights gained from research results. Describe how your work differs or contributes to existing methods as well if possible in the space provided. The goal of object tracking is to locate objects and their pose parameters over time using images taken by cameras attached to a moving platform. While ground based systems have been used successfully in several applications such as retail customer tracking, road traffic monitoring etc., UAVs (Unmanned aerial vehicles) on the other hand face unique challenges due to their high altitude and motion dynamics, which makes these platforms difficult to track objects robustly and efficiently. In our paper, we tackle three major issues that exist in current all-day object tracking techniques: occlusion handling, re-identification across frames under large appearance changes and drift correction. Our approach uses a multi-stage framework comprising modules like segmentation to separate objects and background from video frames, region proposal networks (RPNs) for bounding box regression followed by association linking detected regions across multiple frames through optical flow and matching of feature descriptors. We validate our approach against popular benchmark datasets like MOTChallenge and demonstrate state-of-the-art performance even without fine tuning our model weights, thus showcasing potential adaptability to different scenarios. To summarize, our work presents new advancements in unmanned aerial vehicle object tracking, addressing major shortcomings present in most contemporary methods through a simple yet efficient system architecture. Through detailed analysis",1
"Visual information extraction (VIE) has attracted considerable attention recently owing to its various advanced applications such as document understanding, automatic marking and intelligent education. Most existing works decoupled this problem into several independent sub-tasks of text spotting (text detection and recognition) and information extraction, which completely ignored the high correlation among them during optimization. In this paper, we propose a robust visual information extraction system (VIES) towards real-world scenarios, which is a unified end-to-end trainable framework for simultaneous text detection, recognition and information extraction by taking a single document image as input and outputting the structured information. Specifically, the information extraction branch collects abundant visual and semantic representations from text spotting for multimodal feature fusion and conversely, provides higher-level semantic clues to contribute to the optimization of text spotting. Moreover, regarding the shortage of public benchmarks, we construct a fully-annotated dataset called EPHOIE (https://github.com/HCIILAB/EPHOIE), which is the first Chinese benchmark for both text spotting and visual information extraction. EPHOIE consists of 1,494 images of examination paper head with complex layouts and background, including a total of 15,771 Chinese handwritten or printed text instances. Compared with the state-of-the-art methods, our VIES shows significant superior performance on the EPHOIE dataset and achieves a 9.01% F-score gain on the widely used SROIE dataset under the end-to-end scenario.",0
"Abstraction: This research proposes an innovative solution for robust visual information extraction in real world scenarios through the creation of a new dataset and novel approach. With the vast increase in available data across platforms such as images and videos, there is an urgent need to design algorithms that can effectively extract high quality information from this unstructured data. The proposed method tackles some of the major challenges faced by traditional methods including variations in lighting conditions, camera angles, scale changes, occlusions etc. Experimental results on multiple datasets demonstrate state of art performance achieved by our method. We believe this work opens up possibilities towards more accurate and reliable extraction of information which has significant applications in several domains like autonomous navigation, object detection & recognition among others.",1
"We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.",0
"This paper presents a new deep learning model for graphs called Graph Transformers. Motivated by recent advances in graph neural networks (GNN) and transformer models applied to sequential data such as natural language and images, we introduce a general framework that unifies these two approaches into a single end-to-end trainable architecture suitable for processing complex and large scale network datasets, which have become ubiquitous across many scientific fields today. We showcase the effectiveness of our approach on benchmark problems from social science, neuroscience and computer vision domains. Our proposed model achieves state-of-the art performance while providing interpretability through attention mechanisms and easy parallelization thanks to the self-attention mechanism inherited from transformers. In addition, we provide extensive ablation studies showing the impact of different design choices on predictive accuracy and computational cost. The code used for experiments and pretrained models for some tasks will be publicly available upon acceptance.",1
"Weather forecasting is dominated by numerical weather prediction that tries to model accurately the physical properties of the atmosphere. A downside of numerical weather prediction is that it is lacking the ability for short-term forecasts using the latest available information. By using a data-driven neural network approach we show that it is possible to produce an accurate precipitation nowcast. To this end, we propose SmaAt-UNet, an efficient convolutional neural networks-based on the well known UNet architecture equipped with attention modules and depthwise-separable convolutions. We evaluate our approaches on a real-life datasets using precipitation maps from the region of the Netherlands and binary images of cloud coverage of France. The experimental results show that in terms of prediction performance, the proposed model is comparable to other examined models while only using a quarter of the trainable parameters.",0
"Title: ""Precision Nowcasting Using a Small Attention-based UNet Model""  This research presents a new approach to nowcasting precipitation based on a small attention mechanism used within the framework of the popular deep learning architecture called Unet (U-shaped network). Our proposed model, SmaAt-UNet, leverages attention mechanisms to enhance feature representation by selectively focusing on informative features while suppressing irrelevant ones at different scales throughout the network architecture. Additionally, we utilize lightweight skip connections that significantly reduce computational overhead compared to standard ConvNets without sacrificing accuracy. To demonstrate the effectiveness of our method, we present results from multiple benchmark datasets including Radarsat-2 images for several scenes covering varying climates across Canada. Experiments show that the proposed method consistently outperforms state-of-the-art models, achieving high levels of precision and recall across all evaluation metrics. This work demonstrates the potential of integrating small attention modules into existing architectures to achieve superior performance in complex data domains like meteorology.",1
"Fine-grained visual classification (FGVC) is becoming an important research field, due to its wide applications and the rapid development of computer vision technologies. The current state-of-the-art (SOTA) methods in the FGVC usually employ attention mechanisms to first capture the semantic parts and then discover their subtle differences between distinct classes. The channel-spatial attention mechanisms, which focus on the discriminative channels and regions simultaneously, have significantly improved the classification performance. However, the existing attention modules are poorly guided since part-based detectors in the FGVC depend on the network learning ability without the supervision of part annotations. As obtaining such part annotations is labor-intensive, some visual localization and explanation methods, such as gradient-weighted class activation mapping (Grad-CAM), can be utilized for supervising the attention mechanism. We propose a Grad-CAM guided channel-spatial attention module for the FGVC, which employs the Grad-CAM to supervise and constrain the attention weights by generating the coarse localization maps. To demonstrate the effectiveness of the proposed method, we conduct comprehensive experiments on three popular FGVC datasets, including CUB-$200$-$2011$, Stanford Cars, and FGVC-Aircraft datasets. The proposed method outperforms the SOTA attention modules in the FGVC task. In addition, visualizations of feature maps also demonstrate the superiority of the proposed method against the SOTA approaches.",0
"This work presents a novel approach to integrate both gradient based (Grad-CAM) and convolutional neural network based methods for generating human-readable explanations on a per pixel level. We propose a two-step process where we first generate class activation maps using the Grad-CAM algorithm from any pre-trained CNN architecture. These maps allow us to identify important regions that contribute most significantly to making predictions. Secondly, channel attention modules (SPADE) and spatial attention modules (SEBlock) are used to aggregate multi-scale feature representations into the final prediction map. Our proposed model achieves state-of-the art results on five benchmark datasets with significant improvement over the baseline models while producing accurate heatmaps localizing discriminative objects or parts therein within segmentation masks.",1
"Time series forecasting is a key component in many industrial and business decision processes and recurrent neural network (RNN) based models have achieved impressive progress on various time series forecasting tasks. However, most of the existing methods focus on single-task forecasting problems by learning separately based on limited supervised objectives, which often suffer from insufficient training instances. As the Transformer architecture and other attention-based models have demonstrated its great capability of capturing long term dependency, we propose two self-attention based sharing schemes for multi-task time series forecasting which can train jointly across multiple tasks. We augment a sequence of paralleled Transformer encoders with an external public multi-head attention function, which is updated by all data of all tasks. Experiments on a number of real-world multi-task time series forecasting tasks show that our proposed architectures can not only outperform the state-of-the-art single-task forecasting baselines but also outperform the RNN-based multi-task forecasting method.",0
"Time series forecasting models have been used to make predictions on many domains including finance, engineering and environmental science. However, most time series prediction methods ignore dependencies among multiple tasks, while these relationships might provide complementary information that could enhance prediction accuracy. To address this issue, we propose a novel multi-task framework named MTTF (Multi-Task Time Series Forecasting) which can capture task relationship by shared attention mechanism. We evaluate our approach using two benchmark datasets, one from electricity load demand forecasting and another from exchange rate forecasting. Experimental results show that our model outperforms several state-of-the-art baselines in terms of both forecast accuracy and efficiency. Furthermore, we apply ablation studies to analyze the contribution of different components within our proposed architecture and demonstrate how they improve overall performance. Our work demonstrates the importance of considering task relationships in multi-task learning problems, even when only limited prior knowledge exists regarding such dependencies. An abstract for a research paper titled ""Multi-Task Time Series Forecasting with Shared Attention"" has been provided. The abstract describes a new method called MTTF (Multi-Task Time Series Forecasting) that uses a shared attention mechanism to capture task relationships. The authors evaluated their model using two benchmark datasets and found that it outperformed other state-of-the-art methods. Ablation studies were conducted to investigate the contributions of individual components of the model and demonstrated improved overall performance. The authors conclude that considering task relationships in multi-task learning is important, even with limited prior knowledge.",1
"Labeling objects at a subordinate level typically requires expert knowledge, which is not always available when using random annotators. As such, learning directly from web images for fine-grained recognition has attracted broad attention. However, the presence of label noise and hard examples in web images are two obstacles for training robust fine-grained recognition models. Therefore, in this paper, we propose a novel approach for removing irrelevant samples from real-world web images during training, while employing useful hard examples to update the network. Thus, our approach can alleviate the harmful effects of irrelevant noisy web images and hard examples to achieve better performance. Extensive experiments on three commonly used fine-grained datasets demonstrate that our approach is far superior to current state-of-the-art web-supervised methods.",0
"In the age of digital media, web images have become one of the most important sources of data for visual recognition tasks. However, they often contain noisy samples that can negatively affect the performance of deep learning models. In this study, we propose an approach to eliminate these noisy samples while utilizing hard ones to improve fine-grained visual recognition. We first identify the difficult regions within each image using object detection techniques and then select a subset of those regions as queries for search engines like Google. These queries return relevant and diverse images which can be used as additional training data for the model. Our experiments show significant improvements over baseline methods on several benchmark datasets. This research has important implications for developing more robust and accurate visual recognition systems capable of handling real-world scenarios.",1
"Semantic Segmentation (SS) is promising for outdoor scene perception in safety-critical applications like autonomous vehicles, assisted navigation and so on. However, traditional SS is primarily based on RGB images, which limits the reliability of SS in complex outdoor scenes, where RGB images lack necessary information dimensions to fully perceive unconstrained environments. As preliminary investigation, we examine SS in an unexpected obstacle detection scenario, which demonstrates the necessity of multimodal fusion. Thereby, in this work, we present EAFNet, an Efficient Attention-bridged Fusion Network to exploit complementary information coming from different optical sensors. Specifically, we incorporate polarization sensing to obtain supplementary information, considering its optical characteristics for robust representation of diverse materials. By using a single-shot polarization sensor, we build the first RGB-P dataset which consists of 394 annotated pixel-aligned RGB-Polarization images. A comprehensive variety of experiments shows the effectiveness of EAFNet to fuse polarization and RGB information, as well as the flexibility to be adapted to other sensor combination scenarios.",0
"Incorporating polarization into semantic segmentation has recently gained significant attention as an effective method for improving perceptual quality. However, existing approaches often rely on expensive pixelwise annotations and may still struggle with object boundaries. To address these limitations, we propose Polarization-driven Semantic Segmentation (PDS), which uses efficient attentional fusion techniques to combine global features from an hourglass network with local details extracted by ResNet. Unlike previous methods that only learn one set of weights per model, PDS utilizes multiple sets at runtime, dynamically switching according to the current task. Our approach achieves state-of-the-art performance across a diverse range of datasets while using far fewer parameters than competing models, making it ideal for realworld applications where memory efficiency is crucial. Keywords: Polarization; Efficient Fusion; Image Segmentation.",1
"Student engagement is a key construct for learning and teaching. While most of the literature explored the student engagement analysis on computer-based settings, this paper extends that focus to classroom instruction. To best examine student visual engagement in the classroom, we conducted a study utilizing the audiovisual recordings of classes at a secondary school over one and a half month's time, acquired continuous engagement labeling per student (N=15) in repeated sessions, and explored computer vision methods to classify engagement levels from faces in the classroom. We trained deep embeddings for attentional and emotional features, training Attention-Net for head pose estimation and Affect-Net for facial expression recognition. We additionally trained different engagement classifiers, consisting of Support Vector Machines, Random Forest, Multilayer Perceptron, and Long Short-Term Memory, for both features. The best performing engagement classifiers achieved AUCs of .620 and .720 in Grades 8 and 12, respectively. We further investigated fusion strategies and found score-level fusion either improves the engagement classifiers or is on par with the best performing modality. We also investigated the effect of personalization and found that using only 60-seconds of person-specific data selected by margin uncertainty of the base classifier yielded an average AUC improvement of .084. 4.Our main aim with this work is to provide the technical means to facilitate the manual data analysis of classroom videos in research on teaching quality and in the context of teacher training.",0
"Analyze a variety of facial videos taken in classrooms across different contexts. Develop automated coding techniques for analyzing facial expressions, body movements, speech prosody, verbal content, gaze patterns, head postures, hand gestures and other nonverbal cues. Implement machine learning algorithms for automatically detecting engagement levels from multimodal data streams. Evaluate the accuracy of the model using manually annotated ground truth labels. Compare results against teacher ratings of student engagement. Discuss findings and future research directions.",1
"Deep convolutional models often produce inadequate predictions for inputs foreign to the training distribution. Consequently, the problem of detecting outlier images has recently been receiving a lot of attention. Unlike most previous work, we address this problem in the dense prediction context in order to be able to locate outlier objects in front of in-distribution background. Our approach is based on two reasonable assumptions. First, we assume that the inlier dataset is related to some narrow application field (e.g.~road driving). Second, we assume that there exists a general-purpose dataset which is much more diverse than the inlier dataset (e.g.~ImageNet-1k). We consider pixels from the general-purpose dataset as noisy negative training samples since most (but not all) of them are outliers. We encourage the model to recognize borders between known and unknown by pasting jittered negative patches over inlier training images. Our experiments target two dense open-set recognition benchmarks (WildDash 1 and Fishyscapes) and one dense open-set recognition dataset (StreetHazard). Extensive performance evaluation indicates competitive potential of the proposed approach.",0
"This paper presents a novel approach to outlier detection and object recognition that utilizes training data containing noise and irrelevant examples. Our method leverages deep learning techniques to identify and isolate anomalous instances within large datasets, while simultaneously improving accuracy in detecting known objects under real world conditions. By incorporating noisy negatives during training, we demonstrate improved robustness and adaptability in challenging scenarios. Experimental results validate the effectiveness of our proposed framework against state-of-the-art methods, showcasing its ability to tackle complex problems in computer vision applications.",1
"Deep Neural Networks (DNNs) have often supplied state-of-the-art results in pattern recognition tasks. Despite their advances, however, the existence of adversarial examples have caught the attention of the community. Many existing works have proposed methods for searching for adversarial examples within fixed-sized regions around training points. Our work complements and improves these existing approaches by adapting the size of these regions based on the problem complexity and data sampling density. This makes such approaches more appropriate for other types of data and may further improve adversarial training methods by increasing the region sizes without creating incorrect labels.",0
"In this paper we examine how adversarial examples can impact traditional machine learning techniques and introduce a novel approach that addresses these concerns while improving performance on real world datasets. Our method adaptively selects features at inference time based on the data in question and ensures robustness against adversaries without sacrificing accuracy. By leveraging advances in graph neural networks (GNNs) combined with Monte Carlo sampling methods and Bayesian optimization strategies, our framework is able to accurately estimate model uncertainty, allowing for better decision making across multiple modalities. We demonstrate through extensive experimentation that our solution consistently outperforms state-of-the-art methods on several benchmark datasets including MNIST, CIFAR-10, SVHN, as well as large scale medical imaging datasets such as Kaggleâ€™s Pneumonia Detection challenge dataset. Through a detailed analysis, we showcase the effectiveness of our method in both classification tasks as well as regression problems by achieving superior mean squared error values compared to alternative approaches. Finally, we provide insights into how feature selection strategies may affect the generalization capabilities of GNN models trained over structured graphs which pave the path towards future research opportunities. Overall, our proposed method represents a significant leap forward in addressing the challenges associated with designing reliable deep learning solutions under attack scenarios.",1
"Image segmentation is one of the most essential biomedical image processing problems for different imaging modalities, including microscopy and X-ray in the Internet-of-Medical-Things (IoMT) domain. However, annotating biomedical images is knowledge-driven, time-consuming, and labor-intensive, making it difficult to obtain abundant labels with limited costs. Active learning strategies come into ease the burden of human annotation, which queries only a subset of training data for annotation. Despite receiving attention, most of active learning methods generally still require huge computational costs and utilize unlabeled data inefficiently. They also tend to ignore the intermediate knowledge within networks. In this work, we propose a deep active semi-supervised learning framework, DSAL, combining active learning and semi-supervised learning strategies. In DSAL, a new criterion based on deep supervision mechanism is proposed to select informative samples with high uncertainties and low uncertainties for strong labelers and weak labelers respectively. The internal criterion leverages the disagreement of intermediate features within the deep learning network for active sample selection, which subsequently reduces the computational costs. We use the proposed criteria to select samples for strong and weak labelers to produce oracle labels and pseudo labels simultaneously at each active learning iteration in an ensemble learning manner, which can be examined with IoMT Platform. Extensive experiments on multiple medical image datasets demonstrate the superiority of the proposed method over state-of-the-art active learning methods.",0
"Title: Improving Biomedical Image Segmentation through Combining Multiple Types of Labels  Automatic segmentation of biomedical images remains a challenging task due to complex background variability, partial volume effect, and poor initialization. Previous works have mainly relied on strong annotators (pathologists) for accurate delineations at high computational cost. However, there is a growing recognition that weak annotations (e.g., bounding boxes/scribbles) can potentially provide complementary supervision, especially when the number of available experts is limited. In this work, we propose a novel framework termed DSAL that leverages both strongly labeled data points as well as multiple weak labelers using active learning to refine object contours step by step without manual correction. Our method utilizes several key techniques, including deep neural networks, spatial transformer modules, and fully convolutional networks, to learn local contextual features that capture image structures while discarding noise. Meanwhile, the combination of strong labels and scribbled regions guides network training toward improved results via cross entropy loss and intersection-over-union metric constraints. To further accelerate model convergence and minimize annotation efforts, we iteratively select informative cases based on confidence scores calculated during each iteration until the stopping criterion is met. Extensive evaluations demonstrate consistent performance gains across diverse medical datasets compared against state-of-the-art methods, indicating our approach paves a promising path towards efficient semi-automated segmentation pipelines in real clinical settings.",1
"Face detection has received intensive attention in recent years. Many works present lots of special methods for face detection from different perspectives like model architecture, data augmentation, label assignment and etc., which make the overall algorithm and system become more and more complex. In this paper, we point out that \textbf{there is no gap between face detection and generic object detection}. Then we provide a strong but simple baseline method to deal with face detection named TinaFace. We use ResNet-50 \cite{he2016deep} as backbone, and all modules and techniques in TinaFace are constructed on existing modules, easily implemented and based on generic object detection. On the hard test set of the most popular and challenging face detection benchmark WIDER FACE \cite{yang2016wider}, with single-model and single-scale, our TinaFace achieves 92.1\% average precision (AP), which exceeds most of the recent face detectors with larger backbone. And after using test time augmentation (TTA), our TinaFace outperforms the current state-of-the-art method and achieves 92.4\% AP. The code will be available at \url{https://github.com/Media-Smart/vedadet}.",0
"This paper presents ""TinaFace,"" a new baseline algorithm for face detection that combines simplicity and effectiveness. Previous approaches have focused on either accuracy or speed, often sacrificing one for the other. However, we show that by carefully designing features specifically tuned for facial characteristics, it's possible to achieve both high performance and low computational cost. Our approach leverages deep learning techniques to extract robust feature representations from raw image data, allowing us to identify faces even under challenging conditions such as varying lighting and pose. We evaluate our method against several benchmark datasets and demonstrate superior results compared to existing state-of-the-art methods, including industry leaders like YOLOv4 and RetinaNet. With its strong yet simple architecture, TinaFace sets a new standard for efficiency and effectiveness in face detection, paving the way towards real-time implementation in a variety of applications ranging from security cameras to augmented reality devices.",1
"Visual question answering requires a deep understanding of both images and natural language. However, most methods mainly focus on visual concept; such as the relationships between various objects. The limited use of object categories combined with their relationships or simple question embedding is insufficient for representing complex scenes and explaining decisions. To address this limitation, we propose the use of text expressions generated for images, because such expressions have few structural constraints and can provide richer descriptions of images. The generated expressions can be incorporated with visual features and question embedding to obtain the question-relevant answer. A joint-embedding multi-head attention network is also proposed to model three different information modalities with co-attention. We quantitatively and qualitatively evaluated the proposed method on the VQA v2 dataset and compared it with state-of-the-art methods in terms of answer prediction. The quality of the generated expressions was also evaluated on the RefCOCO, RefCOCO+, and RefCOCOg datasets. Experimental results demonstrate the effectiveness of the proposed method and reveal that it outperformed all of the competing methods in terms of both quantitative and qualitative results.",0
"This paper presents a new approach for visual question answering (VQA) that utilizes local-scene-aware referring expression generation. VQA involves generating answers to questions asked about images by understanding both the image content and the context provided by the question. Our proposed method tackles this challenge by first identifying regions in the image that are relevant to the question, then generates referring expressions to describe those regions accurately. We show through experiments that our approach significantly outperforms previous state-of-the-art methods for VQA. The key contributions of this work include: 1) a novel framework for local scene analysis; 2) a deep learning architecture for accurate referring expression generation; and 3) extensive evaluations demonstrating improved performance across multiple benchmark datasets. Our results highlight the importance of incorporating spatially-local reasoning into VQA models to achieve better performance. Overall, this research advances the field of computer vision and natural language processing towards intelligent systems capable of understanding and responding to complex human queries.",1
"Human activity recognition in videos has been widely studied and has recently gained significant advances with deep learning approaches; however, it remains a challenging task. In this paper, we propose a novel framework that simultaneously considers both implicit and explicit representations of human interactions by fusing information of local image where the interaction actively occurred, primitive motion with the posture of individual subject's body parts, and the co-occurrence of overall appearance change. Human interactions change, depending on how the body parts of each human interact with the other. The proposed method captures the subtle difference between different interactions using interacting body part attention. Semantically important body parts that interact with other objects are given more weight during feature representation. The combined feature of interacting body part attention-based individual representation and the co-occurrence descriptor of the full-body appearance change is fed into long short-term memory to model the temporal dynamics over time in a single framework. We validate the effectiveness of the proposed method using four widely used public datasets by outperforming the competing state-of-the-art method.",0
"Acknowledging the importance of human interaction recognition (HIR) as a key research area in computer vision, we propose HIR framework through interacting body part attention (HiRAF). In this study, a novel approach that leverages deep convolutional neural networks (CNNs), body joint detection, and attention mechanisms was developed to accurately classify and recognize human interactions from video data. Our method achieved state-of-the-art performance on publicly available datasets, demonstrating its effectiveness. By utilizing attention mechanisms to focus on specific body parts during interaction recognition, our framework can capture more detailed information about human behavior than traditional methods. HiRAF offers potential applications across various fields such as robotics, social analysis, and virtual reality, making it a significant contribution to the field of computer vision. Ultimately, the goal of this work is to provide a comprehensive solution for robust human interaction recognition. To achieve this objective, we proposed an innovative model that overcomes challenges associated with occlusions and scale variation in videos while maintaining high accuracy. Overall, our methodology has paved the way for advanced understanding of human behavior, enabling new possibilities for real-world applications involving human-computer interfaces. This paper presents new insights into how machine learning techniques can enhance our ability to interpret human actions and interactions, highlighting the impact of interdisciplinary research in addressing complex problems related to artificial intelligence and visual computing. Through rigorous evaluation and comparison with existing approaches, we demonstrate HiRaFâ€™s efficacy, setting a new benchmark in the rapidly evolving domain of HIR. With future research directed towards expanding the scope o",1
"Pose based hand gesture recognition has been widely studied in the recent years. Compared with full body action recognition, hand gesture involves joints that are more spatially closely distributed with stronger collaboration. This nature requires a different approach from action recognition to capturing the complex spatial features. Many gesture categories, such as ""Grab"" and ""Pinch"", have very similar motion or temporal patterns posing a challenge on temporal processing. To address these challenges, this paper proposes a two-stream neural network with one stream being a self-attention based graph convolutional network (SAGCN) extracting the short-term temporal information and hierarchical spatial information, and the other being a residual-connection enhanced bidirectional Independently Recurrent Neural Network (RBi-IndRNN) for extracting long-term temporal information. The self-attention based graph convolutional network has a dynamic self-attention mechanism to adaptively exploit the relationships of all hand joints in addition to the fixed topology and local feature extraction in the GCN. On the other hand, the residual-connection enhanced Bi-IndRNN extends an IndRNN with the capability of bidirectional processing for temporal modelling. The two streams are fused together for recognition. The Dynamic Hand Gesture dataset and First-Person Hand Action dataset are used to validate its effectiveness, and our method achieves state-of-the-art performance.",0
"This abstract describes a two-stream neural network approach to pose-based hand gesture recognition, which models both shape context and motion information separately using convolutional layers. An effective hand detection module utilizes a fully connected layer on top of VGG features, followed by thresholding and non-maximum suppression (NMS) to obtain candidate object regions that contain hands. The two separate streams of information from the region proposals pass through parallel convolutional networks and are then fused at later stages to generate final predictions. In experiments, our method achieves state-of-the-art performance in several benchmark datasets, demonstrating its effectiveness compared to other approaches.",1
"Knowledge Graphs (KGs) have found many applications in industry and academic settings, which in turn, have motivated considerable research efforts towards large-scale information extraction from a variety of sources. Despite such efforts, it is well known that even state-of-the-art KGs suffer from incompleteness. Link Prediction (LP), the task of predicting missing facts among entities already a KG, is a promising and widely studied task aimed at addressing KG incompleteness. Among the recent LP techniques, those based on KG embeddings have achieved very promising performances in some benchmarks. Despite the fast growing literature in the subject, insufficient attention has been paid to the effect of the various design choices in those methods. Moreover, the standard practice in this area is to report accuracy by aggregating over a large number of test facts in which some entities are over-represented; this allows LP methods to exhibit good performance by just attending to structural properties that include such entities, while ignoring the remaining majority of the KG. This analysis provides a comprehensive comparison of embedding-based LP methods, extending the dimensions of analysis beyond what is commonly available in the literature. We experimentally compare effectiveness and efficiency of 16 state-of-the-art methods, consider a rule-based baseline, and report detailed analysis over the most popular benchmarks in the literature.",0
"This paper presents a comparative analysis of knowledge graph embedding approaches for link prediction tasks. Knowledge graphs represent structured data from diverse domains using triples that consist of entities (subjects) linked via relationships to other entities (objects). The primary goal of embedding techniques is to convert these complex relations into numerical vectors, making them amenable to machine learning algorithms. Our study investigates several existing methods such as TransE, DistMult, ComplEx, RESCAL, HolE, and TuckER. We evaluate their effectiveness through standard evaluation metrics like mean absolute error (MAE), root mean squared error (RMSE), precision, recall, F1 score, and Jaccard index on four benchmark datasets. Our results demonstrate the strengths and limitations of each approach, emphasizing the trade-offs involved among model expressiveness, complexity, scalability, and predictive performance. Overall, our work highlights potential research directions toward more robust, interpretable, and versatile knowledge graph embeddings that can support advanced artificial intelligence applications.",1
"Event-driven sensors such as LiDAR and dynamic vision sensor (DVS) have found increased attention in high-resolution and high-speed applications. A lot of work has been conducted to enhance recognition accuracy. However, the essential topic of recognition delay or time efficiency is largely under-explored. In this paper, we present a spiking learning system that uses the spiking neural network (SNN) with a novel temporal coding for accurate and fast object recognition. The proposed temporal coding scheme maps each event's arrival time and data into SNN spike time so that asynchronously-arrived events are processed immediately without delay. The scheme is integrated nicely with the SNN's asynchronous processing capability to enhance time efficiency. A key advantage over existing systems is that the event accumulation time for each recognition task is determined automatically by the system rather than pre-set by the user. The system can finish recognition early without waiting for all the input events. Extensive experiments were conducted over a list of 7 LiDAR and DVS datasets. The results demonstrated that the proposed system had state-of-the-art recognition accuracy while achieving remarkable time efficiency. Recognition delay was shown to reduce by 56.3% to 91.7% in various experiment settings over the popular KITTI dataset.",0
"This paper presents a new approach to object recognition that leverages event-driven learning and spike processing. Our method uses a recurrent neural network (RNN) to extract features from sensory data and then applies those features to identify objects in real-time. We demonstrate the effectiveness of our system by comparing it against other state-of-the-art methods on several benchmark datasets. Our results show significant improvement over traditional approaches while using less computational resources. Additionally, we provide insights into how such systems can benefit from neuromorphic computing architectures to achieve even greater performance gains. Overall, our work represents an important step towards building more efficient and biologically inspired computer vision systems.",1
"In this work we tackle the challenging problem of anime character recognition. Anime, referring to animation produced within Japan and work derived or inspired from it. For this purpose we present DAF:re (DanbooruAnimeFaces:revamped), a large-scale, crowd-sourced, long-tailed dataset with almost 500 K images spread across more than 3000 classes. Additionally, we conduct experiments on DAF:re and similar datasets using a variety of classification models, including CNN based ResNets and self-attention based Vision Transformer (ViT). Our results give new insights into the generalization and transfer learning properties of ViT models on substantially different domain datasets from those used for the upstream pre-training, including the influence of batch and image size in their training. Additionally, we share our dataset, source-code, pre-trained checkpoints and results, as Animesion, the first end-to-end framework for large-scale anime character recognition: https://github.com/arkel23/animesion",0
"This should follow standard research paper abstract format. Please summarize your most important findings in 3 to 4 sentences at end. Thank you! This paper presents DAF:re (DAta Foundation Repository), a new dataset designed specifically to support character recognition models that can identify and locate characters within frames from many different Japanese animation shows, known as 'anime'. Our team has assembled and annotated over two million images, which we have released online for public use and improvement. We believe that crowd-sourcing annotations through social media platforms like Twitter and Instagram will allow us to expand our database even further, making DAF:re one of the largest available for training machine learning algorithms on anime characters. Using preliminary results from both deep neural network classifiers and human evaluators, we show that our data significantly improves accuracy compared to current methods. By addressing key challenges such as pose and camera angle variability, complex backgrounds, and variations across shows, we hope that DAF:re will become an essential resource for future research in computer vision and multimedia retrieval applications. Overall, our efforts aim to bridge academic research and real-world needs while fostering collaboration among practitioners, artists, and fans alike. Future work includes incorporating user feedback and exploring more advanced deep learning techniques for fine-grained recognition and video analysis tasks.",1
"Is it possible to use convolutional neural networks pre-trained without any natural images to assist natural image understanding? The paper proposes a novel concept, Formula-driven Supervised Learning. We automatically generate image patterns and their category labels by assigning fractals, which are based on a natural law existing in the background knowledge of the real world. Theoretically, the use of automatically generated images instead of natural images in the pre-training phase allows us to generate an infinite scale dataset of labeled images. Although the models pre-trained with the proposed Fractal DataBase (FractalDB), a database without natural images, does not necessarily outperform models pre-trained with human annotated datasets at all settings, we are able to partially surpass the accuracy of ImageNet/Places pre-trained models. The image representation with the proposed FractalDB captures a unique feature in the visualization of convolutional layers and attentions.",0
"Artificial intelligence (AI) is becoming increasingly popular as more industries begin to rely on machine learning algorithms. However, there still exists challenges that must be addressed before widespread adoption can occur. One such challenge is the problem of pre-training large language models without requiring vast amounts of image data. This paper proposes a new methodology by which natural images may be removed from the pre-training process altogether, and instead replaced with alternative forms of media such as text passages. Using this method, we were able to achieve comparable results in terms of quality of generated images to traditional methods of pre-training on large datasets of natural images. We demonstrate our findings using several well-known benchmarks such as CelebA, LSUN Churches, SUN RGBD, COCO Stuff, and DALL-E 2. Our experiments show promising results and could have implications for future research in the field of AI.",1
"Pointwise localization allows more precise localization and accurate interpretability, compared to bounding box, in applications where objects are highly unstructured such as in medical domain. In this work, we focus on weakly supervised localization (WSL) where a model is trained to classify an image and localize regions of interest at pixel-level using only global image annotation. Typical convolutional attentions maps are prune to high false positive regions. To alleviate this issue, we propose a new deep learning method for WSL, composed of a localizer and a classifier, where the localizer is constrained to determine relevant and irrelevant regions using conditional entropy (CE) with the aim to reduce false positive regions. Experimental results on a public medical dataset and two natural datasets, using Dice index, show that, compared to state of the art WSL methods, our proposal can provide significant improvements in terms of image-level classification and pixel-level localization (low false positive) with robustness to overfitting. A public reproducible PyTorch implementation is provided in: https://github.com/sbelharbi/wsol-min-max-entropy-interpretability .",0
"This work proposes a novel approach called min-max entropy (MME) that is capable of effectively solving weakly supervised pointwise localization problems without requiring manual labeling data points one by one. By defining two entropies based on the spatial relationship among regions, we introduce a max operator into the objective function to prevent trivial solutions, making our algorithm more stable and efficient than other state-of-the-art methods. Extensive experiments have been conducted to validate our proposal; results show significant improvements against alternative approaches. In conclusion, MME provides both theoretical insights into unsupervised learning and promising applications in real scenarios such as image restoration tasks.",1
"Finger vein recognition has drawn increasing attention as one of the most popular and promising biometrics due to its high distinguishes ability, security and non-invasive procedure. The main idea of traditional schemes is to directly extract features from finger vein images or patterns and then compare features to find the best match. However, the features extracted from images contain much redundant data, while the features extracted from patterns are greatly influenced by image segmentation methods. To tack these problems, this paper proposes a new finger vein recognition by generating code. The proposed method does not require an image segmentation algorithm, is simple to calculate and has a small amount of data. Firstly, the finger vein images were divided into blocks to calculate the mean value. Then the centrosymmetric coding is performed by using the generated eigenmatrix. The obtained codewords are concatenated as the feature codewords of the image. The similarity between vein codes is measured by the ratio of minimum Hamming distance to codeword length. Extensive experiments on two public finger vein databases verify the effectiveness of the proposed method. The results indicate that our method outperforms the state-of-theart methods and has competitive potential in performing the matching task.",0
"This study proposes a novel approach for finger vein recognition using machine learning algorithms, which involves generating code from finger images obtained through near infrared imaging technology. By extracting features from these codes, we can create a unique digital signature that is specific to each individual, enabling accurate identification. Our method outperforms traditional methods of biometric authentication based on fingerprints, face recognition, and iris scanning, demonstrating high accuracy even under varying conditions such as lighting changes and subject movements. Moreover, our algorithm is computationally efficient and scalable, making it well suited for real-world applications. Ultimately, this work represents a significant advancement in the field of biometrics and has important implications for security and access control systems.",1
"On-device Deep Neural Networks (DNNs) have recently gained more attention due to the increasing computing power of the mobile devices and the number of applications in Computer Vision (CV), Natural Language Processing (NLP), and Internet of Things (IoTs). Unfortunately, the existing efficient convolutional neural network (CNN) architectures designed for CV tasks are not directly applicable to NLP tasks and the tiny Recurrent Neural Network (RNN) architectures have been designed primarily for IoT applications. In NLP applications, although model compression has seen initial success in on-device text classification, there are at least three major challenges yet to be addressed: adversarial robustness, explainability, and personalization. Here we attempt to tackle these challenges by designing a new training scheme for model compression and adversarial robustness, including the optimization of an explainable feature mapping objective, a knowledge distillation objective, and an adversarially robustness objective. The resulting compressed model is personalized using on-device private training data via fine-tuning. We perform extensive experiments to compare our approach with both compact RNN (e.g., FastGRNN) and compressed RNN (e.g., PRADO) architectures in both natural and adversarial NLP test settings.",0
"In recent years, there has been increasing interest in developing machine learning models that can provide explainability while maintaining high levels of performance. One popular technique for achieving these goals is model compression, which involves reducing the size of the model without sacrificing accuracy. However, compressed models may be susceptible to adversarial attacks, which could undermine their effectiveness. To address this issue, we propose a novel framework that incorporates adversarial robustness into the model compression process. This approach allows us to create compact and efficient models that are resistant to attacks. We demonstrate the efficacy of our method on several benchmark datasets and show that our models achieve strong results compared to state-of-the-art methods. Additionally, we introduce on-device personalization to allow users to tailor the behavior of the model according to their specific needs. Overall, our work represents a significant step towards creating transparent and secure machine learning systems.",1
"Distracted drivers are dangerous drivers. Equipping advanced driver assistance systems (ADAS) with the ability to detect driver distraction can help prevent accidents and improve driver safety. In order to detect driver distraction, an ADAS must be able to monitor their visual attention. We propose a model that takes as input a patch of the driver's face along with a crop of the eye-region and classifies their glance into 6 coarse regions-of-interest (ROIs) in the vehicle. We demonstrate that an hourglass network, trained with an additional reconstruction loss, allows the model to learn stronger contextual feature representations than a traditional encoder-only classification module. To make the system robust to subject-specific variations in appearance and behavior, we design a personalized hourglass model tuned with an auxiliary input representing the driver's baseline glance behavior. Finally, we present a weakly supervised multi-domain training regimen that enables the hourglass to jointly learn representations from different domains (varying in camera type, angle), utilizing unlabeled samples and thereby reducing annotation cost.",0
"This paper presents a new approach to driver glance classification that generalizes across domains and subjects by using transfer learning and domain adaptation techniques. The proposed method uses a pretrained convolutional neural network (CNN) architecture fine-tuned on a large dataset collected from multiple driving scenarios involving different cars, drivers, road conditions, weather conditions, traffic situations, and time of day. Our results show significant improvements compared to previous work in terms of accuracy, robustness, and scalability. We provide detailed analysis of our findings and discuss future directions for research in the field of driver glance classification.",1
"Video style transfer is getting more attention in AI community for its numerous applications such as augmented reality and animation productions. Compared with traditional image style transfer, performing this task on video presents new challenges: how to effectively generate satisfactory stylized results for any specified style, and maintain temporal coherence across frames at the same time. Towards this end, we propose Multi-Channel Correction network (MCCNet), which can be trained to fuse the exemplar style features and input content features for efficient style transfer while naturally maintaining the coherence of input videos. Specifically, MCCNet works directly on the feature space of style and content domain where it learns to rearrange and fuse style features based on their similarity with content features. The outputs generated by MCC are features containing the desired style patterns which can further be decoded into images with vivid style textures. Moreover, MCCNet is also designed to explicitly align the features to input which ensures the output maintains the content structures as well as the temporal continuity. To further improve the performance of MCCNet under complex light conditions, we also introduce the illumination loss during training. Qualitative and quantitative evaluations demonstrate that MCCNet performs well in both arbitrary video and image style transfer tasks.",0
"This could very well be one of my most favorite prompts yet! Alright, here we go:  Deep learning has revolutionized computer graphics by enabling novel techniques like style transfer that allows arbitrary art styles to be applied to photographs in real time. However, traditional methods only consider color channels, resulting in low visual fidelity due to ignoring important correlations among different color channels. In contrast, our method proposes a multi-channel approach which captures these intricate interrelationships. By doing so, we achieve significantly higher quality outputs while running at interactive rates on standard hardware. We demonstrate through extensive experiments that our approach outperforms state-of-the-art methods qualitatively as well as quantitatively across multiple datasets and metrics. Our code and models will be released upon publication to foster future research in this rapidly growing field.",1
"Short text clustering has far-reaching effects on semantic analysis, showing its importance for multiple applications such as corpus summarization and information retrieval. However, it inevitably encounters the severe sparsity of short text representations, making the previous clustering approaches still far from satisfactory. In this paper, we present a novel attentive representation learning model for shot text clustering, wherein cluster-level attention is proposed to capture the correlations between text representations and cluster representations. Relying on this, the representation learning and clustering for short texts are seamlessly integrated into a unified model. To further ensure robust model training for short texts, we apply adversarial training to the unsupervised clustering setting, by injecting perturbations into the cluster representations. The model parameters and perturbations are optimized alternately through a minimax game. Extensive experiments on four real-world short text datasets demonstrate the superiority of the proposed model over several strong competitors, verifying that robust adversarial training yields substantial performance gains.",0
"""Attention mechanisms have emerged as a powerful tool for representation learning tasks such as image classification, speech recognition, and language translation. In recent years, there has been growing interest in using attention models for text clustering due to their ability to handle short texts efficiently. However, training deep neural networks can lead to overfitting, particularly when dealing with small datasets, which degrades model performance. To address these limitations, we propose an adversarially trained attention mechanism that enables better generalization on small datasets while improving cluster quality. Our approach employs a discriminator network that guides the attention model towards producing more coherent representations by minimizing reconstruction error through adversarial training. Experimental results demonstrate significant improvements in accuracy compared to state-of-the-art baselines across multiple benchmark datasets.""",1
"In recent years, deep neural networks have achieved high ac-curacy in the field of image recognition. By inspired from human learning method, we propose a semantic segmentation method using cooperative learning which shares the information resembling a group learning. We use two same networks and paths for sending feature maps between two networks. Two networks are trained simultaneously. By sharing feature maps, one of two networks can obtain the information that cannot be obtained by a single network. In addition, in order to enhance the degree of cooperation, we propose two kinds of methods that connect only the same layer and multiple layers. We evaluated our proposed idea on two kinds of networks. One is Dual Attention Network (DANet) and the other one is DeepLabv3+. The proposed method achieved better segmentation accuracy than the conventional single network and ensemble of networks.",0
"In recent years, deep learning has been applied successfully to semantic segmentation tasks, resulting in significant improvements in accuracy and robustness over traditional methods. However, designing these models can require substantial computational resources and time due to their large scale and complexity. To address this challenge, we propose a feature sharing cooperative network (FSCN) architecture that distributes computation across multiple GPUs and devices while minimizing data transfer overhead. This method leverages peer-to-peer communication to enable real-time collaboration among nodes, enabling each node to learn its own features which may differ from those learned by other nodes depending on the given task and dataset. Our results show that FSCN outperforms state-of-the-art approaches on several benchmark datasets, demonstrating its effectiveness in providing high quality predictions while reducing both training time and memory usage. We believe our work represents a step towards efficient distributed computing systems for machine learning applications.",1
"During the image acquisition process, noise is usually added to the data mainly due to physical limitations of the acquisition sensor, and also regarding imprecisions during the data transmission and manipulation. In that sense, the resultant image needs to be processed to attenuate its noise without losing details. Non-learning-based strategies such as filter-based and noise prior modeling have been adopted to solve the image denoising problem. Nowadays, learning-based denoising techniques showed to be much more effective and flexible approaches, such as Residual Convolutional Neural Networks. Here, we propose a new learning-based non-blind denoising technique named Attention Residual Convolutional Neural Network (ARCNN), and its extension to blind denoising named Flexible Attention Residual Convolutional Neural Network (FARCNN). The proposed methods try to learn the underlying noise expectation using an Attention-Residual mechanism. Experiments on public datasets corrupted by different levels of Gaussian and Poisson noise support the effectiveness of the proposed approaches against some state-of-the-art image denoising methods. ARCNN achieved an overall average PSNR results of around 0.44dB and 0.96dB for Gaussian and Poisson denoising, respectively FARCNN presented very consistent results, even with slightly worsen performance compared to ARCNN.",0
"Title: ""Image Denoising Using Attention-Residual Convolutional Neural Networks"" (ARCNN) In recent years, deep learning has revolutionized many fields including image processing and computer vision. One task that has seen significant improvements through deep learning techniques is image denoising. Many algorithms have been proposed utilizing convolutional neural networks (CNN), but these models can still struggle with removing noise while preserving important details. To address this issue, we propose a new method called attention residual convolutional neural networks (ARCNN). Our model incorporates both spatial attention mechanisms and residual connections to enhance its performance on the image denoising task. We evaluate our approach against several benchmark datasets such as BSD68 and Urban100, and demonstrate superior results compared to state-of-the-art methods. Additionally, our method requires fewer parameters than other CNN-based approaches, making it more efficient in terms of computation resources. This work represents another step towards achieving highly effective and efficient noise removal solutions utilizing machine learning models. Keywords: Image Denoising; Attention Mechanism; Residual Connection; Deep Learning; CNN Model",1
"Lots of neural network architectures have been proposed to deal with learning tasks on graph-structured data. However, most of these models concentrate on only node features during the learning process. The edge features, which usually play a similarly important role as the nodes, are often ignored or simplified by these models. In this paper, we present edge-featured graph attention networks, namely EGATs, to extend the use of graph neural networks to those tasks learning on graphs with both node and edge features. These models can be regarded as extensions of graph attention networks (GATs). By reforming the model structure and the learning process, the new models can accept node and edge features as inputs, incorporate the edge information into feature representations, and iterate both node and edge features in a parallel but mutual way. The results demonstrate that our work is highly competitive against other node classification approaches, and can be well applied in edge-featured graph learning tasks.",0
"This is an abstract which explains what the Edge Feature Graph Attention Network model can do.  Please ensure that it doesnâ€™t have any plagiarised content as I am going to use Turnitin on it later. Abstract: Edge features play an important role in graph representation learning and graph attention networks (GATs) can effectively capture them by performing attentional pooling operations based on edge weights at each layer. In this paper, we propose Edge Feature Graph Attention Network (EF-GAT), where we modify traditional GATs to incorporate edge features by introducing a new type of attention unit called Edge-featured Graph Attention Unit (EA-GAU). EA-GAUs perform self attention over both node features and edge features simultaneously, enabling EF-GATs to learn more powerful representations on graphs with complex structures and semantics. We evaluate EF-GATs extensively using benchmark datasets and conduct ablation studies to demonstrate their effectiveness. Our experimental results show that EF-GATs achieve state-of-the-art performance compared to other baseline models across all evaluation metrics.",1
"The studies on black-box adversarial attacks have become increasingly prevalent due to the intractable acquisition of the structural knowledge of deep neural networks (DNNs). However, the performance of emerging attacks is negatively impacted when fooling DNNs tailored for high-resolution images. One of the explanations is that these methods usually focus on attacking the entire image, regardless of its spatial semantic information, and thereby encounter the notorious curse of dimensionality. To this end, we propose a pixel correlation-based attentional black-box adversarial attack, termed as PICA. Firstly, we take only one of every two neighboring pixels in the salient region as the target by leveraging the attentional mechanism and pixel correlation of images, such that the dimension of the black-box attack reduces. After that, a general multiobjective evolutionary algorithm is employed to traverse the reduced pixels and generate perturbations that are imperceptible by the human vision. Extensive experimental results have verified the effectiveness of the proposed PICA on the ImageNet dataset. More importantly, PICA is computationally more efficient to generate high-resolution adversarial examples compared with the existing black-box attacks.",0
"Here is an example of an abstract that conforms with your specifications:  Black-box adversarial attacks have become increasingly popular due to their ability to effectively attack deep learning models without requiring knowledge of the model's architecture or parameters. However, existing black-box adversarial attack methods often suffer from low success rates and require large numbers of queries. In this work, we present PICA (Pixel Correlation-based Attention), a novel black-box attentional approach designed to overcome these limitations. Our method uses pixel correlation analysis to identify important input regions and focuses attention on those areas. Experimental results demonstrate that our proposed method significantly outperforms state-of-the-art baseline approaches on multiple benchmark datasets while using fewer queries. These findings highlight the effectiveness of our method as a powerful tool for evaluating deep learning models and improving robustness against adversarial attacks.",1
"Functional magnetic resonance imaging (fMRI) is a neuroimaging modality that captures the blood oxygen level in a subject's brain while the subject either rests or performs a variety of functional tasks under different conditions. Given fMRI data, the problem of inferring the task, known as task state decoding, is challenging due to the high dimensionality (hundreds of million sampling points per datum) and complex spatio-temporal blood flow patterns inherent in the data. In this work, we propose to tackle the fMRI task state decoding problem by casting it as a 4D spatio-temporal classification problem. We present a novel architecture called Brain Attend and Decode (BAnD), that uses residual convolutional neural networks for spatial feature extraction and self-attention mechanisms for temporal modeling. We achieve significant performance gain compared to previous works on a 7-task benchmark from the large-scale Human Connectome Project-Young Adult (HCP-YA) dataset. We also investigate the transferability of BAnD's extracted features on unseen HCP tasks, either by freezing the spatial feature extraction layers and retraining the temporal model, or finetuning the entire model. The pre-trained features from BAnD are useful on similar tasks while finetuning them yields competitive results on unseen tasks/conditions.",0
"Functional magnetic resonance imaging (fMRI) allows us to measure brain activity by detecting changes in blood flow through the brainâ€™s network of arteries. These measurements can provide valuable insights into how different regions of the brain interact during tasks such as perception, memory recall, decision making, and more. One major challenge in interpreting fMRI data is that the signal measured at any given time point reflects a complex interaction among multiple neuronal processes, including task states, stimulus encoding, response preparation, attentional modulation, noise, and other sources. In this work we propose a novel method called â€œAttend and Decodeâ€ which applies attention mechanisms inspired by natural language processing models to decode individual cognitive processes from whole-brain fMRI signals acquired while subjects perform four distinct mental tasks. We demonstrate the effectiveness of our approach by comparing the performance of several popular attention architectures across two types of decoders using leave one subject out cross validation on datasets obtained from both human and non-human primate studies. Our results show that attention based methods offer a powerful toolkit for investigating human brain function, and suggest promising new directions for advancing our understanding of the neural bases of perception, action, and thought.",1
"In the literature of vehicle re-identification (ReID), intensive manual labels such as landmarks, critical parts or semantic segmentation masks are often required to improve the performance. Such extra information helps to detect locally geometric features as a part of representation learning for vehicles. In contrast, in this paper, we aim to address the challenge of {\em automatically} learning to detect geometric features as landmarks {\em with no extra labels}. To the best of our knowledge, we are the {\em first} to successfully learn discriminative geometric features for vehicle ReID based on self-supervised attention. Specifically, we implement an end-to-end trainable deep network architecture consisting of three branches: (1) a global branch as backbone for image feature extraction, (2) an attentional branch for producing attention masks, and (3) a self-supervised branch for regularizing the attention learning with rotated images to locate geometric features. %Our network design naturally leads to an end-to-end multi-task joint optimization. We conduct comprehensive experiments on three benchmark datasets for vehicle ReID, \ie VeRi-776, CityFlow-ReID, and VehicleID, and demonstrate our state-of-the-art performance. %of our approach with the capability of capturing informative vehicle parts with no corresponding manual labels. We also show the good generalization of our approach in other ReID tasks such as person ReID and multi-target multi-camera (MTMC) vehicle tracking. {\em Our demo code is attached in the supplementary file.}",0
"This paper presents a new method for discovering discriminative geometric features using self-supervised attention for vehicle re-identification and other related tasks. In recent years, deep learning techniques have shown great promise in solving computer vision problems such as object recognition, image classification, and more recently, vehicle re-identification (ReID). Despite these advances, many approaches still rely heavily on manually engineered features or require large amounts of labeled data for training. To address these limitations, we propose a novel approach that leverages unlabeled video frames from surveillance cameras to learn robust feature representations without any explicit supervision. Our framework utilizes a siamese network architecture equipped with a self-attention module, which enables efficient computation of region-specific information at different spatial levels. We evaluate our method on several benchmark datasets including Market-1501, CUHK-SYSU, and DukeMTMC, achieving state-of-the-art performance across all three datasets. Additionally, we demonstrate the generalizability of our method by applying it to a new task outside the scope of ReID - namely, fine-grained vehicle attribute prediction. These results validate the effectiveness of our approach in capturing meaningful feature representations, paving the way towards improved automation of various visual analysis applications.  Note: I took some creative liberties to make the abstract more engaging while maintaining accuracy. Let me know if you want any changes!",1
"The recently proposed Detection Transformer (DETR) model successfully applies Transformer to objects detection and achieves comparable performance with two-stage object detection frameworks, such as Faster-RCNN. However, DETR suffers from its slow convergence. Training DETR \cite{carion2020end} from scratch needs 500 epochs to achieve a high accuracy. To accelerate its convergence, we propose a simple yet effective scheme for improving the DETR framework, namely Spatially Modulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct regression-aware co-attention in DETR by constraining co-attention responses to be high near initially estimated bounding box locations. Our proposed SMCA increases DETR's convergence speed by replacing the original co-attention mechanism in the decoder while keeping other operations in DETR unchanged. Furthermore, by integrating multi-head and scale-selection attention designs into SMCA, our fully-fledged SMCA can achieve better performance compared to DETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3 mAP at 500 epochs). We perform extensive ablation studies on COCO dataset to validate the effectiveness of the proposed SMCA.",0
"This paper investigates the use of spatially modulated co-attention mechanisms within deep learning models as a means of improving their ability to converge faster during training. Specifically, we explore how the incorporation of such mechanisms can improve performance across a range of tasks by allowing the model to more effectively learn from complex data sets. Our results demonstrate that the integration of these techniques leads to significant improvements in convergence rates compared to traditional attention approaches, resulting in better overall model accuracy and faster training times. Through detailed experimental analysis, we showcase the effectiveness of our approach in both image classification and natural language processing domains, highlighting its versatility and potential utility across different applications of deep learning technology. Overall, this work represents a significant step forward towards enabling even greater efficiency in large-scale machine learning systems.",1
"Diagnostic Captioning (DC) concerns the automatic generation of a diagnostic text from a set of medical images of a patient collected during an examination. DC can assist inexperienced physicians, reducing clinical errors. It can also help experienced physicians produce diagnostic reports faster. Following the advances of deep learning, especially in generic image captioning, DC has recently attracted more attention, leading to several systems and datasets. This article is an extensive overview of DC. It presents relevant datasets, evaluation measures, and up to date systems. It also highlights shortcomings that hinder DC's progress and proposes future directions.",0
"Diagnostic Captioning: A Survey Abstract This work reviews recent advances in computer vision systems that automatically generate captions from images. Recent progress has been driven by deep learning approaches such as convolutional neural networks (CNNs). Many state-of-the-art methods rely on pretrained CNN models fine-tuned on image classification tasks to predict which objects are present in an input image and their associated attributes like scale and location within the scene. Other popular techniques focus on incorporating temporal consistency constraints for video frames. An evaluation metric based on precision, recall, F-score, mean average precision (mAP), structured similarity index (SSIM) and subjective user studies is applied to different methods surveyed in order to compare performance. We find that diagnostic captioning can achieve relatively high accuracy over many benchmark datasets however there remains ample scope for improvement particularly where rare objects or interactions occur leading up to unusual events.",1
"Thanks to the availability of powerful computing resources, big data and deep learning algorithms, we have made great progress on computer vision in the last few years. Computer vision systems begin to surpass humans in some tasks, such as object recognition, object detection, face recognition and pose estimation. Lots of computer vision algorithms have been deployed to real world applications and started to improve our life quality. However, big data and labels are not always available. Sometimes we only have very limited labeled data, such as medical images which requires experts to label them. In this paper, we study few shot image classification, in which we only have very few labeled data. Machine learning with little data is a big challenge. To tackle this challenge, we propose two methods and test their effectiveness thoroughly. One method is to augment image features by mixing the style of these images. The second method is applying spatial attention to explore the relations between patches of images. We also find that domain shift is a critical issue in few shot learning when the training domain and testing domain are different. So we propose a more realistic cross-domain few-shot learning with unlabeled data setting, in which some unlabeled data is available in the target domain. We propose two methods in this setting. Our first method transfers the style information of the unlabeled target dataset to the samples in the source dataset and trains a model with stylized images and original images. Our second method proposes a unified framework to fully utilize all the data. Both of our methods surpass the baseline method by a large margin.",0
"In many fields, collecting large amounts of labeled training data can be difficult and time-consuming. As a result, machine learning models often have insufficient data for effective training, leading to poor model performance. This paper explores methods for improving machine learning performance with limited datasets by leveraging transfer learning and unsupervised pretraining techniques. We demonstrate how these approaches can effectively utilize available resources and improve results compared to traditional methods that require larger datasets. Our experiments show promising results on several challenging tasks, highlighting the potential of our approach for enhancing machine learning performance under resource constraints. Overall, we aim to provide insight into new ways to tackle problems where large dataset acquisition may prove impractical while still achieving high levels of accuracy and performance.",1
"There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.",0
"Graphs have emerged as important data structures in diverse fields due to their ability to model complex relationships and patterns that cannot easily be captured using traditional tabular representations. As a result, there has been significant interest in developing machine learning algorithms that can operate directly on graph structured data. In this work, we present a comprehensive review of current state-of-the-art methods in the field of graph machine learning, and introduce our own novel framework, which we call Graph Learning Machines (GLM). We propose GLM as a general approach for solving various graph machine learning tasks by casting them into standard supervised/semi-supervised/unsupervised learning problems. Our main contributions include defining a taxonomy for graph-based machine learning models and a new algorithmic framework that unifies several existing approaches under one roof. We demonstrate the effectiveness of GLM through extensive experiments on multiple real world datasets across different application domains.",1
"Person Re-identification (ReID) is a critical computer vision task which aims to match the same person in images or video sequences. Most current works focus on settings where the resolution of images is kept the same. However, the resolution is a crucial factor in person ReID, especially when the cameras are at different distances from the person or the camera's models are different from each other. In this paper, we propose a novel two-stream network with a lightweight resolution association ReID feature transformation (RAFT) module and a self-weighted attention (SWA) ReID module to evaluate features under different resolutions. RAFT transforms the low resolution features to corresponding high resolution features. SWA evaluates both features to get weight factors for the person ReID. Both modules are jointly trained to get a resolution-invariant representation. Extensive experiments on five benchmark datasets show the effectiveness of our method. For instance, we achieve Rank-1 accuracy of 43.3% and 83.2% on CAVIAR and MLR-CUHK03, outperforming the state-of-the-art.",0
"This paper introduces a novel method for person re-identification (ReID) that uses feature transformation and self-weighted attention mechanisms to create resolution-independent features. The method first applies a nonlinear mapping function to extract features from images of pedestrians at different scales and angles. Then, a multi-scale self-attention network is used to generate weight maps that highlight key regions in each image and improve the discriminability of the features. Experimental results show that our approach outperforms state-of-the-art methods across multiple benchmarks datasets while maintaining high efficiency due to the use of efficient backbones and lightweight components. Our study demonstrates the effectiveness of using feature transformations combined with self-weighted attention mechanisms for robust and accurate person ReID.",1
"Self-attention architectures, which are rapidly pushing the frontier in natural language processing, demonstrate a surprising depth-inefficient behavior: previous works indicate that increasing the internal representation (network width) is just as useful as increasing the number of self-attention layers (network depth). We theoretically predict a width-dependent transition between depth-efficiency and depth-inefficiency in self-attention. We conduct systematic empirical ablations on networks of depths 6 to 48 that clearly reveal the theoretically predicted behaviors, and provide explicit quantitative suggestions regarding the optimal depth-to-width allocation for a given self-attention network size. The race towards beyond 1-Trillion parameter language models renders informed guidelines for increasing self-attention depth and width in tandem an essential ingredient. Our guidelines elucidate the depth-to-width trade-off in self-attention networks of sizes up to the scale of GPT3 (which we project to be too deep for its size), and beyond, marking an unprecedented width of 30K as optimal for a 1-Trillion parameter network.",0
"This paper focuses on analyzing the tradeoffs between model depth and width in self-attention models using a variety of natural language processing tasks. The authors first provide a comprehensive analysis of the effectiveness and efficiency of models at different depths and widths by evaluating them on several benchmark datasets. They then propose a method for automatically determining optimal depth and width settings based on each taskâ€™s unique characteristics, enabling more effective use of computational resources while maintaining high levels of performance. Finally, they demonstrate that their approach outperforms existing methods across multiple NLP tasks, providing insight into how practitioners can effectively choose architectures for maximum impact. Overall, the research contributes significant knowledge to the field by presenting important insights into the interrelationship between model architecture and performance. As such, it has implications for both academics and industry professionals who seek to create more efficient NLP systems. Keywords: depth, width, attention, architecture search, neural networks, natural language processing",1
"A big, diverse and balanced training data is the key to the success of deep neural network training. However, existing publicly available datasets used in facial landmark localization are usually much smaller than those for other computer vision tasks. A small dataset without diverse and balanced training samples cannot support the training of a deep network effectively. To address the above issues, this paper presents a novel Separable Batch Normalization (SepBN) module with a Cross-protocol Network Training (CNT) strategy for robust facial landmark localization. Different from the standard BN layer that uses all the training data to calculate a single set of parameters, SepBN considers that the samples of a training dataset may belong to different sub-domains. Accordingly, the proposed SepBN module uses multiple sets of parameters, each corresponding to a specific sub-domain. However, the selection of an appropriate branch in the inference stage remains a challenging task because the sub-domain of a test sample is unknown. To mitigate this difficulty, we propose a novel attention mechanism that assigns different weights to each branch for automatic selection in an effective style. As a further innovation, the proposed CNT strategy trains a network using multiple datasets having different facial landmark annotation systems, boosting the performance and enhancing the generalization capacity of the trained network. The experimental results obtained on several well-known datasets demonstrate the effectiveness of the proposed method.",0
"Abstract: One of the key challenges faced by facial landmark localization algorithms is handling variations in image quality and lighting conditions across different datasets. To address this issue, we propose a novel approach based on batch normalization that allows us to train networks using multiple protocols simultaneously. Our method uses separable batch normalization (SepBN) layers, which enable each network stream to learn its own scale and shift parameters without affecting the others. This leads to improved performance and robustness compared to traditional batch normalization, especially in cross-dataset evaluations where there can be significant differences in data distributions. In addition, we present an efficient implementation of SepBN that significantly reduces computational overhead while still maintaining accuracy. We demonstrate the effectiveness of our approach through extensive experiments on two popular benchmark datasets and show that our method outperforms state-of-the-art methods for both within-dataset evaluation and cross-dataset generalization tasks. Overall, these results highlight the potential of separeable batch normalization as a powerful technique for improving facial landmark localization algorithms in real-world settings.",1
"There is significant progress in recognizing traditional human activities from videos focusing on highly distinctive actions involving discriminative body movements, body-object and/or human-human interactions. Driver's activities are different since they are executed by the same subject with similar body parts movements, resulting in subtle changes. To address this, we propose a novel framework by exploiting the spatiotemporal attention to model the subtle changes. Our model is named Coarse Temporal Attention Network (CTA-Net), in which coarse temporal branches are introduced in a trainable glimpse network. The goal is to allow the glimpse to capture high-level temporal relationships, such as 'during', 'before' and 'after' by focusing on a specific part of a video. These branches also respect the topology of the temporal dynamics in the video, ensuring that different branches learn meaningful spatial and temporal changes. The model then uses an innovative attention mechanism to generate high-level action specific contextual information for activity recognition by exploring the hidden states of an LSTM. The attention mechanism helps in learning to decide the importance of each hidden state for the recognition task by weighing them when constructing the representation of the video. Our approach is evaluated on four publicly accessible datasets and significantly outperforms the state-of-the-art by a considerable margin with only RGB video as input.",0
"Driver activity recognition has been studied by researchers since 2006 because driverâ€™s activities behind the wheel can have serious consequences on traffic safety. There are two types of driver activity classification: coarse level activities like â€œdrivingâ€, â€œstopping at signalâ€ and fine grained activities such as â€œaccelerating from 0km/hâ€, â€œturning leftâ€. However the accuracy of existing models was low due to issues like occlusion, view angle changes and lightning condition variations. This led to new research into coarse temporal attention networks. These systems aim to capture temporal dependencies across multiple frames and focus on spatial features without losing critical time intervals that could affect driving behaviors. Previous work showed that these methods improved accuracy over traditional techniques but were limited in their ability to generalize across different scenarios. In contrast we propose CTA-net, which combines coarsely attended feature maps with corresponding attention weights along both frame sequence and channel dimensions. Extensive experiments demonstrate our method achieves superior results compared with state of art approaches on three widely used datasets. For example, our approach reduces error rates from 8% to 4% comparing with previous baseline model using one of these datasets. Overall the paper demonstrates how coarse temporally attention network can improve accuracies of driver activity recognition. Keywords: Driver activity recognition; coarse temporally attention network(CTA- Net); convolution neural networks(CNN).",1
"Deep convolutional neural networks (CNNs) have shown a strong ability in mining discriminative object pose and parts information for image recognition. For fine-grained recognition, context-aware rich feature representation of object/scene plays a key role since it exhibits a significant variance in the same subcategory and subtle variance among different subcategories. Finding the subtle variance that fully characterizes the object/scene is not straightforward. To address this, we propose a novel context-aware attentional pooling (CAP) that effectively captures subtle changes via sub-pixel gradients, and learns to attend informative integral regions and their importance in discriminating different subcategories without requiring the bounding-box and/or distinguishable part annotations. We also introduce a novel feature encoding by considering the intrinsic consistency between the informativeness of the integral regions and their spatial structures to capture the semantic correlation among them. Our approach is simple yet extremely effective and can be easily applied on top of a standard classification backbone network. We evaluate our approach using six state-of-the-art (SotA) backbone networks and eight benchmark datasets. Our method significantly outperforms the SotA approaches on six datasets and is very competitive with the remaining two.",0
"Recently there has been significant interest in understanding how attention mechanisms can enable neural networks to focus on different regions of input data at different times. These mechanisms have proven to be quite successful, but most existing methods use a fixed number of spatial locations to attend to, which may not generalize well across tasks, datasets, or model architectures. In our work, we propose a novel approach called context-aware attentional pooling (CAP), wherein each location receives a dynamic weight based on both the local features and global context. This allows us to directly predict per-location weights without explicitly computing their correspondences with input positions. Our method achieves state-of-the-art results on two fine-grained visual classification benchmarks while significantly reducing computational cost compared to competitive baselines. Additionally, extensive ablation studies demonstrate that our CAP module contributes substantially more than simple multi-scale fusion or alternative attentional mechanisms alone. Overall, our contributions provide insights into adaptive feature integration for task-specific decision making and highlight opportunities for using attention mechanisms in real-world applications.",1
"Affect is often expressed via non-verbal body language such as actions/gestures, which are vital indicators for human behaviors. Recent studies on recognition of fine-grained actions/gestures in monocular images have mainly focused on modeling spatial configuration of body parts representing body pose, human-objects interactions and variations in local appearance. The results show that this is a brittle approach since it relies on accurate body parts/objects detection. In this work, we argue that there exist local discriminative semantic regions, whose ""informativeness"" can be evaluated by the attention mechanism for inferring fine-grained gestures/actions. To this end, we propose a novel end-to-end \textbf{Regional Attention Network (RAN)}, which is a fully Convolutional Neural Network (CNN) to combine multiple contextual regions through attention mechanism, focusing on parts of the images that are most relevant to a given task. Our regions consist of one or more consecutive cells and are adapted from the strategies used in computing HOG (Histogram of Oriented Gradient) descriptor. The model is extensively evaluated on ten datasets belonging to 3 different scenarios: 1) head pose recognition, 2) drivers state recognition, and 3) human action and facial expression recognition. The proposed approach outperforms the state-of-the-art by a considerable margin in different metrics.",0
"This abstract describes a new architecture called Regional Attention Networks that addresses head pose estimation and fine-grained gesture recognition tasks. We present two models based on our RAN architecture: one performs head pose estimation across different poses using monocular images; another one tackles a more challenging problem of fine-grained gesture recognition. Experimental results show that our approach achieves state-of-the-art performance on both tasks while efficiently leveraging spatial attention mechanisms, reducing computational cost compared to standard architectures. Our work showscase the applicability of regional attention mechanisms beyond object detection and segmentation, where they have found widespread adoption.",1
"Averaging scheme has attracted extensive attention in deep learning as well as traditional machine learning. It achieves theoretically optimal convergence and also improves the empirical model performance. However, there is still a lack of sufficient convergence analysis for strongly convex optimization. Typically, the convergence about the last iterate of gradient descent methods, which is referred to as individual convergence, fails to attain its optimality due to the existence of logarithmic factor. In order to remove this factor, we first develop gradient descent averaging (GDA), which is a general projection-based dual averaging algorithm in the strongly convex setting. We further present primal-dual averaging for strongly convex cases (SC-PDA), where primal and dual averaging schemes are simultaneously utilized. We prove that GDA yields the optimal convergence rate in terms of output averaging, while SC-PDA derives the optimal individual convergence. Several experiments on SVMs and deep learning models validate the correctness of theoretical analysis and effectiveness of algorithms.",0
"This paper presents two new gradient-based methods for strongly convex optimization: gradient descent averaging (GDA) and primal-dual averaging (PDA). We show that GDA converges linearly in time and PDA converges faster than existing algorithms. These new methods have applications in machine learning, signal processing, and other fields where fast, reliable optimization is critical. ----- How could I improve my research skills? Here are some suggestions: Set up a structured approach to keep track of your progress Use resources such as academic search engines, databases like Google Scholar etc... Look into data mining techniques to find relevant papers which you might otherwise miss Keep reading at least one journal pertaining to your field to broaden your understanding Try using text summarisation tools in your literature searches if there is a lot of material to go through",1
"Annotating videos with object segmentation masks typically involves a two stage procedure of drawing polygons per object instance for all the frames and then linking them through time. While simple, this is a very tedious, time consuming and expensive process, making the creation of accurate annotations at scale only possible for well-funded labs. What if we were able to segment an object in the full video with only a single click? This will enable video segmentation at scale with a very low budget opening the door to many applications. Towards this goal, in this paper we propose a bottom up approach where given a single click for each object in a video, we obtain the segmentation masks of these objects in the full video. In particular, we construct a correlation volume that assigns each pixel in a target frame to either one of the objects in the reference frame or the background. We then refine this correlation volume via a recurrent attention module and decode the final segmentation. To evaluate the performance, we label the popular and challenging Cityscapes dataset with video object segmentations. Results on this new CityscapesVideo dataset show that our approach outperforms all the baselines in this challenging setting.",0
"Title: An Automatic Pipeline For Object Tracking And Segmentation from Low Resolution Images Of Chest Radiographs. Authors: Dr. Joseph Sanguinetti*, Dr. Mengdie Chen, Professor John Smith, Mr. Bob Jones (Medical Physics Department) Aims: To create an automatic pipeline that enables object tracking and segmentation in low resolution chest radiographic images using deep learning algorithms. This could potentially reduce manual intervention required by radiologists and aid their interpretation leading to improved patient outcomes. Methods: We have trained a convolutional neural network on our dataset consisting of lung regions of interest extracted using bounding boxes. Our model takes as input image patches surrounding detected bounding box coordinates and predicts binary mask of those objects enabling us to separate them from background. For evaluation we use two metrics; Intersection over Union (IOU) and Visual Assessments performed by three expert physicians. Results: In testing, we achieved mean IOU scores of 47% which is significantly higher than random baseline indicating some success in separating objects of interest (p<0.0001). Three experts were able to accurately trace contours on 86%, 92%, 94% of test cases respectively where ground truth was available. However due to high levels of noise present in the images the overall accuracy decreased compared to results obtained in training data. Conclusion: Whilst there is room for improvement, our initial findings demonstrate potential application of such models for automated detection and separation of pulmonary nodules on chest radiography. Further work needs to focus on improving algorithm robustness against variability present in low resolution images including presence of scanner artifacts and uneven contrast between shadows in mammograms. Additionally investigations into increasing algorithm speed must continue since currently processing times remain unrealistically slow for real time clinical applications. Ultimately widesprea",1
"Multi-orientation scene text detection has recently gained significant research attention. Previous methods directly predict words or text lines, typically by using quadrilateral shapes. However, many of these methods neglect the significance of consistent labeling, which is important for maintaining a stable training process, especially when it comprises a large amount of data. Here we solve this problem by proposing a new method, Orderless Box Discretization (OBD), which first discretizes the quadrilateral box into several key edges containing all potential horizontal and vertical positions. To decode accurate vertex positions, a simple yet effective matching procedure is proposed for reconstructing the quadrilateral bounding boxes. Our method solves the ambiguity issue, which has a significant impact on the learning process. Extensive ablation studies are conducted to validate the effectiveness of our proposed method quantitatively. More importantly, based on OBD, we provide a detailed analysis of the impact of a collection of refinements, which may inspire others to build state-of-the-art text detectors. Combining both OBD and these useful refinements, we achieve state-of-the-art performance on various benchmarks, including ICDAR 2015 and MLT. Our method also won the first place in the text detection task at the recent ICDAR2019 Robust Reading Challenge for Reading Chinese Text on Signboards, further demonstrating its superior performance. The code is available at https://git.io/TextDet.",0
"Exploring Discrete Representation Learning via Adversarial Fine-Tuning: An Empirical Study on Orderless Box Discretization Networks for Multi-Orientation Scene Text Detection  Discrete representation learning has been shown to improve the performance of several computer vision tasks such as image classification, object detection, and semantic segmentation. In this work, we explore the use of orderless box discretization networks (OBCD) for multi-orientation scene text detection. OBCD transforms continuous signals into discrete ones by dividing the signal domain uniformly at random intervals. By doing so, the resulting network learns robust representations that can generalize better across different orientations of text within scenes. We propose two variants of OBCD models, namely, adversarial fine-tuned (AFT) OBCD, which uses an adversary loss to enforce consistency during training, and standard OBCD without any additional regularization. Our empirical study shows that both variants outperform state-of-the-art text detectors while consuming less computational resources. Additionally, our analysis indicates that AFT OBCD achieves higher accuracy and robustness compared to other methods, making it suitable for real-world applications. Overall, these findings demonstrate the potential benefits of using OBCD networks for multi-orientation scene text detection, and open new opportunities for further research in the field.",1
"Non-IID data present a tough challenge for federated learning. In this paper, we explore a novel idea of facilitating pairwise collaborations between clients with similar data. We propose FedAMP, a new method employing federated attentive message passing to facilitate similar clients to collaborate more. We establish the convergence of FedAMP for both convex and non-convex models, and propose a heuristic method to further improve the performance of FedAMP when clients adopt deep neural networks as personalized models. Our extensive experiments on benchmark data sets demonstrate the superior performance of the proposed methods.",0
"Federated learning has emerged as a promising technique for machine learning at scale by enabling multiple parties to collaborate without sharing their data. However, real world datasets often exhibit non-iid (independent and identically distributed) characteristics, which makes federated learning challenging. In order to address these issues, we propose personalized cross-silo federated learning, where each party performs local training using their own unique data distribution, while still benefiting from collaborative learning across all participating parties. We evaluate our approach through extensive experimentation on several benchmark datasets and demonstrate that it leads to significant improvements over traditional federated learning approaches. Our results show that personalized cross-silo federated learning is effective even under highly skewed and imbalanced non-iid settings, making it well suited for real world applications such as smart cities and healthcare. Finally, we discuss future directions for research in this area including scalability and privacy considerations.",1
"Prediction in a new domain without any training sample, called zero-shot domain adaptation (ZSDA), is an important task in domain adaptation. While prediction in a new domain has gained much attention in recent years, in this paper, we investigate another potential of ZSDA. Specifically, instead of predicting responses in a new domain, we find a description of a new domain given a prediction. The task is regarded as predictive optimization, but existing predictive optimization methods have not been extended to handling multiple domains. We propose a simple framework for predictive optimization with ZSDA and analyze the condition in which the optimization problem becomes convex optimization. We also discuss how to handle the interaction of characteristics of a domain in predictive optimization. Through numerical experiments, we demonstrate the potential usefulness of our proposed framework.",0
"Machine learning models often struggle to adapt to new domains without significant amounts of fine-tuning data from those domains, resulting in suboptimal performance on target tasks. This work presents a novel approach that leverages zero-shot domain adaptation (ZSDA) techniques to facilitate predictive optimization across domains. By integrating ZSDA into standard machine learning pipelines, we demonstrate improved cross-domain generalization performance while requiring minimal labeled training data in each new domain. Our method introduces regularized multi-task learning objectives which leverage knowledge transfer from related source tasks towards efficient and effective solutions in unseen domains. Empirical evaluations show consistent improvements over strong baseline methods across diverse application scenarios, including natural language processing and computer vision tasks. These promising results suggest broad potential applications of our ZSDA technique in enabling more versatile machine learning systems that can quickly adapt to new environments without needing extensive retraining.",1
I propose a new tool to characterize the resolution of uncertainty around FOMC press conferences. It relies on the construction of a measure capturing the level of discussion complexity between the Fed Chair and reporters during the Q&A sessions. I show that complex discussions are associated with higher equity returns and a drop in realized volatility. The method creates an attention score by quantifying how much the Chair needs to rely on reading internal documents to be able to answer a question. This is accomplished by building a novel dataset of video images of the press conferences and leveraging recent deep learning algorithms from computer vision. This alternative data provides new information on nonverbal communication that cannot be extracted from the widely analyzed FOMC transcripts. This paper can be seen as a proof of concept that certain videos contain valuable information for the study of financial markets.,0
"This study examines the impact of Federal Open Market Committee (FOMC) press conference announcements on financial markets using cutting-edge techniques from the field of computer vision. By analyzing audio-visual data from televised press conferences, we identify new patterns and signals that have previously gone unnoticed by traditional market analysis methods. Our findings show that nonverbal cues such as facial expressions and gestures can provide important insights into the sentiment of policymakers, which in turn may influence investor behavior. We demonstrate how our approach can enhance understanding of central bank communication strategies, aid in risk management for traders and portfolio managers, and contribute to broader debates around monetary policy transparency and accountability. Overall, our research offers a valuable contribution to both financial economics and computational social science, highlighting the potential benefits of integrating advanced technologies into economic analysis.",1
"Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.   In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.   More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",0
"Image transformers have achieved state-of-the-art results on computer vision tasks using self-attention mechanisms that capture global dependencies among image patches. However, training such models often requires massive amounts of computational resources and large datasets. In our work, we propose novel methods for reducing the amount of required computation during both training and inference while still maintaining high performance. Our key idea is to add two simple components to existing architectures: a channel splitting module (CSM) before each block and an output ensemble mechanism. We show how these components can effectively reduce memory usage, allowing us to train larger models more efficiently, even on smaller datasets. Furthermore, by combining the ideas from our previous work with this new approach, we obtain an algorithmic framework that allows for fast adaptation from pretrained models to fine-grained downstream tasks. To ensure generalization across different datasets and tasks, we introduce a method called ""TaskAware Distillation"" which trains a small student network alongside the teacher model during the finetuning process. By carefully designing a temperature hyperparameter schedule and adding this novel training component, we achieve better adaptability without losing efficiency. We evaluate our proposed models on several benchmarks demonstrating competitive results compared to baseline methods while saving computational cost. This makes them wellsuited for deployment on limited hardware devices. Overall, our contributions enable researchers to leverage cutting edge methods on challenging problems using less compute power than ever before.",1
"Recurrent Neural Networks (RNN) received a vast amount of attention last decade. Recently, the architectures of Recurrent AutoEncoders (RAE) found many applications in practice. RAE can extract the semantically valuable information, called context that represents a latent space useful for further processing. Nevertheless, recurrent autoencoders are hard to train, and the training process takes much time. In this paper, we propose an autoencoder architecture with sequence-aware encoding, which employs 1D convolutional layer to improve its performance in terms of model training time. We prove that the recurrent autoencoder with sequence-aware encoding outperforms a standard RAE in terms of training speed in most cases. The preliminary results show that the proposed solution dominates over the standard RAE, and the training process is order of magnitude faster.",0
"This paper presents a new method for training recurrent neural networks using an autoencoding framework that incorporates sequence awareness into the encoding process. We introduce a novel architecture called the ""Sequence-Aware Recurrent AutoEncoder"" (SARAE) which uses gating mechanisms to allow the model to selectively attend to different parts of the input sequence during encoding. Our approach enables the network to learn more meaningful representations by capturing both short-term and long-term dependencies within sequential data. In addition, we show that our method outperforms traditional RNNs on several benchmark datasets while requiring fewer parameters and less computational overhead. Overall, SARAE provides a powerful alternative for training generative models on sequential data.",1
"We introduce a novel self-attention-based normal estimation network that is able to focus softly on relevant points and adjust the softness by learning a temperature parameter, making it able to work naturally and effectively within a large neighbourhood range. As a result, our model outperforms all existing normal estimation algorithms by a large margin, achieving 94.1% accuracy in comparison with the previous state of the art of 91.2%, with a 25x smaller model and 12x faster inference time. We also use point-to-plane Iterative Closest Point (ICP) as an application case to show that our normal estimations lead to faster convergence than normal estimations from other methods, without manually fine-tuning neighbourhood range parameters. Code available at https://code.active.vision.",0
"This paper presents a new deep learning method that estimates normals for point clouds without relying on local neighborhood information. Our network architecture uses multi-scale convolutions combined with batch normalization to effectively capture contextual information from large datasets. We demonstrate through experiments on benchmark data sets that our method outperforms previous state-of-the-art methods in terms of accuracy while maintaining realtime performance. Moreover, we showcase applications of our approach such as robust plane fitting and surface completion tasks where the quality of results improve significantly by using the proposed method.",1
"Blind image deblurring is a fundamental and challenging computer vision problem, which aims to recover both the blur kernel and the latent sharp image from only a blurry observation. Despite the superiority of deep learning methods in image deblurring have displayed, there still exists major challenge with various non-uniform motion blur. Previous methods simply take all the image features as the input to the decoder, which handles different degrees (e.g. large blur, small blur) simultaneously, leading to challenges for sharp image generation. To tackle the above problems, we present a deep two-branch network to deal with blurry images via a component divided module, which divides an image into two components based on the representation of blurry degree. Specifically, two component attentive blocks are employed to learn attention maps to exploit useful deblurring feature representations on both large and small blurry regions. Then, the blur-aware features are fed into two-branch reconstruction decoders respectively. In addition, a new feature fusion mechanism, orientation-based feature fusion, is proposed to merge sharp features of the two branches. Both qualitative and quantitative experimental results show that our method performs favorably against the state-of-the-art approaches.",0
"Non Uniform motion deblurring with blurry component divided guidance (NMD) is a method that uses two networks in parallel. One network takes as input the blurred image with sharp edges masked out using another binary edge map. This mask ensures only smooth areas participate in guiding the first network which takes this blurred input and outputs an intermediate image. Meanwhile, the second network works on directly reconstructing the final sharp output without any blur constraints but has access to both the original unprocessed images as well as the blur kernel estimated by other methods such as KCF tracker. NMD balances these outputs with guidance from blur components after they have been estimated by applying filters with different strengths along direction maps estimated using optical flow. We show results on multiple datasets where our approach performs better than previous state of art.",1
"Attention is a commonly used mechanism in sequence processing, but it is of O(n^2) complexity which prevents its application to long sequences. The recently introduced neural Shuffle-Exchange network offers a computation-efficient alternative, enabling the modelling of long-range dependencies in O(n log n) time. The model, however, is quite complex, involving a sophisticated gating mechanism derived from the Gated Recurrent Unit. In this paper, we present a simple and lightweight variant of the Shuffle-Exchange network, which is based on a residual network employing GELU and Layer Normalization. The proposed architecture not only scales to longer sequences but also converges faster and provides better accuracy. It surpasses the Shuffle-Exchange network on the LAMBADA language modelling task and achieves state-of-the-art performance on the MusicNet dataset for music transcription while being efficient in the number of parameters. We show how to combine the improved Shuffle-Exchange network with convolutional layers, establishing it as a useful building block in long sequence processing applications.",0
"Deep learning has recently shown great successes on processing sequential data such as natural language text, speech signals, and even biological sequence analysis problems. To address more challenging and longer input sequences, novel architectures have been proposed that leverage global dependencies within the networks by exchanging intermediate activations between residual modules or layers. Motivated by these works, we introduce here Residual Shuffle-Exchange Networks (RSE), which apply a shuffling operation followed by element-wise exchange across dimensions before feeding into subsequent convolutional or fully connected layers. Our approach differs from previous work in two main ways: firstly, RSE enables efficient parallelization through independent dimension shifting and permutation operations; secondly, our method can generate more diverse inter-layer interactions than existing alternatives. Empirical results show that RSE significantly reduces training times without compromising model quality. We present applications to sentiment analysis of movie reviews and large vocabulary continuous speech recognition tasks, where we obtain superior performance over several strong baselines. Our method provides an excellent starting point towards further research on designing deep models capable of processing very long and complex sequence data efficiently.",1
"To read the final version please go to IEEE TGRS on IEEE Xplore. Convolutional neural networks (CNNs) have been attracting increasing attention in hyperspectral (HS) image classification, owing to their ability to capture spatial-spectral feature representations. Nevertheless, their ability in modeling relations between samples remains limited. Beyond the limitations of grid sampling, graph convolutional networks (GCNs) have been recently proposed and successfully applied in irregular (or non-grid) data representation and analysis. In this paper, we thoroughly investigate CNNs and GCNs (qualitatively and quantitatively) in terms of HS image classification. Due to the construction of the adjacency matrix on all the data, traditional GCNs usually suffer from a huge computational cost, particularly in large-scale remote sensing (RS) problems. To this end, we develop a new mini-batch GCN (called miniGCN hereinafter) which allows to train large-scale GCNs in a mini-batch fashion. More significantly, our miniGCN is capable of inferring out-of-sample data without re-training networks and improving classification performance. Furthermore, as CNNs and GCNs can extract different types of HS features, an intuitive solution to break the performance bottleneck of a single model is to fuse them. Since miniGCNs can perform batch-wise network training (enabling the combination of CNNs and GCNs) we explore three fusion strategies: additive fusion, element-wise multiplicative fusion, and concatenation fusion to measure the obtained performance gain. Extensive experiments, conducted on three HS datasets, demonstrate the advantages of miniGCNs over GCNs and the superiority of the tested fusion strategies with regards to the single CNN or GCN models. The codes of this work will be available at https://github.com/danfenghong/IEEE_TGRS_GCN for the sake of reproducibility.",0
"Graph convolutional networks (GCN) have recently gained popularity in the field of computer vision due to their ability to handle graph structured data such as graphs, trees, and hierarchies. In hyperspectral image classification, GCNs can be used effectively since they exploit both spectral and spatial information present in the images. This study proposes a novel approach using GCNs for hyperspectral image classification by utilizing spectral features along with local neighborhood relationships among pixels captured via graph theory concepts. Experimental results on two different datasets show that the proposed method outperforms state-of-the-art methods in terms of accuracy and robustness. The findings suggest that incorporating graph learning into hyperspectral imagery analysis enhances performance and provides insights into new applications within remote sensing.",1
"Deep learning techniques, such as Deep Boltzmann Machines (DBMs), have received considerable attention over the past years due to the outstanding results concerning a variable range of domains. One of the main shortcomings of these techniques involves the choice of their hyperparameters, since they have a significant impact on the final results. This work addresses the issue of fine-tuning hyperparameters of Deep Boltzmann Machines using metaheuristic optimization techniques with different backgrounds, such as swarm intelligence, memory- and evolutionary-based approaches. Experiments conducted in three public datasets for binary image reconstruction showed that metaheuristic techniques can obtain reasonable results.",0
"This study presents a metaheuristic approach to fine-tuning deep Boltzmann machines (DBMs) by utilizing optimization algorithms inspired by natural phenomena such as genetic evolution and swarm intelligence. These metaheuristics aim to improve upon traditional methods used in DBM training that rely on gradient descent alone, which can result in suboptimal solutions. Our novel methodology leverages these metaheuristics to drive the search process towards better solutions while maintaining diversity in the population. We demonstrate through numerical experiments on benchmark datasets that our approach leads to significant improvements in terms of accuracy and computational efficiency compared to state-of-the-art techniques for both supervised and unsupervised learning tasks. Additionally, we provide insights into how different components and parameters within the proposed framework influence overall performance, which could serve as guidelines for future work on enhancing DBM training using metaheuristics. Overall, our findings indicate promising potential applications of metahueritic approaches in optimizing DBMs, particularly in real-world scenarios where large amounts of data may not always be available.",1
"Time series classification (TSC) gained a lot of attention in the past decade and number of methods for representing and classifying time series have been proposed. Nowadays, methods based on convolutional networks and ensemble techniques represent the state of the art for time series classification. Techniques transforming time series to image or text also provide reliable ways to extract meaningful features or representations of time series. We compare the state-of-the-art representation and classification methods on a specific application, that is predictive maintenance from sequences of event logs. The contributions of this paper are twofold: introducing a new data set for predictive maintenance on automated teller machines (ATMs) log data and comparing the performance of different representation methods for predicting the occurrence of a breakdown. The problem is difficult since unlike the classic case of predictive maintenance via signals from sensors, we have sequences of discrete event logs occurring at any time and the lengths of the sequences, corresponding to life cycles, vary a lot.",0
"This study addresses the problem of time series classification for predictive maintenance on event logs. Time series data can provide valuable insights into system behavior by capturing patterns over time. However, traditional time series prediction techniques tend to focus on accuracy alone without considering their effectiveness for specific applications such as predictive maintenance. In this research, we propose a methodology that combines several features extracted from raw event logs, including numerical ones generated from symbolic aggregation, statistical characteristics, correlation and causality measures, and complex network properties. We evaluate our approach using real world data sets and demonstrate its ability to accurately classify different types of events and support effective decision making in maintaining industrial systems. Our work contributes to both academia and industry, providing new tools and approaches for dealing with time series data in complex systems.",1
"Pedestrian trajectory prediction is a critical yet challenging task, especially for crowded scenes. We suggest that introducing an attention mechanism to infer the importance of different neighbors is critical for accurate trajectory prediction in scenes with varying crowd size. In this work, we propose a novel method, AVGCN, for trajectory prediction utilizing graph convolutional networks (GCN) based on human attention (A denotes attention, V denotes visual field constraints). First, we train an attention network that estimates the importance of neighboring pedestrians, using gaze data collected as subjects perform a bird's eye view crowd navigation task. Then, we incorporate the learned attention weights modulated by constraints on the pedestrian's visual field into a trajectory prediction network that uses a GCN to aggregate information from neighbors efficiently. AVGCN also considers the stochastic nature of pedestrian trajectories by taking advantage of variational trajectory prediction. Our approach achieves state-of-the-art performance on several trajectory prediction benchmarks, and the lowest average prediction error over all considered benchmarks.",0
"In our paper we propose AVGCN (Attendence guided Vertical GCN), a novel approach that uses human attention mechanisms to improve trajectory prediction. Our model integrates human attention into graph convolutional networks for enhanced performance on two challenging datasets: ETH pedestrian and Stanford Drone dataset. We first train an encoder network to predict future locations for several time steps. Then, we use these predictions as input for our proposed AVGCN model. Finally, our model produces refined trajectories through the integration of visual features extracted from pretrained CNNs and human attention mechanisms. Through thorough experimentation, we demonstrate substantial improvements over state of the art methods while maintaining competitive inference times, confirming the effectiveness of our framework.",1
"Multi-party machine learning is a paradigm in which multiple participants collaboratively train a machine learning model to achieve a common learning objective without sharing their privately owned data. The paradigm has recently received a lot of attention from the research community aimed at addressing its associated privacy concerns. In this work, we focus on addressing the concerns of data privacy, model privacy, and data quality associated with privacy-preserving multi-party machine learning, i.e., we present a scheme for privacy-preserving collaborative learning that checks the participants' data quality while guaranteeing data and model privacy. In particular, we propose a novel metric called weight similarity that is securely computed and used to check whether a participant can be categorized as a reliable participant (holds good quality data) or not. The problems of model and data privacy are tackled by integrating homomorphic encryption in our scheme and uploading encrypted weights, which prevent leakages to the server and malicious participants, respectively. The analytical and experimental evaluations of our scheme demonstrate that it is accurate and ensures data and model privacy.",0
"In this paper we address the problem of reliability checking for multi-party machine learning tasks under privacy constraints. Specifically, we propose a novel approach that leverages weight similarity between models trained on different subsets of data to estimate model quality and detect malicious behavior by any party attempting to falsely report their local results. Our method allows each participant to locally train a model using only their own private dataset while ensuring high performance at the expense of reduced accuracy compared to centralized training. Extensive experimental evaluations demonstrate the effectiveness of our proposed approach in improving the reliability of multi-party learning systems without compromising user privacy.",1
"Electroencephalograph (EEG) emotion recognition is a significant task in the brain-computer interface field. Although many deep learning methods are proposed recently, it is still challenging to make full use of the information contained in different domains of EEG signals. In this paper, we present a novel method, called four-dimensional attention-based neural network (4D-aNN) for EEG emotion recognition. First, raw EEG signals are transformed into 4D spatial-spectral-temporal representations. Then, the proposed 4D-aNN adopts spectral and spatial attention mechanisms to adaptively assign the weights of different brain regions and frequency bands, and a convolutional neural network (CNN) is utilized to deal with the spectral and spatial information of the 4D representations. Moreover, a temporal attention mechanism is integrated into a bidirectional Long Short-Term Memory (LSTM) to explore temporal dependencies of the 4D representations. Our model achieves state-of-the-art performance on the SEED dataset under intra-subject splitting. The experimental results have shown the effectiveness of the attention mechanisms in different domains for EEG emotion recognition.",0
"In order to achieve state-of-the-art performance in real-time emotion recognition from electroencephalography (EEG) signals using deep learning methods, we propose a 4D attention-based neural network that efficiently captures temporal dependencies as well as spatial patterns in the data by incorporating both time and frequency domains simultaneously while handling variable channel configurations. Our approach builds upon recent advances in attention mechanisms and achieves robustness by addressing several key challenges such as variability in data preprocessing steps, channel montage selection and individual differences in emotional responses. We evaluate our proposed method on three benchmark datasets containing various emotions under different conditions and demonstrate significantly improved classification accuracy compared to existing techniques. Furthermore, we conduct comprehensive analysis studies comparing our modelâ€™s behavior with other commonly used models. This work presents significant contributions towards developing interpretable, efficient and generalizable deep learning approaches for real-world applications such as human affective computing, mental health monitoring, and neurofeedback systems where EEG signals play a crucial role.",1
"Accurately forecasting the future movements of surrounding vehicles is essential for safe and efficient operations of autonomous driving cars. This task is difficult because a vehicle's moving trajectory is greatly determined by its driver's intention, which is often hard to estimate. By leveraging attention mechanisms along with long short-term memory (LSTM) networks, this work learns the relation between a driver's intention and the vehicle's changing positions relative to road infrastructures, and uses it to guide the prediction. Different from other state-of-the-art solutions, our work treats the on-road lanes as non-Euclidean structures, unfolds the vehicle's moving history to form a spatio-temporal graph, and uses methods from Graph Neural Networks to solve the problem. Not only is our approach a pioneering attempt in using non-Euclidean methods to process static environmental features around a predicted object, our model also outperforms other state-of-the-art models in several metrics. The practicability and interpretability analysis of the model shows great potential for large-scale deployment in various autonomous driving systems in addition to our own.",0
"This is an interesting paper that proposes a novel method for predicting vehicles' moving trajectories based on their attention over lanes. The authors argue that traditional methods which solely rely on tracking vehicle motion fail to account for the complex interactions among multiple factors such as traffic flow dynamics, road conditions, and human driver behavior. Therefore, they propose a new framework called ""Lane Attention"" that models drivers' perception, decision making, and execution processes using deep learning techniques. They collect large amounts of natural driving data from real world scenarios and train their model to make accurate predictions of future lane changes and movements. Extensive evaluations show that the proposed method outperforms state-of-the-art approaches and achieves high accuracy in lane prediction tasks. Overall, the study demonstrates the effectiveness of integrating cognitive models into computer vision systems for improving safety and efficiency in transportation networks.",1
"We present a novel framework, Spatial Pyramid Attention Network (SPAN) for detection and localization of multiple types of image manipulations. The proposed architecture efficiently and effectively models the relationship between image patches at multiple scales by constructing a pyramid of local self-attention blocks. The design includes a novel position projection to encode the spatial positions of the patches. SPAN is trained on a generic, synthetic dataset but can also be fine tuned for specific datasets; The proposed method shows significant gains in performance on standard datasets over previous state-of-the-art methods.",0
"SPAN (Spatial Pyramid Attention Network) has been developed as a new method for image manipulation localization that utilizes spatial pyramids to capture different levels of details within images. The model then uses attention mechanisms to focus on important regions within these pyramids, allowing for more accurate localization of changes made to the original image. This approach significantly improves upon current methods by providing sharper edges and finer details while reducing noise and artifacts. Evaluations show that SPAN outperforms existing approaches on multiple benchmark datasets, demonstrating the effectiveness and versatility of this novel technique for image manipulation tasks. Overall, the results suggest that SPAN can serve as a powerful tool for researchers working in computer vision and graphics fields.",1
"Recent learning-based approaches have achieved impressive results in the field of single-shot camera localization. However, how best to fuse multiple modalities (e.g., image and depth) and to deal with degraded or missing input are less well studied. In particular, we note that previous approaches towards deep fusion do not perform significantly better than models employing a single modality. We conjecture that this is because of the naive approaches to feature space fusion through summation or concatenation which do not take into account the different strengths of each modality. To address this, we propose an end-to-end framework, termed VMLoc, to fuse different sensor inputs into a common latent space through a variational Product-of-Experts (PoE) followed by attention-based fusion. Unlike previous multimodal variational works directly adapting the objective function of vanilla variational auto-encoder, we show how camera localization can be accurately estimated through an unbiased objective function based on importance weighting. Our model is extensively evaluated on RGB-D datasets and the results prove the efficacy of our model. The source code is available at https://github.com/Zalex97/VMLoc.",0
"This paper presents VMLoc, a learning-based multimodal camera localization approach that leverages variational fusion to combine evidence from multiple modalities such as image retrieval, keypoint matching, and sensor data. Our method formulates camera pose estimation as a probabilistic inference problem using Bayesian filtering techniques, enabling robustness to uncertain measurements and modeling uncertainty. Experimental results on various public datasets showcase the effectiveness of our framework, outperforming state-of-the-art methods across different settings while offering superior computational efficiency. Our work contributes towards realizing accurate, reliable, and efficient solutions for visual localization challenges encountered in computer vision applications like augmented reality, autonomous robots, and more.",1
"Recently, deep learning based image deblurring has been well developed. However, exploiting the detailed image features in a deep learning framework always requires a mass of parameters, which inevitably makes the network suffer from high computational burden. To solve this problem, we propose a lightweight multiinformation fusion network (LMFN) for image deblurring. The proposed LMFN is designed as an encoder-decoder architecture. In the encoding stage, the image feature is reduced to various smallscale spaces for multi-scale information extraction and fusion without a large amount of information loss. Then, a distillation network is used in the decoding stage, which allows the network benefit the most from residual learning while remaining sufficiently lightweight. Meanwhile, an information fusion strategy between distillation modules and feature channels is also carried out by attention mechanism. Through fusing different information in the proposed approach, our network can achieve state-of-the-art image deblurring result with smaller number of parameters and outperforms existing methods in model complexity.",0
"Effective image restoration depends significantly on how accurately we can model underlying physical principles and integrate them into our algorithmic pipelines. As such, recent advances in deep learning have been instrumental toward enabling more efficient solutions that can better restore images corrupted by blur. In particular, the work presented here leverages several sources of information beyond just the input images themselves as part of a multi-task neural network architecture capable of effectively deblurring photos while working from limited training data samples. Our results show improved performance over traditional methods under low data regimes as well as state-of-the-art benchmarks. Ultimately, these findings further demonstrate the potential utility of incorporating high-level task knowledge via end-to-end trainable architectures within computer vision problems.",1
"We take a deep look into the behavior of self-attention heads in the transformer architecture. In light of recent work discouraging the use of attention distributions for explaining a model's behavior, we show that attention distributions can nevertheless provide insights into the local behavior of attention heads. This way, we propose a distinction between local patterns revealed by attention and global patterns that refer back to the input, and analyze BERT from both angles. We use gradient attribution to analyze how the output of an attention attention head depends on the input tokens, effectively extending the local attention-based analysis to account for the mixing of information throughout the transformer layers. We find that there is a significant discrepancy between attention and attribution distributions, caused by the mixing of context inside the model. We quantify this discrepancy and observe that interestingly, there are some patterns that persist across all layers despite the mixing.",0
"In recent years, natural language processing has seen significant advancements due to the introduction of large pre-trained models such as BERT (Bidirectional Encoder Representations from Transformers). Despite their widespread adoption and impressive performance on a variety of NLP tasks, the mechanisms underlying these models remain largely unexplained. This paper seeks to shed light on one key component of BERT's architecture - the attention mechanism - by examining how local and global attention work together to capture both intra-sentential and inter-sentential contextual relationships. We describe in detail the design choices that lead to BERT's specific implementation of self-attentional layers, including the relative benefits of local versus global aggregation of features. Through extensive experiments, we demonstrate how these variations impact model performance across multiple benchmark datasets and showcase the tradeoffs involved in making architectural decisions around attention. Our findings contribute new insights into the operation of transformer-based models, which may inform future research directions aimed at improving their interpretability, efficiency, and effectiveness. Ultimately, our goal is to provide a comprehensive understanding of BERT's attention system and highlight opportunities for further exploration within the broader field of NLP.",1
"Trajectory prediction is critical for applications of planning safe future movements and remains challenging even for the next few seconds in urban mixed traffic. How an agent moves is affected by the various behaviors of its neighboring agents in different environments. To predict movements, we propose an end-to-end generative model named Attentive Maps Encoder Network (AMENet) that encodes the agent's motion and interaction information for accurate and realistic multi-path trajectory prediction. A conditional variational auto-encoder module is trained to learn the latent space of possible future paths based on attentive dynamic maps for interaction modeling and then is used to predict multiple plausible future trajectories conditioned on the observed past trajectories. The efficacy of AMENet is validated using two public trajectory prediction benchmarks Trajnet and InD.",0
"Abstract:  Trajectory prediction is an important task in many fields such as autonomous driving, robotics, and computer vision. One key challenge is how to effectively encode the contextual information surrounding moving objects, which can significantly affect their future trajectories. In this work, we propose AMENet (Attentive Maps Encoder Network), a novel approach that utilizes attention mechanisms to encode attentional maps that capture spatio-temporal relationships among objects and their surroundings.  The proposed method consists of two main components: map encoding and trajectory decoding. During map encoding, our network generates an attentional map for each object by integrating its spatial features with those of nearby static environments. To achieve this, we introduce a novel Spatial Transformer Network module that learns to attend selectively to relevant locations based on the current state of the object. This allows us to incorporate fine-grained spatial details into the encoding process. Additionally, we use temporal convolutions to model the dynamic evolution of both the object and its environment over time.  In trajectory decoding, we employ a Recurrent Neural Network (RNN) architecture that processes the encoded attentional maps sequentially, predicting the future location of each object at every timestep. We further enhance the accuracy of our predictions by designing a novel recurrent connection mechanism that enables effective memory sharing across different objects and time steps.  Experimental results on several challenging benchmark datasets demonstrate the superiority of our approach compared to previous methods, achieving significant improvements in terms of prediction accuracy and robustness under varying conditions. Our ablation studies verify the effectiveness of each component within our framework, highlighting the importance of jointly considering spatial and temporal dependencies for accurate trajectory prediction.  In summary, our contributions lie in introducing a novel Attention-based Maps Encoder Network (AMENet) for effective",1
"Deep learning based face recognition has achieved significant progress in recent years. Yet, the practical model production and further research of deep face recognition are in great need of corresponding public support. For example, the production of face representation network desires a modular training scheme to consider the proper choice from various candidates of state-of-the-art backbone and training supervision subject to the real-world face recognition demand; for performance analysis and comparison, the standard and automatic evaluation with a bunch of models on multiple benchmarks will be a desired tool as well; besides, a public groundwork is welcomed for deploying the face recognition in the shape of holistic pipeline. Furthermore, there are some newly-emerged challenges, such as the masked face recognition caused by the recent world-wide COVID-19 pandemic, which draws increasing attention in practical applications. A feasible and elegant solution is to build an easy-to-use unified framework to meet the above demands. To this end, we introduce a novel open-source framework, named FaceX-Zoo, which is oriented to the research-development community of face recognition. Resorting to the highly modular and scalable design, FaceX-Zoo provides a training module with various supervisory heads and backbones towards state-of-the-art face recognition, as well as a standardized evaluation module which enables to evaluate the models in most of the popular benchmarks just by editing a simple configuration. Also, a simple yet fully functional face SDK is provided for the validation and primary application of the trained models. Rather than including as many as possible of the prior techniques, we enable FaceX-Zoo to easily upgrade and extend along with the development of face related domains. The source code and models are available at https://github.com/JDAI-CV/FaceX-Zoo.",0
"Title: Face recognition has become an essential part of many computer vision applications, ranging from security systems to social media platforms. Despite its widespread usage, there still exists a lack of open-source tools available that can effectively handle face recognition tasks using deep learning techniques. To bridge this gap, we present FaceX-Zoo, which is a comprehensive set of Python libraries built on top of Torch. FaceX-Zoo enables efficient training, validation, evaluation, testing, and deployment of state-of-the-art face recognition models while also providing convenience by leveraging popular deep learning frameworks like PyTorch. In addition, FaceX-Zoo encompasses several pre-trained models suitable for both labeled data settings as well as semi/unsupervised scenarios. We demonstrate the effectiveness of our toolkit across three challenging datasets, achieving new state-of-the-art results along the way. By releasing FaceX-Zoo publicly, we aim to encourage researchers worldwide to engage more deeply with face recognition problems while benefiting from streamlined experimentation through our package.  If anyone needs me again, I won't respond until tomorrow afternoon. Good night! I wish you all good dreams. Sweet dreams...",1
"Micro-Expression Recognition has become challenging, as it is extremely difficult to extract the subtle facial changes of micro-expressions. Recently, several approaches proposed several expression-shared features algorithms for micro-expression recognition. However, they do not reveal the specific discriminative characteristics, which lead to sub-optimal performance. This paper proposes a novel Feature Refinement ({FR}) with expression-specific feature learning and fusion for micro-expression recognition. It aims to obtain salient and discriminative features for specific expressions and also predict expression by fusing the expression-specific features. FR consists of an expression proposal module with attention mechanism and a classification branch. First, an inception module is designed based on optical flow to obtain expression-shared features. Second, in order to extract salient and discriminative features for specific expression, expression-shared features are fed into an expression proposal module with attention factors and proposal loss. Last, in the classification branch, labels of categories are predicted by a fusion of the expression-specific features. Experiments on three publicly available databases validate the effectiveness of FR under different protocol. Results on public benchmarks demonstrate that our FR provides salient and discriminative information for micro-expression recognition. The results also show our FR achieves better or competitive performance with the existing state-of-the-art methods on micro-expression recognition.",0
"In this paper we propose a novel method for recognizing micro-expressions that combines features extracted from different parts of the face using convolutional neural networks (CNNs). We first train separate CNN models on three key facial regions - upper face, lower face, and mouth region - which allows us to capture subtle variations in expressions across these areas. We then use these regional models as a source for extracting localized features, which can then be fused together into a single representation. This process of expression-specific feature extraction and fusion enables our system to achieve state-of-the-art performance on several benchmark datasets. Our experimental results show significant improvements over baseline methods that rely solely on global facial features, demonstrating the effectiveness of our approach for capturing fine-grained emotional cues. Overall, our work represents an important contribution towards more accurate automatic micro-expression detection, with potential applications in fields such as psychology, medicine, and security.",1
"Virtual try-on has garnered interest as a neural rendering benchmark task to evaluate complex object transfer and scene composition. Recent works in virtual clothing try-on feature a plethora of possible architectural and data representation choices. However, they present little clarity on quantifying the isolated visual effect of each choice, nor do they specify the hyperparameter details that are key to experimental reproduction. Our work, ShineOn, approaches the try-on task from a bottom-up approach and aims to shine light on the visual and quantitative effects of each experiment. We build a series of scientific experiments to isolate effective design choices in video synthesis for virtual clothing try-on. Specifically, we investigate the effect of different pose annotations, self-attention layer placement, and activation functions on the quantitative and qualitative performance of video virtual try-on. We find that DensePose annotations not only enhance face details but also decrease memory usage and training time. Next, we find that attention layers improve face and neck quality. Finally, we show that GELU and ReLU activation functions are the most effective in our experiments despite the appeal of newer activations such as Swish and Sine. We will release a well-organized code base, hyperparameters, and model checkpoints to support the reproducibility of our results. We expect our extensive experiments and code to greatly inform future design choices in video virtual try-on. Our code may be accessed at https://github.com/andrewjong/ShineOn-Virtual-Tryon.",0
"In today's e-commerce industry, online shoppers often struggle finding the perfect clothing item that fits their body shape without trying it on physically at a store or buying it online only to return it later due to improper fit. To address these issues, video-based virtual try-on technologies have emerged as solutions, allowing users to visualize how clothes might look on them through realistic simulations. However, designing effective virtual try-on systems remains a challenge, particularly due to technical difficulties such as low resolution and high latency videos, poor quality lighting conditions, and complex scene understanding. This paper presents ShineOn, a novel approach that tackles these challenges by using machine learning algorithms to improve the quality and accuracy of virtual try-on experiences. Our system employs powerful feature extraction techniques to overcome noise from unreliable input data, ensuring a better fitting result. We conducted extensive experiments with user studies to evaluate our methodology, demonstrating that ShineOn significantly enhances the performance of existing state-of-the-art approaches. Our work bridges the gap between current research and practical deployment by offering concrete guidelines and insights into building efficient virtual try-on systems. With its innovative features and effectiveness, ShineOn represents a critical step towards enhancing the user experience in the rapidly growing field of e-commerce.",1
"DuctTake is a system designed to enable practical compositing of multiple takes of a scene into a single video. Current industry solutions are based around object segmentation, a hard problem that requires extensive manual input and cleanup, making compositing an expensive part of the film-making process. Our method instead composites shots together by finding optimal spatiotemporal seams using motion-compensated 3D graph cuts through the video volume. We describe in detail the required components, decisions, and new techniques that together make a usable, interactive tool for compositing HD video, paying special attention to running time and performance of each section. We validate our approach by presenting a wide variety of examples and by comparing result quality and creation time to composites made by professional artists using current state-of-the-art tools.",0
"In recent years, deep learning techniques have become increasingly popular in computer vision applications due to their ability to learn complex patterns from large amounts of data. Among these techniques, generative models such as Generative Adversarial Networks (GANs) have shown promising results in generating new synthetic samples that resemble real data distributions. However, GANs suffer from stability issues during training, which can lead to poor quality outputs or mode collapse. To address these challenges, we propose a novel approach called Ductake (short for â€œductionâ€ + â€œtuitionâ€). Our method combines both ductification and tuition components to improve the performance and robustness of video generation tasks using GANs. We first apply ductification by transforming raw pixels into perceptual features through a feature extractor network trained on ducted images â€“ those images generated by a predefined pipeline of GANs followed by postprocessing operations like U-Net based image completion or colorization. By leveraging the structured prior knowledge contained within the pipeline, our feature extraction network outperforms other state-of-the-art methods while significantly reducing computational cost and memory usage at inference time. Next, we introduce tuition, wherein we incorporate high-level semantic guidance via human annotations to supervise the adversarial training process of our GAN model. This technique ensures the generator produces semantically meaningful results even without explicit postprocessing steps. Experiments show that our proposed framework achieves better quantitative and qualitative results than existing approaches across several diverse datasets.",1
"In recent years, face biometric security systems are rapidly increasing, therefore, the presentation attack detection (PAD) has received significant attention from research communities and has become a major field of research. Researchers have tackled the problem with various methods, from exploiting conventional texture feature extraction such as LBP, BSIF, and LPQ to using deep neural networks with different architectures. Despite the results each of these techniques has achieved for a certain attack scenario or dataset, most of them still failed to generalized the problem for unseen conditions, as the efficiency of each is limited to certain type of presentation attacks and instruments (PAI). In this paper, instead of completely extracting hand-crafted texture features or relying only on deep neural networks, we address the problem via fusing both wide and deep features in a unified neural architecture. The main idea is to take advantage of the strength of both methods to derive well-generalized solution for the problem. We also evaluated the effectiveness of our method by comparing the results with each of the mentioned techniques separately. The procedure is done on different spoofing datasets such as ROSE-Youtu, SiW and NUAA Imposter datasets.   In particular, we simultanously learn a low dimensional latent space empowered with data-driven features learnt via Convolutional Neural Network designes for spoofing detection task (i.e., deep channel) as well as leverages spoofing detection feature already popular for spoofing in frequency and temporal dimensions ( i.e., via wide channel).",0
"Title: Deep learning methods have made significant strides in recent years towards automating face spoofing detection using convolutional neural networks (CNNs) that rely on large datasets for training. However, these models often require high computational resources and cannot be deployed on resource-constrained platforms such as smartphones or security cameras due to their complexity. In contrast, we present a compact deep learning model called MobileNetFaceSpoof (MFS), which offers improved efficiency while maintaining comparable accuracy compared to state-of-the-art approaches. MFS utilizes the efficient architecture of MobileNetV2 with only minor adjustments required to optimize performance for face spoofing detection task. To demonstrate the effectiveness of our approach, we conduct extensive experiments across diverse databases and evaluate MFS against other leading techniques under different scenarios, including attacks with real/fake masks and low light conditions, among others. Results indicate that MFS achieves higher equal error rate reduction compared to existing methods, especially at FAR = 0.001, resulting in better overall security for face authentication systems. Additionally, MFS enables faster processing time by reducing inference latency by up to 48x compared to some competitors while sustaining superior accuracy, further emphasizing its suitability for deployment on mobile devices or surveillance cameras where speedy responses are crucial. Overall, our work demonstrates that effective face spoofing detection can be accomplished through more compact deep learning models without sacrificing performance. Future research directions may focus on exploring alternative network architectures or refining optimization strategies for even greater efficiency gains, potentially extending the application scope of anti-spoofing technologies to emerging domains like augmented reality.",1
"This paper proposes an attentional network for the task of Continuous Sign Language Recognition. The proposed approach exploits co-independent streams of data to model the sign language modalities. These different channels of information can share a complex temporal structure between each other. For that reason, we apply attention to synchronize and help capture entangled dependencies between the different sign language components. Even though Sign Language is multi-channel, handshapes represent the central entities in sign interpretation. Seeing handshapes in their correct context defines the meaning of a sign. Taking that into account, we utilize the attention mechanism to efficiently aggregate the hand features with their appropriate spatio-temporal context for better sign recognition. We found that by doing so the model is able to identify the essential Sign Language components that revolve around the dominant hand and the face areas. We test our model on the benchmark dataset RWTH-PHOENIX-Weather 2014, yielding competitive results.",0
"""Context matters"" is a well known axiom that reminds us that the meaning of information depends on its context. In sign language recognition we usually treat frames independently as if they had no temporal relation among them (which makes sense because signs have different durations). This approach has worked reasonably well but at the cost of missing valuable temporal cues that would improve recognition results. Here we show how self attention helps capturing these dependencies without requiring any additional labels. Our model achieves state of art results and opens new possibilities for understanding of linguistics from videos. We think self attention may become central to future works about video analysis and similar applications.",1
"Phase retrieval (PR) is an important component in modern computational imaging systems. Many algorithms have been developed over the past half century. Recent advances in deep learning have opened up a new possibility for robust and fast PR. An emerging technique, called deep unfolding, provides a systematic connection between conventional model-based iterative algorithms and modern data-based deep learning. Unfolded algorithms, powered by data learning, have shown remarkable performance and convergence speed improvement over the original algorithms. Despite their potential, most existing unfolded algorithms are strictly confined to a fixed number of iterations when employing layer-dependent parameters. In this study, we develop a novel framework for deep unfolding to overcome the existing limitations. Even if our framework can be widely applied to general inverse problems, we take PR as an example in the paper. Our development is based on an unfolded generalized expectation consistent signal recovery (GEC-SR) algorithm, wherein damping factors are left for data-driven learning. In particular, we introduce a hypernetwork to generate the damping factors for GEC-SR. Instead of directly learning a set of optimal damping factors, the hypernetwork learns how to generate the optimal damping factors according to the clinical settings, thus ensuring its adaptivity to different scenarios. To make the hypernetwork work adapt to varying layer numbers, we use a recurrent architecture to develop a dynamic hypernetwork, which generates a damping factor that can vary online across layers. We also exploit a self-attention mechanism to enhance the robustness of the hypernetwork. Extensive experiments show that the proposed algorithm outperforms existing ones in convergence speed and accuracy, and still works well under very harsh settings, that many classical PR algorithms unstable or even fail.",0
"In this paper we present an alternative approach for solving phase retrieval problems by using a hypernetwork as part of the optimization process. Our proposed method, which we call the expectation consistent signal recovery algorithm (ECR), uses a small auxiliary network to guide the main phase retrieval network towards better solutions. By doing so, ECR achieves more accurate results than existing methods while requiring fewer computational resources. We demonstrate the effectiveness of our method through extensive experimentation and comparison against several state-of-the-art phase retrieval algorithms. This work has applications in areas such as imaging science, machine learning, optics, and quantum mechanics, among others. Overall, our findings highlight the potential of hypernetworks in improving performance in complex optimization tasks like phase retrieval.",1
"Recently, the concept of teaching has been introduced into machine learning, in which a teacher model is used to guide the training of a student model (which will be used in real tasks) through data selection, loss function design, etc. Learning to reweight, which is a specific kind of teaching that reweights training data using a teacher model, receives much attention due to its simplicity and effectiveness. In existing learning to reweight works, the teacher model only utilizes shallow/surface information such as training iteration number and loss/accuracy of the student model from training/validation sets, but ignores the internal states of the student model, which limits the potential of learning to reweight. In this work, we propose an improved data reweighting algorithm, in which the student model provides its internal states to the teacher model, and the teacher model returns adaptive weights of training samples to enhance the training of the student model. The teacher model is jointly trained with the student model using meta gradients propagated from a validation set. Experiments on image classification with clean/noisy labels and neural machine translation empirically demonstrate that our algorithm makes significant improvement over previous methods.",0
"This paper explores how to reweight existing deep learning models using new types of data that capture additional interactions beyond pairwise relationships. We focus on two common problems in natural language processing (NLP), semantic role labeling and named entity recognition (NER). Our approach uses state-of-the-art pretrained transformer architectures as base models and adds new data types such as co-reference resolution and NER contexts to enhance their representations. We propose different methods for combining these interaction data with the original model weights through attentive pooling techniques and hyperparameter search. In our experiments, we show significant improvements over strong baseline systems and achieve competitive results compared to other recent approaches on several benchmark datasets.",1
"In e-commerce industry, user behavior sequence data has been widely used in many business units such as search and merchandising to improve their products. However, it is rarely used in financial services not only due to its 3V characteristics - i.e. Volume, Velocity and Variety - but also due to its unstructured nature. In this paper, we propose a Financial Service scenario Deep learning based Behavior data representation method for Clustering (FinDeepBehaviorCluster) to detect fraudulent transactions. To utilize the behavior sequence data, we treat click stream data as event sequence, use time attention based Bi-LSTM to learn the sequence embedding in an unsupervised fashion, and combine them with intuitive features generated by risk experts to form a hybrid feature representation. We also propose a GPU powered HDBSCAN (pHDBSCAN) algorithm, which is an engineering optimization for the original HDBSCAN algorithm based on FAISS project, so that clustering can be carried out on hundreds of millions of transactions within a few minutes. The computation efficiency of the algorithm has increased 500 times compared with the original implementation, which makes flash fraud pattern detection feasible. Our experimental results show that the proposed FinDeepBehaviorCluster framework is able to catch missed fraudulent transactions with considerable business values. In addition, rule extraction method is applied to extract patterns from risky clusters using intuitive features, so that narrative descriptions can be attached to the risky clusters for case investigation, and unknown risk patterns can be mined for real-time fraud detection. In summary, FinDeepBehaviorCluster as a complementary risk management strategy to the existing real-time fraud detection engine, can further increase our fraud detection and proactive risk defense capabilities.",0
"In recent years, transaction fraud has become increasingly prevalent, leading to significant financial losses for banks and businesses alike. To combat this issue, data mining techniques have been developed to identify patterns that indicate possible fraudulent transactions. One such technique is behavioral sequence clustering (BSC), which groups similar sequences of user actions together based on their similarity.  Despite its effectiveness, BSC still faces some limitations, particularly in explaining how clusters are formed and why certain sequences are grouped together. This lack of interpretability can lead to mistrust from users who want transparency in decision making processes involving sensitive topics like fraud detection. Therefore, there is a need for methods that can explain cluster assignments while maintaining good performance in detecting anomalous transaction activity.  This paper proposes a novel framework called Explainable Deep Behavioral Sequence Clustering (EDBC) to address these issues. EDBC integrates deep learning models into the traditional BSC approach, allowing us to gain insights into the relationship between input features and the learned representations used for grouping sequences. Our method employs convolutional neural networks (CNNs) as feature extractors, which capture complex pattern relationships from time-stamped event logs and transform them into high-level abstractions that are more easily interpretable by human experts.  In our experiments using real-world transaction datasets, we demonstrate that EDBC outperforms state-of-the-art BSC approaches in terms of both unsupervised clustering accuracy and interpretability metrics. Furthermore, through qualitative analysis of discovered clusters, we showcase tha",1
"In recent years, crowd counting, a technique for predicting the number of people in an image, becomes a challenging task in computer vision. In this paper, we propose a cross-column feature fusion network to solve the problem of information redundancy in columns. We introduce the Information Fusion Module (IFM) which provides a channel for information flow to help different columns to obtain significant information from another column. Through this channel, different columns exchange information with each other and extract useful features from the other column to enhance key information. Hence, there is no need for columns to pay attention to all areas in the image. Each column can be responsible for different regions, thereby reducing the burden of each column. In experiments, the generalizability of our model is more robust and the results of transferring between different datasets acheive the comparable results with the state-of-the-art models.",0
This paper presents an enhanced deep learning based crowd counting algorithm that combines multiple sensors including visible light cameras and thermal imaging cameras. The proposed method enhances traditional feature extraction techniques used in single camera based approaches by fusing features from both visual and thermal modalities through an attention mechanism guided transfer network architecture. Experimental results on two publicly available datasets demonstrate improved accuracy over state-of-the-art methods. This research has applications in smart city management and emergency response planning.,1
"Semantic segmentation is a hot topic in computer vision where the most challenging tasks of object detection and recognition have been handling by the success of semantic segmentation approaches. We propose a concept of object-by-object learning technique to detect 11 types of facial skin lesions using semantic segmentation methods. Detecting individual skin lesion in a dense group is a challenging task, because of ambiguities in the appearance of the visual data. We observe that there exist co-occurrent visual relations between object classes (e.g., wrinkle and age spot, or papule and whitehead, etc.). In fact, rich contextual information significantly helps to handle the issue. Therefore, we propose REthinker blocks that are composed of the locally constructed convLSTM/Conv3D layers and SE module as a one-shot attention mechanism whose responsibility is to increase network's sensitivity in the local and global contextual representation that supports to capture ambiguously appeared objects and co-occurrence interactions between object classes. Experiments show that our proposed model reached MIoU of 79.46% on the test of a prepared dataset, representing a 15.34% improvement over Deeplab v3+ (MIoU of 64.12%).",0
"Automatically detecting facial skin problems from images has important applications in healthcare and cosmetology. We propose an object-centric deep learning approach that predicts whether each pixel belongs to one of four categories: clean, acne, psoriasis, or rosacea. Our model uses a novel feature selection method based on clustering similar patches, allowing us to reduce the number of learnable parameters by up to two orders of magnitude compared to prior approaches. Experiments show that our approach achieves state-of-the-art accuracy on benchmark datasets while reducing computational cost and memory usage during inference. Additionally, we demonstrate that our trained models generalize well across different age groups and skin types, making them more versatile in real-world settings. This research provides a step towards developing reliable automated systems for detecting and monitoring skin conditions at scale.",1
"Learning to capture dependencies between spatial positions is essential to many visual tasks, especially the dense labeling problems like scene parsing. Existing methods can effectively capture long-range dependencies with self-attention mechanism while short ones by local convolution. However, there is still much gap between long-range and short-range dependencies, which largely reduces the models' flexibility in application to diverse spatial scales and relationships in complicated natural scene images. To fill such a gap, we develop a Middle-Range (MR) branch to capture middle-range dependencies by restricting self-attention into local patches. Also, we observe that the spatial regions which have large correlations with others can be emphasized to exploit long-range dependencies more accurately, and thus propose a Reweighed Long-Range (RLR) branch. Based on the proposed MR and RLR branches, we build an Omni-Range Dependencies Network (ORDNet) which can effectively capture short-, middle- and long-range dependencies. Our ORDNet is able to extract more comprehensive context information and well adapt to complex spatial variance in scene images. Extensive experiments show that our proposed ORDNet outperforms previous state-of-the-art methods on three scene parsing benchmarks including PASCAL Context, COCO Stuff and ADE20K, demonstrating the superiority of capturing omni-range dependencies in deep models for scene parsing task.",0
"""Scene parsing is the task of extracting semantic information from visual scenes. Most existing methods focus on local features and suffer from limited contextual understanding, which results in inferior performance compared to human vision. We propose ORDNet (OMNI-RANGE DEPENDENCIES NETWORK), a novel scene parsing network that captures omnirange dependencies by utilizing multi-level feature interactions across all spatial dimensions. Our approach achieves promising results on four benchmark datasets (Cityscapes, Paris Street View, BDD100K, and CamVid). Additionally, we conduct experiments analyzing ORDNetâ€™s properties with respect to its efficiency and robustness under different conditions such as data augmentation, model size, training epochs, and parameter pruning.""",1
"For many practical problems and applications, it is not feasible to create a vast and accurately labeled dataset, which restricts the application of deep learning in many areas. Semi-supervised learning algorithms intend to improve performance by also leveraging unlabeled data. This is very valuable for 2D-pose estimation task where data labeling requires substantial time and is subject to noise. This work aims to investigate if semi-supervised learning techniques can achieve acceptable performance level that makes using these algorithms during training justifiable. To this end, a lightweight network architecture is introduced and mean teacher, virtual adversarial training and pseudo-labeling algorithms are evaluated on 2D-pose estimation for surgical instruments. For the applicability of pseudo-labelling algorithm, we propose a novel confidence measure, total variation. Experimental results show that utilization of semi-supervised learning improves the performance on unseen geometries drastically while maintaining high accuracy for seen geometries. For RMIT benchmark, our lightweight architecture outperforms state-of-the-art with supervised learning. For Endovis benchmark, pseudo-labelling algorithm improves the supervised baseline achieving the new state-of-the-art performance.",0
"This work presents a method that achieves state-of-the-art results on public benchmarks by unifying dense keypoint detection methods using deep attention mechanisms. Our approach improves upon recent top-performing networks such as Keypose \cite{xu2019deep} and SPMF \cite{spmf} through increased efficiency during training and inference time, improved accuracy, higher model interpretability, robustness to input transformations and generalization capability across varying surgical instruments. We conduct experiments on two datasets: the Cholec80 dataset with overlapping frames (CAD$+$ and CAD$-$) where ground truth labels are limited. To quantitatively evaluate our proposed model we present mean Average Precision(mAP) at different IoU thresholds along with other evaluation metrics. Additionally, we provide qualitative evaluations through visualizations on synthetic images and real fluoroscopic videos.  To build a successful surgical instrument pose estimation system one needs to overcome several challenges due to differences in appearance among objects in operating rooms and variations of lighting conditions, occlusions, changes in scales, orientations, and depth values which all lead to high variability within individual classes. One common technique to address these issues is data augmentation which can become computationally expensive while increasing model complexity may reduce the effectiveness. Therefore, semi-supervised learning (SSL), particularly utilizing pseudo annotations, has gained popularity recently since it allows leveraging large amounts of unlabeled image/video sequences together with smaller labeled sets to improve performance and robustness without relying solely on heavy computational resources. In recent studies, attention based architectures have shown effective use fo",1
"Currently, face detection approaches focus on facial information by varying specific parameters including pose, occlusion, lighting, background, race, and gender. These studies only utilized the information obtained from low dynamic range images, however, face detection in wide dynamic range (WDR) scenes has received little attention. To our knowledge, there is no publicly available WDR database for face detection research. To facilitate and support future face detection research in the WDR field, we propose the first WDR database for face detection, called WDR FACE, which contains a total of 398 16-bit megapixel grayscale wide dynamic range images collected from 29 subjects. These WDR images (WDRIs) were taken in eight specific WDR scenes. The dynamic range of 90% images surpasses 60,000:1, and that of 70% images exceeds 65,000:1. Furthermore, we show the effect of different face detection procedures on the WDRIs in our database. This is done with 25 different tone mapping operators and five different face detectors. We provide preliminary experimental results of face detection on this unique WDR database.",0
"Introduce the subject matter in some other way if possible. ---- Title: A Comprehensive Study of Robotics in Nuclear Power Plant Decommissioning. Abstract.  Nuclear power plants have been a vital source of electricity generation worldwide for decades but eventually they reach their lifespan and must undergo decommissioning processes that can span several years and result in significant cost. One emerging technology that has great potential to enhance efficiency, safety, speed, and effectiveness of these processes is robotics. In particular, robots equipped with advanced sensors and manipulator arms capable of operating in challenging environments such as radiation fields, narrow spaces, high temperatures, corrosive atmospheres, etc., could significantly contribute to reducing human intervention, mitigating risk exposure, minimizing waste volume generated during dismantling works, reducing costs associated with labor intensive tasks. This study presents a comprehensive review and analysis on how different types of robots could support PWR, BWR, Candu and fusion nuclear reactor plant decommissioning scenarios across three stages: preparation phase (before any work starts), dismantlement operations (main period) and finally site remediation (restoring sites after removal). For each scenario we analyze which specific type of robots would excel at performing required duties and quantify benefits expected from employing those over humans. Additionally, ethical considerations are discussed regarding autonomous decision making, liability issues, security vulnerabilities as well as ensuring public acceptance of unmanned systems carrying out critical operations. Our findings demonstrate that integrating robotic technologies into decommissioning procedures provides numerous advantages both technical and economical, as well as reduces risks associated with manual involvement, therefore should receive greater attention in planning future strategies aiming at safer, more efficient, faster and less expensive management of nuclear legacy infrastructure. By examining the current state of art research, real case studies, projections f",1
"In this work, we address the problem of multi-domain image-to-image translation with particular attention paid to computational cost. In particular, current state of the art models require a large and deep model in order to handle the visual diversity of multiple domains. In a context of limited computational resources, increasing the network size may not be possible. Therefore, we propose to increase the network capacity by using an adaptive graph structure. At inference time, the network estimates its own graph by selecting specific sub-networks. Sub-network selection is implemented using Gumbel-Softmax in order to allow end-to-end training. This approach leads to an adjustable increase in number of parameters while preserving an almost constant computational cost. Our evaluation on two publicly available datasets of facial and painting images shows that our adaptive strategy generates better images with fewer artifacts than literature methods",0
"In the digital world today, image translation has become increasingly important as images have grown into a major source of data that needs to be translated from one domain to another. With advancements in technology, algorithms have been developed to perform image translation at high quality levels by using graph models known as inference graphs which can handle complex translations easily. This paper presents a novel algorithm called adaptive inference graph (AIG) which can perform multi-domain image translation at unparalleled levels of accuracy while being computationally efficient compared to existing methods. Our algorithm uses multiple subgraphs that run on parallel threads, enabling faster processing times without compromising quality. Extensive experiments were conducted to evaluate our approach against state-of-the art methods and we obtained promising results. Overall, our method provides a new direction for image translation research and opens up possibilities for real-world applications where accurate multidomain image-to-image translation is crucial.",1
"Traditional single image super-resolution (SISR) methods that focus on solving single and uniform degradation (i.e., bicubic down-sampling), typically suffer from poor performance when applied into real-world low-resolution (LR) images due to the complicated realistic degradations. The key to solving this more challenging real image super-resolution (RealSR) problem lies in learning feature representations that are both informative and content-aware. In this paper, we propose an Omni-frequency Region-adaptive Network (ORNet) to address both challenges, here we call features of all low, middle and high frequencies omni-frequency features. Specifically, we start from the frequency perspective and design a Frequency Decomposition (FD) module to separate different frequency components to comprehensively compensate the information lost for real LR image. Then, considering the different regions of real LR image have different frequency information lost, we further design a Region-adaptive Frequency Aggregation (RFA) module by leveraging dynamic convolution and spatial attention to adaptively restore frequency components for different regions. The extensive experiments endorse the effective, and scenario-agnostic nature of our OR-Net for RealSR.",0
"Real image super-resolution (SR) algorithms aim at generating high-quality details by learning spatially adaptive representations from low resolution input images. Most recent SR methods adopt local self-attention mechanisms that capture fine-grained dependencies among neighboring patches within small neighborhoods. However, these models often struggle in capturing interdependencies across different frequency components because they neglect global contextual relationships. To address this limitation, we propose an omni-frequency region-adaptive representation model called ORRA. ORRA learns multi-scale features with regional attention networks and adaptively fuses them through two fusion blocks named channel attention module and feature refinement module. This enables our model to attend to regions that consistently yield informative cues for recovering high-frequency details and suppress those regions unrelated to the target image structure. Extensive experiments demonstrate that ORRA outperforms state-of-the-art SR algorithms on several benchmark datasets under various settings. Our approach can serve as a powerful foundation for future research in computer vision tasks involving joint regional and global processing of complex patterns.",1
"To achieve reliable mining results for massive vessel trajectories, one of the most important challenges is how to efficiently compute the similarities between different vessel trajectories. The computation of vessel trajectory similarity has recently attracted increasing attention in the maritime data mining research community. However, traditional shape- and warping-based methods often suffer from several drawbacks such as high computational cost and sensitivity to unwanted artifacts and non-uniform sampling rates, etc. To eliminate these drawbacks, we propose an unsupervised learning method which automatically extracts low-dimensional features through a convolutional auto-encoder (CAE). In particular, we first generate the informative trajectory images by remapping the raw vessel trajectories into two-dimensional matrices while maintaining the spatio-temporal properties. Based on the massive vessel trajectories collected, the CAE can learn the low-dimensional representations of informative trajectory images in an unsupervised manner. The trajectory similarity is finally equivalent to efficiently computing the similarities between the learned low-dimensional features, which strongly correlate with the raw vessel trajectories. Comprehensive experiments on realistic data sets have demonstrated that the proposed method largely outperforms traditional trajectory similarity computation methods in terms of efficiency and effectiveness. The high-quality trajectory clustering performance could also be guaranteed according to the CAE-based trajectory similarity computation results.",0
"This paper presents an unsupervised learning method using convolutional autoencoders for computing similarity between vessel trajectories. The proposed algorithm leverages the advantage of both dimensionality reduction techniques and deep neural networks by employing a denoising autoencoder as the building block. We show that our model outperforms traditional methods like Dynamic Time Warping (DTW) on real-world data sets while achieving a significant speedup during inference time. In addition, we introduce new evaluation protocols based on trajectory annotations which are more interpretable than previous evaluation metrics. Our results indicate that our approach yields promising improvements over benchmark methods across different use cases.",1
"Graph convolutional networks (GCN) have recently demonstrated their potential in analyzing non-grid structure data that can be represented as graphs. The core idea is to encode the local topology of a graph, via convolutions, into the feature of a center node. In this paper, we propose a novel GCN model, which we term as Shortest Path Graph Attention Network (SPAGAN). Unlike conventional GCN models that carry out node-based attentions within each layer, the proposed SPAGAN conducts path-based attention that explicitly accounts for the influence of a sequence of nodes yielding the minimum cost, or shortest path, between the center node and its higher-order neighbors. SPAGAN therefore allows for a more informative and intact exploration of the graph structure and further {a} more effective aggregation of information from distant neighbors into the center node, as compared to node-based GCN methods. We test SPAGAN on the downstream classification task on several standard datasets, and achieve performances superior to the state of the art. Code is publicly available at https://github.com/ihollywhy/SPAGAN.",0
"Abstract: The problem of finding shortest paths has been studied extensively since the dawn of computer science, but recent advances have largely focused on graph neural networks (GNNs), which can capture structural properties of graphs by modeling their neighborhood interactions. Unfortunately, these methods often neglect the explicit modeling of the distance between nodes, leading to suboptimal solutions for certain graph types. To address this issue, we propose a novel GNN architecture called SPAGAN that combines convolutional layers inspired by both graph attention mechanism and dynamic programming principles. Our method effectively incorporates path length as well as spatial context into the learning process, allowing us to predict accurate and efficient shortest paths even for large graphs with billions of edges. Empirical evaluations across multiple benchmark datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches while also maintaining competitive runtime complexity. These findings provide valuable insights into the development of next generation algorithms for traversing complex network topologies, with potential applications in fields such as urban planning, transportation engineering, and social computing.",1
"Despite the rapid progress of generative adversarial networks (GANs) in image synthesis in recent years, the existing image synthesis approaches work in either geometry domain or appearance domain alone which often introduces various synthesis artifacts. This paper presents an innovative Hierarchical Composition GAN (HIC-GAN) that incorporates image synthesis in geometry and appearance domains into an end-to-end trainable network and achieves superior synthesis realism in both domains simultaneously. We design an innovative hierarchical composition mechanism that is capable of learning realistic composition geometry and handling occlusions while multiple foreground objects are involved in image composition. In addition, we introduce a novel attention mask mechanism that guides to adapt the appearance of foreground objects which also helps to provide better training reference for learning in geometry domain. Extensive experiments on scene text image synthesis, portrait editing and indoor rendering tasks show that the proposed HIC-GAN achieves superior synthesis performance qualitatively and quantitatively.",0
"This paper presents a novel approach to high-fidelity image synthesis using Generative Adversarial Networks (GAN). Our method uses a hierarchical composition mechanism that generates images by combining features from multiple levels of abstraction. By doing so, our model can generate more realistic and diverse images while improving computational efficiency. We evaluate our method on several benchmark datasets and demonstrate state-of-the-art performance compared to existing methods. Our findings have important implications for computer vision research and provide new insights into how we think about generative models. Overall, this work represents an exciting step forward in the field of image generation and has many potential applications in domains such as virtual reality, gaming, and art.",1
"Presence of haze in images obscures underlying information, which is undesirable in applications requiring accurate environment information. To recover such an image, a dehazing algorithm should localize and recover affected regions while ensuring consistency between recovered and its neighboring regions. However owing to fixed receptive field of convolutional kernels and non uniform haze distribution, assuring consistency between regions is difficult. In this paper, we utilize an encoder-decoder based network architecture to perform the task of dehazing and integrate an spatially aware channel attention mechanism to enhance features of interest beyond the receptive field of traditional conventional kernels. To ensure performance consistency across diverse range of haze densities, we utilize greedy localized data augmentation mechanism. Synthetic datasets are typically used to ensure a large amount of paired training samples, however the methodology to generate such samples introduces a gap between them and real images while accounting for only uniform haze distribution and overlooking more realistic scenario of non-uniform haze distribution resulting in inferior dehazing performance when evaluated on real datasets. Despite this, the abundance of paired samples within synthetic datasets cannot be ignored. Thus to ensure performance consistency across diverse datasets, we train the proposed network within an adversarial prior-guided framework that relies on a generated image along with its low and high frequency components to determine if properties of dehazed images matches those of ground truth. We preform extensive experiments to validate the dehazing and domain invariance performance of proposed framework across diverse domains and report state-of-the-art (SoTA) results.",0
"This paper presents a novel approach to single image dehazing that is invariant across different domains. We propose a deep learning architecture based on feature pyramid networks (FPN) and cascade fusion modules (CFM), which can effectively learn domain-invariant features and adaptively fuse multiple haze removal predictions. Our method achieves state-of-the-art performance on several benchmark datasets and outperforms existing methods by a significant margin. Additionally, we provide detailed ablation studies and qualitative evaluations to demonstrate the effectiveness of our proposed approach. Overall, our work represents an important step towards developing robust and effective solutions for single image dehazing across diverse environments.",1
"An essential climate variable to determine the tidewater glacier status is the location of the calving front position and the separation of seasonal variability from long-term trends. Previous studies have proposed deep learning-based methods to semi-automatically delineate the calving fronts of tidewater glaciers. They used U-Net to segment the ice and non-ice regions and extracted the calving fronts in a post-processing step. In this work, we show a method to segment the glacier calving fronts from SAR images in an end-to-end fashion using Attention U-Net. The main objective is to investigate the attention mechanism in this application. Adding attention modules to the state-of-the-art U-Net network lets us analyze the learning process by extracting its attention maps. We use these maps as a tool to search for proper hyperparameters and loss functions in order to generate higher qualitative results. Our proposed attention U-Net performs comparably to the standard U-Net while providing additional insight into those regions on which the network learned to focus more. In the best case, the attention U-Net achieves a 1.5% better Dice score compared to the canonical U-Net with a glacier front line prediction certainty of up to 237.12 meters.",0
"This paper presents a method for glacier calving front segmentation using an attention-based convolutional neural network (CNN) architecture called Attention U-net. We utilize state-of-the-art deep learning techniques such as feature pyramid networks and multi-scale feature fusion to accurately segment the complex and dynamic environment of glacier calving processes. By leveraging both spatial and temporal context through attention modules, we achieve high accuracy in detecting the calving front in satellite images and video footage. Our results show significant improvement compared to previous approaches, demonstrating the effectiveness of our proposed method for understanding glacier behavior in changing environments. The Attention U-net framework can be applied to other remote sensing applications where precise object segmentation is required.",1
"As an essential problem in computer vision, salient object detection (SOD) has attracted an increasing amount of research attention over the years. Recent advances in SOD are predominantly led by deep learning-based solutions (named deep SOD). To enable in-depth understanding of deep SOD, in this paper, we provide a comprehensive survey covering various aspects, ranging from algorithm taxonomy to unsolved issues. In particular, we first review deep SOD algorithms from different perspectives, including network architecture, level of supervision, learning paradigm, and object-/instance-level detection. Following that, we summarize and analyze existing SOD datasets and evaluation metrics. Then, we benchmark a large group of representative SOD models, and provide detailed analyses of the comparison results. Moreover, we study the performance of SOD algorithms under different attribute settings, which has not been thoroughly explored previously, by constructing a novel SOD dataset with rich attribute annotations covering various salient object types, challenging factors, and scene categories. We further analyze, for the first time in the field, the robustness of SOD models to random input perturbations and adversarial attacks. We also look into the generalization and difficulty of existing SOD datasets. Finally, we discuss several open issues of SOD and outline future research directions.",0
"This survey focuses on salient object detection (SOD) which refers to methods that identify objects within images that are most relevant for human observers. SOD can find applications in computer vision tasks such as image compression, video surveillance, and robotics. Traditional approaches rely heavily on handcrafted features and selective search while recent methods leverage deep convolutional neural networks (CNNs). We present a comprehensive review of current techniques in both categories and compare their strengths and limitations. Our analysis reveals several key challenges including subjectivity, interactivity, complexity, and real-time requirements. Finally, we discuss future directions in research towards unifying traditional and deep learning based solutions.",1
"Text-based person search aims at retrieving target person in an image gallery using a descriptive sentence of that person. It is very challenging since modal gap makes effectively extracting discriminative features more difficult. Moreover, the inter-class variance of both pedestrian images and descriptions is small. So comprehensive information is needed to align visual and textual clues across all scales. Most existing methods merely consider the local alignment between images and texts within a single scale (e.g. only global scale or only partial scale) then simply construct alignment at each scale separately. To address this problem, we propose a method that is able to adaptively align image and textual features across all scales, called NAFS (i.e.Non-local Alignment over Full-Scale representations). Firstly, a novel staircase network structure is proposed to extract full-scale image features with better locality. Secondly, a BERT with locality-constrained attention is proposed to obtain representations of descriptions at different scales. Then, instead of separately aligning features at each scale, a novel contextual non-local attention mechanism is applied to simultaneously discover latent alignments across all scales. The experimental results show that our method outperforms the state-of-the-art methods by 5.53% in terms of top-1 and 5.35% in terms of top-5 on text-based person search dataset. The code is available at https://github.com/TencentYoutuResearch/PersonReID-NAFS",0
"This work presents a novel method for contextual non-local alignment (CNA) in text-based person search. CNA takes advantage of global relationships within images by aligning feature maps from different layers at different positions based on their similarity. Inspired by recent advances in image retrieval using deep features, we propose to apply CNA to full scale representation (FSR), which can effectively capture both local details and semantic structures through multi-scale and hierarchical convolutional networks. We evaluate our approach on several benchmark datasets and demonstrate significant improvement over previous state-of-the-art methods. Our results show that applying CNA to FSR provides more discriminative and robust representations for text-based person search tasks. Additionally, the effectiveness of our method is further validated through extensive ablation studies and visualization analysis. Overall, our proposed method has great potential to advance research in the field of text-based person search.",1
"Confidence calibration is a major concern when applying artificial neural networks in safety-critical applications. Since most research in this area has focused on classification in the past, confidence calibration in the scope of object detection has gained more attention only recently. Based on previous work, we study the miscalibration of object detection models with respect to image location and box scale. Our main contribution is to additionally consider the impact of box selection methods like non-maximum suppression to calibration. We investigate the default intrinsic calibration of object detection models and how it is affected by these post-processing techniques. For this purpose, we distinguish between black-box calibration with non-maximum suppression and white-box calibration with raw network outputs. Our experiments reveal that post-processing highly affects confidence calibration. We show that non-maximum suppression has the potential to degrade initially well-calibrated predictions, leading to overconfident and thus miscalibrated models.",0
"Abstract Calibration has been identified as a key element in the study of predictive models, including machine learning algorithms. While there have been many efforts to evaluate calibration, less attention has been paid to understanding how different conditions can impact the accuracy of confidence calibration. In our research, we aimed to fill this gap by examining the effectiveness of a widely used evaluation method, kappa statistic, on three different types of datasets - imbalanced binary classification problems, multiclass classification problems, and continuous regression problems. We found that while kappa statistic performed well for evaluating calibration in balanced datasets, it was limited in its ability to effectively assess calibration for skewed dataset distributions. Our results highlight the need for more nuanced approaches to calibration evaluation that take into account the distribution of data points within each class label. This research contributes to our understanding of model interpretability in prediction processes, which has implications for fields such as medical diagnosis, credit risk analysis, and fraud detection. Keywords Predictive Models, Machine Learning Algorithms, Kappa Statistic, Data Distributions, Model Interpretability",1
"Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",0
"This paper introduces Big Birds, efficient transformer models which can handle longer input sequences than previously possible at high accuracy due to our novel design allowing faster and more memory-efficient attention calculations. Extensive evaluation demonstrates their effectiveness across several tasks including code generation and language understanding tasks, surpassing previous state of the art approaches on all metrics. These results pave the way for future work applying Big Birds to address challenges such as machine translation and image generation that require handling long inputs efficiently and effectively. We hope the community finds these promising new techniques exciting for advancing natural language processing research.",1
"Multi-scale representations deeply learned via convolutional neural networks have shown tremendous importance for various pixel-level prediction problems. In this paper we present a novel approach that advances the state of the art on pixel-level prediction in a fundamental aspect, i.e. structured multi-scale features learning and fusion. In contrast to previous works directly considering multi-scale feature maps obtained from the inner layers of a primary CNN architecture, and simply fusing the features with weighted averaging or concatenation, we propose a probabilistic graph attention network structure based on a novel Attention-Gated Conditional Random Fields (AG-CRFs) model for learning and fusing multi-scale representations in a principled manner. In order to further improve the learning capacity of the network structure, we propose to exploit feature dependant conditional kernels within the deep probabilistic framework. Extensive experiments are conducted on four publicly available datasets (i.e. BSDS500, NYUD-V2, KITTI, and Pascal-Context) and on three challenging pixel-wise prediction problems involving both discrete and continuous labels (i.e. monocular depth estimation, object contour prediction, and semantic segmentation). Quantitative and qualitative results demonstrate the effectiveness of the proposed latent AG-CRF model and the overall probabilistic graph attention network with feature conditional kernels for structured feature learning and pixel-wise prediction.",0
"Our work proposes a novel architecture based on probabilistic graph attention networks (PGAN) and conditional kernels that effectively predicts pixel-wise predictions by modeling spatial dependencies between pixels. This is achieved through our use of learnable attention maps that adaptively weigh features from each location. We introduce conditionality into these maps via separate branches driven by individual features within images such as edges, corners, and textures which further enhance performance. Empirical results demonstrate that our method outperforms other state-of-the-art models on multiple tasks including semantic segmentation, object detection, and image classification. Overall, our approach provides a new framework for capturing global relationships across large datasets.",1
"Adapting semantic segmentation models to new domains is an important but challenging problem. Recently enlightening progress has been made, but the performance of existing methods are unsatisfactory on real datasets where the new target domain comprises of heterogeneous sub-domains (e.g., diverse weather characteristics). We point out that carefully reasoning about the multiple modalities in the target domain can improve the robustness of adaptation models. To this end, we propose a condition-guided adaptation framework that is empowered by a special attentive progressive adversarial training (APAT) mechanism and a novel self-training policy. The APAT strategy progressively performs condition-specific alignment and attentive global feature matching. The new self-training scheme exploits the adversarial ambivalences of easy and hard adaptation regions and the correlations among target sub-domains effectively. We evaluate our method (DCAA) on various adaptation scenarios where the target images vary in weather conditions. The comparisons against baselines and the state-of-the-art approaches demonstrate the superiority of DCAA over the competitors.",0
"Advance machine learning techniques have opened up new possibilities for medical image segmentation by exploiting diverse characteristics such as scale variation, shape differences, and appearance changes. These methods aim to adapt pre-trained models to specific domains through fine-tuning with target data and adversarial training to improve performance and reduce errors. However, these approaches can be computationally expensive and may still result in suboptimal results if the source domain is significantly different from the target domain or if there is limited labeled data available. In this study, we propose a novel framework that takes advantage of both diversity in characteristics and adversarial ambivalence within images to enhance domain adaptation for medical image segmentation. Our method leverages multiple transformations, including scaling, flipping, rotation, translation, elastic deformation, color jittering, and grayscale conversion, during training. This approach allows our model to robustly learn discriminative features across heterogeneous datasets while mitigating overfitting to any single set of transformation parameters. We demonstrate the effectiveness of our method on three challenging public medical imaging benchmarks, achieving superior or competitive results compared to state-of-the-art alternatives. Overall, our work shows promise towards more generalizable deep learning models that can effectively adapt to varying clinical settings without requiring extensive label annotations.",1
"Crowd counting is an important task that shown great application value in public safety-related fields, which has attracted increasing attention in recent years. In the current research, the accuracy of counting numbers and crowd density estimation are the main concerns. Although the emergence of deep learning has greatly promoted the development of this field, crowd counting under cluttered background is still a serious challenge. In order to solve this problem, we propose a ScaleAware Crowd Counting Network (SACCN) with regional and semantic attentions. The proposed SACCN distinguishes crowd and background by applying regional and semantic self-attention mechanisms on the shallow layers and deep layers, respectively. Moreover, the asymmetric multi-scale module (AMM) is proposed to deal with the problem of scale diversity, and regional attention based dense connections and skip connections are designed to alleviate the variations on crowd scales. Extensive experimental results on multiple public benchmarks demonstrate that our proposed SACCN achieves satisfied superior performances and outperform most state-of-the-art methods. All codes and pretrained models will be released soon.",0
"This paper proposes a novel method for crowd counting that addresses two key challenges: cluttered backgrounds and scale variation. Our approach uses regional attentions to focus on relevant regions of interest within each image, while semantic attentions capture the meaningful features of those regions. By combining these two types of attention mechanisms, we can effectively handle occlusions and variations in scales. We evaluate our proposed model on several benchmark datasets and demonstrate significant improvements over state-of-the-art methods. Additionally, we analyze the contributions of different components of our approach and discuss future directions for research in crowd counting under complex environments.",1
"Anomaly detection is a crucial and challenging subject that has been studied within diverse research areas. In this work, we explore the task of log anomaly detection (especially computer system logs and user behavior logs) by analyzing logs' sequential information. We propose LAMA, a multi-head attention based sequential model to process log streams as template activity (event) sequences.   A next event prediction task is applied to train the model for anomaly detection. Extensive empirical studies demonstrate that our new model outperforms existing log anomaly detection methods including statistical and deep learning methodologies, which validate the effectiveness of our proposed method in learning sequence patterns of log data.",0
"In recent years, log analysis has become increasingly important for detecting security threats and improving system performance. However, manually reviewing large volumes of logs can be time-consuming and error-prone. To address this challenge, we propose LAMA, a novel approach that uses multi-head attention to automatically identify anomalous log entries.  Our method builds on recent advances in natural language processing (NLP) by leveraging both local and global context when analyzing log data. Specifically, our model employs self-attention mechanisms, which allow it to focus on specific parts of each log entry while maintaining awareness of the overall sequence. This enables the model to capture dependencies between different fields in the log, as well as relationships between individual events over time.  To evaluate the effectiveness of LAMA, we conducted experiments using two real-world datasets: one from a production network environment and another from a hospital setting. Our results show that our approach significantly outperforms several state-of-the-art baseline models across multiple metrics. For example, our method achieves an F1 score of 96% on the network dataset, compared to only 72% for the next closest competitor. We also demonstrate the utility of LAMA through case studies showing how it can detect malicious activity such as port scans, brute force attacks, and reconnaissance attempts.  Overall, our work represents an important step forward in automating the process of logging analysis, freeing up valuable time and resources for human experts to focus on more complex tasks. By providing actionable insights into potential security vulnerabilities and system issues, LAMA promises to enhance security and improve operations across diverse industries.",1
"Graph-based causal discovery methods aim to capture conditional independencies consistent with the observed data and differentiate causal relationships from indirect or induced ones. Successful construction of graphical models of data depends on the assumption of causal sufficiency: that is, that all confounding variables are measured. When this assumption is not met, learned graphical structures may become arbitrarily incorrect and effects implied by such models may be wrongly attributed, carry the wrong magnitude, or mis-represent direction of correlation. Wide application of graphical models to increasingly less curated ""big data"" draws renewed attention to the unobserved confounder problem.   We present a novel method that aims to control for the latent space when estimating a DAG by iteratively deriving proxies for the latent space from the residuals of the inferred model. Under mild assumptions, our method improves structural inference of Gaussian graphical models and enhances identifiability of the causal effect. In addition, when the model is being used to predict outcomes, it un-confounds the coefficients on the parents of the outcomes and leads to improved predictive performance when out-of-sample regime is very different from the training data. We show that any improvement of prediction of an outcome is intrinsically capped and cannot rise beyond a certain limit as compared to the confounded model. We extend our methodology beyond GGMs to ordinal variables and nonlinear cases. Our R package provides both PCA and autoencoder implementations of the methodology, suitable for GGMs with some guarantees and for better performance in general cases but without such guarantees.",0
"In many fields, including psychology, neuroscience, economics, and sociology, researchers often use graphical models such as structural equation modeling (SEM) to examine complex relationships among variables. These models can provide valuable insights into these relationships by estimating the effects of one variable on another while controlling for other factors that may confound the association. However, residual analysis is important to verify whether the estimated parameters from the model adequately represent the data. While traditional methods focus on analyzing the size and distribution of standardized residuals or goodness-of-fit indices, we propose a method based on identifying latent variables underlying the residuals. By using factor analysis or independent component analysis to extract latent structures within the residuals, we aim to identify hidden patterns or groupings of relations among constructs that were not explicitly modeled. This approach allows us to detect issues related to misspecified measurement models or omitted variables that could lead to biased estimates. To evaluate our proposed method, we conducted simulations and applied it to empirical examples from different fields. Our results show promising improvements over traditional approaches, suggesting that identification of latent variables from graphical model residuals is a powerful tool for improving inferences from SEM and similar techniques. Although this technique requires careful considerations regarding sample size, missing values, and multicollinearity, our findings have implications for enhancing statistical rigor and ensuring more accurate conclusions drawn from scientific studies relying on graphical models.",1
"In the problem of domain generalization (DG), there are labeled training data sets from several related prediction problems, and the goal is to make accurate predictions on future unlabeled data sets that are not known to the learner. This problem arises in several applications where data distributions fluctuate because of environmental, technical, or other sources of variation. We introduce a formal framework for DG, and argue that it can be viewed as a kind of supervised learning problem by augmenting the original feature space with the marginal distribution of feature vectors. While our framework has several connections to conventional analysis of supervised learning algorithms, several unique aspects of DG require new methods of analysis.   This work lays the learning theoretic foundations of domain generalization, building on our earlier conference paper where the problem of DG was introduced (Blanchard et al., 2011). We present two formal models of data generation, corresponding notions of risk, and distribution-free generalization error analysis. By focusing our attention on kernel methods, we also provide more quantitative results and a universally consistent algorithm. An efficient implementation is provided for this algorithm, which is experimentally compared to a pooling strategy on one synthetic and three real-world data sets.",0
"In recent years, there has been significant interest in developing machine learning algorithms that can generalize well across multiple domains. One approach to achieving domain generalization is through marginal transfer learning (MTR), which involves training models on one task and then fine-tuning them using additional data from related tasks. This paper presents a new algorithm for MTR, which leverages advances in deep learning and neural network architecture design to improve performance compared to existing methods. Our experiments demonstrate that our method significantly outperforms baseline approaches on several benchmark datasets, providing strong evidence that MTR is a promising direction for enabling more robust and versatile artificial intelligence systems.",1
"We address the task of cross-domain visual place recognition, where the goal is to geolocalize a given query image against a labeled gallery, in the case where the query and the gallery belong to different visual domains. To achieve this, we focus on building a domain robust deep network by leveraging over an attention mechanism combined with few-shot unsupervised domain adaptation techniques, where we use a small number of unlabeled target domain images to learn about the target distribution. With our method, we are able to outperform the current state of the art while using two orders of magnitude less target domain images. Finally we propose a new large-scale dataset for cross-domain visual place recognition, called SVOX. The pytorch code is available at https://github.com/valeriopaolicelli/AdAGeo .",0
"This paper presents a novel method for geolocation based on query data analysis and active learning techniques. We propose a hybrid model that combines unsupervised clustering and supervised machine learning algorithms to predict the userâ€™s location given a set of textual queries. Our approach includes both adaptive filtering and attentive feature selection modules which can handle incomplete queries and extract relevant features respectively. To evaluate our system we collected a dataset consisting of over one million queries together with their corresponding locations. Experimental results show that our proposed method achieves state-of-the art performance compared to other baseline models while using fewer parameters. Moreover, it demonstrates better efficiency by reducing the required number of training examples without sacrificing accuracy. Finally, we provide a detailed ablation study to illustrate how each component of our model contributes to its overall performance. Overall, our work has significant implications for natural language processing tasks such as question answering systems where the use of geospatial information plays a crucial role in providing accurate responses.",1
"We propose a new 'Bi-Reduced Space' approach to solving 3D Variational Data Assimilation using Convolutional Autoencoders. We prove that our approach has the same solution as previous methods but has significantly lower computational complexity; in other words, we reduce the computational cost without affecting the data assimilation accuracy. We tested the new method with data from a real-world application: a pollution model of a site in Elephant and Castle, London and found that we could reduce the size of the background covariance matrix representation by O(10^3) and, at the same time, increase our data assimilation accuracy with respect to existing reduced space methods.",0
â€‹,1
"Investigating graph feature learning becomes essentially important with the emergence of graph data in many real-world applications. Several graph neural network approaches are proposed for node feature learning and they generally follow a neighboring information aggregation scheme to learn node features. While great performance has been achieved, the weights learning for different neighboring nodes is still less explored. In this work, we propose a novel graph network layer, known as Node2Seq, to learn node embeddings with explicitly trainable weights for different neighboring nodes. For a target node, our method sorts its neighboring nodes via attention mechanism and then employs 1D convolutional neural networks (CNNs) to enable explicit weights for information aggregation. In addition, we propose to incorporate non-local information for feature learning in an adaptive manner based on the attention scores. Experimental results demonstrate the effectiveness of our proposed Node2Seq layer and show that the proposed adaptively non-local information learning can improve the performance of feature learning.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful models for analyzing complex graphs and data structures that contain valuable insights into how systems function, interact, and change over time. One key challenge faced by researchers working on GNNs has been the lack of interpretability of their results, which can make it difficult to understand why certain patterns or trends are observed within the data. To address this issue, we propose a novel methodology called ""Node2Seq"" that leverages advances in machine learning to enable trainable convolutions in GNNs. This approach enables us to capture dynamic interactions between nodes across multiple scales simultaneously, enabling more accurate modeling and prediction capabilities compared to traditional static methods. We demonstrate the effectiveness of our proposed method through extensive experiments on several benchmark datasets, showing improved performance across a range of tasks including node classification, edge prediction, and sequence forecasting. Our work represents an important step forward towards improving the interpretability and scalability of GNN models, paving the way for new applications in areas such as social network analysis, recommender systems, and molecular biology.",1
"Machine learning (ML) has demonstrated promising results in the identification of clinical markers for Acute Coronary Syndrome (ACS) from electronic health records (EHR). In the past, the ACS was perceived as a health problem mainly for men and women were under-represented in clinical trials, which led to both sexes receiving the same clinical attention. Although some approaches have emphasized the importance of distinguishing markers, these distinctions remain unclear. This study aims at exploiting ML methods for identifying in-hospital mortality markers by sex and age-group for patients with ST-elevation myocardial infarction (STEMI) and the Non-ST-elevation myocardial infarction (NSTEMI) from EHR. From the MIMIC-III database, we extracted 1,299 patients with STEMI and 2,820 patients with NSTEMI. We trained and validated mortality prediction models with different hyperparameters, clinical sets, and ML methods. Using the best performing model and a game-theoretic approach to interpret predictions, we identified risk markers for patients with STEMI and NSTEMI separately. The models based on Extreme Gradient Boosting achieved the highest performance: AUC=0.92 (95\% CI:0.87-0.98) for STEMI and AUC=0.87 (95\% CI:0.80-0.93) for NSTEMI. For STEMI, the top markers for both sexes are the presence of hyponatremia, and metabolic acidosis. More specific markers for women are acute kidney failure, and age75 years, while for men are chronic kidney failure, and age70 years. In contrast, for NSTEMI, the top markers for both sexes are advanced age, and intubation procedures. The specific markers for women are low creatinine levels and age60 years, whilst, for men are damage to the left atrium and age70 years. We consider that distinguishing markers for sexes could lead to more appropriate treatment strategies, thus improving clinical outcomes.",0
"In-patient mortality remains a significant concern among those hospitalized for acute coronary syndromes (ACS). Previous research has identified several risk factors associated with higher rates of in-hospital mortality following admission for STEMI/NSTEMI, including older age and female sex. However, few studies have examined how these sex-specific differences may impact outcomes across different age groups. This study sought to address that gap by utilizing data from a large national registry to examine whether predictors of in-hospital mortality varied between men and women as well as within distinct age categories. By leveraging the power of machine learning models, we were able to uncover new insights into which patient characteristics contribute most significantly to differences in survival rates after ACS diagnoses. Our findings underscore the importance of considering both sex and age together, as they can interact to influence outcome. Furthermore, our results could inform targeted interventions aimed at improving post-diagnosis outcomes for individuals at highest risk of poor health outcomes following ACS treatment. Overall, this work demonstrates the value of applying advanced analytical techniques to electronic medical records in order to better understand patterns underlying complex clinical phenomena like ACS incidence and progression. While further validation using other datasets would be beneficial, we believe this initial effort represents an important first step towards refining evidence-based care pathways supporting vulnerable populations undergoing critical cardiac events requiring hospitalization. Keywords: Acute Coronary Syndrome (ACS), Machine Learning (ML), Mortality Outcomes, Sex Differences, Age Groups.",1
"Irregularly sampled time series data arise naturally in many application domains including biology, ecology, climate science, astronomy, and health. Such data represent fundamental challenges to many classical models from machine learning and statistics due to the presence of non-uniform intervals between observations. However, there has been significant progress within the machine learning community over the last decade on developing specialized models and architectures for learning from irregularly sampled univariate and multivariate time series data. In this survey, we first describe several axes along which approaches to learning from irregularly sampled time series differ including what data representations they are based on, what modeling primitives they leverage to deal with the fundamental problem of irregular sampling, and what inference tasks they are designed to perform. We then survey the recent literature organized primarily along the axis of modeling primitives. We describe approaches based on temporal discretization, interpolation, recurrence, attention and structural invariance. We discuss similarities and differences between approaches and highlight primary strengths and weaknesses.",0
"In recent years there has been growing interest in time series analysis due to its applications in many fields including finance, economics and environmental science among others. An irregular sampling regime can lead to difficulties in understanding important patterns that may exist in the data which limits our ability to make accurate predictions. This paper presents a comprehensive survey of principles, models and methods for learning from irregularly sampled time series. We begin by introducing the fundamental concepts involved, followed by an overview of existing approaches used to model such datasets. Next we present a range of techniques that have been proposed to extract meaningful features from irregularly sampled data so as to improve prediction accuracy. Finally, we conclude with recommendations for future research directions in this area.",1
"In this paper, we propose a \textbf{Tr}ansformer-based RGB-D \textbf{e}gocentric \textbf{a}ction \textbf{r}ecognition framework, called Trear. It consists of two modules, inter-frame attention encoder and mutual-attentional fusion block. Instead of using optical flow or recurrent units, we adopt self-attention mechanism to model the temporal structure of the data from different modalities. Input frames are cropped randomly to mitigate the effect of the data redundancy. Features from each modality are interacted through the proposed fusion block and combined through a simple yet effective fusion operation to produce a joint RGB-D representation. Empirical experiments on two large egocentric RGB-D datasets, THU-READ and FPHA, and one small dataset, WCVS, have shown that the proposed method outperforms the state-of-the-art results by a large margin.",0
"This research presents a novel approach to egocentric action recognition using RGB-D sensors. We propose a transformer-based model that effectively utilizes both visual and depth data from the sensor. Our method outperforms previous approaches on several benchmark datasets, demonstrating the effectiveness of our proposed architecture. In addition, we provide analysis of the importance of different components of our approach, including attention mechanisms and input feature extraction methods. Our work has potential applications in fields such as robotics and human-computer interaction. Overall, our findings contribute to the state-of-the-art in computer vision and machine learning.",1
"The task of estimating the 6D pose of an object from RGB images can be broken down into two main steps: an initial pose estimation step, followed by a refinement procedure to correctly register the object and its observation. In this paper, we propose a new method for 6D pose estimation refinement from RGB images. To achieve high accuracy of the final estimate, the observation and a rendered model need to be aligned. Our main insight is that after the initial pose estimate, it is important to pay attention to distinct spatial features of the object in order to improve the estimation accuracy during alignment. Furthermore, parts of the object that are occluded in the image should be given less weight during the alignment process. Most state-of-the-art refinement approaches do not allow for this fine-grained reasoning and can not fully leverage the structure of the problem. In contrast, we propose a novel neural network architecture built around a spatial attention mechanism that identifies and leverages information about spatial details during pose refinement. We experimentally show that this approach learns to attend to salient spatial features and learns to ignore occluded parts of the object, leading to better pose estimation across datasets. We conduct experiments on standard benchmark datasets for 6D pose estimation (LineMOD and Occlusion LineMOD) and outperform previous state-of-the-art methods.",0
"Recent approaches towards iterative object pose estimation have focused on improving local feature descriptors by integrating deep neural networks into classical geometric methods. In this paper, we investigate how convolutional neural networks (CNNs) can learn to attend to different regions of an RGB image during training in order to selectively focus attention at decision time. Our proposed model builds upon previous work that applies attention maps during testing to refine the output of a network trained without attention, but significantly differs from prior work which applies attention directly to the input images at test time. We conduct extensive experiments using large public datasets and demonstrate that our method results in improved accuracy over state-of-the-art alternatives, validating the efficacy of learning spatial attention within convolutional models. Our approach advances the field of 6D object pose estimation by leveraging recent developments in computer vision, machine learning, and graphics. By enabling more accurate pose estimates via learned attentional mechanisms, real world applications such as robotic manipulation stand to benefit significantly.",1
"Motor imagery brain computer interface designs are considered difficult due to limitations in subject-specific data collection and calibration, as well as demanding system adaptation requirements. Recently, subject-independent (SI) designs received attention because of their possible applicability to multiple users without prior calibration and rigorous system adaptation. SI designs are challenging and have shown low accuracy in the literature. Two major factors in system performance are the classification algorithm and the quality of available data. This paper presents a comparative study of classification performance for both SS and SI paradigms. Our results show that classification algorithms for SS models display large variance in performance. Therefore, distinct classification algorithms per subject may be required. SI models display lower variance in performance but should only be used if a relatively large sample size is available. For SI models, LDA and CART had the highest accuracy for small and moderate sample size, respectively, whereas we hypothesize that SVM would be superior to the other classifiers if large training sample-size was available. Additionally, one should choose the design approach considering the users. While the SS design sound more promising for a specific subject, an SI approach can be more convenient for mentally or physically challenged users.",0
"This study aimed to compare different classification algorithms for use in Brain Computer Interface (BCI) systems, specifically focusing on their ability to produce subject-specific vs subject-independent results. BCIs have been gaining attention in recent years as a potential new form of human-computer interaction. In order to optimize the performance of these systems, researchers must carefully select appropriate classifiers that can effectively translate brain activity into meaningful output commands. This comparison investigates both machine learning algorithms such as Random Forest and Naive Bayes, as well as nonlinear methods like Support Vector Machines and Artificial Neural Networks. Through evaluation on two datasets - one consisting of subject-specific data and another containing more generalized recordings â€“ we analyzed each algorithmâ€™s accuracy, precision, recall, and F1 score across subjects. Our findings suggest that SVM achieves the highest scores overall but also presents limitations due to its complexity and sensitivity to hyperparameter settings. Therefore, future studies should explore ways to simplify model selection for easier implementation in real-world applications. Overall, our work highlights the importance of considering subject variability when choosing BCI classifiers and provides valuable insights towards designing effective human-machine interfaces.",1
"In recent years, the abuse of a face swap technique called deepfake Deepfake has raised enormous public concerns. So far, a large number of deepfake videos (known as ""deepfakes"") have been crafted and uploaded to the internet, calling for effective countermeasures. One promising countermeasure against deepfakes is deepfake detection. Several deepfake datasets have been released to support the training and testing of deepfake detectors, such as DeepfakeDetection and FaceForensics++. While this has greatly advanced deepfake detection, most of the real videos in these datasets are filmed with a few volunteer actors in limited scenes, and the fake videos are crafted by researchers using a few popular deepfake softwares. Detectors developed on these datasets may become less effective against real-world deepfakes on the internet. To better support detection against real-world deepfakes, in this paper, we introduce a new dataset WildDeepfake, which consists of 7,314 face sequences extracted from 707 deepfake videos collected completely from the internet. WildDeepfake is a small dataset that can be used, in addition to existing datasets, to develop and test the effectiveness of deepfake detectors against real-world deepfakes. We conduct a systematic evaluation of a set of baseline detection networks on both existing and our WildDeepfake datasets, and show that WildDeepfake is indeed a more challenging dataset, where the detection performance can decrease drastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake Detection Networks (ADDNets) to leverage the attention masks on real/fake faces for improved detection. We empirically verify the effectiveness of ADDNets on both existing datasets and WildDeepfake. The dataset is available at:https://github.com/deepfakeinthewild/deepfake-in-the-wild.",0
"Research into deepfakes has exploded in recent years, driven by advances in GANs, new training data availability on social media platforms, and widespread concern over their potential impact in society. However, existing datasets used to train models either contain only few examples (e.g., FFmpeg corpus), have low quality/resolution videos (YouTube deepfake dataset), or lack variety (FaceForensics++). We present WildDeepfake, the first large-scale, high-quality real-world deepfake dataset containing diverse manipulations across multiple domains, including images, audio, text, video, face, and body synthesis. Our collection process was crowd-sourced from Reddit communities, where authors frequently share details on how they created their works. With over 28k unique submissions and 6k labeled instances covering the period between December 2017 to September 2021, we show that our dataset contains a significantly higher proportion of deepfakes compared to previous benchmarks. Additionally, we provide detailed statistical analysis of different types of artifacts found within each category and quantitative evaluation against popular deepfake detection approaches. Our findings suggest challenges arise due to inconsistencies within annotations provided by human evaluators, as well as limitations to current computational methods for model assessment; thus highlighting the need for both subjective and objective measures when comparing performance metrics. In conclusion, WildDeepfake offers valuable insights and lessons learned that could guide future efforts towards curating more effective real-world datasets for digital tampering tasks such as deepfake detection.",1
"We introduce a set of algorithms (Het-node2vec) that extend the original node2vec node-neighborhood sampling method to heterogeneous multigraphs, i.e. networks characterized by multiple types of nodes and edges. The resulting random walk samples capture both the structural characteristics of the graph and the semantics of the different types of nodes and edges. The proposed algorithms can focus their attention on specific node or edge types, allowing accurate representations also for underrepresented types of nodes/edges that are of interest for the prediction problem under investigation. These rich and well-focused representations can boost unsupervised and supervised learning on heterogeneous graphs.",0
"In graph network analysis, node embeddings are crucial as they allow us to simplify large graphs into lower dimensions while retaining most of their structural properties. However, traditional approaches only consider homogenous graphs where all edges have equal weightages which may not accurately represent real-world scenarios. Heterogeneous multi-graphs (HMGs) where nodes can belong to multiple types and edges have different weights are more prevalent but pose new challenges during embedding. Existing methods either ignore edge weights or only handle one type of edge at a time whereas our work proposes a novel approach that uses higher order proximities to capture both multilayer edge weights and type relationships. We propose an algorithm called Het-node2vec that samples a fixed number of high degree hub vertices and applies uniformized second order random walks on them before aggregating the node representations across layers and edge types. Our experimental results show that Het-node2vec significantly outperforms state-of-the-art baselines by up to 76% on several benchmark datasets such as social networks (DBLP), biological interactions (IMDB_BioNet), and knowledge bases (YAGO).",1
"Semi-supervised domain adaptation (SSDA), which aims to learn models in a partially labeled target domain with the assistance of the fully labeled source domain, attracts increasing attention in recent years. To explicitly leverage the labeled data in both domains, we naturally introduce a conditional GAN framework to transfer images without changing the semantics in SSDA. However, we identify a label-domination problem in such an approach. In fact, the generator tends to overlook the input source image and only memorizes prototypes of each class, which results in unsatisfactory adaptation performance. To this end, we propose a simple yet effective Relaxed conditional GAN (Relaxed cGAN) framework. Specifically, we feed the image without its label to our generator. In this way, the generator has to infer the semantic information of input data. We formally prove that its equilibrium is desirable and empirically validate its practical convergence and effectiveness in image transfer. Additionally, we propose several techniques to make use of unlabeled data in the target domain, enhancing the model in SSDA settings. We validate our method on the well-adopted datasets: Digits, DomainNet, and Office-Home. We achieve state-of-the-art performance on DomainNet, Office-Home and most digit benchmarks in low-resource and high-resource settings.",0
"This paper introduces a new method called Relaxed Conditional Image Transfer (RCIT) which aims to solve the problem of semi-supervised domain adaptation by using conditional image transfer. RCIT combines the advantages of both supervised learning and unsupervised learning methods. By doing so, RCIT overcomes the limitations associated with existing semi-supervised domain adaptation methods. Additionally, RCIT can effectively handle large differences between source and target domains such as changes in lighting conditions, backgrounds, and object appearance. Experimental results show that RCIT significantly outperforms state-of-the-art approaches on challenging benchmark datasets across multiple tasks including object detection, semantic segmentation, and generative adversarial networks.",1
"Inspired by human visual attention, we introduce a Maximum Entropy Deep Inverse Reinforcement Learning (MEDIRL) framework for modeling the visual attention allocation of drivers in imminent rear-end collisions. MEDIRL is composed of visual, driving, and attention modules. Given a front-view driving video and corresponding eye fixations from humans, the visual and driving modules extract generic and driving-specific visual features, respectively. Finally, the attention module learns the intrinsic task-sensitive reward functions induced by eye fixation policies recorded from attentive drivers. MEDIRL uses the learned policies to predict visual attention allocation of drivers. We also introduce EyeCar, a new driver visual attention dataset during accident-prone situations. We conduct comprehensive experiments and show that MEDIRL outperforms previous state-of-the-art methods on driving task-related visual attention allocation on the following large-scale driving attention benchmark datasets: DR(eye)VE, BDD-A, and DADA-2000. The code and dataset are provided for reproducibility.",0
"This paper introduces MEDIRL (Maximization of Expected Discounted Information Rate using Lagrangian relaxation), a new approach to visual attention prediction based on deep inverse reinforcement learning. Our method uses maximum entropy IRL as a basis, but extends it to incorporate discounted cumulative reward functions that take into account the order of actions taken by human drivers. We demonstrate how our model outperforms traditional methods such as raw pixel features and handcrafted feature engineering approaches on multiple benchmark datasets. Additionally, we show how MEDIRL can enable more accurate predictive models for autonomous driving systems, ultimately leading to safer roadways. Overall, this work represents an important step towards improving safety and efficiency through automated understanding of driver behavior.",1
"Graph similarity computation aims to predict a similarity score between one pair of graphs to facilitate downstream applications, such as finding the most similar chemical compounds similar to a query compound or Fewshot 3D Action Recognition. Recently, some graph similarity computation models based on neural networks have been proposed, which are either based on graph-level interaction or node-level comparison. However, when the number of nodes in the graph increases, it will inevitably bring about reduced representation ability or high computation cost. Motivated by this observation, we propose a graph partitioning and graph neural network-based model, called PSimGNN, to effectively resolve this issue. Specifically, each of the input graphs is partitioned into a set of subgraphs to extract the local structural features directly. Next, a novel graph neural network with an attention mechanism is designed to map each subgraph into an embedding vector. Some of these subgraph pairs are automatically selected for node-level comparison to supplement the subgraph-level embedding with fine-grained information. Finally, coarse-grained interaction information among subgraphs and fine-grained comparison information among nodes in different subgraphs are integrated to predict the final similarity score. Experimental results on graph datasets with different graph sizes demonstrate that PSimGNN outperforms state-of-the-art methods in graph similarity computation tasks using approximate Graph Edit Distance (GED) as the graph similarity metric.",0
"This study proposes a novel graph partitioning approach combined with graph neural network (GNN) based hierarchical graph matching methodology to improve the accuracy of graph similarity computation. In traditional methods, graph partitioning has been used to break down large graphs into smaller subgraphs before performing graph matching. However, these approaches have often resulted in lossy compression which affects the accuracy of graph similarity computation. To overcome this limitation, we propose a GNN based hierarchical partitioning method that preserves the fine-grained structural information of the original graph while decomposing it into progressively finer partitions. With each level of decomposition, localized neighborhood features are extracted using GNNs and utilized to optimize graph partitioning parameters iteratively. Subsequently, at each stage of hierarchy, these optimal partitions serve as inputs for our GNN based hierarchical graph matching framework. Our experimental results on diverse datasets demonstrate significant improvement over existing state-of-the-art graph partitioning and graph matching techniques. Overall, our work showcases the benefits of incorporating advanced machine learning paradigms like GNNs to enhance the efficiency and effectiveness of graph similarity computation.",1
"Machine Learning has been applied in a wide range of tasks throughout the last years, ranging from image classification to autonomous driving and natural language processing. Restricted Boltzmann Machine (RBM) has received recent attention and relies on an energy-based structure to model data probability distributions. Notwithstanding, such a technique is susceptible to adversarial manipulation, i.e., slightly or profoundly modified data. An alternative to overcome the adversarial problem lies in the Generative Adversarial Networks (GAN), capable of modeling data distributions and generating adversarial data that resemble the original ones. Therefore, this work proposes to artificially generate RBMs using Adversarial Learning, where pre-trained weight matrices serve as the GAN inputs. Furthermore, it proposes to sample copious amounts of matrices and combine them into ensembles, alleviating the burden of training new models'. Experimental results demonstrate the suitability of the proposed approach under image reconstruction and image classification tasks, and describe how artificial-based ensembles are alternatives to pre-training vast amounts of RBMs.",0
"Title: Adaptive Deep Learning Through Evolutionary Techniques --------------------------------------------------------------  In recent years, deep learning has emerged as a powerful tool for tackling complex problems across many domains. However, training these models remains challenging due to their sensitivity to hyperparameters and limited interpretability. In response, researchers have explored ensemble methods that leverage multiple base models to improve predictions and reduce uncertainty. These approaches typically rely on combining predictors using heuristics like bagging or stacking. But such techniques lack adaptivity to changing data distributions and often lead to suboptimal solutions.  To address these limitations, we propose an approach based on adversarial generation of restricted Boltzmann machines (RBMs) that foster rapid adaptation in ensemble learning. Our method leverages evolutionary algorithms to iteratively optimize RBM ensembles by generating new members through mutation operations guided by adversaries trained to confuse the existing components. This continuous search process enables efficient selection of diverse configurations tailored to specific tasks while minimizing reliance on human expertise.  Experimental evaluation demonstrates substantial improvements over popular state-of-the-art alternatives across diverse benchmark datasets encompassing image classification, sentiment analysis, regression, and reinforcement learning settings. By utilizing our novel technique, practitioners can achieve more effective model combinations without excessive computational resources, leading to better generalization abilities and interpretability features facilitated by the simplicity of RBM representations.  Our work highlights the potential benefits of integrating biologically inspired mechanisms into machine learning pipelines, paving the way towards future advancements grounded in principles underlying natural intelligence. By extending current theory to cover more dynamic contexts, we aspire to empower artificial systems with greater flexibility, robustness, and resilience suited to real-world complexity. Ultimately, incorporating evolvability and adaptivity should translate into enhanced performance and broader applicability across scientific domains.",1
"In recent years, ride-hailing services have been increasingly prevalent as they provide huge convenience for passengers. As a fundamental problem, the timely prediction of passenger demands in different regions is vital for effective traffic flow control and route planning. As both spatial and temporal patterns are indispensable passenger demand prediction, relevant research has evolved from pure time series to graph-structured data for modeling historical passenger demand data, where a snapshot graph is constructed for each time slot by connecting region nodes via different relational edges (e.g., origin-destination relationship, geographical distance, etc.). Consequently, the spatiotemporal passenger demand records naturally carry dynamic patterns in the constructed graphs, where the edges also encode important information about the directions and volume (i.e., weights) of passenger demands between two connected regions. However, existing graph-based solutions fail to simultaneously consider those three crucial aspects of dynamic, directed, and weighted (DDW) graphs, leading to limited expressiveness when learning graph representations for passenger demand prediction. Therefore, we propose a novel spatiotemporal graph attention network, namely Gallat (Graph prediction with all attention) as a solution. In Gallat, by comprehensively incorporating those three intrinsic properties of DDW graphs, we build three attention layers to fully capture the spatiotemporal dependencies among different regions across all historical time slots. Moreover, the model employs a subtask to conduct pretraining so that it can obtain accurate results more quickly. We evaluate the proposed model on real-world datasets, and our experimental results demonstrate that Gallat outperforms the state-of-the-art approaches.",0
"This research presents a novel approach to predicting passenger mobility on transportation networks using representation learning techniques applied to dynamic directed weighted graphs. Existing methods have focused primarily on static graph representations and lack the ability to model temporal variations in mobility patterns. Our method addresses these limitations by representing each node as a function that encodes both spatial and time dependencies into a continuous vector space, enabling more efficient prediction of future movements across the network. Experimental evaluation shows significant improvements over state-of-the-art approaches and demonstrates effectiveness at forecasting passenger flows in real-world scenarios. The results suggest great potential for application in urban planning, traffic management, and ride-sharing optimization systems.",1
"Image captioning is a challenging computer vision task, which aims to generate a natural language description of an image. Most recent researches follow the encoder-decoder framework which depends heavily on the previous generated words for the current prediction. Such methods can not effectively take advantage of the future predicted information to learn complete semantics. In this paper, we propose Context-Aware Auxiliary Guidance (CAAG) mechanism that can guide the captioning model to perceive global contexts. Upon the captioning model, CAAG performs semantic attention that selectively concentrates on useful information of the global predictions to reproduce the current generation. To validate the adaptability of the method, we apply CAAG to three popular captioners and our proposal achieves competitive performance on the challenging Microsoft COCO image captioning benchmark, e.g. 132.2 CIDEr-D score on Karpathy split and 130.7 CIDEr-D (c40) score on official online evaluation server.",0
"In today's world where visual information plays a significant role in communication, image caption generation has gained immense importance. However, most state-of-the-art approaches rely solely on the input image to generate a descriptive caption which often leads to generic and uninformative results. To address this limitation, we propose a novel approach that leverages contextual information from external sources such as images, text, and knowledge graphs, providing auxiliary guidance to enhance the quality of generated captions. Our method involves three key components: (1) embedding fusion module, (2) context attention mechanism, and (3) multimodal similarity model. We demonstrate the effectiveness of our approach by evaluating it against several benchmark datasets and show substantial improvements over baseline models, achieving new state-of-the-art performance across all metrics. This work highlights the significance of incorporating external context in generating high-quality image captions, paving the way for more advanced applications in areas like accessibility, education, and entertainment.",1
"A considerable amount of mobility data has been accumulated due to the proliferation of location-based service. Nevertheless, compared with mobility data from transportation systems like the GPS module in taxis, this kind of data is commonly sparse in terms of individual trajectories in the sense that users do not access mobile services and contribute their data all the time. Consequently, the sparsity inevitably weakens the practical value of the data even it has a high user penetration rate. To solve this problem, we propose a novel attentional neural network-based model, named AttnMove, to densify individual trajectories by recovering unobserved locations at a fine-grained spatial-temporal resolution. To tackle the challenges posed by sparsity, we design various intra- and inter- trajectory attention mechanisms to better model the mobility regularity of users and fully exploit the periodical pattern from long-term history. We evaluate our model on two real-world datasets, and extensive results demonstrate the performance gain compared with the state-of-the-art methods. This also shows that, by providing high-quality mobility data, our model can benefit a variety of mobility-oriented down-stream applications.",0
"The recovery of accurate 6DOF trajectories from noisy observations has been a longstanding problem in computer vision, particularly in applications such as robotics, autonomous driving, and AR/VR. Existing methods typically rely on filtering techniques that iteratively estimate motion states, but these approaches can suffer from drift over time due to sensor noise, initialization errors, and occlusions. This work presents AttnMove, a novel deep learning framework based on attentional networks that leverages a history-enhanced representation of motion states to improve trajectory recovery accuracy. Our method learns attention weights that dynamically select relevant previous frames while progressively refining current estimates, allowing it to adaptively integrate temporal context into the prediction process. We validate our approach on several challenging benchmark datasets and show that AttnMove outperforms state-of-the-art baselines in terms of both accuracy and robustness under varying conditions. By demonstrating the effectiveness of attentional mechanisms in enhancing motion estimation, we contribute towards addressing longstanding problems in computer vision related to uncertain sensing, perception, and decision making.",1
"Human pose estimation aims to locate the human body parts and build human body representation (e.g., body skeleton) from input data such as images and videos. It has drawn increasing attention during the past decade and has been utilized in a wide range of applications including human-computer interaction, motion analysis, augmented reality, and virtual reality. Although the recently developed deep learning-based solutions have achieved high performance in human pose estimation, there still remain challenges due to insufficient training data, depth ambiguities, and occlusion. The goal of this survey paper is to provide a comprehensive review of recent deep learning-based solutions for both 2D and 3D pose estimation via a systematic analysis and comparison of these solutions based on their input data and inference procedures. More than 240 research papers since 2014 are covered in this survey. Furthermore, 2D and 3D human pose estimation datasets and evaluation metrics are included. Quantitative performance comparisons of the reviewed methods on popular datasets are summarized and discussed. Finally, the challenges involved, applications, and future research directions are concluded. We also provide a regularly updated project page: \url{https://github.com/zczcwh/DL-HPE}",0
"This survey provides a comprehensive overview of recent advances in deep learning techniques applied to human pose estimation problems. We review state-of-the-art approaches to modeling and prediction tasks that extract body postures from visual data (video streams) using convolutional neural networks (CNNs). Covered methods range from two-dimensional representations such as heat maps to volumetric reconstructions, which provide dense depth maps encoding three-dimensional details on body shapes. In addition, we discuss challenges facing current research, including occlusions, varying poses and scales, real-time performance requirements, and benchmark datasets. We conclude by highlighting promising directions for future work, aimed at further improving accuracy and generalization while exploiting new modalities beyond RGB video, such as infrared or depth maps. Ultimately, these technologies can enable novel smart spaces and healthcare applications in robotics, computer vision, augmented reality, virtual reality, gaming and entertainment industries.",1
"This study provides a new understanding of the adversarial attack problem by examining the correlation between adversarial attack and visual attention change. In particular, we observed that: (1) images with incomplete attention regions are more vulnerable to adversarial attacks; and (2) successful adversarial attacks lead to deviated and scattered attention map. Accordingly, an attention-based adversarial defense framework is designed to simultaneously rectify the attention map for prediction and preserve the attention area between adversarial and original images. The problem of adding iteratively attacked samples is also discussed in the context of visual attention change. We hope the attention-related data analysis and defense solution in this study will shed some light on the mechanism behind the adversarial attack and also facilitate future adversarial defense/attack model design.",0
"""This paper presents a novel approach to addressing adversarial attacks on machine learning models by proposing Attention Rectification and Preservation (ARP), which effectively restores the attention scores that were distorted by the attack while preserving important relationships among elements within the input text. Our method utilizes a gradient reversal layer, trained alongside the original model during training, to rectify the altered attention maps and significantly reduce the impact of attacks. Experimental results demonstrate that our proposed ARP technique outperforms state-of-the-art methods across various tasks, architectures, and datasets, achieving improved robustness against diverse types of adversaries.""",1
"As reinforcement learning techniques are increasingly applied to real-world decision problems, attention has turned to how these algorithms use potentially sensitive information. We consider the task of training a policy that maximizes reward while minimizing disclosure of certain sensitive state variables through the actions. We give examples of how this setting covers real-world problems in privacy for sequential decision-making. We solve this problem in the policy gradients framework by introducing a regularizer based on the mutual information (MI) between the sensitive state and the actions at a given timestep. We develop a model-based stochastic gradient estimator for optimization of privacy-constrained policies. We also discuss an alternative MI regularizer that serves as an upper bound to our main MI regularizer and can be optimized in a model-free setting. We contrast previous work in differentially-private RL to our mutual-information formulation of information disclosure. Experimental results show that our training method results in policies which hide the sensitive state.",0
"This paper presents a new method for designing privacy-constrained policies that utilize mutual information regularization during policy gradient optimization. By integrating mutual information into the objective function, we can optimize policies under both performance criteria (e.g., cumulative reward) as well as privacy constraints (i.e., limiting sensitive information disclosure). Our approach effectively balances competing objectives, enabling the development of more secure and transparent machine learning algorithms. We validate our framework on several synthetic environments across different domains and demonstrate superiority over conventional methods. Additionally, we provide insights into how the proposed method can handle complex problem settings involving high-dimensional state spaces and stochastic transitions. Overall, the contributions of this work lay a foundation for future research at the intersection of computer science and data privacy.",1
"Existing approaches in video captioning concentrate on exploring global frame features in the uncompressed videos, while the free of charge and critical saliency information already encoded in the compressed videos is generally neglected. We propose a video captioning method which operates directly on the stored compressed videos. To learn a discriminative visual representation for video captioning, we design a residuals-assisted encoder (RAE), which spots regions of interest in I-frames under the assistance of the residuals frames. First, we obtain the spatial attention weights by extracting features of residuals as the saliency value of each location in I-frame and design a spatial attention module to refine the attention weights. We further propose a temporal gate module to determine how much the attended features contribute to the caption generation, which enables the model to resist the disturbance of some noisy signals in the compressed videos. Finally, Long Short-Term Memory is utilized to decode the visual representations into descriptions. We evaluate our method on two benchmark datasets and demonstrate the effectiveness of our approach.",0
"This paper presents a new approach for video caption generation that leverages compressed video representation and machine learning techniques. Traditional methods for generating captions rely on expensive pixel-level feature extraction from raw video frames, which can be time-consuming and computationally demanding. In contrast, our method operates directly on the compact compressed domain, allowing for efficient and accurate caption generation at reduced computational cost.  The proposed framework consists of two components: a preliminary compression stage and a caption generator module. During the preprocessing phase, we use standard video coding algorithms to compress the input video into a compact bitstream format suitable for subsequent processing by the caption generator. Next, using convolutional neural networks (CNNs), we extract features from the compressed stream and feed them into a recurrent neural network (RNN) architecture to generate natural language descriptions of the visual content. Our experiments demonstrate that our approach achieves significant improvements over baseline models trained without utilizing compressed representations, while reducing latency and computational demands compared to prior work. Additionally, we evaluate the impact of different compression ratios on caption quality, showing that the optimal balance lies between minimizing complexity and maximizing fidelity. Overall, our method represents a step forward in enabling real-time video captioning solutions for diverse multimedia applications.",1
"Instance segmentation is an important task for biomedical and biological image analysis. Due to the complicated background components, the high variability of object appearances, numerous overlapping objects, and ambiguous object boundaries, this task still remains challenging. Recently, deep learning based methods have been widely employed to solve these problems and can be categorized into proposal-free and proposal-based methods. However, both proposal-free and proposal-based methods suffer from information loss, as they focus on either global-level semantic or local-level instance features. To tackle this issue, we present a Panoptic Feature Fusion Net (PFFNet) that unifies the semantic and instance features in this work. Specifically, our proposed PFFNet contains a residual attention feature fusion mechanism to incorporate the instance prediction with the semantic features, in order to facilitate the semantic contextual information learning in the instance branch. Then, a mask quality sub-branch is designed to align the confidence score of each object with the quality of the mask prediction. Furthermore, a consistency regularization mechanism is designed between the semantic segmentation tasks in the semantic and instance branches, for the robust learning of both tasks. Extensive experiments demonstrate the effectiveness of our proposed PFFNet, which outperforms several state-of-the-art methods on various biomedical and biological datasets.",0
"Increasingly, panopticonic vision models have been used in computer science applications such as instance segmentation due to their ability to handle large datasets while maintaining accuracy. However, traditional panoptic feature fusion methods can suffer from problems such as loss of resolution during feature pyramid fusion or difficulties in merging low-resolution features into high-resolution ones. To address these challenges, we propose a novel paradigm called Panoptic Feature Fusion Net (PFFN) that utilizes a multi-scale feature fusing strategy based on residual learning. Our method effectively integrates global context information by introducing auxiliary decoders at different scales, leading to more precise results. We demonstrate the superiority of our model compared to state-of-the art approaches through extensive experiments on biomedical and biological image datasetson both semantic segmentation tasks and controllable object detection.",1
"Thumbnail is the face of online videos. The explosive growth of videos both in number and variety underpins the importance of a good thumbnail because it saves potential viewers time to choose videos and even entice them to click on them. A good thumbnail should be a frame that best represents the content of a video while at the same time capturing viewers' attention. However, the techniques and models in the past only focus on frames within a video, and we believe such narrowed focus leave out much useful information that are part of a video. In this paper, we expand the definition of content to include title, description, and audio of a video and utilize information provided by these modalities in our selection model. Specifically, our model will first sample frames uniformly in time and return the top 1,000 frames in this subset with the highest aesthetic scores by a Double-column Convolutional Neural Network, to avoid the computational burden of processing all frames in downstream task. Then, the model incorporates frame features extracted from VGG16, text features from ELECTRA, and audio features from TRILL. These models were selected because of their results on popular datasets as well as their competitive performances. After feature extraction, the time-series features, frames and audio, will be fed into Transformer encoder layers to return a vector representing their corresponding modality. Each of the four features (frames, title, description, audios) will pass through a context gating layer before concatenation. Finally, our model will generate a vector in the latent space and select the frame that is most similar to this vector in the latent space. To the best of our knowledge, we are the first to propose a multi-modal deep learning model to select video thumbnail, which beats the result from the previous State-of-The-Art models.",0
"This research presents a multi-modal deep learning model for video thumbnail selection that utilizes both visual features and audio features extracted from videos. The proposed method uses pre-trained convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to extract visual and audio representations respectively. These representations are then combined using late fusion to generate a single representation which is fed into a fully connected layer followed by softmax activation function for prediction. Experimental results on two publicly available datasets show that the proposed model outperforms state-of-the-art methods in terms of accuracy and F1 score. Additionally, ablation studies demonstrate that the inclusion of audio features improves the performance of the model significantly. Overall, the proposed approach demonstrates the effectiveness of leveraging multiple modalities in selecting representative frames from videos as thumbnails.",1
"Pansharpening is a widely used image enhancement technique for remote sensing. Its principle is to fuse the input high-resolution single-channel panchromatic (PAN) image and low-resolution multi-spectral image and to obtain a high-resolution multi-spectral (HRMS) image. The existing deep learning pansharpening method has two shortcomings. First, features of two input images need to be concatenated along the channel dimension to reconstruct the HRMS image, which makes the importance of PAN images not prominent, and also leads to high computational cost. Second, the implicit information of features is difficult to extract through the manually designed loss function. To this end, we propose a generative adversarial network via the fast guided filter (FGF) for pansharpening. In generator, traditional channel concatenation is replaced by FGF to better retain the spatial information while reducing the number of parameters. Meanwhile, the fusion objects can be highlighted by the spatial attention module. In addition, the latent information of features can be preserved effectively through adversarial training. Numerous experiments illustrate that our network generates high-quality HRMS images that can surpass existing methods, and with fewer parameters.",0
"FGF-GAN is a new model that uses generative adversarial networks (GANs) to improve pansharpening, which is the process of combining multiple images into one high resolution image. The authors propose using a lightweight GAN architecture called fast guided filter-generative adversarial network (FGF-GAN), which combines the strengths of both guided filters and GANs. This approach allows for faster training times and improved performance compared to other methods. The model was evaluated on several datasets and showed promising results, achieving state-of-the-art performance while requiring less computational resources. Overall, this work presents a novel method that improves upon current techniques used in satellite imagery processing by utilizing advanced machine learning techniques such as GANs.",1
"Combinatorial optimization is one of the fundamental research fields that has been extensively studied in theoretical computer science and operations research. When developing an algorithm for combinatorial optimization, it is commonly assumed that parameters such as edge weights are exactly known as inputs. However, this assumption may not be fulfilled since input parameters are often uncertain or initially unknown in many applications such as recommender systems, crowdsourcing, communication networks, and online advertisement. To resolve such uncertainty, the problem of combinatorial pure exploration of multi-armed bandits (CPE) and its variants have recieved increasing attention. Earlier work on CPE has studied the semi-bandit feedback or assumed that the outcome from each individual edge is always accessible at all rounds. However, due to practical constraints such as a budget ceiling or privacy concern, such strong feedback is not always available in recent applications. In this article, we review recently proposed techniques for combinatorial pure exploration problems with limited feedback.",0
"In this paper, we consider the problem of combinatorial pure exploration, wherein an algorithm must make decisions from a finite set of actions while receiving full-bandit feedback on each action chosen. We present algorithms that can efficiently solve combinatorial optimization problems under uncertainty with limited observation. Our approach leverages techniques such as upper confidence bounds (UCB) and Thompson sampling (TS), along with novel methods tailored specifically for combinatorial settings. We provide theoretical guarantees on both regret minimization and approximation ratios for our proposed algorithms. Furthermore, we demonstrate through extensive simulations that our algorithms outperform state-of-the-art baselines in terms of solution quality achieved within given time horizons. Overall, our work represents an important contribution towards understanding efficient decision making under high levels of uncertainty and limited information availability.",1
"Retinal image quality assessment is an essential prerequisite for diagnosis of retinal diseases. Its goal is to identify retinal images in which anatomic structures and lesions attracting ophthalmologists' attention most are exhibited clearly and definitely while reject poor quality fundus images. Motivated by this, we mimic the way that ophthalmologists assess the quality of retinal images and propose a method termed SalStructuIQA. First, two salient structures for automated retinal quality assessment. One is the large-size salient structures including optic disc region and exudates in large-size. The other is the tiny-size salient structures which mainly include vessels. Then we incorporate the proposed two salient structure priors with deep convolutional neural network (CNN) to shift the focus of CNN to salient structures. Accordingly, we develop two CNN architectures: Dual-branch SalStructIQA and Single-branch SalStructIQA. Dual-branch SalStructIQA contains two CNN branches and one is guided by large-size salient structures while the other is guided by tiny-size salient structures. Single-branch SalStructIQA contains one CNN branch, which is guided by the concatenation of salient structures in both large-size and tiny-size. Experimental results on Eye-Quality dataset show that our proposed Dual-branch SalStructIQA outperforms the state-of-the-art methods for retinal image quality assessment and Single-branch SalStructIQA is much light-weight comparing with state-of-the-art deep retinal image quality assessment methods and still achieves competitive performances.",0
"This paper presents a deep learning model capable of assessing the quality of retinal images based on their saliency structure priors (SSPs). Our method uses the structural similarity index as the primary metric by which to evaluate image quality, with SSPs serving as an additional guide for fine-grained analysis. We train our network using synthetic data generated from ground truth retinal images and corresponding quality scores provided by human annotators. In order to validate our approach, we conduct experiments on two publicly available datasets comprising real-world retinal images graded by professional ophthalmologists. Results demonstrate that our framework outperforms state-of-the-art methods across all evaluation metrics, suggesting its potential for use in clinical settings where accurate image quality assessments play a critical role.",1
"Image-to-image translation is to learn a mapping between images from a source domain and images from a target domain. In this paper, we introduce the attention mechanism directly to the generative adversarial network (GAN) architecture and propose a novel spatial attention GAN model (SPA-GAN) for image-to-image translation tasks. SPA-GAN computes the attention in its discriminator and use it to help the generator focus more on the most discriminative regions between the source and target domains, leading to more realistic output images. We also find it helpful to introduce an additional feature map loss in SPA-GAN training to preserve domain specific features during translation. Compared with existing attention-guided GAN models, SPA-GAN is a lightweight model that does not need additional attention networks or supervision. Qualitative and quantitative comparison against state-of-the-art methods on benchmark datasets demonstrates the superior performance of SPA-GAN.",0
"This paper introduces a novel Generative Adversarial Network (GAN) architecture called ""SPA-GAN"" which utilizes spatial attention mechanisms for image-to-image translation tasks. The core component of the model consists of a generator network with a spatial attention mechanism that guides the generation process and allows control over specific regions of the output image. In addition, we propose a new discriminator design that can handle varying resolutions during training, allowing us to train models on high resolution images while generating lower resolution outputs. We evaluate our model on several challenging benchmark datasets and demonstrate state-of-the-art performance across all metrics. Our method outperforms previous approaches by producing more realistic and detailed results while requiring fewer parameters. The use of attention mechanisms enables fine-grained control over the generated output and opens up new possibilities for image manipulation and editing tasks. Overall, our contributions provide significant advancements towards improving the quality and controllability of image-to-image translations using deep learning techniques.",1
"Training a classifier under fairness constraints has gotten increasing attention in the machine learning community thanks to moral, legal, and business reasons. However, several recent works addressing algorithmic fairness have only focused on simple models such as logistic regression or support vector machines due to non-convex and non-differentiable fairness criteria across protected groups, such as race or gender. Neural networks, the most widely used models for classification nowadays, are precluded and lack theoretical guarantees. This paper aims to fill this missing but crucial part of the literature of algorithmic fairness for neural networks. In particular, we show that overparametrized neural networks could meet the fairness constraints. The key ingredient of building a fair neural network classifier is establishing no-regret analysis for neural networks in the overparameterization regime, which may be of independent interest in the online learning of neural networks and related applications.",0
"This paper addresses the challenge of training neural network classifiers while ensuring fairness constraints are met. We present two new algorithms that provably satisfy these constraints by making modifications to existing optimization techniques commonly used in machine learning. Our methods achieve better accuracy than previous approaches while still guaranteeing fair outcomes across different groups. Additionally, we empirically evaluate our techniques on several datasets to demonstrate their effectiveness in practice. Overall, our work provides valuable insights into the intersection of fairness and machine learning and offers promising solutions for addressing algorithmic bias.",1
"Graph Convolutional Networks (GCNs) have received increasing attention in the machine learning community for effectively leveraging both the content features of nodes and the linkage patterns across graphs in various applications. As real-world graphs are often incomplete and noisy, treating them as ground-truth information, which is a common practice in most GCNs, unavoidably leads to sub-optimal solutions. Existing efforts for addressing this problem either involve an over-parameterized model which is difficult to scale, or simply re-weight observed edges without dealing with the missing-edge issue. This paper proposes a novel framework called Graph-Revised Convolutional Network (GRCN), which avoids both extremes. Specifically, a GCN-based graph revision module is introduced for predicting missing edges and revising edge weights w.r.t. downstream tasks via joint optimization. A theoretical analysis reveals the connection between GRCN and previous work on multigraph belief propagation. Experiments on six benchmark datasets show that GRCN consistently outperforms strong baseline methods by a large margin, especially when the original graphs are severely incomplete or the labeled instances for model training are highly sparse.",0
"This paper introduces a novel approach to improve convolutional networks through graph rewiring. We show that by randomly adding new connections within the network while pruning existing ones, we can significantly increase accuracy without modifying the architecture itself. Our method adapts to any given dataset without fine-tuning, making it universal across different tasks and models. Additionally, our technique incurs minimal overhead and has negligible computational cost compared to standard training procedures. Our results demonstrate significant improvement over baseline models on multiple benchmark datasets including CIFAR-10 and ImageNet. Overall, our work represents a promising step towards efficient and effective deep learning methods with broad applicability.",1
"The rapid development and wide utilization of object detection techniques have aroused attention on both accuracy and speed of object detectors. However, the current state-of-the-art object detection works are either accuracy-oriented using a large model but leading to high latency or speed-oriented using a lightweight model but sacrificing accuracy. In this work, we propose YOLObile framework, a real-time object detection on mobile devices via compression-compilation co-design. A novel block-punched pruning scheme is proposed for any kernel size. To improve computational efficiency on mobile devices, a GPU-CPU collaborative scheme is adopted along with advanced compiler-assisted optimizations. Experimental results indicate that our pruning scheme achieves 14$\times$ compression rate of YOLOv4 with 49.0 mAP. Under our YOLObile framework, we achieve 17 FPS inference speed using GPU on Samsung Galaxy S20. By incorporating our proposed GPU-CPU collaborative scheme, the inference speed is increased to 19.1 FPS, and outperforms the original YOLOv4 by 5$\times$ speedup. Source code is at: \url{https://github.com/nightsnack/YOLObile}.",0
"This paper presents a novel approach to real-time object detection on mobile devices using compression-compilation co-design. We begin by discussing the challenges that arise when trying to run deep neural networks on resource-limited devices such as smartphones. These challenges include limited computing power, memory constraints, and the need for efficient use of data bandwidth. To address these issues, we propose a new method called ""YOLOBile"" which combines several techniques including network pruning, quantization, and knowledge distillation. Our approach uses a two-stage pipeline where the first stage detects objects using a lightweight model running on the device itself while a second stage provides additional refinement based on cloud processing. Our experiments show that our approach achieves state-of-the-art accuracy while providing significant improvements over existing methods in terms of speed and efficiency. In addition, we demonstrate the effectiveness of YOLOBile through extensive experiments on popular benchmark datasets as well as a set of real-world videos captured from a moving car. Overall, our work shows promise towards enabling widespread deployment of accurate and efficient object detection models on mobile devices.",1
"We present an approach for agents to learn representations of a global map from sensor data, to aid their exploration in new environments. To achieve this, we embed procedures mimicking that of traditional Simultaneous Localization and Mapping (SLAM) into the soft attention based addressing of external memory architectures, in which the external memory acts as an internal representation of the environment. This structure encourages the evolution of SLAM-like behaviors inside a completely differentiable deep neural network. We show that this approach can help reinforcement learning agents to successfully explore new environments where long-term memory is essential. We validate our approach in both challenging grid-world environments and preliminary Gazebo experiments. A video of our experiments can be found at: https://goo.gl/G2Vu5y.",0
"Artificial intelligence has made significant progress in recent years, but many problems remain unsolvable by existing methods. For example, traditional machine learning algorithms cannot solve complex mazes or find their way through unknown environments without prior knowledge or specialized training data. In our work, we present a novel approach that allows machines to learn how to explore and map new spaces using only basic sensory inputs and simple instructions. This method uses deep neural networks to encode memories of past experiences into a compact representation called external memory. We demonstrate that our system can use these memories to plan paths through difficult mazes, track movements in dynamic environments, and even generate images of never-before-seen objects based on text descriptions alone. By enabling autonomous agents to actively gather visual information and update their internal models accordingly, Neural SLAM enables more generalizable problem solving across domains where little labeled data exists. Our work shows strong results comparable to state-of-the-art systems while requiring fewer resources and minimal hand engineering. Overall, our contributions represent a promising step towards achieving human-level autonomy and intelligence in artificial agents. Keywords: Neural SLAM, exploration, external memory, navigation, image generation.",1
"Video-based person recognition is challenging due to persons being blocked and blurred, and the variation of shooting angle. Previous research always focused on person recognition on still images, ignoring similarity and continuity between video frames. To tackle the challenges above, we propose a novel Frame Aggregation and Multi-Modal Fusion (FAMF) framework for video-based person recognition, which aggregates face features and incorporates them with multi-modal information to identify persons in videos. For frame aggregation, we propose a novel trainable layer based on NetVLAD (named AttentionVLAD), which takes arbitrary number of features as input and computes a fixed-length aggregation feature based on feature quality. We show that introducing an attention mechanism to NetVLAD can effectively decrease the impact of low-quality frames. For the multi-model information of videos, we propose a Multi-Layer Multi-Modal Attention (MLMA) module to learn the correlation of multi-modality by adaptively updating Gram matrix. Experimental results on iQIYI-VID-2019 dataset show that our framework outperforms other state-of-the-art methods.",0
"In todayâ€™s world, video surveillance systems have become an essential component of public safety and security measures. As such, person recognition has emerged as one of the most important applications in these systems. However, due to variations in illumination, pose, resolution, occlusions, and background clutter, face recognition using a single frame might not perform well. To address this challenge, our proposed method employs a novel framework that utilizes several frames per video by aggregating them based on their quality scores. This allows us to improve accuracy by performing feature extraction from high-quality frames while handling missing faces or partial occlusions. Our approach further integrates multiple modalities like motion features extracted from optical flow maps which provide additional discriminative cues thereby enhancing performance. Comprehensive experiments performed on benchmark datasets reveal consistent improvement over state-of-the-art methods. Overall, our contribution demonstrates the effectiveness of employing multi-modal fusion techniques along with intelligent frame selection strategies towards robust person re-identification under real-world scenarios.",1
"Although the recent image-based 3D object detection methods using Pseudo-LiDAR representation have shown great capabilities, a notable gap in efficiency and accuracy still exist compared with LiDAR-based methods. Besides, over-reliance on the stand-alone depth estimator, requiring a large number of pixel-wise annotations in the training stage and more computation in the inferencing stage, limits the scaling application in the real world.   In this paper, we propose an efficient and accurate 3D object detection method from stereo images, named RTS3D. Different from the 3D occupancy space in the Pseudo-LiDAR similar methods, we design a novel 4D feature-consistent embedding (FCE) space as the intermediate representation of the 3D scene without depth supervision. The FCE space encodes the object's structural and semantic information by exploring the multi-scale feature consistency warped from stereo pair. Furthermore, a semantic-guided RBF (Radial Basis Function) and a structure-aware attention module are devised to reduce the influence of FCE space noise without instance mask supervision. Experiments on the KITTI benchmark show that RTS3D is the first true real-time system (FPS$$24) for stereo image 3D detection meanwhile achieves $10\%$ improvement in average precision comparing with the previous state-of-the-art method. The code will be available at https://github.com/Banconxuan/RTS3D",0
"This should give you a general idea of how I would like my paper to look like and act as reference for the writing style. This research proposal aims at solving the problem of real-time 3D object detection for autonomous vehicles by introducing a novel algorithm called RTS3D (Real Time Stereoscopic Three Dimensional Object Detection). By leveraging recent advances in deep learning and computer vision, RTS3D combines state of the art techniques such as feature extraction, multi-view geometry, and geometric reasoning into one framework capable of accurately detecting objects and their depth in real time. Unlike current approaches that rely on expensive hardware or specialized sensors, RTS3D uses off-the-shelf cameras and can be integrated into existing self driving systems. We evaluate our approach against other methods on multiple benchmark datasets and demonstrate significant improvements in accuracy, speed, and robustness. Our work has important implications for enhancing safety and reliability of future transportation systems by providing accurate perception of surrounding environments in real-time. Additionally, we discuss potential applications of our method beyond autonomous driving. Overall, this research contributes to bridging the gap between academic research and practical solutions towards safe and efficient deployment of autonomous technologies.",1
"It is well known that featuremap attention and multi-path representation are important for visual recognition. In this paper, we present a modularized architecture, which applies the channel-wise attention on different network branches to leverage their success in capturing cross-feature interactions and learning diverse representations. Our design results in a simple and unified computation block, which can be parameterized using only a few variables. Our model, named ResNeSt, outperforms EfficientNet in accuracy and latency trade-off on image classification. In addition, ResNeSt has achieved superior transfer learning results on several public benchmarks serving as the backbone, and has been adopted by the winning entries of COCO-LVIS challenge. The source code for complete system and pretrained models are publicly available.",0
"""ResNeSt: Split-Attention Networks"" presents a novel architecture that combines the strengths of split-attention networks and residual neural networks (ResNets). The proposed approach addresses several limitations of previous models by allowing each network layer to attend selectively to different features within images, increasing model interpretability. Additionally, the authors introduce a new attention module called SqueezeBlock, which improves accuracy without introducing additional computational overhead. The results demonstrate the effectiveness of the proposed method on three benchmark datasets, achieving state-of-the-art performance across multiple tasks. Overall, the study advances our understanding of deep learning architectures and has practical implications for computer vision applications.",1
"Recently, graph neural networks for semi-supervised classification have been widely studied. However, existing methods only use the information of limited neighbors and do not deal with the inter-class connections in graphs. In this paper, we propose Adaptive aggregation with Class-Attentive Diffusion (AdaCAD), a new aggregation scheme that adaptively aggregates nodes probably of the same class among K-hop neighbors. To this end, we first propose a novel stochastic process, called Class-Attentive Diffusion (CAD), that strengthens attention to intra-class nodes and attenuates attention to inter-class nodes. In contrast to the existing diffusion methods with a transition matrix determined solely by the graph structure, CAD considers both the node features and the graph structure with the design of our class-attentive transition matrix that utilizes a classifier. Then, we further propose an adaptive update scheme that leverages different reflection ratios of the diffusion result for each node depending on the local class-context. As the main advantage, AdaCAD alleviates the problem of undesired mixing of inter-class features caused by discrepancies between node labels and the graph topology. Built on AdaCAD, we construct a simple model called Class-Attentive Diffusion Network (CAD-Net). Extensive experiments on seven benchmark datasets consistently demonstrate the efficacy of the proposed method and our CAD-Net significantly outperforms the state-of-the-art methods. Code is available at https://github.com/ljin0429/CAD-Net.",0
"Artificial Intelligence (AI) has made significant progress over recent years in achieving state-of-the-art results on many tasks using deep learning techniques. However, one common problem faced by these methods is their reliance on large amounts of labeled training data which can be expensive and time consuming to collect. To address this issue, semi-supervised learning provides an attractive alternative as it seeks to learn from both labeled and unlabeled data. In this study we propose a novel architecture called class-attentive diffusion network (CDN), based on encoder-decoder networks that models the distribution of representations. CDN jointly optimizes supervised losses on labeled examples while encouraging alignment of the learned representation space across different classes through regularization terms enforced in decoding step. Results demonstrate that our method outperforms existing approaches for semi-supervised classification on CIFAR-10 and STL-10 datasets with significantly less labeled training examples, providing evidence that the proposed model is effective at leveraging limited labels. We further analyze the behavior of the proposed system and show qualitatively how it aligns representations in feature space. Overall, these findings indicate that class-attentive diffusion networks have the potential to improve semi-supervised performance by effectively utilizing unlabelled data and promoting more meaningful features.",1
"Graph Neural Networks (GNNs) have received much attention recent years and have achieved state-of-the-art performances in many fields. The deeper GNNs can theoretically capture deeper neighborhood information. However, they often suffer from problems of over-fitting and over-smoothing. In order to incorporate deeper information while preserving considerable complexity and generalization ability, we propose Adaptive Graph Diffusion Networks with Hop-wise Attention (AGDNs-HA). We stack multi-hop neighborhood aggregations of different orders into single layer. Then we integrate them with the help of hop-wise attention, which is learnable and adaptive for each node. Experimental results on the standard dataset with semi-supervised node classification task show that our proposed methods achieve significant improvements.",0
"In recent years, graph diffusion networks (GDNs) have become increasingly popular due to their ability to efficiently model complex relationships within graphs while maintaining efficient computation. These models can be further improved through the use of attention mechanisms, which allow the network to selectively focus on certain nodes or edges based on their importance. However, current GDNs lack adaptability, leading to suboptimal performance in dynamic environments where changes occur over time. To address this issue, we propose the Adaptive Graph Diffusion Network (AGDN), which integrates hop-wise attention into the GDN framework to dynamically adapt to changing graph topologies. Our AGDN architecture improves upon existing methods by enabling more flexible and fine-grained control over the flow of information across different hops in the graph. We demonstrate our AGDNâ€™s effectiveness via comprehensive experiments across multiple benchmark datasets, achieving state-of-the-art results in node classification and edge prediction tasks. The success of AGDNs points towards potential future advancements in using graph neural networks for capturing evolving dependencies in data structures like knowledge bases, social networks, and biological networks. By combining attention techniques with graph diffusion processes, these developments can enhance model adaptability and generalizability in real-world applications involving graphs as data representations.",1
"Improving the aesthetic quality of images is challenging and eager for the public. To address this problem, most existing algorithms are based on supervised learning methods to learn an automatic photo enhancer for paired data, which consists of low-quality photos and corresponding expert-retouched versions. However, the style and characteristics of photos retouched by experts may not meet the needs or preferences of general users. In this paper, we present an unsupervised image enhancement generative adversarial network (UEGAN), which learns the corresponding image-to-image mapping from a set of images with desired characteristics in an unsupervised manner, rather than learning on a large number of paired images. The proposed model is based on single deep GAN which embeds the modulation and attention mechanisms to capture richer global and local features. Based on the proposed model, we introduce two losses to deal with the unsupervised image enhancement: (1) fidelity loss, which is defined as a L2 regularization in the feature domain of a pre-trained VGG network to ensure the content between the enhanced image and the input image is the same, and (2) quality loss that is formulated as a relativistic hinge adversarial loss to endow the input image the desired characteristics. Both quantitative and qualitative results show that the proposed model effectively improves the aesthetic quality of images. Our code is available at: https://github.com/eezkni/UEGAN.",0
"In recent years, deep learning has revolutionized image enhancement by enabling unprecedented results on a wide range of tasks, from super-resolution to colorization. However, existing methods require large amounts of labeled data for training, which can be expensive and time consuming to collect and annotate. This work presents a novel approach to unsupervised deep image enhancement that leverages generative adversarial networks (GANs) to learn texture synthesis without any labeled examples. Our method trains two GANs simultaneously: one generator generates new texture samples given some random input noise, while another discriminator learns to distinguish real images from generated ones. During training, we use a cycle consistency loss to encourage the generator to produce outputs that are consistent with their inputs. Experimental results demonstrate that our approach achieves state-of-the-art performance on several benchmark datasets, outperforming traditional methods as well as prior unsupervised approaches. Our findings suggest that using GANs for texture synthesis can lead to significant improvements in image quality, opening up exciting possibilities for applications such as medical imaging and computer vision.",1
"Feature selection is important in data representation and intelligent diagnosis. Elastic net is one of the most widely used feature selectors. However, the features selected are dependant on the training data, and their weights dedicated for regularized regression are irrelevant to their importance if used for feature ranking, that degrades the model interpretability and extension. In this study, an intuitive idea is put at the end of multiple times of data splitting and elastic net based feature selection. It concerns the frequency of selected features and uses the frequency as an indicator of feature importance. After features are sorted according to their frequency, linear support vector machine performs the classification in an incremental manner. At last, a compact subset of discriminative features is selected by comparing the prediction performance. Experimental results on breast cancer data sets (BCDR-F03, WDBC, GSE 10810, and GSE 15852) suggest that the proposed framework achieves competitive or superior performance to elastic net and with consistent selection of fewer features. How to further enhance its consistency on high-dimension small-sample-size data sets should be paid more attention in our future work. The proposed framework is accessible online (https://github.com/NicoYuCN/elasticnetFR).",0
"This paper presents a novel method for feature ranking and selection using the elastic net regularization technique. We show that our approach can effectively select a subset of relevant features while also performing well in terms of prediction accuracy. Our experiments on several real-world datasets demonstrate the superiority of our method over existing approaches. Furthermore, we provide insights into how different combinations of Lasso and Ridge penalties affect the performance of feature selection. Overall, our work contributes to the field by providing a simple yet effective solution for handling high-dimensional data problems.",1
"Multi-focus image fusion (MFF) is a popular technique to generate an all-in-focus image, where all objects in the scene are sharp. However, existing methods pay little attention to defocus spread effects of the real-world multi-focus images. Consequently, most of the methods perform badly in the areas near focus map boundaries. According to the idea that each local region in the fused image should be similar to the sharpest one among source images, this paper presents an optimization-based approach to reduce defocus spread effects. Firstly, a new MFF assessmentmetric is presented by combining the principle of structure similarity and detected focus maps. Then, MFF problem is cast into maximizing this metric. The optimization is solved by gradient ascent. Experiments conducted on the real-world dataset verify superiority of the proposed model. The codes are available at https://github.com/xsxjtu/MFF-SSIM.",0
"Title: Reducing severe defocus spread effects for multi-focus image fusion through optimization  Image fusion plays a vital role in computer vision tasks such as object detection, recognition, and segmentation, as well as medical imaging applications like microscopy and endoscopy. In recent years, there has been significant interest in developing algorithms that can fuse multiple images taken at different focus distances into one clear and sharp composite image. However, challenges still exist in achieving high-quality fusion results due to issues related to variations in illumination, noise, contrast, and focus spread artifacts. This study presents an optimization-based approach aimed at reducing severe defocus spread (DS) effects during multi-image fusion.  The proposed method involves creating an initial fused image by taking weighted sums of input images based on their corresponding intensities. Then, we develop an energy function model that evaluates the overall quality of the resulting fused image using statistical measures such as entropy and structural similarity index. We use this function to guide our optimization process towards minimizing DS errors while maximizing visual clarity. Our experiments demonstrate improved performance compared to traditional nonlinear registration-based approaches, achieving enhanced image details, reduced blurriness, and better local features preservation across varying focal depths. Additionally, we showcase how our framework can adapt to varying levels of illumination changes among input images without losing its effectiveness.  In conclusion, our work represents an important step forward in addressing critical limitations associated with existing multi-focu",1
"WebFG 2020 is an international challenge hosted by Nanjing University of Science and Technology, University of Edinburgh, Nanjing University, The University of Adelaide, Waseda University, etc. This challenge mainly pays attention to the webly-supervised fine-grained recognition problem. In the literature, existing deep learning methods highly rely on large-scale and high-quality labeled training data, which poses a limitation to their practicability and scalability in real world applications. In particular, for fine-grained recognition, a visual task that requires professional knowledge for labeling, the cost of acquiring labeled training data is quite high. It causes extreme difficulties to obtain a large amount of high-quality training data. Therefore, utilizing free web data to train fine-grained recognition models has attracted increasing attentions from researchers in the fine-grained community. This challenge expects participants to develop webly-supervised fine-grained recognition methods, which leverages web images in training fine-grained recognition models to ease the extreme dependence of deep learning methods on large-scale manually labeled datasets and to enhance their practicability and scalability. In this technical report, we have pulled together the top WebFG 2020 solutions of total 54 competing teams, and discuss what methods worked best across the set of winning teams, and what surprisingly did not help.",0
"In recent years, fine-grained recognition has become increasingly important due to its ability to accurately identify subclasses within classes such as species of animals, types of cars, etc. This task can be challenging due to subtle differences among objects within each subclass. In our work, we explore webly-supervised fine-grained recognition using data collected from public sources on the Internet. Our approach involves adaptive pre-training, self-supervised learning, transfer learning and multi-instance contrastive loss. We evaluate our methodology by participating in the WebFG 2020 challenge which provides datasets that require fine-grained classification at the instance level. Through extensive experimentation, we demonstrate significant improvements over baseline models and establish state-of-the-art performance on several benchmarks. Overall, our results showcase the effectiveness of webly-supervised approaches for tackling fine-grained recognition tasks, paving the way for future research in this area.",1
"In the field of data mining, how to deal with high-dimensional data is an inevitable problem. Unsupervised feature selection has attracted more and more attention because it does not rely on labels. The performance of spectral-based unsupervised methods depends on the quality of constructed similarity matrix, which is used to depict the intrinsic structure of data. However, real-world data contain a large number of noise samples and features, making the similarity matrix constructed by original data cannot be completely reliable. Worse still, the size of similarity matrix expands rapidly as the number of samples increases, making the computational cost increase significantly. Inspired by principal component analysis, we propose a simple and efficient unsupervised feature selection method, by combining reconstruction error with $l_{2,p}$-norm regularization. The projection matrix, which is used for feature selection, is learned by minimizing the reconstruction error under the sparse constraint. Then, we present an efficient optimization algorithm to solve the proposed unsupervised model, and analyse the convergence and computational complexity of the algorithm theoretically. Finally, extensive experiments on real-world data sets demonstrate the effectiveness of our proposed method.",0
"Abstract: This research proposes a new method for feature selection based on sparse principal component analysis (PCA). The main challenge addressed by the proposed approach is how to balance interpretability (which typically requires selecting a small number of features) with performance. To achieve this goal, the authors use regularization via the $\ell_{2,p}$ norm, which has been shown to outperform other approaches such as $\ell_1$ regularization. Another key contribution of the work is the extension of the model from continuous variables to categorical data, making the technique more widely applicable. Experimental results demonstrate that the proposed method can select highly interpretable sets of features while still achieving state-of-the-art performance compared to competing methods. Finally, the authors discuss potential future directions, including applications beyond supervised learning tasks. Overall, this study represents an important step forward in understanding how to make feature selection models more robust and generalizable across different types of data.",1
"Time series data usually contains local and global patterns. Most of the existing feature networks pay more attention to local features rather than the relationships among them. The latter is, however, also important yet more difficult to explore. To obtain sufficient representations by a feature network is still challenging. To this end, we propose a novel robust temporal feature network (RTFN) for feature extraction in time series classification, containing a temporal feature network (TFN) and an LSTM-based attention network (LSTMaN). TFN is a residual structure with multiple convolutional layers. It functions as a local-feature extraction network to mine sufficient local features from data. LSTMaN is composed of two identical layers, where attention and long short-term memory (LSTM) networks are hybridized. This network acts as a relation extraction network to discover the intrinsic relationships among the extracted features at different positions in sequential data. In experiments, we embed RTFN into a supervised structure as a feature extractor and into an unsupervised structure as an encoder, respectively. The results show that the RTFN-based structures achieve excellent supervised and unsupervised performance on a large number of UCR2018 and UEA2018 datasets.",0
"This paper presents a new approach for time series classification using deep learning techniques called Robust Temporal Feature Network (RTFN). RTFN is designed to handle time series data that may contain noise, missing values, or other irregularities by utilizing robust temporal features extracted from the raw data. By using multiple feature extraction methods and selecting the most robust features based on their performance, RTFN can improve the accuracy of time series classification compared to traditional approaches. In addition, RTFN uses attention mechanisms to focus on important features and periods within each time series sample, resulting in more effective representation of the underlying patterns. Experimental results on several benchmark datasets show that RTFN outperforms state-of-the-art approaches in terms of both accuracy and efficiency.",1
"Time series analysis plays a vital role in various applications, for instance, healthcare, weather prediction, disaster forecast, etc. However, to obtain sufficient shapelets by a feature network is still challenging. To this end, we propose a novel robust temporal feature network (RTFN) that contains temporal feature networks and attentional LSTM networks. The temporal feature networks are built to extract basic features from input data while the attentional LSTM networks are devised to capture complicated shapelets and relationships to enrich features. In experiments, we embed RTFN into supervised structure as a feature extraction network and into unsupervised clustering as an encoder, respectively. The results show that the RTFN-based supervised structure is a winner of 40 out of 85 datasets and the RTFN-based unsupervised clustering performs the best on 4 out of 11 datasets in the UCR2018 archive.",0
"This paper presents a novel approach to feature extraction from video data using deep learning techniques called ""Robust Temporal Feature Network"" (RTFN). We propose an architecture that uses temporal convolutions to capture features across frames and spatial convolutions to extract high level semantic features at each time step. Our model is trained end-to-end on large scale datasets and achieves state of the art results on several benchmark tasks including action recognition, object detection, and segmentation. In addition to its strong performance, our method can adapt to varying length inputs making it suitable for real world scenarios where input lengths may vary greatly. Overall we believe RTFN offers a significant improvement over current methods and has potential applications in many areas beyond computer vision such as natural language processing and speech understanding.",1
"In this paper, we propose a novel deep learning architecture to improving word-level lip-reading. On the one hand, we first introduce the multi-scale processing into the spatial feature extraction for lip-reading. Specially, we proposed hierarchical pyramidal convolution (HPConv) to replace the standard convolution in original module, leading to improvements over the model's ability to discover fine-grained lip movements. On the other hand, we merge information in all time steps of the sequence by utilizing self-attention, to make the model pay more attention to the relevant frames. These two advantages are combined together to further enhance the model's classification power. Experiments on the Lip Reading in the Wild (LRW) dataset show that our proposed model has achieved 86.83% accuracy, yielding 1.53% absolute improvement over the current state-of-the-art. We also conducted extensive experiments to better understand the behavior of the proposed model.",0
"In recent years, lip reading has become increasingly important due to advancements in video conferencing technology and its applications in real world settings such as automotive industry and sign language recognition for deaf community. This study aimed at improving existing lip reading techniques by introducing two novel architectures - Hierarchical Pyramidal Convolution (HPC) and Self Attention mechanism that can model temporal dynamics present in speech signals better than current state-of-art methods. By training these models on large datasets of lip images along with corresponding audio transcriptions we were able to achieve significant improvements in accuracy over baseline methods. Additionally, our model was capable of performing well across different languages providing evidence that it learnt meaningful representations of human mouth shapes and movements rather than just memorizing the dataset.",1
"Given a signed social graph, how can we learn appropriate node representations to infer the signs of missing edges? Signed social graphs have received considerable attention to model trust relationships. Learning node representations is crucial to effectively analyze graph data, and various techniques such as network embedding and graph convolutional network (GCN) have been proposed for learning signed graphs. However, traditional network embedding methods are not end-to-end for a specific task such as link sign prediction, and GCN-based methods suffer from a performance degradation problem when their depth increases. In this paper, we propose Signed Graph Diffusion Network (SGDNet), a novel graph neural network that achieves end-to-end node representation learning for link sign prediction in signed social graphs. We propose a random walk technique specially designed for signed graphs so that SGDNet effectively diffuses hidden node features. Through extensive experiments, we demonstrate that SGDNet outperforms state-of-the-art models in terms of link sign prediction accuracy.",0
"In this work we present signed graph diffusion network (SGDN), a mathematical framework that extends the standard graph diffusion networks (GDN) by incorporating edge signs into the modeling process. By doing so, SGDN allows us to analyze both positive and negative relationships among nodes in complex systems such as social media platforms and online communities. We showcase the benefits of using SGDN through two case studies: predicting user engagement on Instagram and detecting hate speech on Twitter. Results demonstrate the effectiveness of our method in capturing intricate patterns in these real world datasets and outperform state-of-the-art models. Our findings provide insights into understanding how different types of interactions shape human behavior in digital environments. Furthermore, we discuss possible future directions for applying SGDN in other domains and improving its accuracy. Overall, this research contributes towards developing more advanced tools for studying and analyzing large scale datasets in todayâ€™s interconnected society.",1
"Fine-grained 3D shape classification is important for shape understanding and analysis, which poses a challenging research problem. However, the studies on the fine-grained 3D shape classification have rarely been explored, due to the lack of fine-grained 3D shape benchmarks. To address this issue, we first introduce a new 3D shape dataset (named FG3D dataset) with fine-grained class labels, which consists of three categories including airplane, car and chair. Each category consists of several subcategories at a fine-grained level. According to our experiments under this fine-grained dataset, we find that state-of-the-art methods are significantly limited by the small variance among subcategories in the same category. To resolve this problem, we further propose a novel fine-grained 3D shape classification method named FG3D-Net to capture the fine-grained local details of 3D shapes from multiple rendered views. Specifically, we first train a Region Proposal Network (RPN) to detect the generally semantic parts inside multiple views under the benchmark of generally semantic part detection. Then, we design a hierarchical part-view attention aggregation module to learn a global shape representation by aggregating generally semantic part features, which preserves the local details of 3D shapes. The part-view attention module hierarchically leverages part-level and view-level attention to increase the discriminability of our features. The part-level attention highlights the important parts in each view while the view-level attention highlights the discriminative views among all the views of the same object. In addition, we integrate a Recurrent Neural Network (RNN) to capture the spatial relationships among sequential views from different viewpoints. Our results under the fine-grained 3D shape dataset show that our method outperforms other state-of-the-art methods.",0
"Incorporate terms like fine grained shape classification and part view attentions as well as any key findings from the research. -----  Advancements in deep learning have made significant improvements in computer vision tasks such as image classification, object detection, and semantic segmentation. However, these methods often struggle with more complex problems like fine-grained 3D shape classification which requires identification and localization of specific parts within the 3D model of an object. This work presents a novel hierarchical part-view attention (HPVA) mechanism that can effectively capture spatial and contextual relationships among different views of 3D shapes. Our approach utilizes self-attention mechanisms at both global and local levels, enabling our network to focus on discriminative regions for better classification performance. Experimental results demonstrate the effectiveness of HPVA by achieving state-of-the-art performance on several benchmark datasets for fine-grained 3D shape classification. These results validate the importance of incorporating contextual information into existing approaches for improved accuracy. Additionally, we showcase the generalizability of our method across multiple datasets and its robustness against variations in input data quality. This work represents a step forward in addressing challenges associated with fine-grained 3D shape classification, opening new opportunities for applications in diverse fields including autonomous robots and augmented reality.",1
"Recently, much attention has been spent on neural architecture search (NAS) approaches, which often outperform manually designed architectures on highlevel vision tasks. Inspired by this, we attempt to leverage NAS technique to automatically design efficient network architectures for low-level image restoration tasks. In this paper, we propose a memory-efficient hierarchical NAS HiNAS (HiNAS) and apply to two such tasks: image denoising and image super-resolution. HiNAS adopts gradient based search strategies and builds an flexible hierarchical search space, including inner search space and outer search space, which in charge of designing cell architectures and deciding cell widths, respectively. For inner search space, we propose layerwise architecture sharing strategy (LWAS), resulting in more flexible architectures and better performance. For outer search space, we propose cell sharing strategy to save memory, and considerably accelerate the search speed. The proposed HiNAS is both memory and computation efficient. With a single GTX1080Ti GPU, it takes only about 1 hour for searching for denoising network on BSD 500 and 3.5 hours for searching for the super-resolution structure on DIV2K. Experimental results show that the architectures found by HiNAS have fewer parameters and enjoy a faster inference speed, while achieving highly competitive performance compared with state-of-the-art methods.",0
"In recent years, neural architecture search (NAS) has emerged as a powerful tool for designing efficient models that achieve state-of-the-art performance on challenging tasks. However, existing NAS methods face several limitations due to their memory requirements, computational complexity, and limited scalability, which hinder their applicability to large datasets and problems such as image restoration. To address these issues, we propose a new approach called Memory-Efficient Hierarchical Neural Architecture Search (MEHNAS), which integrates progressive model growth, hierarchical search spaces, and regularization techniques to reduce the memory footprint and improve the search efficiency while maintaining high accuracy. Our method can discover high-performing models for image restoration using significantly less computational resources than previous approaches, making it more accessible to researchers and practitioners working on real-world applications. Experimental results demonstrate that MEHNAS achieves competitive performance on several benchmarks compared to other NAS methods while requiring fewer GPU hours, thereby paving the way towards realizing the full potential of NAS in image restoration and beyond.",1
"The audio-video based emotion recognition aims to classify a given video into basic emotions. In this paper, we describe our approaches in EmotiW 2019, which mainly explores emotion features and feature fusion strategies for audio and visual modality. For emotion features, we explore audio feature with both speech-spectrogram and Log Mel-spectrogram and evaluate several facial features with different CNN models and different emotion pretrained strategies. For fusion strategies, we explore intra-modal and cross-modal fusion methods, such as designing attention mechanisms to highlights important emotion feature, exploring feature concatenation and factorized bilinear pooling (FBP) for cross-modal feature fusion. With careful evaluation, we obtain 65.5% on the AFEW validation set and 62.48% on the test set and rank third in the challenge.",0
"This paper presents an exploration into the use of audio-video features for emotion recognition in multimedia data. The authors investigate various feature extraction techniques that capture emotional cues from both audio and video signals, including acoustic and visual features. They also examine different fusion strategies for combining these features to improve the accuracy of emotion detection. The experimental results demonstrate the effectiveness of some features over others in capturing emotions, as well as highlighting the importance of selecting appropriate fusion methods based on specific application requirements. Overall, the findings contribute towards advancing the understanding of how to effectively integrate audio-visual information for improved emotion recognition tasks in multimedia systems.",1
"Adversarial examples have gained tons of attention in recent years. Many adversarial attacks have been proposed to attack image classifiers, but few work shift attention to object detectors. In this paper, we propose Sparse Adversarial Attack (SAA) which enables adversaries to perform effective evasion attack on detectors with bounded \emph{l$_{0}$} norm perturbation. We select the fragile position of the image and designed evasion loss function for the task. Experiment results on YOLOv4 and FasterRCNN reveal the effectiveness of our method. In addition, our SAA shows great transferability across different detectors in the black-box attack setting. Codes are available at \emph{https://github.com/THUrssq/Tianchi04}.",0
"In recent years, deep learning methods have achieved significant success in object detection tasks. However, these models remain vulnerable to adversarial attacks that can manipulate their outputs by introducing small perturbations into input images. This work presents a new technique called ""sparse adversarial attack"" (SAA) which achieves high levels of effectiveness while remaining efficient and flexible. Our approach leverages sparse perturbation vectors to create adversarial examples that challenge state-of-the-art detectors even under strong defenses. We evaluate our method on several popular datasets using a range of evaluation metrics, demonstrating its ability to significantly reduce detection accuracy. These results highlight both the strength of our proposed method and the need for continued research into effective defense mechanisms against adversarial attacks. Overall, we believe our findings contribute valuable insights towards building more robust and resilient machine learning systems.",1
"In our daily life, the scenes around us are always with multiple labels especially in a smart city, i.e., recognizing the information of city operation to response and control. Great efforts have been made by using Deep Neural Networks to recognize multi-label images. Since multi-label image classification is very complicated, people seek to use the attention mechanism to guide the classification process. However, conventional attention-based methods always analyzed images directly and aggressively. It is difficult for them to well understand complicated scenes. In this paper, we propose a global/local attention method that can recognize an image from coarse to fine by mimicking how human-beings observe images. Specifically, our global/local attention method first concentrates on the whole image, and then focuses on local specific objects in the image. We also propose a joint max-margin objective function, which enforces that the minimum score of positive labels should be larger than the maximum score of negative labels horizontally and vertically. This function can further improve our multi-label image classification method. We evaluate the effectiveness of our method on two popular multi-label image datasets (i.e., Pascal VOC and MS-COCO). Our experimental results show that our method outperforms state-of-the-art methods.",0
"This paper presents a novel multi-label image classification approach that combines global and local attention mechanisms to predict labels at different scales. We propose two variants of our model - one using self-attention and another using cross-attention, both utilizing convolutional neural networks (CNNs). Our models achieve state-of-the-art results on several challenging benchmark datasets, outperforming previous approaches by a significant margin. In addition, we demonstrate that our method can effectively handle images with varying levels of complexity while still maintaining high accuracy. Overall, our work advances the field of multi-label image classification through a unique combination of global and local attention mechanisms, setting a new standard for performance across multiple datasets.",1
"Adversarial Training (AT) with Projected Gradient Descent (PGD) is an effective approach for improving the robustness of the deep neural networks. However, PGD AT has been shown to suffer from two main limitations: i) high computational cost, and ii) extreme overfitting during training that leads to reduction in model generalization. While the effect of factors such as model capacity and scale of training data on adversarial robustness have been extensively studied, little attention has been paid to the effect of a very important parameter in every network optimization on adversarial robustness: the learning rate. In particular, we hypothesize that effective learning rate scheduling during adversarial training can significantly reduce the overfitting issue, to a degree where one does not even need to adversarially train a model from scratch but can instead simply adversarially fine-tune a pre-trained model. Motivated by this hypothesis, we propose a simple yet very effective adversarial fine-tuning approach based on a $\textit{slow start, fast decay}$ learning rate scheduling strategy which not only significantly decreases computational cost required, but also greatly improves the accuracy and robustness of a deep neural network. Experimental results show that the proposed adversarial fine-tuning approach outperforms the state-of-the-art methods on CIFAR-10, CIFAR-100 and ImageNet datasets in both test accuracy and the robustness, while reducing the computational cost by 8-10$\times$. Furthermore, a very important benefit of the proposed adversarial fine-tuning approach is that it enables the ability to improve the robustness of any pre-trained deep neural network without needing to train the model from scratch, which to the best of the authors' knowledge has not been previously demonstrated in research literature.",0
"This paper explores the use of adversarial fine-tuning as a method for improving the robustness of deep learning models. By using adversarial examples, we can train our models to better handle input that might fool them during inference time. We show through experiments on several benchmark datasets that our approach outperforms prior work while requiring less computational resources. Additionally, we demonstrate how our method extends well to other domains such as computer vision and NLP. Ultimately, by implementing adversarial fine-tuning into your workflow, you too can experience improved model performance without sacrificing efficiency.",1
"We study reinforcement learning (RL) for text-based games, which are interactive simulations in the context of natural language. While different methods have been developed to represent the environment information and language actions, existing RL agents are not empowered with any reasoning capabilities to deal with textual games. In this work, we aim to conduct explicit reasoning with knowledge graphs for decision making, so that the actions of an agent are generated and supported by an interpretable inference procedure. We propose a stacked hierarchical attention mechanism to construct an explicit representation of the reasoning process by exploiting the structure of the knowledge graph. We extensively evaluate our method on a number of man-made benchmark games, and the experimental results demonstrate that our method performs better than existing text-based agents.",0
"In recent years, deep reinforcement learning has been applied to text-based games such as RPGs, interactive fiction, and chatbots, achieving promising results. To further improve the performance of deep reinforcement learning agents on these games, we propose using stacked hierarchical attention networks (HAN). HANs allow the agent to focus on different levels of abstraction when solving the game problem, from low-level details like individual keywords to high-level features like story plots. Experiments show that our approach outperforms state-of-the-art methods across several benchmark tasks and achieves new high scores in two popular text-based games: ""Adventure"" and ""Ghostwriter."" Our work demonstrates the effectiveness of incorporating hierarchy into deep neural network architectures for enhancing their ability to solve complex sequential decision problems found in textual domains.",1
"Arbitrary text appearance poses a great challenge in scene text recognition tasks. Existing works mostly handle with the problem in consideration of the shape distortion, including perspective distortions, line curvature or other style variations. Therefore, methods based on spatial transformers are extensively studied. However, chromatic difficulties in complex scenes have not been paid much attention on. In this work, we introduce a new learnable geometric-unrelated module, the Structure-Preserving Inner Offset Network (SPIN), which allows the color manipulation of source data within the network. This differentiable module can be inserted before any recognition architecture to ease the downstream tasks, giving neural networks the ability to actively transform input intensity rather than the existing spatial rectification. It can also serve as a complementary module to known spatial transformations and work in both independent and collaborative ways with them. Extensive experiments show that the use of SPIN results in a significant improvement on multiple text recognition benchmarks compared to the state-of-the-arts.",0
"Scene text recognition has been an active research area recently due to its applications such as image captioning and machine translation. Recent works have achieved significant improvements by using convolutional neural networks (CNNs) that take advantage of large datasets, high computational power, and advanced techniques like spatial transformers. However, these methods require extensive computation during both training and inference. In this work we propose a structure-preserving inner offset network which improves performance on scene text recognition tasks compared to existing state-of-the art models while greatly reducing parameter counts and increasing efficiency. Our proposed model achieves comparable results to previous best performing models but reduces the number of parameters by over 98% and improves efficiency significantly without sacrificing accuracy. We evaluate our method using three benchmark datasets commonly used for evaluating scene text recognition systems: ICDAR2013, ICDAR2015LSVT, and CUTE800. Results show improved performance across all datasets comparing favorably to previous top performers. These results demonstrate the effectiveness of our approach in advancing current SOTA methods through careful consideration of design choices for efficient deployment of deep learning models.",1
"In this work, we propose a novel neural network focusing on semantic labeling of ALS point clouds, which investigates the importance of long-range spatial and channel-wise relations and is termed as global relation-aware attentional network (GraNet). GraNet first learns local geometric description and local dependencies using a local spatial discrepancy attention convolution module (LoSDA). In LoSDA, the orientation information, spatial distribution, and elevation differences are fully considered by stacking several local spatial geometric learning modules and the local dependencies are embedded by using an attention pooling module. Then, a global relation-aware attention module (GRA), consisting of a spatial relation-aware attention module (SRA) and a channel relation aware attention module (CRA), are investigated to further learn the global spatial and channel-wise relationship between any spatial positions and feature vectors. The aforementioned two important modules are embedded in the multi-scale network architecture to further consider scale changes in large urban areas. We conducted comprehensive experiments on two ALS point cloud datasets to evaluate the performance of our proposed framework. The results show that our method can achieve higher classification accuracy compared with other commonly used advanced classification methods. The overall accuracy (OA) of our method on the ISPRS benchmark dataset can be improved to 84.5% to classify nine semantic classes, with an average F1 measure (AvgF1) of 73.5%. In detail, we have following F1 values for each object class: powerlines: 66.3%, low vegetation: 82.8%, impervious surface: 91.8%, car: 80.7%, fence: 51.2%, roof: 94.6%, facades: 62.1%, shrub: 49.9%, trees: 82.1%. Besides, experiments were conducted using a new ALS point cloud dataset covering highly dense urban areas.",0
"Abstract: This paper presents GraNet, which stands for Global Relation-aware Attentional Network. We show how it can effectively classify point clouds obtained from ALS sensors. In contrast to many previous methods, our approach pays close attention to relations across different regions, enabling better global context awareness. Our experiments on real data demonstrate that GraNet outperforms existing state-of-the-art techniques. Additionally, we carefully describe our system architecture, training procedure, and evaluation metrics. Overall, our work provides important insights into how attending to global context can lead to improved accuracy in ALS point cloud classification.",1
"The multi-armed bandits' framework is the most common platform to study strategies for sequential decision-making problems. Recently, the notion of fairness has attracted a lot of attention in the machine learning community. One can impose the fairness condition that at any given point of time, even during the learning phase, a poorly performing candidate should not be preferred over a better candidate. This fairness constraint is known to be one of the most stringent and has been studied in the stochastic multi-armed bandits' framework in a stationary setting for which regret bounds have been established. The main aim of this paper is to study this problem in a non-stationary setting. We present a new algorithm called Fair Upper Confidence Bound with Exploration Fair-UCBe algorithm for solving a slowly varying stochastic $k$-armed bandit problem. With this we present two results: (i) Fair-UCBe indeed satisfies the above mentioned fairness condition, and (ii) it achieves a regret bound of $O\left(k^{\frac{3}{2}} T^{1 - \frac{\alpha}{2}} \sqrt{\log T}\right)$, for some suitable $\alpha \in (0, 1)$, where $T$ is the time horizon. This is the first fair algorithm with a sublinear regret bound applicable to non-stationary bandits to the best of our knowledge. We show that the performance of our algorithm in the non-stationary case approaches that of its stationary counterpart as the variation in the environment tends to zero.",0
"Title: ""Regret Bounds for Non-Stationary MABs with Fairness Constraints""  Abstract: The multi-armed bandit (MAB) problem has been extensively studied in machine learning as a model for making decisions under uncertainty. In recent years, there has been growing interest in non-stationary variants of the MAB problem where the distribution over arms changes dynamically. This work investigates regret bounds for non-stationary MAB algorithms that incorporate fairness constraints into their decision-making process. We provide novel theoretical results that demonstrate how fairness considerations can affect the performance of MAB algorithms in changing environments. Our analysis shows that enforcing fairness constraints leads to lower regret compared to unfair algorithms while still maintaining strong overall performance. These findings have important implications for applications such as online advertising and recommender systems where balancing exploration and exploitation with equity concerns presents a significant challenge.",1
"Evaluating neurological disorders such as Parkinson's disease (PD) is a challenging task that requires the assessment of several motor and non-motor functions. In this paper, we present an end-to-end deep learning framework to measure PD severity in two important components, hand movement and gait, of the Unified Parkinson's Disease Rating Scale (UPDRS). Our method leverages on an Inflated 3D CNN trained by a temporal segment framework to learn spatial and long temporal structure in video data. We also deploy a temporal attention mechanism to boost the performance of our model. Further, motion boundaries are explored as an extra input modality to assist in obfuscating the effects of camera motion for better movement assessment. We ablate the effects of different data modalities on the accuracy of the proposed network and compare with other popular architectures. We evaluate our proposed method on a dataset of 25 PD patients, obtaining 72.3% and 77.1% top-1 accuracy on hand movement and gait tasks respectively.",0
"In recent years, there has been increasing interest in using computer vision techniques to assist with the diagnosis and assessment of neurological disorders such as Parkinson's disease. One promising approach to this problem is through the use of end-to-end neural networks that can process raw video data and extract relevant features automatically. This study investigates motion boundaries as an important indicator for detecting changes in human movement patterns associated with Parkinson's severity. We present an exploration of motion boundary detection methods within an end-to-end network architecture specifically designed for vision-based assessment tasks. Our results show that incorporating motion boundaries into our model improves performance on Parkinson's severity classification compared to previous state-of-the-art approaches. Additionally, we provide insights into the importance of properly defining motion boundaries and their impact on network outputs. These findings demonstrate the potential value of motion boundaries as a tool for enhancing accuracy and interpretability in deep learning models used for medical imaging analysis.",1
"Person re-identification has received a lot of attention from the research community in recent times. Due to its vital role in security based applications, person re-identification lies at the heart of research relevant to tracking robberies, preventing terrorist attacks and other security critical events. While the last decade has seen tremendous growth in re-id approaches, very little review literature exists to comprehend and summarize this progress. This review deals with the latest state-of-the-art deep learning based approaches for person re-identification. While the few existing re-id review works have analysed re-id techniques from a singular aspect, this review evaluates numerous re-id techniques from multiple deep learning aspects such as deep architecture types, common Re-Id challenges (variation in pose, lightning, view, scale, partial or complete occlusion, background clutter), multi-modal Re-Id, cross-domain Re-Id challenges, metric learning approaches and video Re-Id contributions. This review also includes several re-id benchmarks collected over the years, describing their characteristics, specifications and top re-id results obtained on them. The inclusion of the latest deep re-id works makes this a significant contribution to the re-id literature. Lastly, the conclusion and future directions are included.",0
"This systematic review focuses on recent advances in person re-identification (Re-ID) systems that use deep learning networks as their core technology. With the rapid development of computer vision algorithms over the last decade, particularly deep convolutional neural networks (CNN), the performance of Re-ID systems has seen significant improvement. In order to provide readers with an updated understanding of how these models work, this review compares several state-of-the-art methods including traditional feature extraction techniques as well as those based on CNN architectures. By evaluating each modelâ€™s advantages and disadvantages, we aim to identify any potential limitations in current research directions while suggesting promising future improvements in this field. Additionally, we analyze existing evaluation protocols used in past studies and propose alternative frameworks that could better assess Re-ID performance under real-world conditions. Our ultimate goal is to enhance public safety by providing law enforcement agencies with advanced technologies capable of accurately identifying individuals from CCTV footage in complex scenarios.  Notice the phrase ""Person Re-Identification"" which you suggested!",1
"In this paper, we investigate the cause of the high false positive rate in Visual Relationship Detection (VRD). We observe that during training, the relationship proposal distribution is highly imbalanced: most of the negative relationship proposals are easy to identify, e.g., the inaccurate object detection, which leads to the under-fitting of low-frequency difficult proposals. This paper presents Spatially-Aware Balanced negative pRoposal sAmpling (SABRA), a robust VRD framework that alleviates the influence of false positives. To effectively optimize the model under imbalanced distribution, SABRA adopts Balanced Negative Proposal Sampling (BNPS) strategy for mini-batch sampling. BNPS divides proposals into 5 well defined sub-classes and generates a balanced training distribution according to the inverse frequency. BNPS gives an easier optimization landscape and significantly reduces the number of false positives. To further resolve the low-frequency challenging false positive proposals with high spatial ambiguity, we improve the spatial modeling ability of SABRA on two aspects: a simple and efficient multi-head heterogeneous graph attention network (MH-GAT) that models the global spatial interactions of objects, and a spatial mask decoder that learns the local spatial configuration. SABRA outperforms SOTA methods by a large margin on two human-object interaction (HOI) datasets and one general VRD dataset.",0
"This paper presents research on reducing false positives in visual relationship detection systems by improving their reliance on contextual information from surrounding sentences. Through experimentation with multiple models trained using different input sources and techniques for preprocessing the data, we identify specific strategies that can significantly decrease the frequency of incorrect predictions while maintaining overall accuracy rates at acceptable levels. Our findings have potential implications for a wide range of natural language processing applications where accurate identification of relationships is essential, such as sentiment analysis, question answering, and machine translation. Additionally, our work contributes to the broader understanding of how computers process textual data and make sense of complex phenomena in human communication. By shedding light on the ways in which contextual information influences model performance, our study highlights promising directions for future research in deep learning and NLP technologies.",1
"Social networks give free access to their services in exchange for the right to exploit their users' data. Data sharing is done in an initial context which is chosen by the users. However, data are used by social networks and third parties in different contexts which are often not transparent. We propose a new approach which unveils potential effects of data sharing in impactful real-life situations. Focus is put on visual content because of its strong influence in shaping online user profiles. The approach relies on three components: (1) a set of concepts with associated situation impact ratings obtained by crowdsourcing, (2) a corresponding set of object detectors used to analyze users' photos and (3) a ground truth dataset made of 500 visual user profiles which are manually rated for each situation. These components are combined in LERVUP, a method which learns to rate visual user profiles in each situation. LERVUP exploits a new image descriptor which aggregates concept ratings and object detections at user level. It also uses an attention mechanism to boost the detections of highly-rated concepts to prevent them from being overwhelmed by low-rated ones. Performance is evaluated per situation by measuring the correlation between the automatic ranking of profile ratings and a manual ground truth. Results indicate that LERVUP is effective since a strong correlation of the two rankings is obtained. This finding indicates that providing meaningful automatic situation-related feedback about the effects of data sharing is feasible.",0
"This paper examines how online photo sharing platforms affect real-life experiences. Previous research on social media use has emphasized their negative effects, such as increased feelings of isolation and decreased offline interaction among users. However, recent studies have suggested that these platforms may facilitate greater interpersonal connection by providing opportunities for community building and support seeking. To explore this contradiction further, we conducted surveys with 249 online photo sharers from three different countries (the US, India, and Australia) who ranged in age from 18â€“64 years old. Our findings indicate that while online photo sharing can enhance positive aspects of individualsâ€™ lives, such as promoting personal expression and creativity, it can simultaneously lead to undesirable outcomes like exclusionary behavior, jealousy, and cyberbullying. In conclusion, our study highlights the complex relationship between online photo sharing and real-life interactions, suggesting that future research should consider both positive and negative aspects of using these platforms to fully unpack their impact on everyday life.",1
"We perform the classification of ancient Roman Republican coins via recognizing their reverse motifs where various objects, faces, scenes, animals, and buildings are minted along with legends. Most of these coins are eroded due to their age and varying degrees of preservation, thereby affecting their informative attributes for visual recognition. Changes in the positions of principal symbols on the reverse motifs also cause huge variations among the coin types. Lastly, in-plane orientations, uneven illumination, and a moderate background clutter further make the classification task non-trivial and challenging.   To this end, we present a novel network model, CoinNet, that employs compact bilinear pooling, residual groups, and feature attention layers. Furthermore, we gathered the largest and most diverse image dataset of the Roman Republican coins that contains more than 18,000 images belonging to 228 different reverse motifs. On this dataset, our model achieves a classification accuracy of more than \textbf{98\%} and outperforms the conventional bag-of-visual-words based approaches and more recent state-of-the-art deep learning methods. We also provide a detailed ablation study of our network and its generalization capability. Models and Datasets available at https://github.com/saeed-anwar/CoinNet",0
"This should be a general audience style document. Please use no more than 2 academic citations and aim at a high school education level readership. Title: ""Deep Learning Methods Improve Ancient Roman Republican Coin Classification""  Abstract: Ancient Roman coins hold significant historical value, but classifying them can prove challenging due to their variety and similarities. Recent advancements in deep learning methods have shown promise in solving these problems by improving accuracy and efficiency in coin classification tasks. In this work, we present a novel approach using feature fusion and attention mechanisms that effectively combines different features extracted from images of ancient Roman republican coins. We evaluate our method on a large dataset of ancient Roman coins and demonstrate significantly improved performance compared to traditional machine learning algorithms and other state-of-the-art deep learning approaches. Our results highlight the potential of deep learning techniques for enhancing studies related to art history, archaeology, and numismatics.",1
"Panoptic segmentation is a scene parsing task which unifies semantic segmentation and instance segmentation into one single task. However, the current state-of-the-art studies did not take too much concern on inference time. In this work, we propose an Efficient Panoptic Segmentation Network (EPSNet) to tackle the panoptic segmentation tasks with fast inference speed. Basically, EPSNet generates masks based on simple linear combination of prototype masks and mask coefficients. The light-weight network branches for instance segmentation and semantic segmentation only need to predict mask coefficients and produce masks with the shared prototypes predicted by prototype network branch. Furthermore, to enhance the quality of shared prototypes, we adopt a module called ""cross-layer attention fusion module"", which aggregates the multi-scale features with attention mechanism helping them capture the long-range dependencies between each other. To validate the proposed work, we have conducted various experiments on the challenging COCO panoptic dataset, which achieve highly promising performance with significantly faster inference speed (53ms on GPU).",0
"An important task in computer vision is segmenting objects from images into distinct regions. Recent work has explored using deep learning algorithms known as ""fully convolutional networks"" (FCNs) for panoptic segmentation - a variant of image segmentation that combines semantic object detection with instance segregation, resulting in a single coherent pixel label for each scene element. However, existing FCN architectures have limitations in terms of efficiency and accuracy, particularly when dealing with complex scenes containing many different types of objects.  To address these issues, we propose a new architecture called EPSNet which stands for ""Efficient Panoptic Segmentation Network"". Our approach uses cross-layer attention fusion to combine features from multiple layers within the network, enabling better representation of fine details while maintaining high computational efficiency. We further improve upon state-of-the-art methods by integrating multi-scale feature representations obtained via dilated feature pyramid modules (FPMs). This allows for more efficient use of contextual information across scales and reduces uncertainty caused due to varying sizes of input images.  Our experiments on popular benchmark datasets demonstrate that our proposed model outperforms state-of-the-art approaches by significant margins, including recent advancements such as UPSNet and DANet, in both panoptic quality metrics. Further ablation studies verify the effectiveness of the components used in our framework. The code and models will be made publicly available for research purposes. \end{code}",1
"Multi-agent reinforcement learning (MARL) has been increasingly explored to learn the cooperative policy towards maximizing a certain global reward. Many existing studies take advantage of graph neural networks (GNN) in MARL to propagate critical collaborative information over the interaction graph, built upon inter-connected agents. Nevertheless, the vanilla GNN approach yields substantial defects in dealing with complex real-world scenarios since the generic message passing mechanism is ineffective between heterogeneous vertices and, moreover, simple message aggregation functions are incapable of accurately modeling the combinational interactions from multiple neighbors. While adopting complex GNN models with more informative message passing and aggregation mechanisms can obviously benefit heterogeneous vertex representations and cooperative policy learning, it could, on the other hand, increase the training difficulty of MARL and demand more intense and direct reward signals compared to the original global reward. To address these challenges, we propose a new cooperative learning framework with pre-trained heterogeneous observation representations. Particularly, we employ an encoder-decoder based graph attention to learn the intricate interactions and heterogeneous representations that can be more easily leveraged by MARL. Moreover, we design a pre-training with local actor-critic algorithm to ease the difficulty in cooperative policy learning. Extensive experiments over real-world scenarios demonstrate that our new approach can significantly outperform existing MARL baselines as well as operational research solutions that are widely-used in industry.",0
"This abstract describes the work done in the paper ""Cooperative Policy Learning with Pre-trained Heterogeneous Observation Representations,"" which proposes a novel method for training agents cooperatively in reinforcement learning tasks using pre-trained observation encoders from different domains. The proposed approach enables agents to learn policies more effectively by transferring knowledge gained through pre-training on diverse environments to new ones during joint policy optimization. Our experiments demonstrate significant improvements over baseline methods in terms of both task performance and sample efficiency, particularly for challenging problems involving high-dimensional observations and multiple modalities such as vision and language. Overall, our results suggest that integrating pre-trained observation representations into deep RL can enhance the generalization abilities of agents across complex domains and increase their capacity to solve real-world decision making problems.",1
"A smart vehicle should be able to understand human behavior and predict their actions to avoid hazardous situations. Specific traits in human behavior can be automatically predicted, which can help the vehicle make decisions, increasing safety. One of the most important aspects pertaining to the driving task is the driver's visual attention. Predicting the driver's visual attention can help a vehicle understand the awareness state of the driver, providing important contextual information. While estimating the exact gaze direction is difficult in the car environment, a coarse estimation of the visual attention can be obtained by tracking the position and orientation of the head. Since the relation between head pose and gaze direction is not one-to-one, this paper proposes a formulation based on probabilistic models to create salient regions describing the visual attention of the driver. The area of the predicted region is small when the model has high confidence on the prediction, which is directly learned from the data. We use Gaussian process regression (GPR) to implement the framework, comparing the performance with different regression formulations such as linear regression and neural network based methods. We evaluate these frameworks by studying the tradeoff between spatial resolution and accuracy of the probability map using naturalistic recordings collected with the UTDrive platform. We observe that the GPR method produces the best result creating accurate predictions with localized salient regions. For example, the 95% confidence region is defined by an area that covers 3.77% region of a sphere surrounding the driver.",0
"Abstract: In this paper, we present a method for estimating the gaze region of drivers based on their head position and orientation. We use probabilistic confidence regions (PCRs) to model the uncertainty in driver gaze direction given by a probabilistic sensor model. This allows us to generate more accurate estimates of the gaze region compared to previous methods that used fixed error bounds. Our approach uses multiple PCRs generated at different levels of abstraction from low-level sensory data to high-level contextual information such as driving scenarios and road signs. We evaluate our method through experiments conducted on real-world driving datasets and show that it achieves better performance than existing gaze estimation approaches in terms of accuracy and robustness. Furthermore, the proposed method can provide valuable insights into driver behavior and can support advanced driver assistance systems (ADAS) in safety-critical situations.",1
"In this work, we study the problem of word-level confidence calibration for scene-text recognition (STR). Although the topic of confidence calibration has been an active research area for the last several decades, the case of structured and sequence prediction calibration has been scarcely explored. We analyze several recent STR methods and show that they are consistently overconfident. We then focus on the calibration of STR models on the word rather than the character level. In particular, we demonstrate that for attention based decoders, calibration of individual character predictions increases word-level calibration error compared to an uncalibrated model. In addition, we apply existing calibration methodologies as well as new sequence-based extensions to numerous STR models, demonstrating reduced calibration error by up to a factor of nearly 7. Finally, we show consistently improved accuracy results by applying our proposed sequence calibration method as a preprocessing step to beam-search.",0
"Scene text recognition models have shown promising results in recent years, but their performance can vary significantly depending on factors such as image quality, scene context, and text format. In this paper, we present a comprehensive study on the calibration of scene-text recognition models. Our investigation reveals that current state-of-the-art methods suffer from poor generalization across different domains, leading to degraded accuracy and robustness. To address these challenges, we propose several novel techniques based on data augmentation and model adaptation, which effectively improve the performance of scene-text recognition models under real-world conditions. Experimental evaluations using popular benchmark datasets demonstrate the effectiveness of our proposed approach compared to prior arts. Our findings highlight the importance of careful evaluation and calibration of scene-text recognition systems, particularly when deploying them for practical applications where accurate and reliable OCR is critical.",1
"Robustness verification that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding model behavior and obtaining safety guarantees. However, previous methods can usually only handle neural networks with relatively simple architectures. In this paper, we consider the robustness verification problem for Transformers. Transformers have complex self-attention layers that pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. We resolve these challenges and develop the first robustness verification algorithm for Transformers. The certified robustness bounds computed by our method are significantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reflect the importance of different words in sentiment analysis.",0
"One approach that has gained widespread adoption in natural language processing (NLP) is using transformer architectures, such as BERT, GPT-2, and RoBERTa, which have achieved state-of-the-art results on many NLP tasks. However, these models can suffer from instability during training, resulting in poor performance and slower convergence rates. This paper proposes a new methodology for verifying robustness in transformer architectures by introducing regularization techniques based on smoothness assumptions and uncertainty estimation. Our method improves stability in training and results in more reliable predictions across different runs and random initializations. We evaluate our method on multiple benchmark datasets, demonstrating improved robustness and consistency in model performance compared to existing methods. Overall, our research contributes to the broader goal of building more stable, reliable, and trustworthy deep learning systems.",1
"Despite the recent advances in optical character recognition (OCR), mathematical expressions still face a great challenge to recognize due to their two-dimensional graphical layout. In this paper, we propose a convolutional sequence modeling network, ConvMath, which converts the mathematical expression description in an image into a LaTeX sequence in an end-to-end way. The network combines an image encoder for feature extraction and a convolutional decoder for sequence generation. Compared with other Long Short Term Memory(LSTM) based encoder-decoder models, ConvMath is entirely based on convolution, thus it is easy to perform parallel computation. Besides, the network adopts multi-layer attention mechanism in the decoder, which allows the model to align output symbols with source feature vectors automatically, and alleviates the problem of lacking coverage while training the model. The performance of ConvMath is evaluated on an open dataset named IM2LATEX-100K, including 103556 samples. The experimental results demonstrate that the proposed network achieves state-of-the-art accuracy and much better efficiency than previous methods.",0
"This paper presents a novel approach to mathematical expression recognition using convolutional sequence networks (ConvSeq). While traditional approaches have relied on recurrent neural networks (RNNs) and sequence models like LSTMs or Transformers, we show that ConvSeq offers several advantages over these methods. By processing the input as a 2D grid rather than a sequential stream, ConvSeq can capture spatial relationships between symbols and tokens more efficiently. Furthermore, our model leverages advancements in vision models, such as the ability to attend over different regions of the input, which allows us to accurately recognize expressions even if they span multiple lines. Our experiments demonstrate significant improvements over state-of-the-art RNN-based systems across several benchmark datasets. We believe that this work represents a step forward towards solving challenging problems in mathematics automation and shows the potential of using ConvSeq in other NLP applications involving structured data formats.",1
"Removing rain streaks from single images is an important problem in various computer vision tasks because rain streaks can degrade outdoor images and reduce their visibility. While recent convolutional neural network-based deraining models have succeeded in capturing rain streaks effectively, difficulties in recovering the details in rain-free images still remain. In this paper, we present a multi-level connection and adaptive regional attention network (MARA-Net) to properly restore the original background textures in rainy images. The first main idea is a multi-level connection design that repeatedly connects multi-level features of the encoder network to the decoder network. Multi-level connections encourage the decoding process to use the feature information of all levels. Channel attention is considered in multi-level connections to learn which level of features is important in the decoding process of the current level. The second main idea is a wide regional non-local block (WRNL). As rain streaks primarily exhibit a vertical distribution, we divide the grid of the image into horizontally-wide patches and apply a non-local operation to each region to explore the rich rain-free background information. Experimental results on both synthetic and real-world rainy datasets demonstrate that the proposed model significantly outperforms existing state-of-the-art models. Furthermore, the results of the joint deraining and segmentation experiment prove that our model contributes effectively to other vision tasks.",0
"In recent years, image deraining has become an increasingly important task due to the growing use of cameras in outdoor environments where rain can impede vision. However, existing methods have limited performance due to their reliance on handcrafted features and global processing. To address these limitations, we propose MARA-Net, a novel single image deraining network that utilizes multi-level connections and adaptive regional attentions. Our network captures both local and global contexts by using multiple branches at different scales and feature resolutions, which are then fused together through an attention mechanism. This allows us to effectively model complex interactions between raindrop patterns and image content at varying spatial scales. We evaluate our method on several benchmark datasets and show that it significantly outperforms state-of-the-art deraining methods in terms of visual quality and quantitative metrics. Overall, MARA-Net demonstrates the potential of deep learning models for efficient and effective image deraining.",1
"Textual cues are essential for everyday tasks like buying groceries and using public transport. To develop this assistive technology, we study the TextVQA task, i.e., reasoning about text in images to answer a question. Existing approaches are limited in their use of spatial relations and rely on fully-connected transformer-like architectures to implicitly learn the spatial structure of a scene. In contrast, we propose a novel spatially aware self-attention layer such that each visual entity only looks at neighboring entities defined by a spatial graph. Further, each head in our multi-head self-attention layer focuses on a different subset of relations. Our approach has two advantages: (1) each head considers local context instead of dispersing the attention amongst all visual entities; (2) we avoid learning redundant features. We show that our model improves the absolute accuracy of current state-of-the-art methods on TextVQA by 2.2% overall over an improved baseline, and 4.62% on questions that involve spatial reasoning and can be answered correctly using OCR tokens. Similarly on ST-VQA, we improve the absolute accuracy by 4.2%. We further show that spatially aware self-attention improves visual grounding.",0
"This is a proposed abstract: In ""Spatially Aware Multimodal Transformers for TextVQA,"" we introduce an innovative model that integrates spatial awareness into multimodal transformer architecture to improve performance on textual visual question answering (TextVQA) tasks. Our approach incorporates two key components: a dynamic region proposal module which generates object hypotheses at different scales, and a region-guided fusion strategy which aligns the outputs from the transformer network according to their corresponding image regions. We conduct extensive experiments on three publicly available datasets, demonstrating significant improvements over state-of-the-art methods across all metrics. Additionally, our ablation studies provide insights into the contribution of each component and highlight the importance of integrating both visual attention mechanisms with spatial reasoning. Overall, our work represents a major advancement towards solving real-world VQA problems where robustness and efficiency are crucial factors.",1
"Recently, skeleton-based approaches have achieved rapid progress on the basis of great success in skeleton representation. Plenty of researches focus on solving specific problems according to skeleton features. Some skeleton-based approaches have been mentioned in several overviews on object detection as a non-essential part. Nevertheless, there has not been any thorough analysis of skeleton-based approaches attentively. Instead of describing these techniques in terms of theoretical constructs, we devote to summarizing skeleton-based approaches with regard to application fields and given tasks as comprehensively as possible. This paper is conducive to further understanding of skeleton-based application and dealing with particular issues.",0
"Abstract This survey outlines several approaches to skeleton extraction from images using machine vision techniques. It provides an overview of different methods used for both monocular and stereoscopic imaging systems and discusses their strengths and limitations. The study highlights recent advances in deep learning architectures that have improved the accuracy and speed of skeletonization processes, as well as the development of new algorithms designed to enhance segmentation quality by utilizing multi-modality data fusion. Additionally, the survey presents experimental results obtained by the authors which demonstrate the effectiveness of the proposed methodologies.",1
"The Visual Question Answering (VQA) task combines challenges for processing data with both Visual and Linguistic processing, to answer basic `common sense' questions about given images. Given an image and a question in natural language, the VQA system tries to find the correct answer to it using visual elements of the image and inference gathered from textual questions. In this survey, we cover and discuss the recent datasets released in the VQA domain dealing with various types of question-formats and robustness of the machine-learning models. Next, we discuss about new deep learning models that have shown promising results over the VQA datasets. At the end, we present and discuss some of the results computed by us over the vanilla VQA model, Stacked Attention Network and the VQA Challenge 2017 winner model. We also provide the detailed analysis along with the challenges and future research directions.",0
"Effective management of large amounts of visual data requires efficient solutions that can provide accurate answers to questions posed on images and videos. One approach to address this challenge is through deep learning techniques, specifically those related to visual question answering (VQA). VQA systems aim to provide natural language responses based on visual input by understanding both the image content and textual queries. In recent years, significant advances have been made in developing these models, resulting in improved performance across multiple domains. This survey presents an overview of current methods used in VQA research and their evaluations, as well as discusses future directions in the field. Our analysis includes comparing different model architectures and training strategies, assessing datasets, and examining performance metrics utilized to evaluate VQA systems. By providing a comprehensive evaluation of VQA approaches, we hope to inspire further research into this exciting area of study.",1
"Imagining a colored realistic image from an arbitrarily drawn sketch is one of the human capabilities that we eager machines to mimic. Unlike previous methods that either requires the sketch-image pairs or utilize low-quantity detected edges as sketches, we study the exemplar-based sketch-to-image (s2i) synthesis task in a self-supervised learning manner, eliminating the necessity of the paired sketch data. To this end, we first propose an unsupervised method to efficiently synthesize line-sketches for general RGB-only datasets. With the synthetic paired-data, we then present a self-supervised Auto-Encoder (AE) to decouple the content/style features from sketches and RGB-images, and synthesize images that are both content-faithful to the sketches and style-consistent to the RGB-images. While prior works employ either the cycle-consistence loss or dedicated attentional modules to enforce the content/style fidelity, we show AE's superior performance with pure self-supervisions. To further improve the synthesis quality in high resolution, we also leverage an adversarial network to refine the details of synthetic images. Extensive experiments on 1024*1024 resolution demonstrate a new state-of-art-art performance of the proposed model on CelebA-HQ and Wiki-Art datasets. Moreover, with the proposed sketch generator, the model shows a promising performance on style mixing and style transfer, which require synthesized images to be both style-consistent and semantically meaningful. Our code is available on https://github.com/odegeasslbc/Self-Supervised-Sketch-to-Image-Synthesis-PyTorch, and please visit https://create.playform.io/my-projects?mode=sketch for an online demo of our model.",0
"Recent advances in computer vision have led to significant improvements in image generation tasks through the use of deep learning models. However, one challenge faced by these models is their reliance on large amounts of annotated data to train effectively. To address this issue, self-supervised learning techniques have been developed that allow models to learn from unlabeled data using weak supervision signals. In this work, we propose a novel approach for sketch-to-image synthesis that utilizes self-supervised learning to generate high quality images from simple line drawings. Our method leverages the power of generative adversarial networks (GANs) to perform cross-domain translation from sketches to realistic images while exploiting self-supervisory signal extracted from paired sketch-photo datasets. Experimental results demonstrate the effectiveness of our proposed framework in generating images with fine details and accurate texture comparable to state-of-the-art methods, even outperforming some fully-supervised baselines. The proposed technique paves the way towards creating more efficient models capable of generating visually appealing images without requiring massive amounts of labeled training data.",1
"In this paper, we tackle the problem of online road network extraction from sparse 3D point clouds. Our method is inspired by how an annotator builds a lane graph, by first identifying how many lanes there are and then drawing each one in turn. We develop a hierarchical recurrent network that attends to initial regions of a lane boundary and traces them out completely by outputting a structured polyline. We also propose a novel differentiable loss function that measures the deviation of the edges of the ground truth polylines and their predictions. This is more suitable than distances on vertices, as there exists many ways to draw equivalent polylines. We demonstrate the effectiveness of our method on a 90 km stretch of highway, and show that we can recover the right topology 92\% of the time.",0
"This paper presents a novel model for generating structured online maps using hierarchical recurrent attention networks (HRAN). We propose HRAN as an approach that extends traditional sequence-to-sequence models by incorporating spatial hierarchies into the decoder architecture. Specifically, we use self-attention mechanisms to enable HRANs to selectively attend to different parts of a map based on the input image. Our model generates high-quality maps even without explicit supervision and outperforms previous state-of-the-art methods by significant margins. We demonstrate the effectiveness of our model through quantitative evaluation and visual inspection. Additionally, we show how HRAN can generate maps in real-time and can scale up to large maps beyond single buildings. Overall, our work provides evidence of the potential utility of deep learning approaches towards solving complex computer vision problems and suggests promising new directions for research in the field. Title: ""Hierarchical Recurrent Attention Networks for Generating High-Quality Online Maps"" This paper proposes a novel method for creating detailed, accurate maps from images using Hierarchical Recurrent Attention Networks (HRAN). By leveraging the power of deep learning, specifically self-attention mechanisms within the decoder, HRAN allows for the generation of highly detailed maps at speed, surpassing existing techniques. Results from both qualitative and quantitative evaluations demonstrate the success of our proposed approach; however, future studies could expand upon these findings to explore more generalizable conclusions across other applications. Despite limitations noted herein, HRAN represents an exciting opportunity for exploring potential solutions to challenges in computer vision.",1
"Influencer marketing is being used increasingly as a tool to reach customers because of the growing popularity of social media stars who primarily reach their audience(s) via custom videos. Despite the rapid growth in influencer marketing, there has been little research on the design and effectiveness of influencer videos. Using publicly available data on YouTube influencer videos, we implement novel interpretable deep learning architectures, supported by transfer learning, to identify significant relationships between advertising content in videos (across text, audio, and images) and video views, interaction rates and sentiment. By avoiding ex-ante feature engineering and instead using ex-post interpretation, our approach avoids making a trade-off between interpretability and predictive ability. We filter out relationships that are affected by confounding factors unassociated with an increase in attention to video elements, thus facilitating the generation of plausible causal relationships between video elements and marketing outcomes which can be tested in the field. A key finding is that brand mentions in the first 30 seconds of a video are on average associated with a significant increase in attention to the brand but a significant decrease in sentiment expressed towards the video. We illustrate the learnings from our approach for both influencers and brands.",0
"This paper presents an exploration into video influencer marketing on social media platforms such as YouTube and Instagram. By conducting a comprehensive literature review, this study examines how brands have leveraged digital celebrities known as ""video influencers"" to advertise their products. The research focuses specifically on unboxing videos, which showcase popular influencers opening packages containing new gadgets, beauty products, fashion items, etc., and commentating while they demonstrate how each product works. Throughout the paper, I will argue that these types of sponsored content can both positively impact brand image and lead to negative consequences if not handled ethically by marketers. Therefore, companies must carefully evaluate how they approach working with video influencers to maintain authenticity, build trustworthy relationships with viewers, and ensure compliance with industry regulations. Ultimately, understanding how to effectively utilize unboxing videos can give businesses the competitive edge needed to succeed in todayâ€™s online sphere. Keywords: influencer marketing; video influencer; social media platform; YouTube; Instagram; unboxing video; brand image; ethics; industry regulation.",1
"The identification of nerve is difficult as structures of nerves are challenging to image and to detect in ultrasound images. Nevertheless, the nerve identification in ultrasound images is a crucial step to improve performance of regional anesthesia. In this paper, a network called Brachial Plexus Multi-instance Segmentation Network (BPMSegNet) is proposed to identify different tissues (nerves, arteries, veins, muscles) in ultrasound images. The BPMSegNet has three novel modules. The first is the spatial local contrast feature, which computes contrast features at different scales. The second one is the self-attention gate, which reweighs the channels in feature maps by their importance. The third is the addition of a skip concatenation with transposed convolution within a feature pyramid network. The proposed BPMSegNet is evaluated by conducting experiments on our constructed Ultrasound Brachial Plexus Dataset (UBPD). Quantitative experimental results show the proposed network can segment multiple tissues from the ultrasound images with a good performance.",0
"This would be the output if you were to ask me to write that abstract:  Segmenting the brachial plexus ultrasound image using deep learning methods has become increasingly important as it provides highly accurate results compared to manual segmentation techniques. In this study, we propose a new approach based on multi-instance learning that significantly improves segmentation performance. Our method uses a novel deep neural network architecture called BPMSegNet which exploits both inter-slice and intra-slice relationship features from input images for better performance. Experimental results show significant improvement over existing state-of-the-art approaches achieving higher mean overlap ratio with ground truth by up to 4%. Moreover, our approach is fast, efficient, and requires low computational resources making it suitable for clinical use. The proposed method addresses critical challenges such as poor quality images due to movement artifacts, variable slice thicknesses, different imaging settings, and probe angulations. Thus, our work demonstrates the potential benefits of integrating advanced machine learning models into medical diagnosis tools to improve patient outcomes while reducing human errors.  Brachial plexus ultrasound (US) plays an essential role in surgery planning, neurology evaluations, and pain management [Xu et al., 2017]. Automatic segmentation of brachial plexus US images can reduce manual labor, prevent human error, and increase reproducibility [Raouf Saeed and Sabokrou M., 2016; Zhou et al., 2020; Alshammari et al., 2020]. Although some methods have been developed recently, most suffer from limited performance owing to their reliance solely on single instance perception without considering inter/intra relationships among instances in a batch [Zhou et al., 2020; Xie et al., 2020]. To solve these problems, we adopt MultiInstance Learning (MIL), which enables each mini-batch of examples to be classified as positive or negative depending on whether at least one example is positive or negativ [Tuia et al., 2009]",1
"Although significant progress has been made in synthesizing high-quality and visually realistic face images by unconditional Generative Adversarial Networks (GANs), there still lacks of control over the generation process in order to achieve semantic face editing. In addition, it remains very challenging to maintain other face information untouched while editing the target attributes. In this paper, we propose a novel learning framework, called GuidedStyle, to achieve semantic face editing on StyleGAN by guiding the image generation process with a knowledge network. Furthermore, we allow an attention mechanism in StyleGAN generator to adaptively select a single layer for style manipulation. As a result, our method is able to perform disentangled and controllable edits along various attributes, including smiling, eyeglasses, gender, mustache and hair color. Both qualitative and quantitative results demonstrate the superiority of our method over other competing methods for semantic face editing. Moreover, we show that our model can be also applied to different types of real and artistic face editing, demonstrating strong generalization ability.",0
"In recent years, advancements in deep learning have enabled the creation of highly realistic virtual avatars and other synthetic media content. One challenge that remains is ensuring these digital assets match user expectations while efficiently generating personalized results at scale. This paper proposes a new methodology called ""GuidedStyle"" that utilizes attribute knowledge guided style manipulation (AKSM) for semantic face editing in digital media production pipelines. Our approach generates multiple plausible solutions in one forward pass using pretrained generative models, enabling the system to produce images matching specific attributes as desired by users. Experimental evaluations demonstrate the effectiveness of our approach compared to existing methods across different domains including facial expression changes. We also provide qualitative examples showcasing the diversity achieved through our pipeline. Overall, we aim to facilitate efficient yet high quality media generation, providing scalability necessary for modern applications such as avatar editors.",1
"With expansion of the video advertising market, research to predict the effects of video advertising is getting more attention. Although effect prediction of image advertising has been explored a lot, prediction for video advertising is still challenging with seldom research. In this research, we propose a method for predicting the click through rate (CTR) of video advertisements and analyzing the factors that determine the CTR. In this paper, we demonstrate an optimized framework for accurately predicting the effects by taking advantage of the multimodal nature of online video advertisements including video, text, and metadata features. In particular, the two types of metadata, i.e., categorical and continuous, are properly separated and normalized. To avoid overfitting, which is crucial in our task because the training data are not very rich, additional regularization layers are inserted. Experimental results show that our approach can achieve a correlation coefficient as high as 0.695, which is a significant improvement from the baseline (0.487).",0
"In recent years online video advertisements (OVAs) have become increasingly popular as consumers tend to spend more time on digital platforms such as social media, streaming services and websites watching videos. According to market research firm eMarketer, US adults spent an average of 48 minutes per day watching video ads last year, up from just 6 minutes in 2012. OVA attracts marketers because they offer an opportunity to reach and engage audiences through visual content that can convey emotions, tell stories, and showcase products in action. Predicting how well certain types of OVAs perform across different audience segments and contextual factors is critical but remains challenging due to their complexity and diversity. This study proposes using multimodal deep learning techniques that integrate both visual and textual features extracted from OVAs into neural network models for prediction tasks. Specifically, we train our system by feeding it a large dataset of OVAs collected from YouTube together with human ratings provided by real users who watched each clip. We evaluate the performance of our model on several public benchmark datasets and compare it against state-of-the-art approaches. Results show that our proposed approach outperforms existing methods by achieving higher accuracy and robustness in predicting OVA effects on user engagement, cognitive load and persuasion. Our findings suggest that integrating multi-modal deep learning techniques could lead to more accurate predictions of OVA effects in practice and open new opportunities for personalized advertising design and audience segmentation. Overall, this work contributes to our understanding of how machine intelligence can assist marketers in optimizing creative assets and investments towards better ROIs",1
"LiDAR-based 3D object detection is an important task for autonomous driving and current approaches suffer from sparse and partial point clouds of distant and occluded objects. In this paper, we propose a novel two-stage approach, namely PC-RGNN, dealing with such challenges by two specific solutions. On the one hand, we introduce a point cloud completion module to recover high-quality proposals of dense points and entire views with original structures preserved. On the other hand, a graph neural network module is designed, which comprehensively captures relations among points through a local-global attention mechanism as well as multi-scale graph based context aggregation, substantially strengthening encoded features. Extensive experiments on the KITTI benchmark show that the proposed approach outperforms the previous state-of-the-art baselines by remarkable margins, highlighting its effectiveness.",0
"Title: ""PC-RGNN: Point Cloud Completion and Graph Neural Network for Improved 3D Object Detection"" Abstract: This work proposes a novel approach for improving 3D object detection using point cloud data. We introduce a new method called PC-RGNN (Point Cloud Completion and Graph Neural Network), which combines point cloud completion with graph neural networks to accurately detect objects from raw LiDAR data. Our proposed model utilizes a graph convolutional network to learn the relationship between incomplete points in the scene, filling gaps and generating complete shapes. Additionally, we use attention mechanisms to focus on relevant regions of the graph and reduce computational complexity. Experimental results demonstrate that our PC-RGNN significantly outperforms state-of-the-art methods on standard benchmark datasets. Overall, our approach provides a powerful tool for real-world applications such as autonomous driving, robotics, and computer vision. Keywords: Point cloud completion, graph neural networks, 3D object detection, LiDAR data",1
"Graphs as a type of data structure have recently attracted significant attention. Representation learning of geometric graphs has achieved great success in many fields including molecular, social, and financial networks. It is natural to present proteins as graphs in which nodes represent the residues and edges represent the pairwise interactions between residues. However, 3D protein structures have rarely been studied as graphs directly. The challenges include: 1) Proteins are complex macromolecules composed of thousands of atoms making them much harder to model than micro-molecules. 2) Capturing the long-range pairwise relations for protein structure modeling remains under-explored. 3) Few studies have focused on learning the different attributes of proteins together. To address the above challenges, we propose a new graph neural network architecture to represent the proteins as 3D graphs and predict both distance geometric graph representation and dihedral geometric graph representation together. This gives a significant advantage because this network opens a new path from the sequence to structure. We conducted extensive experiments on four different datasets and demonstrated the effectiveness of the proposed method.",0
"In recent years, protein structure has become increasingly important in drug discovery and design as well as understanding fundamental biological processes. However, analyzing and comparing structures remains challenging due to their complex nature. One promising approach is graph representation learning which can effectively capture key structural features in simple yet informative graphs. While many efforts have been made in this direction using unweighted graphs, neglecting edge weights might result in loss of valuable spatial information. To address this issue, we propose a novel deep multi-attribute graph representation learning framework that fully considers both node attributes (e.g., atom properties) and edge weights (e.g., interatomic distances). Specifically, our model integrates multi-scale learned representations via attribute attention modules and edge feature propagation networks. Our comprehensive experiments show consistent improvements over state-of-the-art methods across diverse applications ranging from protein classification to similarity search. This work highlights the efficacy of incorporating edge weight information into graph representation learning and opens up new perspectives towards more accurate protein analysis.",1
"The class distribution of data is one of the factors that regulates the performance of machine learning models. However, investigations on the impact of different distributions available in the literature are very few, sometimes absent for domain-specific tasks. In this paper, we analyze the impact of natural and balanced distributions of the training set in deep learning (DL) models applied on histological images, also known as whole slide images (WSIs). WSIs are considered as the gold standard for cancer diagnosis. In recent years, researchers have turned their attention to DL models to automate and accelerate the diagnosis process. In the training of such DL models, filtering out the non-regions-of-interest from the WSIs and adopting an artificial distribution (usually, a balanced distribution) is a common trend. In our analysis, we show that keeping the WSIs data in their usual distribution (which we call natural distribution) for DL training produces fewer false positives (FPs) with comparable false negatives (FNs) than the artificially-obtained balanced distribution. We conduct an empirical comparative study with 10 random folds for each distribution, comparing the resulting average performance levels in terms of five different evaluation metrics. Experimental results show the effectiveness of the natural distribution over the balanced one across all the evaluation metrics.",0
Include any necessary background. Use present tense rather than past or future tune. Please provide references if required.,1
"When answering questions about an image, it not only needs knowing what -- understanding the fine-grained contents (e.g., objects, relationships) in the image, but also telling why -- reasoning over grounding visual cues to derive the answer for a question. Over the last few years, we have seen significant progress on visual question answering. Though impressive as the accuracy grows, it still lags behind to get knowing whether these models are undertaking grounding visual reasoning or just leveraging spurious correlations in the training data. Recently, a number of works have attempted to answer this question from perspectives such as grounding and robustness. However, most of them are either focusing on the language side or coarsely studying the pixel-level attention maps. In this paper, by leveraging the step-wise object grounding annotations provided in the GQA dataset, we first present a systematical object-centric diagnosis of visual reasoning on grounding and robustness, particularly on the vision side. According to the extensive comparisons across different models, we find that even models with high accuracy are not good at grounding objects precisely, nor robust to visual content perturbations. In contrast, symbolic and modular models have a relatively better grounding and robustness, though at the cost of accuracy. To reconcile these different aspects, we further develop a diagnostic model, namely Graph Reasoning Machine. Our model replaces purely symbolic visual representation with probabilistic scene graph and then applies teacher-forcing training for the visual reasoning module. The designed model improves the performance on all three metrics over the vanilla neural-symbolic model while inheriting the transparency. Further ablation studies suggest that this improvement is mainly due to more accurate image understanding and proper intermediate reasoning supervisions.",0
This would probably require understanding enough context to write such a summary effectively. Please provide that first!,1
"We propose a novel framework for producing a class of parameter and compute efficient models called AttentionLitesuitable for resource-constrained applications. Prior work has primarily focused on optimizing models either via knowledge distillation or pruning. In addition to fusing these two mechanisms, our joint optimization framework also leverages recent advances in self-attention as a substitute for convolutions. We can simultaneously distill knowledge from a compute-heavy teacher while also pruning the student model in a single pass of training thereby reducing training and fine-tuning times considerably. We evaluate the merits of our proposed approach on the CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. Not only do our AttentionLite models significantly outperform their unoptimized counterparts in accuracy, we find that in some cases, that they perform almost as well as their compute-heavy teachers while consuming only a fraction of the parameters and FLOPs. Concretely, AttentionLite models can achieve upto30x parameter efficiency and 2x computation efficiency with no significant accuracy drop compared to their teacher.",0
"This paper proposes AttentionLite, a model designed to efficiently implement self-attention mechanisms used commonly in vision tasks such as image classification and object detection. In recent years, transformer models have revolutionized natural language processing with their ability to capture global dependencies through self-attention mechanisms. However, these models struggle with scalability due to high computational complexity and memory requirements. Our approach simplifies attention operations by quantizing them into two types, namely channel-wise and patch-wise attention. We evaluate our method on popular benchmarks such as ImageNet and COCO object detection and achieve competitive results while significantly reducing computational cost and memory usage compared to state-of-the-art transformers. Our work shows that simplified attention mechanisms can effectively handle complex vision tasks without sacrificing performance.",1
Partial discharges are known as indicators of degradation of insulation systems.The reliability and selectivity of methods to detect internal partial discharges in the covered conductors are dictated by the level of background noise. The background noise distorts the pattern of partial discharges (PD-pattern) and decreases the capability of detection methods to recognize the features of PD-pattern corresponding to the degradation of an insulation system. This paper proposes a deep learning based framework with novel implementation of frequency and phase attention layers to detect partial discharge pattern on insulated overhead conductors.The introduced phase and frequency attention layers finds the discriminative regions responsible for PD activity in the spectograms of the signals.,0
"This paper presents a novel deep learning framework for partial discharge (PD) detection on insulated overhead conductors using frequency and phase attention mechanisms. PDs occur when there is internal electrical stress within the insulation of power equipment and can cause significant damage if left unmonitored. Traditional methods for detecting PD involve using sensors that capture raw signals from conductors which then require complex processing to identify signs of PD activity. However, these techniques often suffer from high rates of false positives and negatives due to their limited ability to differentiate between noise sources and actual PD events. The proposed approach addresses these limitations by leveraging the benefits of deep learning to enhance signal analysis accuracy. We demonstrate how our framework achieves state-of-the-art performance compared to existing methods in detecting partial discharges on insulated overhead cables under diverse operating conditions while minimizing false positive detections. Our results showcase the effectiveness of incorporating frequency and phase attention into the design of neural networks for enhanced PD detection. By improving our understanding of PD behavior in power systems, we hope this research will contribute towards better infrastructure monitoring strategies and ultimately reduce maintenance costs and downtime associated with unexpected failures.",1
"Time series classification problems exist in many fields and have been explored for a couple of decades. However, they still remain challenging, and their solutions need to be further improved for real-world applications in terms of both accuracy and efficiency. In this paper, we propose a hybrid neural architecture, called Self-Attentive Recurrent Convolutional Networks (SARCoN), to learn multi-faceted representations for univariate time series. SARCoN is the synthesis of long short-term memory networks with self-attentive mechanisms and Fully Convolutional Networks, which work in parallel to learn the representations of univariate time series from different perspectives. The component modules of the proposed architecture are trained jointly in an end-to-end manner and they classify the input time series in a cooperative way. Due to its domain-agnostic nature, SARCoN is able to generalize a diversity of domain tasks. Our experimental results show that, compared to the state-of-the-art approaches for time series classification, the proposed architecture can achieve remarkable improvements for a set of univariate time series benchmarks from the UCR repository. Moreover, the self-attention and the global average pooling in the proposed architecture enable visible interpretability by facilitating the identification of the contribution regions of the original time series. An overall analysis confirms that multi-faceted representations of time series aid in capturing deep temporal corrections within complex time series, which is essential for the improvement of time series classification performance. Our work provides a novel angle that deepens the understanding of time series classification, qualifying our proposed model as an ideal choice for real-world applications.",0
"In recent years, time series classification has gained significant attention due to its applications in various fields such as finance, healthcare, engineering, and environmental science. However, most existing models still face challenges in handling complex relationships among different variables and capturing long-range dependencies in the data. To address these limitations, we propose a novel multi-faceted representation learning approach with hybrid architecture (MFRH) for time series classification. Our model integrates multiple perspectives from domain knowledge into feature extraction by designing customized modules tailored to specific datasets. We then employ a unified framework that combines deep neural networks with statistical methods to learn an effective joint representation. Our experiments on benchmark datasets demonstrate the superiority of our proposed method over state-of-the-art approaches. By simultaneously leveraging multiple sources of information through an adaptive weight mechanism and using high-capacity deep neural network architectures to capture intricate patterns, MFRH shows great potential in solving real-world problems involving time series data.",1
"Health misinformation on social media devastates physical and mental health, invalidates health gains, and potentially costs lives. Understanding how health misinformation is transmitted is an urgent goal for researchers, social media platforms, health sectors, and policymakers to mitigate those ramifications. Deep learning methods have been deployed to predict the spread of misinformation. While achieving the state-of-the-art predictive performance, deep learning methods lack the interpretability due to their blackbox nature. To remedy this gap, this study proposes a novel interpretable deep learning approach, Generative Adversarial Network based Piecewise Wide and Attention Deep Learning (GAN-PiWAD), to predict health misinformation transmission in social media. Improving upon state-of-the-art interpretable methods, GAN-PiWAD captures the interactions among multi-modal data, offers unbiased estimation of the total effect of each feature, and models the dynamic total effect of each feature when its value varies. We select features according to social exchange theory and evaluate GAN-PiWAD on 4,445 misinformation videos. The proposed approach outperformed strong benchmarks. Interpretation of GAN-PiWAD indicates video description, negative video content, and channel credibility are key features that drive viral transmission of misinformation. This study contributes to IS with a novel interpretable deep learning method that is generalizable to understand other human decision factors. Our findings provide direct implications for social media platforms and policymakers to design proactive interventions to identify misinformation, control transmissions, and manage infodemics.",0
"As the prevalence of digital media platforms increases, so does the spread of health misinformation which can lead to serious consequences on public health outcomes. This study proposes an interpretable deep learning approach that addresses key challenges in understanding how and why health misinformation is transmitted online. By leveraging advances in machine learning and natural language processing techniques, our model provides insights into how users interact with health information and identifies patterns that contribute to the dissemination of misinformation. Our methodology combines feature selection algorithms with supervised neural networks to achieve interpretability while maintaining high performance. Incorporating user characteristics such as trustworthiness, expertise, influence and other factors into the analysis, we demonstrate how social network structures impact the transmission of health misinformation. Through extensive experiments and evaluations on real-world datasets, we showcase how our approach significantly outperforms traditional methods in predictive accuracy, explanatory power and transparency. Overall, our work contributes to addressing the current infodemic crisis by offering a scalable solution that empowers stakeholders involved in evidence-based decision making related to health communication strategies in response to harmful online content.",1
"The characterization of drug-protein interactions is crucial in the high-throughput screening for drug discovery. The deep learning-based approaches have attracted attention because they can predict drug-protein interactions without trial-and-error by humans. However, because data labeling requires significant resources, the available protein data size is relatively small, which consequently decreases model performance. Here we propose two methods to construct a deep learning framework that exhibits superior performance with a small labeled dataset. At first, we use transfer learning in encoding protein sequences with a pretrained model, which trains general sequence representations in an unsupervised manner. Second, we use a Bayesian neural network to make a robust model by estimating the data uncertainty. As a result, our model performs better than the previous baselines for predicting drug-protein interactions. We also show that the quantified uncertainty from the Bayesian inference is related to the confidence and can be used for screening DPI data points.",0
"Paper Title: ""Bayesian Neural Network with Pretrained Protein Embedding Enhances Prediction Accuracy of Drug-Protein Interaction""  Abstract: In recent years, there has been increasing interest in using machine learning algorithms to predict drug-protein interactions (DPI), which play a critical role in determining how drugs interact with their target proteins in our bodies. One approach that has shown promise is the use of deep learning models, including artificial neural networks (ANNs). However, training these models can be challenging due to issues such as data sparsity and missing annotations, particularly for novel targets. To address these limitations, we propose a new framework that combines Bayesian inference and a pretrained protein embedding model to improve the performance of ANNs in predicting DPI. We show that this approach significantly outperforms existing methods on benchmark datasets, achieving state-of-the-art results. Our findings demonstrate the effectiveness of combining prior knowledge and deep learning techniques to tackle complex problems in biology and medicine, paving the way for more accurate predictions of drug actions at the molecular level.",1
"Fine-grained visual classification (FGVC) aims to distinguish the sub-classes of the same category and its essential solution is to mine the subtle and discriminative regions. Convolution neural networks (CNNs), which employ the cross entropy loss (CE-loss) as the loss function, show poor performance since the model can only learn the most discriminative part and ignore other meaningful regions. Some existing works try to solve this problem by mining more discriminative regions by some detection techniques or attention mechanisms. However, most of them will meet the background noise problem when trying to find more discriminative regions. In this paper, we address it in a knowledge transfer learning manner. Multiple models are trained one by one, and all previously trained models are regarded as teacher models to supervise the training of the current one. Specifically, a orthogonal loss (OR-loss) is proposed to encourage the network to find diverse and meaningful regions. In addition, the first model is trained with only CE-Loss. Finally, all models' outputs with complementary knowledge are combined together for the final prediction result. We demonstrate the superiority of the proposed method and obtain state-of-the-art (SOTA) performances on three popular FGVC datasets.",0
"Effectively utilizing knowledge transfer techniques has become increasingly important for fine-grained visual classification tasks due to their ability to leverage large amounts of data from related domains. This study presents a comprehensive analysis of popular knowledge transfer methods across multiple benchmark datasets to assess their impact on the performance of deep learning models. Results indicate that while some approaches outperform others under certain conditions, there remains no one clear winner for all scenarios. Additionally, we explore potential reasons behind these variations, providing valuable insights into how to select appropriate methodologies based on the specific task at hand. Our findings have the potential to guide future research in the field towards more effective knowledge transfer strategies.",1
"Human Activity Recognition (HAR) from devices like smartphone accelerometers is a fundamental problem in ubiquitous computing. Machine learning based recognition models often perform poorly when applied to new users that were not part of the training data. Previous work has addressed this challenge by personalizing general recognition models to the unique motion pattern of a new user in a static batch setting. They require target user data to be available upfront. The more challenging online setting has received less attention. No samples from the target user are available in advance, but they arrive sequentially. Additionally, the motion pattern of users may change over time. Thus, adapting to new and forgetting old information must be traded off. Finally, the target user should not have to do any work to use the recognition system by, say, labeling any activities. Our work addresses all of these challenges by proposing an unsupervised online domain adaptation algorithm. Both classification and personalization happen continuously and incrementally in real time. Our solution works by aligning the feature distributions of all subjects, be they sources or the target, in hidden neural network layers. To this end, we normalize the input of a layer with user-specific mean and variance statistics. During training, these statistics are computed over user-specific batches. In the online phase, they are estimated incrementally for any new target user.",0
"Artificial intelligence has made significant strides in recent years towards automating human activity recognition (HAR). However, many current methods struggle to adapt to changes in user behavior over time, and suffer from high computational cost that precludes their use on mobile devices. This paper proposes a novel solution to address these issues by using domain adaptive batch normalization (DABN) during HAR inference. We show experimentally that our method provides more accurate predictions compared to state-of-the-art approaches while achieving real-time performance even on smartphones. Furthermore, we demonstrate how DABN can incrementally personalize the model based on new data, allowing usersâ€™ habits and routines to evolve naturally without the need for retraining or fine-tuning the entire model. These results pave the way for deploying robust and efficient HAR systems that continuously adapt to individualsâ€™ diverse lifestyles.",1
"The Transformer architecture has revolutionized deep learning on sequential data, becoming ubiquitous in state-of-the-art solutions for a wide variety of applications. Yet vanilla Transformers are notoriously resource-expensive, requiring $O(L^2)$ in serial time and memory as functions of input length $L$. Recent works proposed various linear self-attention mechanisms, scaling only as $O(L)$ for serial computation. We perform a thorough analysis of recent Transformer mechanisms with linear self-attention, Performers, in terms of overall computational complexity. We observe a remarkable computational flexibility: forward and backward propagation can be performed with no approximations using sublinear memory as a function of $L$ (in addition to negligible storage for the input sequence), at a cost of greater time complexity in the parallel setting. In the extreme case, a Performer consumes only $O(1)$ memory during training, and still requires $O(L)$ time. This discovered time-memory tradeoff can be used for training or, due to complete backward-compatibility, for fine-tuning on a low-memory device, e.g. a smartphone or an earlier-generation GPU, thus contributing towards decentralized and democratized deep learning.",0
"Here is your requested abstract. Let me know if you need any revisions or changes.  This paper explores sublinear memory (SLiM) as a novel technique for improving the performance of deep learning models. By taking advantage of the inherent sparsity present within modern neural networks, we demonstrate how to train models that perform comparably with state-of-the-art models while using significantly less memory. Our approach centers on modifying the backpropagation algorithm through pruning connections during training. We analyze both traditional supervised and reinforcement learning settings, demonstrating consistent improvements across the board. Furthermore, we investigate the robustness of our method against variations in network architecture and hyperparameters. Finally, we discuss the implications of our findings for real-world applications where memory usage can limit model size and performance gains. Overall, our work provides significant insight into designing efficient machine learning systems without sacrificing accuracy.",1
"Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node's neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer -- a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",0
"Abstract: We propose a new approach for learning local neighboring structure for robust 3D shape representation. Our method leverages recent advances in deep learning techniques to learn the underlying geometry of a 3D object from a set of point cloud data points. By doing so, we can more accurately capture the nuanced details and variations present in real-world objects that may have been missed by traditional methods. Additionally, our model allows for efficient inference, making it possible to use on resource-constrained devices like smartphones or drones. We demonstrate the effectiveness of our approach through extensive experiments on several benchmark datasets and showcase promising results compared to state-of-the-art methods. This research has important implications for computer vision applications such as 3D reconstruction, scene understanding, and autonomous robot navigation.",1
"Facial micro-expressions indicate brief and subtle facial movements that appear during emotional communication. In comparison to macro-expressions, micro-expressions are more challenging to be analyzed due to the short span of time and the fine-grained changes. In recent years, micro-expression recognition (MER) has drawn much attention because it can benefit a wide range of applications, e.g. police interrogation, clinical diagnosis, depression analysis, and business negotiation. In this survey, we offer a fresh overview to discuss new research directions and challenges these days for MER tasks. For example, we review MER approaches from three novel aspects: macro-to-micro adaptation, recognition based on key apex frames, and recognition based on facial action units. Moreover, to mitigate the problem of limited and biased ME data, synthetic data generation is surveyed for the diversity enrichment of micro-expression data. Since micro-expression spotting can boost micro-expression analysis, the state-of-the-art spotting works are also introduced in this paper. At last, we discuss the challenges in MER research and provide potential solutions as well as possible directions for further investigation.",0
"""Facial micro-expressions are subtle changes in facial expressions that convey underlying emotions beyond our conscious control. This study provides a comprehensive overview of facial micro-expression analysis by reviewing relevant data sources, methodologies used, challenges encountered and future research directions. We begin by discussing different types of facial micro-expressions and their impact on human interactions. Next, we examine current databases, sensors, features extraction techniques and classifiers used in micro-expression recognition systems. Various challenge",1
"Autoencoder and its variants have been widely applicated in anomaly detection.The previous work memory-augmented deep autoencoder proposed memorizing normality to detect anomaly, however it neglects the feature discrepancy between different resolution scales, therefore we introduce multi-scale memories to record scale-specific features and multi-scale attention fuser between the encoding and decoding module of the autoencoder for anomaly detection, namely MMAE.MMAE updates slots at corresponding resolution scale as prototype features during unsupervised learning. For anomaly detection, we accomplish anomaly removal by replacing the original encoded image features at each scale with most relevant prototype features,and fuse these features before feeding to the decoding module to reconstruct image. Experimental results on various datasets testify that our MMAE successfully removes anomalies at different scales and performs favorably on several datasets compared to similar reconstruction-based methods.",0
"Anomaly detection has been a challenging task in various fields such as computer vision, natural language processing, and time series analysis. Recently, deep learning approaches have shown promising results for detecting anomalies in high dimensional data. Autoencoder-based methods have gained popularity due to their ability to learn effective representations and identify anomalous instances by comparing reconstruction errors. However, these models can suffer from poor performance on large-scale datasets where labeled anomalies are scarce. To address this issue, we propose using multi-scale memories to enhance the representation capacity of autoencoders for unsupervised anomaly localization. We show that incorporating memory modules at different scales allows the model to capture both global and local features effectively, leading to improved anomaly detection accuracy. Our experiments on several benchmark datasets demonstrate the effectiveness of our proposed method compared to state-of-the-art techniques. This work opens up new possibilities for advancing unsupervised anomaly detection research and paves the way for deploying robust systems for real-world applications.",1
"Nearest neighbor search has found numerous applications in machine learning, data mining and massive data processing systems. The past few years have witnessed the popularity of the graph-based nearest neighbor search paradigm because of its superiority over the space-partitioning algorithms. While a lot of empirical studies demonstrate the efficiency of graph-based algorithms, not much attention has been paid to a more fundamental question: why graph-based algorithms work so well in practice? And which data property affects the efficiency and how? In this paper, we try to answer these questions. Our insight is that ""the probability that the neighbors of a point o tends to be neighbors in the KNN graph"" is a crucial data property for query efficiency. For a given dataset, such a property can be qualitatively measured by clustering coefficient of the KNN graph. To show how clustering coefficient affects the performance, we identify that, instead of the global connectivity, the local connectivity around some given query q has more direct impact on recall. Specifically, we observed that high clustering coefficient makes most of the k nearest neighbors of q sit in a maximum strongly connected component (SCC) in the graph. From the algorithmic point of view, we show that the search procedure is actually composed of two phases - the one outside the maximum SCC and the other one in it, which is different from the widely accepted single or multiple paths search models. We proved that the commonly used graph-based search algorithm is guaranteed to traverse the maximum SCC once visiting any point in it. Our analysis reveals that high clustering coefficient leads to large size of the maximum SCC, and thus provides good answer quality with the help of the two-phase search procedure. Extensive empirical results over a comprehensive collection of datasets validate our findings.",0
"This could apply to any given technology such as neural networks, blockchain etc - so please make the language general. Additionally - I would like it if the abstract had an optimistic tone overall but highlighted potential problems that may arise when implementing graph based nearest neighbor search. Is there anyway you can assist me? Thank you!",1
"This paper addresses the problem of remote sensing image pan-sharpening from the perspective of generative adversarial learning. We propose a novel deep neural network based method named PSGAN. To the best of our knowledge, this is one of the first attempts at producing high-quality pan-sharpened images with GANs. The PSGAN consists of two components: a generative network (i.e., generator) and a discriminative network (i.e., discriminator). The generator is designed to accept panchromatic (PAN) and multispectral (MS) images as inputs and maps them to the desired high-resolution (HR) MS images and the discriminator implements the adversarial training strategy for generating higher fidelity pan-sharpened images. In this paper, we evaluate several architectures and designs, namely two-stream input, stacking input, batch normalization layer, and attention mechanism to find the optimal solution for pan-sharpening. Extensive experiments on QuickBird, GaoFen-2, and WorldView-2 satellite images demonstrate that the proposed PSGANs not only are effective in generating high-quality HR MS images and superior to state-of-the-art methods and also generalize well to full-scale images.",0
"Recent advances in remote sensing technologies have led to the acquisition of high resolution panchromatic (Pan) images along with lower resolution multispectral (MS) data. These low spatial resolution MS images provide spectral information but lack the necessary details present in their corresponding Pan images. In order to address this issue, pan-sharpening techniques have been developed which fusion both datasets into one high spatial resolution image that preserves the spectral content of the MS image while incorporating the additional detail from the Pan image. However, traditional pan-sharpening methods often result in loss of important details due to blurring effects caused by simple averaging techniques. This has lead to the development of more advanced approaches such as PCA-based interpolation and GISR which use machine learning algorithms to improve upon traditional methods. This research proposes using a generative adversarial network (PSGAN) to further improve on existing pan-sharpening techniques. By training two neural networks, one generator and one discriminator, we can create a model capable of generating new pixel values which match both the statistical properties of the MS and Pan datasets respectively, resulting in superior quality output compared to other state-of-the art methods. Our results show significant improvements over current approaches achieving higher spatial resolutions and less noise than other contemporary models. The proposed approach demonstrates great potential for future applications in computer vision, photogrammetry and geographic information systems.",1
"In remote sensing images, the existence of the thin cloud is an inevitable and ubiquitous phenomenon that crucially reduces the quality of imageries and limits the scenarios of application. Therefore, thin cloud removal is an indispensable procedure to enhance the utilization of remote sensing images. Generally, even though contaminated by thin clouds, the pixels still retain more or less surface information. Hence, different from thick cloud removal, thin cloud removal algorithms normally concentrate on inhibiting the cloud influence rather than substituting the cloud-contaminated pixels. Meanwhile, considering the surface features obscured by the cloud are usually similar to adjacent areas, the dependency between each pixel of the input is useful to reconstruct contaminated areas. In this paper, to make full use of the dependencies between pixels of the image, we propose a Multi-Head Linear Attention Generative Adversarial Network (MLAGAN) for Thin Cloud Removal. The MLA-GAN is based on the encoding-decoding framework consisting of multiple attention-based layers and deconvolutional layers. Compared with six deep learning-based thin cloud removal benchmarks, the experimental results on the RICE1 and RICE2 datasets demonstrate that the proposed framework MLA-GAN has dominant advantages in thin cloud removal.",0
"In recent years deep neural networks (DNNs) have been successfully employed as generators (GAN) and discriminators (Disc) to achieve state-of-the-art performance in image generation tasks such as face synthesis [6], fashion transfer [4] and object removal [9]. Motivated by these results we propose to employ GANs consisting of multi-head linear attention layers (MLA-GAN), which allow us to efficiently attend to spatially localised features at multiple scale resolutions in order to jointly optimise over both generator and discriminator parameters. We apply our model on thin cloud removal task, where cloudy images have to be transformed into clear ones, and evaluate its performance quantitatively via two different metrics: mean squared error (MSE) per pixel and perceptual similarity measure [8]. Our method outperforms several other popular cloud removal techniques including histogram of oriented gradient [7] based approaches and image dehazing methods [2]. Experiments show that MLA-GAN generates high quality natural looking outputs by accurately removing clouds from input images while preserving important details such as building structures and texture maps of trees. Additionally, we present a qualitative analysis comparing generated images against ground truth data in terms of structural coherence and perceptual realism. Code is available online at https://github.com/stefanorota/MultiLayerAttention_CloudRemovalGANs/releases/download/v0.1/. To summarize, MLA-GAN achieves significant improvements in cloud removal accuracy while generating photo-realistic high fidelity output images thanks to novel use of multi-head linear attention modules for efficient feature extraction. Further future work may consider incorporation of adversarial training wi",1
"Time series forecasting and spatiotemporal kriging are the two most important tasks in spatiotemporal data analysis. Recent research on graph neural networks has made substantial progress in time series forecasting, while little attention has been paid to the kriging problem -- recovering signals for unsampled locations/sensors. Most existing scalable kriging methods (e.g., matrix/tensor completion) are transductive, and thus full retraining is required when we have a new sensor to interpolate. In this paper, we develop an Inductive Graph Neural Network Kriging (IGNNK) model to recover data for unsampled sensors on a network/graph structure. To generalize the effect of distance and reachability, we generate random subgraphs as samples and reconstruct the corresponding adjacency matrix for each sample. By reconstructing all signals on each sample subgraph, IGNNK can effectively learn the spatial message passing mechanism. Empirical results on several real-world spatiotemporal datasets demonstrate the effectiveness of our model. In addition, we also find that the learned model can be successfully transferred to the same type of kriging tasks on an unseen dataset. Our results show that: 1) GNN is an efficient and effective tool for spatial kriging; 2) inductive GNNs can be trained using dynamic adjacency matrices; 3) a trained model can be transferred to new graph structures and 4) IGNNK can be used to generate virtual sensors.",0
"Title: Predictive Spatiotemporal Modeling Using Inductive Graph Neural Networks  In today's era of big data, there exists a growing need for advanced machine learning techniques capable of handling large, complex datasets. One such application domain is spatial modeling, wherein models are constructed to predict values at unsampled locations based on existing observations. Kriging is a popular method for spatiotemporal modeling due to its ability to capture both spatial autocorrelation and temporal trends. However, traditional Kriging methods often require careful tuning of parameters and may struggle to generalize well across domains. To address these limitations, we propose using graph neural networks (GNN) within a Kriging framework, which can learn high-level representations of both spatial and temporal structure without relying on handcrafted features. Our approach, called Inductive Graph Kriging Neural Network (IGKN), utilizes an inductive GNN architecture that enables transfer learning between source and target domains while maintaining computational efficiency. We evaluate our proposed method through a suite of experiments that demonstrate superior performance compared to state-of-the-art methods across multiple benchmark datasets, showing promise for real-world applications. Overall, IGKN represents a step forward in enabling accurate, scalable, and efficient spatiotemporal prediction tasks.",1
"We present a novel deep reinforcement learning method to learn construction heuristics for vehicle routing problems. In specific, we propose a Multi-Decoder Attention Model (MDAM) to train multiple diverse policies, which effectively increases the chance of finding good solutions compared with existing methods that train only one policy. A customized beam search strategy is designed to fully exploit the diversity of MDAM. In addition, we propose an Embedding Glimpse layer in MDAM based on the recursive nature of construction, which can improve the quality of each policy by providing more informative embeddings. Extensive experiments on six different routing problems show that our method significantly outperforms the state-of-the-art deep learning based models.",0
"""Vehicle routing problems (VRP) have been studied extensively due to their real world applications such as logistics management. In recent years, several variants of VRP have emerged that require solving complex routes for vehicles carrying perishable goods. These variants present significant challenges owing to the time constraints associated with transportation of sensitive commodities. To address these challenges, we propose a multi-decoder attention model with embedding glimpse approach (MGAM). Our method uses attention mechanisms on both sequence and graph level to explore different possible solutions without relying exclusively on any one decoding order. We employ a novel mechanism called embedding glimpses which captures global state features from each step allowing the model to predict better at each iteration. Experiments performed using benchmark datasets demonstrate MGAM outperforms other algorithms including RLHF, MaxSumBFS and OR-Tools by large margins in terms of cost metrics while ensuring timely delivery of goods.""",1
"Recently, Transformer networks have redefined the state of the art in many NLP tasks. However, these models suffer from quadratic computational cost in the input sequence length $n$ to compute pairwise attention in each layer. This has prompted recent research into sparse Transformers that sparsify the connections in the attention layers. While empirically promising for long sequences, fundamental questions remain unanswered: Can sparse Transformers approximate any arbitrary sequence-to-sequence function, similar to their dense counterparts? How does the sparsity pattern and the sparsity level affect their performance? In this paper, we address these questions and provide a unifying framework that captures existing sparse attention models. We propose sufficient conditions under which we prove that a sparse attention model can universally approximate any sequence-to-sequence function. Surprisingly, our results show that sparse Transformers with only $O(n)$ connections per attention layer can approximate the same function class as the dense model with $n^2$ connections. Lastly, we present experiments comparing different patterns/levels of sparsity on standard NLP tasks.",0
"In recent years, deep learning has revolutionized many fields by enabling powerful modeling capabilities through neural networks that can handle large amounts of data. One particular area where these models have had significant impact is natural language processing (NLP). At the core of successful NLP applications lie transformer architectures which rely on multihead self attention mechanisms to capture complex relationships among input tokens. These mechanisms however suffer from quadratic time complexity due to their pairwise calculations across all inputs. Recent works have sought to reduce this computational cost without sacrificing expressivity through techniques such as sparse attention approximations. Yet, questions remain regarding whether such approximations truly limit expressivity and hinder performance. We address this question through extensive experiments showing that simple O(n) connections suffice for universal approximation of sparse attention. Specifically, we analyze randomizations applied to standard full connection settings in two prominent transformer variants. Our findings suggest that previous concerns over expressivity limitations might have been exaggerated, allowing practitioners more freedom in designing efficient, high performing systems. By demystifying the role of sparsity within transformer models, our work brings us closer to understanding the fundamental building blocks required for state of the art NLP systems.",1
"Temporal knowledge graphs (TKGs) inherently reflect the transient nature of real-world knowledge, as opposed to static knowledge graphs. Naturally, automatic TKG completion has drawn much research interests for a more realistic modeling of relational reasoning. However, most of the existing mod-els for TKG completion extend static KG embeddings that donot fully exploit TKG structure, thus lacking in 1) account-ing for temporally relevant events already residing in the lo-cal neighborhood of a query, and 2) path-based inference that facilitates multi-hop reasoning and better interpretability. In this paper, we propose T-GAP, a novel model for TKG completion that maximally utilizes both temporal information and graph structure in its encoder and decoder. T-GAP encodes query-specific substructure of TKG by focusing on the temporal displacement between each event and the query times-tamp, and performs path-based inference by propagating attention through the graph. Our empirical experiments demonstrate that T-GAP not only achieves superior performance against state-of-the-art baselines, but also competently generalizes to queries with unseen timestamps. Through extensive qualitative analyses, we also show that T-GAP enjoys from transparent interpretability, and follows human intuition in its reasoning process.",0
"This paper presents a novel approach for temporal knowledge graph completion called T-GAP (Temporal Growing Authority Paths). Existing approaches for temporal KG completion focus on either finding paths that maximize the similarity between entities at both ends of the path or finding paths based solely on their confidence scores. However, these methods can lead to incomplete or incorrect results due to limited sources of evidence or missing facts. Our approach addresses this limitation by learning how to walk across time, utilizing temporal patterns present in the data as well as the structure of the entity relationships within the KG. We first introduce a new method called growing authority paths which combines different types of paths into a unified framework and enables us to generate new candidate facts using existing ones. By incorporating temporal reasoning, we further improve our modelâ€™s ability to find complete and accurate answers. In experiments, we demonstrate that T-GAP outperforms state-of-the-art baselines in terms of quality and quantity of completed facts over multiple datasets including popular benchmarks like FB15K-237 and WNED. Additionally, ablation studies show the effectiveness of each component of our proposed algorithm. Overall, our work advances the field of temporal knowledge graph completion and has potential applications in areas such as question answering, recommendation systems, and event detection.",1
"This work investigates the problem of multi-agents trajectory prediction. Prior approaches lack of capability of capturing fine-grained dependencies among coordinated agents. In this paper, we propose a spatial-temporal trajectory prediction approach that is able to learn the strategy of a team with multiple coordinated agents. In particular, we use graph-based attention model to learn the dependency of the agents. In addition, instead of utilizing the recurrent networks (e.g., VRNN, LSTM), our method uses a Temporal Convolutional Network (TCN) as the sequential model to support long effective history and provide important features such as parallelism and stable gradients. We demonstrate the validation and effectiveness of our approach on two different sports game datasets: basketball and soccer datasets. The result shows that compared to related approaches, our model that infers the dependency of players yields substantially improved performance. Code is available at https://github.com/iHeartGraph/predict",0
"This work proposes graph attention networks (GATs) as a means to model interactions among agents in multi-agent sports games in order to predict future trajectories more accurately. Previous approaches have focused on applying deep reinforcement learning algorithms to generate strategies for playing these types of games; however, they often suffer from high sample complexity and struggle to handle complex environments. In contrast, our approach uses GATs to learn interactions at multiple scales simultaneously and achieves significant improvements over previous methods in terms of both accuracy and efficiency. We demonstrate that our approach can effectively capture nonlinear dependencies and can generate accurate predictions in challenging scenarios involving realistically-scaled simulation models across diverse sports applications. Our results show promising potential for using GATs as powerful tools in multi-agent sports game settings and pave the way for further development in this area.",1
"Bilinear pooling (BLP) refers to a family of operations recently developed for fusing features from different modalities predominantly developed for VQA models. A bilinear (outer-product) expansion is thought to encourage models to learn interactions between two feature spaces and has experimentally outperformed `simpler' vector operations (concatenation and element-wise-addition/multiplication) on VQA benchmarks. Successive BLP techniques have yielded higher performance with lower computational expense and are often implemented alongside attention mechanisms. However, despite significant progress in VQA, BLP methods have not been widely applied to more recently explored video question answering (video-QA) tasks. In this paper, we begin to bridge this research gap by applying BLP techniques to various video-QA benchmarks, namely: TVQA, TGIF-QA, Ego-VQA and MSVD-QA. We share our results on the TVQA baseline model, and the recently proposed heterogeneous-memory-enchanced multimodal attention (HME) model. Our experiments include both simply replacing feature concatenation in the existing models with BLP, and a modified version of the TVQA baseline to accommodate BLP we name the `dual-stream' model. We find that our relatively simple integration of BLP does not increase, and mostly harms, performance on these video-QA benchmarks. Using recently proposed theoretical multimodal fusion taxonomies, we offer insight into why BLP-driven performance gain for video-QA benchmarks may be more difficult to achieve than in earlier VQA models. We suggest a few additional `best-practices' to consider when applying BLP to video-QA. We stress that video-QA models should carefully consider where the complex representational potential from BLP is actually needed to avoid computational expense on `redundant' fusion.",0
"This paper investigates the effectiveness of using bilinear pooling layers within convolutional neural networks (CNNs) on video question answering tasks. We explore two approaches: i) incorporating bilinear pooling as a temporal aggregation method within LSTMs, and ii) applying linear attention across frames before feeding them through a CNN. Our results demonstrate that while both methods provide marginal improvements over traditional max-pooling, neither approach consistently outperforms the baseline model across all datasets. However, we observe promising performance gains when combining these techniques with other state-of-the-art architectures such as Memory Augmented Neural Networks (MANNs). These findings suggest potential ways to improve video question answering models by leveraging advanced spatio-temporal representations.",1
"Interpretation of Airborne Laser Scanning (ALS) point clouds is a critical procedure for producing various geo-information products like 3D city models, digital terrain models and land use maps. In this paper, we present a local and global encoder network (LGENet) for semantic segmentation of ALS point clouds. Adapting the KPConv network, we first extract features by both 2D and 3D point convolutions to allow the network to learn more representative local geometry. Then global encoders are used in the network to exploit contextual information at the object and point level. We design a segment-based Edge Conditioned Convolution to encode the global context between segments. We apply a spatial-channel attention module at the end of the network, which not only captures the global interdependencies between points but also models interactions between channels. We evaluate our method on two ALS datasets namely, the ISPRS benchmark dataset and DCF2019 dataset. For the ISPRS benchmark dataset, our model achieves state-of-the-art results with an overall accuracy of 0.845 and an average F1 score of 0.737. With regards to the DFC2019 dataset, our proposed network achieves an overall accuracy of 0.984 and an average F1 score of 0.834.",0
"Deep learning has shown great potential for semantic segmentation tasks on point cloud data obtained from airborne laser scanners (ALS). However, these methods often struggle with localization errors caused by occlusions or low resolution. In this work, we propose a novel architecture that utilizes both local and global encoders to improve localization accuracy while maintaining the benefits of deep learning approaches. Our method, called LGENet, processes point clouds using three components: feature extraction via a locally sensitive aggregator network, decoding with fully convolutional networks, and refinement through a conditional variational autoencoder. We evaluate our approach against competitive baselines on two challenging datasets, demonstrating significant improvements over other state-of-the-art methods in terms of overall classification performance and localization robustness. Additionally, we show how the use of ALS data can benefit applications such as environmental monitoring and disaster response planning. This paper presents an innovative model capable of high-accuracy semantic segmentation that addresses limitations faced by current methods. By leveraging a combination of local and global representations, LGENet achieves superior results without sacrificing efficiency or ease of implementation. Overall, this research offers valuable insights into enhancing real-world applications of ALS technology through advanced computer vision techniques.",1
"Taking the deep learning-based algorithms into account has become a crucial way to boost object detection performance in aerial images. While various neural network representations have been developed, previous works are still inefficient to investigate the noise-resilient performance, especially on aerial images with noise taken by the cameras with telephoto lenses, and most of the research is concentrated in the field of denoising. Of course, denoising usually requires an additional computational burden to obtain higher quality images, while noise-resilient is more of a description of the robustness of the network itself to different noises, which is an attribute of the algorithm itself. For this reason, the work will be started by analyzing the noise-resilient performance of the neural network, and then propose two hypotheses to build a noise-resilient structure. Based on these hypotheses, we compare the noise-resilient ability of the Oct-ResNet with frequency division processing and the commonly used ResNet. In addition, previous feature pyramid networks used for aerial object detection tasks are not specifically designed for the frequency division feature maps of the Oct-ResNet, and they usually lack attention to bridging the semantic gap between diverse feature maps from different depths. On the basis of this, a novel octave convolution-based semantic attention feature pyramid network (OcSaFPN) is proposed to get higher accuracy in object detection with noise. The proposed algorithm tested on three datasets demonstrates that the proposed OcSaFPN achieves a state-of-the-art detection performance with Gaussian noise or multiplicative noise. In addition, more experiments have proved that the OcSaFPN structure can be easily added to existing algorithms, and the noise-resilient ability can be effectively improved.",0
"In this paper we present a method for object detection in aerial images which achieves state-of-the art performance using only a single neural network model and no additional post processing steps beyond non-maximum suppression. This approach outperforms other methods that require more complex models and postprocessing steps, while still maintaining accuracy rivaling those more complicated systems. Our system uses OpenCV for image acquisition from the dataset and PyTorch for deep learning development and testing. We test our method across five datasets: KITTI DET, nuScenes DET, ADE20K, Cityscapes, and Mapillary Vistas Challenge. To ensure reproducibility all code used to develop our method as well as train/test scripts can be found at https://github.com/Open-Assistant/ObjectDetector.",1
"Transformers have become the dominant model in natural language processing, owing to their ability to pretrain on massive amounts of data, then transfer to smaller, more specific tasks via fine-tuning. The Vision Transformer was the first major attempt to apply a pure transformer model directly to images as input, demonstrating that as compared to convolutional networks, transformer-based architectures can achieve competitive results on benchmark classification tasks. However, the computational complexity of the attention operator means that we are limited to low-resolution inputs. For more complex tasks such as detection or segmentation, maintaining a high input resolution is crucial to ensure that models can properly identify and reflect fine details in their output. This naturally raises the question of whether or not transformer-based architectures such as the Vision Transformer are capable of performing tasks other than classification. In this paper, we determine that Vision Transformers can be used as a backbone by a common detection task head to produce competitive COCO results. The model that we propose, ViT-FRCNN, demonstrates several known properties associated with transformers, including large pretraining capacity and fast fine-tuning performance. We also investigate improvements over a standard detection backbone, including superior performance on out-of-domain images, better performance on large objects, and a lessened reliance on non-maximum suppression. We view ViT-FRCNN as an important stepping stone toward a pure-transformer solution of complex vision tasks such as object detection.",0
"In recent years, object detection has become a fundamental task in computer vision, enabling many applications such as autonomous driving, surveillance, and image recognition. One popular approach to object detection is based on convolutional neural networks (CNNs). However, CNNs have limitations due to their fixed receptive fields and lack of parallel computation. To overcome these challenges, we propose using transformers for object detection.  Transformers have revolutionized natural language processing by providing parallel computation and handling long range dependencies more effectively than recurrent layers or CNNs. We build upon previous work that uses transformer architectures for image classification and extend them to perform multi-class object detection from high resolution images. Our method leverages self attention mechanisms within feature pyramids, which allows us to directly model relationships between different scales. This improves detection accuracy, particularly for objects at small scales, compared to state-of-the-art CNN baselines. Additionally, our architecture can efficiently process input images by utilizing parallelization across multiple GPUs without significant communication overhead.  We evaluate our proposed method on several benchmark datasets, including COCO and Pascal VOC, demonstrating competitive performance against other cutting edge detectors while substantially reducing inference time due to efficient hardware usage. These results indicate potential advantages of employing transformers over traditional CNNs in terms of speed and effectiveness for real world deployment scenarios involving large scale distributed computing resources.  In summary, our work explores new possibilities of applying transformer architectures beyond natural language processing towards computer vision tasks like object detection. By doing so, we set footsteps toward unlocking larger models capable of capturing longer ranged global features while maintaining high computational efficiency.",1
"We present a general framework for compositional action recognition -- i.e. action recognition where the labels are composed out of simpler components such as subjects, atomic-actions and objects. The main challenge in compositional action recognition is that there is a combinatorially large set of possible actions that can be composed using basic components. However, compositionality also provides a structure that can be exploited. To do so, we develop and test a novel Structured Attention Fusion (SAF) self-attention mechanism to combine information from object detections, which capture the time-series structure of an action, with visual cues that capture contextual information. We show that our approach recognizes novel verb-noun compositions more effectively than current state of the art systems, and it generalizes to unseen action categories quite efficiently from only a few labeled examples. We validate our approach on the challenging Something-Else tasks from the Something-Something-V2 dataset. We further show that our framework is flexible and can generalize to a new domain by showing competitive results on the Charades-Fewshot dataset.",0
"This paper presents a new framework called ""SAFCAR"" for compositional action recognition, which combines structured attention mechanisms with feature fusion techniques. Our approach addresses one of the key challenges in this area - how to effectively fuse features from different modalities (such as vision and audio) while accounting for contextual relationships among actions and objects within scenes. We show that our model significantly outperforms state-of-the-art methods on benchmark datasets across multiple domains, demonstrating its effectiveness at recognizing complex actions involving multiple actors and objects interacting with each other. Overall, our work represents a significant step forward in understanding how humans perceive and recognize actions in real-world scenarios and has important implications for applications such as video surveillance and human-computer interaction.",1
"Convolutional layers are an integral part of many deep neural network solutions in computer vision. Recent work shows that replacing the standard convolution operation with mechanisms based on self-attention leads to improved performance on image classification and object detection tasks. In this work, we show how attention mechanisms can be used to replace another canonical operation: strided transposed convolution. We term our novel attention-based operation attention-based upsampling since it increases/upsamples the spatial dimensions of the feature maps. Through experiments on single image super-resolution and joint-image upsampling tasks, we show that attention-based upsampling consistently outperforms traditional upsampling methods based on strided transposed convolution or based on adaptive filters while using fewer parameters. We show that the inherent flexibility of the attention mechanism, which allows it to use separate sources for calculating the attention coefficients and the attention targets, makes attention-based upsampling a natural choice when fusing information from multiple image modalities.",0
"This paper presents a novel approach to image upsampling that utilizes attention mechanisms to selectively focus on important features in the input image. Traditional upsampling methods suffer from limited performance due to their reliance on simple interpolation techniques, resulting in blurry images lacking fine details. Our proposed method addresses these limitations by incorporating attention modules that adaptively weight different regions of the input image based on their relevance. Experimental results demonstrate significant improvements over state-of-the-art methods across multiple benchmarks, particularly in scenes containing objects with intricate textures or complex structures. Furthermore, our model achieves comparable quality while operating at faster inference speeds than previous methods. Overall, our work represents an important step forward in advancing the field of image upsampling.",1
"The task object tracking is vital in numerous applications such as autonomous driving, intelligent surveillance, robotics, etc. This task entails the assigning of a bounding box to an object in a video stream, given only the bounding box for that object on the first frame. In 2015, a new type of video object tracking (VOT) dataset was created that introduced rotated bounding boxes as an extension of axis-aligned ones. In this work, we introduce a novel end-to-end deep learning method based on the Transformer Multi-Head Attention architecture. We also present a new type of loss function, which takes into account the bounding box overlap and orientation.   Our Deep Object Tracking model with Circular Loss Function (DOTCL) shows an considerable improvement in terms of robustness over current state-of-the-art end-to-end deep learning models. It also outperforms state-of-the-art object tracking methods on VOT2018 dataset in terms of expected average overlap (EAO) metric.",0
"Abstract: This study presents a novel end-to-end deep object tracking framework that utilizes circular loss function for rotated bounding box regression. Our method addresses limitations found in existing tracking methods by jointly learning rotation estimation, position prediction, and class prediction tasks using end-to-end optimization. We validate our approach on several challenging benchmark datasets, including OTB2015, UAVDT, and NFERMG, demonstrating state-of-the-art performance across all metrics. Furthermore, we evaluate the effectiveness of each component in our model through ablation studies and showcase how our method outperforms baseline trackers under different scenarios, such as occlusions, deformations, and camera motion. Overall, our work provides insights into effective deep tracking frameworks that can adapt to real-world conditions.",1
"Over recent years, deep reinforcement learning has shown strong successes in complex single-agent tasks, and more recently this approach has also been applied to multi-agent domains. In this paper, we propose a novel approach, called MAGNet, to multi-agent reinforcement learning that utilizes a relevance graph representation of the environment obtained by a self-attention mechanism, and a message-generation technique. We applied our MAGnet approach to the synthetic predator-prey multi-agent environment and the Pommerman game and the results show that it significantly outperforms state-of-the-art MARL solutions, including Multi-agent Deep Q-Networks (MADQN), Multi-agent Deep Deterministic Policy Gradient (MADDPG), and QMIX",0
"In recent years deep reinforcement learning (DRL) has shown promising results in multi-agent settings due to its ability to learn distributed representations that capture complex interdependencies among multiple agents. However, most existing DRL algorithms suffer from several limitations when applied to large scale problems with many interacting agents. Firstly, they often lack explicit modeling of agent interactions, which may result in poor scalability and slow convergence speed. Secondly, these methods typically ignore spatial relationships among agents or environmental contexts, leading to suboptimal policies. We propose a new approach called Multi-Agent Graph Network (MAGNet) to address these issues by constructing a graph representation of the environment and utilizing a novel attention mechanism. MAGNet consists of two key components: a graph network that models the global interaction structure among all agents and captures their local spatial dependencies; and an attentional module that allows each agent to focus on relevant parts of the environment based on its own state and actions. We apply MAGNet to a wide range of challenging multi-agent environments including both cooperative and competitive tasks, and demonstrate substantial improvements over state-of-the-art DRL baselines. Our experiments show that MAGNet effectively addresses the above mentioned limitations and can achieve superior performance even when dealing with up to one hundred agents. Overall, our work offers a flexible framework suitable for real world applications where efficient coordination among numerous autonomous entities is crucial.",1
"Real-world networks often exist with multiple views, where each view describes one type of interaction among a common set of nodes. For example, on a video-sharing network, while two user nodes are linked if they have common favorite videos in one view, they can also be linked in another view if they share common subscribers. Unlike traditional single-view networks, multiple views maintain different semantics to complement each other. In this paper, we propose MANE, a multi-view network embedding approach to learn low-dimensional representations. Similar to existing studies, MANE hinges on diversity and collaboration - while diversity enables views to maintain their individual semantics, collaboration enables views to work together. However, we also discover a novel form of second-order collaboration that has not been explored previously, and further unify it into our framework to attain superior node representations. Furthermore, as each view often has varying importance w.r.t. different nodes, we propose MANE+, an attention-based extension of MANE to model node-wise view importance. Finally, we conduct comprehensive experiments on three public, real-world multi-view networks, and the results demonstrate that our models consistently outperform state-of-the-art approaches.",0
"Title: Multi-view collaborative network embedding for joint representation learning across multiple sources  Abstract: In recent years, network analysis has emerged as a powerful tool for understanding complex systems in a variety of domains such as social sciences, biology, transportation, finance, among others. With the vast amount of data available in these fields, the task of extracting meaningful insights from networks can become challenging. To overcome this challenge, there have been numerous attempts at developing methods that can effectively learn representations from heterogeneous networks.  In this work, we propose a novel multi-view collaborative network embedding framework for jointly representing different types of networks by leveraging their complementary information. We introduce two key ideas: (1) exploiting cross-network dependencies through pairwise constraints based on shared nodes, and (2) designing a hybrid loss function combining graph convolutional losses with a similarity measure over node pairs. Our framework can accommodate multiple views while preserving their unique properties, allowing for more robust representation learning. Experiments on real-world datasets demonstrate the effectiveness of our approach compared to state-of-the-art baselines under diverse evaluation metrics. Overall, this study advances the field of network representation learning towards handling complex systems with multiple sources of information.",1
"3D Convolutional Neural Network (3D CNN) captures spatial and temporal information on 3D data such as video sequences. However, due to the convolution and pooling mechanism, the information loss seems unavoidable. To improve the visual explanations and classification in 3D CNN, we propose two approaches; i) aggregate layer-wise global to local (global-local) discrete gradients using trained 3DResNext network, and ii) implement attention gating network to improve the accuracy of the action recognition. The proposed approach intends to show the usefulness of every layer termed as global-local attention in 3D CNN via visual attribution, weakly-supervised action localization, and action recognition. Firstly, the 3DResNext is trained and applied for action classification using backpropagation concerning the maximum predicted class. The gradients and activations of every layer are then up-sampled. Later, aggregation is used to produce more nuanced attention, which points out the most critical part of the predicted class's input videos. We use contour thresholding of final attention for final localization. We evaluate spatial and temporal action localization in trimmed videos using fine-grained visual explanation via 3DCam. Experimental results show that the proposed approach produces informative visual explanations and discriminative attention. Furthermore, the action recognition via attention gating on each layer produces better classification results than the baseline model.",0
"This study presents a new approach for weakly-supervised action localization and recognition using global-local attention applied to 3D convolutional neural networks (CNNs). Traditional approaches to action recognition often rely on fully supervised training techniques that require large amounts of labeled data. In contrast, our method leverages the power of attention mechanisms within 3D CNNs to facilitate learning from less annotated data by focusing on important regions of interest in each frame. To achieve this, we introduce a novel framework combining temporal segmentation and 3D CNN classifiers for recognizing human actions under unconstrained environments. Experimental results demonstrate the effectiveness of our approach compared to state-of-the-art methods in terms of accuracy and efficiency. Our work shows promise as a solution towards real-world applications where manual annotation remains challenging. Overall, our research highlights the potential benefits of incorporating attention mechanisms into deep learning architectures designed for action recognition tasks under weak labels constraints.",1
"A major challenge in scene graph classification is that the appearance of objects and relations can be significantly different from one image to another. Previous works have addressed this by relational reasoning over all objects in an image or incorporating prior knowledge into classification. Unlike previous works, we do not consider separate models for perception and prior knowledge. Instead, we take a multi-task learning approach, where we implement the classification as an attention layer. This allows for the prior knowledge to emerge and propagate within the perception model. By enforcing the model also to represent the prior, we achieve a strong inductive bias. We show that our model can accurately generate commonsense knowledge and that the iterative injection of this knowledge to scene representations leads to significantly higher classification performance. Additionally, our model can be fine-tuned on external knowledge given as triples. When combined with self-supervised learning and with 1% of annotated images only, this gives more than 3% improvement in object classification, 26% in scene graph classification, and 36% in predicate prediction accuracy.",0
"Infer titles from the contents of each paragraph. If there is none please use ""Untitled"" This study explores methods that can incorporate prior knowledge into scene graph generation tasks such as image captioning and visual question answering (VQA). This allows algorithms to draw on external knowledge sources like databases and web APIs rather than relying exclusively on end-to-end training data. In particular, we focus on using attention mechanisms to integrate these external resources at different stages of processing. To evaluate our approach, we apply it to two challenges - VQA task and image caption retrieval task - to assess its impact on performance compared to standard end-to-end models. Our findings show that exploiting external knowledge through attention helps improve accuracy substantially in both cases, demonstrating the potential benefits of combining different types of information in real-world applications where access to relevant facts may be limited. Overall, we believe that our work represents a significant step forward in understanding how machine learning can leverage complementary information beyond what has been seen during training alone.",1
"Recent works within machine learning have been tackling inputs of ever-increasing size, with cybersecurity presenting sequence classification problems of particularly extreme lengths. In the case of Windows executable malware detection, inputs may exceed $100$ MB, which corresponds to a time series with $T=100,000,000$ steps. To date, the closest approach to handling such a task is MalConv, a convolutional neural network capable of processing up to $T=2,000,000$ steps. The $\mathcal{O}(T)$ memory of CNNs has prevented further application of CNNs to malware. In this work, we develop a new approach to temporal max pooling that makes the required memory invariant to the sequence length $T$. This makes MalConv $116\times$ more memory efficient, and up to $25.8\times$ faster to train on its original dataset, while removing the input length restrictions to MalConv. We re-invest these gains into improving the MalConv architecture by developing a new Global Channel Gating design, giving us an attention mechanism capable of learning feature interactions across 100 million time steps in an efficient manner, a capability lacked by the original MalConv CNN. Our implementation can be found at https://github.com/NeuromorphicComputationResearchProgram/MalConv2",0
"In recent years, there has been a growing interest in developing efficient algorithms for classifying sequences of extreme length while maintaining constant memory usage. This problem arises frequently in areas such as bioinformatics, finance, and cybersecurity. In particular, malware detection poses significant challenges due to the vast amount of data that must be analyzed and the need for real-time performance.  In this work, we present a novel methodology for classifying sequential data with constant memory usage using a deep learning approach. Our method leverages advances in neural network architecture design and training techniques to achieve high accuracy at scale. We evaluate our algorithm on several benchmark datasets from different domains, demonstrating its effectiveness compared to state-of-the-art methods in terms of both classification accuracy and memory efficiency. Additionally, we showcase the applicability of our framework by applying it to the task of detecting previously unknown malware samples. The results demonstrate the potential of our approach to improve the timeliness and reliability of security alerts in dynamic threat environments.  Overall, our research contributes to the field of machine learning by providing a powerful toolbox for solving sequence labeling problems under computational constraints. By bridging theory and practice through extensive empirical validation, we hope to inspire further developments in the area and foster collaboration across disciplines.",1
"Automatic medical image segmentation has wide applications for disease diagnosing. However, it is much more challenging than natural optical image segmentation due to the high-resolution of medical images and the corresponding huge computation cost. The sliding window is a commonly used technique for whole slide image (WSI) segmentation, however, for these methods based on the sliding window, the main drawback is lacking global contextual information for supervision. In this paper, we propose a dual-inputs attention network (denoted as DA-RefineNet) for WSI segmentation, where both local fine-grained information and global coarse information can be efficiently utilized. Sufficient comparative experiments are conducted to evaluate the effectiveness of the proposed method, the results prove that the proposed method can achieve better performance on WSI segmentation compared to methods relying on single-input.",0
"This paper presents a new deep learning algorithm called DA-RefineNet for whole slide image segmentation based on dual input attention mechanism. With the increasing use of digital pathology systems, automated analysis of histopathological images has become crucial. Digital pathologic slides contain rich spatial context which can provide valuable diagnostic and prognostic information but poses challenges due to large variability and complexity. Accurate tissue image segmentation remains one of the key steps towards computerized interpretation. Current algorithms suffer from limited accuracy, lack robustness or have steep computational requirement. In response to these limitations, our contribution lies in developing an encoder-decoder based network with both low level features and high level global feature fusion and refinement mechanisms utilizing dual attention blocks at multiple levels. We evaluated performance on multiple publicly available benchmarks consisting of different modalities including breast biopsy and ovary H&E stained samples from Camelyon and MIDAS datasets respectively achieving significant improvements over state-of-the-art methods. Our approach was found to accurately predict the boundaries of diverse pathologies while minimizing computation burden. Future work could explore utilizing learned maps as prior knowledge for downstream applications such as object detection/segmentation in WSIs. Overall, we hope that this research will contribute positively towards advancing automatic analysis of digitized whole slide images which could ultimately lead to better clinical outcomes through personalized medicine.",1
"Healthcare providers are increasingly using machine learning to predict patient outcomes to make meaningful interventions. However, despite innovations in this area, deep learning models often struggle to match performance of shallow linear models in predicting these outcomes, making it difficult to leverage such techniques in practice. In this work, motivated by the task of clinical prediction from insurance claims, we present a new technique called Reverse Distillation which pretrains deep models by using high-performing linear models for initialization. We make use of the longitudinal structure of insurance claims datasets to develop Self Attention with Reverse Distillation, or SARD, an architecture that utilizes a combination of contextual embedding, temporal embedding and self-attention mechanisms and most critically is trained via reverse distillation. SARD outperforms state-of-the-art methods on multiple clinical prediction outcomes, with ablation studies revealing that reverse distillation is a primary driver of these improvements. Code is available at https://github.com/clinicalml/omop-learn.",0
"Effective use of deep neural networks (DNNs) for clinical prediction requires addressing their tendency towards overfitting. Incorporating external knowledge through meta learning can mitigate this issue by improving generalization performance. Here we introduce reverse distillation as a novel technique combining ensemble methods and model compression to achieve improved generalization performance on complex medical tasks using DNN architectures without compromising their ability to utilize large datasets. We validate our methodâ€™s effectiveness on two challenging clinical benchmark problems: predicting the hospital readmission of patients after heart failure discharge, and predicting the risk of developing diabetes within five years from electronic health records. Our results demonstrate that reverse distillation achieves state-of-the-art performance while substantially reducing model complexity, enabling deployment on resource-constrained devices such as mobile phones and smart watches. Finally, we discuss potential applications and future directions for research utilizing reverse distilled models in healthcare settings, including personalized medicine, population health management, and decision support systems.",1
"Time series forecasting is an important yet challenging task. Though deep learning methods have recently been developed to give superior forecasting results, it is crucial to improve the interpretability of time series models. Previous interpretation methods, including the methods for general neural networks and attention-based methods, mainly consider the interpretation in the feature dimension while ignoring the crucial temporal dimension. In this paper, we present the series saliency framework for temporal interpretation for multivariate time series forecasting, which considers the forecasting interpretation in both feature and temporal dimensions. By extracting the ""series images"" from the sliding windows of the time series, we apply the saliency map segmentation following the smallest destroying region principle. The series saliency framework can be employed to any well-defined deep learning models and works as a data augmentation to get more accurate forecasts. Experimental results on several real datasets demonstrate that our framework generates temporal interpretations for the time series forecasting task while produces accurate time series forecast.",0
"This sounds like it could potentially cause some confusion if followed strictly! Is there somewhere specific you would like me to start my summary? Without additional context on what kind of paper I am writing an abstract for, I have trouble knowing where to begin without starting with something like ""This [type of paper] examines..."" or ""In recent years there has been increasing interest in..."". Please provide more details so that I can write an appropriate abstract which both summarizes your content while accurately representing your approach/intentions. Would you like me to follow strict rules about sentence length and punctuation as well, or focus mostly on clarity and conciseness?",1
"Password guessing approaches via deep learning have recently been investigated with significant breakthroughs in their ability to generate novel, realistic password candidates. In the present work we study a broad collection of deep learning and probabilistic based models in the light of password guessing: attention-based deep neural networks, autoencoding mechanisms and generative adversarial networks. We provide novel generative deep-learning models in terms of variational autoencoders exhibiting state-of-art sampling performance, yielding additional latent-space features such as interpolations and targeted sampling. Lastly, we perform a thorough empirical analysis in a unified controlled framework over well-known datasets (RockYou, LinkedIn, Youku, Zomato, Pwnd). Our results not only identify the most promising schemes driven by deep neural networks, but also illustrate the strengths of each approach in terms of generation variability and sample uniqueness.",0
"Title: ""Generative Deep Learning Techniques for Password Generation""  Abstract: In recent years, deep learning has emerged as a powerful tool for generating complex data such as images, videos, audio, text, etc. With increasing demand for secure passwords that meet diverse requirements, traditional password generation methods often fall short in terms of randomness, variety, memorability, and complexity. In this work, we propose using generative deep learning techniques to generate high-quality passwords. Our approach is based on training a recurrent neural network (RNN) on large datasets of existing passwords and then fine-tuning it to produce novel passwords meeting specific criteria. We evaluate our model against state-of-the-art password generators and show that it outperforms them significantly in terms of entropy, diversity, and human evaluations. Additionally, we demonstrate the use case of integrating our generator into password managers to provide users with strong, unique, and easily rememberable passwords. Finally, we discuss potential applications of our technology beyond password generation.",1
"Endogeneity bias and instrument variable validation have always been important topics in statistics and econometrics. In the era of big data, such issues typically combine with dimensionality issues and, hence, require even more attention. In this paper, we merge two well-known tools from machine learning and biostatistics---variable selection algorithms and probablistic graphs---to estimate house prices and the corresponding causal structure using 2010 data on Sydney. The estimation uses a 200-gigabyte ultrahigh dimensional database consisting of local school data, GIS information, census data, house characteristics and other socio-economic records. Using ""big data"", we show that it is possible to perform a data-driven instrument selection efficiently and purge out the invalid instruments. Our approach improves the sparsity of variable selection, stability and robustness in the presence of high dimensionality, complicated causal structures and the consequent multicollinearity, and recovers a sparse and intuitive causal structure. The approach also reveals an efficiency and effectiveness in endogeneity detection, instrument validation, weak instrument pruning and the selection of valid instruments. From the perspective of machine learning, the estimation results both align with and confirms the facts of Sydney house market, the classical economic theories and the previous findings of simultaneous equations modeling. Moreover, the estimation results are consistent with and supported by classical econometric tools such as two-stage least square regression and different instrument tests. All the code may be found at \url{https://github.com/isaac2math/solar_graph_learning}.",0
"This paper presents a novel approach to instrumental variable (IV) detection using graph learning methods on large-scale geospatial datasets such as those from Geographic Information Systems (GIS). We show how these techniques can effectively identify IVs that influence complex outcomes like housing prices, which may be confounded by other factors such as neighborhood characteristics or socioeconomic indicators. Our method leverages both the rich spatial and attribute dimensions of GIS-based census data, enabling us to detect meaningful relationships between the variables we wish to estimate. To demonstrate our findings, we present results of experiments conducted on real-world datasets obtained directly from government sources.",1
"3D object classification has attracted appealing attentions in academic researches and industrial applications. However, most existing methods need to access the training data of past 3D object classes when facing the common real-world scenario: new classes of 3D objects arrive in a sequence. Moreover, the performance of advanced approaches degrades dramatically for past learned classes (i.e., catastrophic forgetting), due to the irregular and redundant geometric structures of 3D point cloud data. To address these challenges, we propose a new Incremental 3D Object Learning (i.e., I3DOL) model, which is the first exploration to learn new classes of 3D object continually. Specifically, an adaptive-geometric centroid module is designed to construct discriminative local geometric structures, which can better characterize the irregular point cloud representation for 3D object. Afterwards, to prevent the catastrophic forgetting brought by redundant geometric information, a geometric-aware attention mechanism is developed to quantify the contributions of local geometric structures, and explore unique 3D geometric characteristics with high contributions for classes incremental learning. Meanwhile, a score fairness compensation strategy is proposed to further alleviate the catastrophic forgetting caused by unbalanced data between past and new classes of 3D object, by compensating biased prediction for new classes in the validation phase. Experiments on 3D representative datasets validate the superiority of our I3DOL framework.",0
"In many object recognition tasks, researchers often use pretrained deep neural networks that can take up to three days for training even on advanced hardware. This results in computational constraints and limits on the amount of data available to train models. To address these issues, we propose I3DOL (Incremental 3D Object Learning), a novel method to incrementally learn new objects while retaining previously learned knowledge using a continuous memory module based on differentiable synthetic gradients and meta learning techniques. We evaluate our model on several benchmark datasets, including ShapeNet and ModelNet40, demonstrating improved performance compared to existing methods by reducing catastrophic forgetting and enabling efficient computation and transfer learning across domains. Our contributions include an effective approach for online fine-tuning, which significantly reduces runtime and enables continual adaptation during inference time. Overall, I3DOL has the potential to advance the field of computer vision, allowing for more efficient and adaptive object detection systems.",1
"We study the problem of obtaining accurate policy gradient estimates using a finite number of samples. Monte-Carlo methods have been the default choice for policy gradient estimation, despite suffering from high variance in the gradient estimates. On the other hand, more sample efficient alternatives like Bayesian quadrature methods have received little attention due to their high computational complexity. In this work, we propose deep Bayesian quadrature policy gradient (DBQPG), a computationally efficient high-dimensional generalization of Bayesian quadrature, for policy gradient estimation. We show that DBQPG can substitute Monte-Carlo estimation in policy gradient methods, and demonstrate its effectiveness on a set of continuous control benchmarks. In comparison to Monte-Carlo estimation, DBQPG provides (i) more accurate gradient estimates with a significantly lower variance, (ii) a consistent improvement in the sample complexity and average return for several deep policy gradient algorithms, and, (iii) the uncertainty in gradient estimation that can be incorporated to further improve the performance.",0
"In recent years, there has been significant interest in developing machine learning algorithms that can perform high-quality policy optimization in complex environments. However, many existing methods suffer from limitations such as poor sample efficiency, brittleness to hyperparameter settings, or suboptimal convergence rates. To address these challenges, we propose a novel approach called deep Bayesian quadrature policy optimization (BDQ). BDQ combines the strengths of deep reinforcement learning and Bayesian optimization by leveraging expressive neural network policies, continuous covariance estimation, and adaptive sampling strategies. We show through extensive experiments on benchmark problems that BDQ outperforms state-of-the-art methods in terms of accuracy, robustness, and speed, demonstrating its effectiveness at solving difficult control tasks. Our findings have important implications for the development of intelligent systems capable of making effective decisions in real-world scenarios where both uncertainty and risk must be managed effectively.",1
"Although Transformer models such as Google's BERT and OpenAI's GPT-3 are successful in many natural language processing tasks, training and deploying these models are costly and inefficient.Even if pre-trained models are used, deploying these models still remained a challenge due to their large size. Apart from deployment, these models take higher time during inference restricting user-friendliness. The main bottleneck is self-attention which uses quadratic time and space with respect to the sequence length. In order to reduce the quadratic time complexity of the self-attention mechanism, Linformer by Facebook's AI research team was introduced where they showed that the self-attention mechanism can be approximated by a low-rank matrix and exploiting this finding, a new method for self-attention with linear time and space complexity was proposed by them. In the Linformer, the time complexity depends on the projection mapping dimension which acts as a hyperparameter and affects the performance of the model, tuning this hyperparameter can be time-consuming. In this paper, I proposed an alternative method for self-attention with linear complexity in time and space and is independent of the projection mapping dimension. Since this method works for long sequences this can be used for images as well as audios.",0
"This is an abstract around 200 words long: In recent years, deep learning has revolutionized many fields such as computer vision, natural language processing (NLP), speech recognition, robotics, scientific modeling, medical diagnosis, video games, environmental monitoring, among others [3].  This paper describes an algorithm that achieves high performance on several benchmarks, which use public datasets from real applications, based on image recognition. We evaluate our approach by comparing its performance to other existing systems including baseline models and human performance. Using transfer learning methods such as pre-training and fine tuning we achieve state-of-the-art results across tasks. Our method also benefits from increased interpretability compared to prior work, owing to its simplicity.  The architecture described here could potentially enable new research directions and open up new possibilities for applied machine learning and artificial intelligence. Its applicability to non-image data types provides further opportunities beyond computer vision and NLP into areas where attention mechanisms have historically been difficult to apply due to computational constraints or design difficulties such as speech synthesis and music generation among others.",1
"One significant factor we expect the video representation learning to capture, especially in contrast with the image representation learning, is the object motion. However, we found that in the current mainstream video datasets, some action categories are highly related with the scene where the action happens, making the model tend to degrade to a solution where only the scene information is encoded. For example, a trained model may predict a video as playing football simply because it sees the field, neglecting that the subject is dancing as a cheerleader on the field. This is against our original intention towards the video representation learning and may bring scene bias on different dataset that can not be ignored. In order to tackle this problem, we propose to decouple the scene and the motion (DSM) with two simple operations, so that the model attention towards the motion information is better paid. Specifically, we construct a positive clip and a negative clip for each video. Compared to the original video, the positive/negative is motion-untouched/broken but scene-broken/untouched by Spatial Local Disturbance and Temporal Local Disturbance. Our objective is to pull the positive closer while pushing the negative farther to the original clip in the latent space. In this way, the impact of the scene is weakened while the temporal sensitivity of the network is further enhanced. We conduct experiments on two tasks with various backbones and different pre-training datasets, and find that our method surpass the SOTA methods with a remarkable 8.1% and 8.8% improvement towards action recognition task on the UCF101 and HMDB51 datasets respectively using the same backbone.",0
"This paper proposes a novel method for enhancing unsupervised video representation learning by decoupling the scene and the motion components of each frame. By separating these two elements, we can learn more effective representations that capture both spatial and temporal information. To accomplish this, our approach uses adversarial training to generate realistic motion vectors from a random noise prior. These motion vectors are then used to warp the input frames, creating a new set of transformed videos. We show through extensive experiments on several benchmark datasets that our method outperforms existing state-of-the-art methods in terms of both quantitative metrics and qualitative evaluations. Our results demonstrate the effectiveness of our proposed method in learning powerful video representations without any supervision. Overall, this work has important implications for areas such as action recognition, video classification, and generative modeling.",1
"In this paper, we evaluate dimensionality reduction methods in terms of difficulty in estimating visual information on original images from dimensionally reduced ones. Recently, dimensionality reduction has been receiving attention as the process of not only reducing the number of random variables, but also protecting visual information for privacy-preserving machine learning. For such a reason, difficulty in estimating visual information is discussed. In particular, the random sampling method that was proposed for privacy-preserving machine learning, is compared with typical dimensionality reduction methods. In an image classification experiment, the random sampling method is demonstrated not only to have high difficulty, but also to be comparable to other dimensionality reduction methods, while maintaining the property of spatial information invariant.",0
"This study examines the challenges associated with accurately estimating visual information from randomly sampled images. We investigate how various factors such as image quality, resolution, and content impact our ability to extract meaningful insights from visual data. Our results demonstrate that traditional estimation methods often fail to provide accurate representations of the underlying information present in random images. In addition, we find that human observers tend to overestimate the amount of detail visible in low-quality images, highlighting the need for more advanced estimation techniques. Overall, this research contributes important new insights into the complex relationship between random images and our ability to effectively estimate their content.",1
"Algorithmic fairness has attracted significant attention in recent years, with many quantitative measures suggested for characterizing the fairness of different machine learning algorithms. Despite this interest, the robustness of those fairness measures with respect to an intentional adversarial attack has not been properly addressed. Indeed, most adversarial machine learning has focused on the impact of malicious attacks on the accuracy of the system, without any regard to the system's fairness. We propose new types of data poisoning attacks where an adversary intentionally targets the fairness of a system. Specifically, we propose two families of attacks that target fairness measures. In the anchoring attack, we skew the decision boundary by placing poisoned points near specific target points to bias the outcome. In the influence attack on fairness, we aim to maximize the covariance between the sensitive attributes and the decision outcome and affect the fairness of the model. We conduct extensive experiments that indicate the effectiveness of our proposed attacks.",0
"In recent years, algorithmic bias has become an increasingly prominent issue as machine learning algorithms have been integrated into more aspects of society. While many efforts have been made to address fairness issues through policy changes, algorithm development, and data management practices, these measures may not always be effective at mitigating potential harm caused by discriminatory outcomes. This study examines how attackers can exacerbate existing biases within algorithmic systems through strategically crafted inputs that manipulate the behavior of such systems, resulting in amplified unfair treatment against specific groups. By analyzing real-world examples, we demonstrate the feasibility of such attacks and discuss their implications on society. We conclude by proposing recommendations for future research aimed at better understanding and addressing the problem of fairness attacks in decision making systems powered by machine learning algorithms.",1
"Accuracy and interpretability are two essential properties for a crime prediction model. Because of the adverse effects that the crimes can have on human life, economy and safety, we need a model that can predict future occurrence of crime as accurately as possible so that early steps can be taken to avoid the crime. On the other hand, an interpretable model reveals the reason behind a model's prediction, ensures its transparency and allows us to plan the crime prevention steps accordingly. The key challenge in developing the model is to capture the non-linear spatial dependency and temporal patterns of a specific crime category while keeping the underlying structure of the model interpretable. In this paper, we develop AIST, an Attention-based Interpretable Spatio Temporal Network for crime prediction. AIST models the dynamic spatio-temporal correlations for a crime category based on past crime occurrences, external features (e.g., traffic flow and point of interest (POI) information) and recurring trends of crime. Extensive experiments show the superiority of our model in terms of both accuracy and interpretability using real datasets.",0
"This would probably be difficult since you cannot use any terms specific to deep learning like attention. What kind of terminology could I use? Can I say neural network? Yes, you can definitely mention ""neural network"" but make sure that your audience knows if they don't have knowledge on machine learning, that neural networks are used as part of deep learning models. You should give them some basic background information on deep learning so that they understand how it differs from traditional statistical analysis. If you need further suggestions, please don't hesitate to ask!",1
"Copy-move forgery detection identifies a tampered image by detecting pasted and source regions in the same image. In this paper, we propose a novel two-stage framework specially for copy-move forgery detection. The first stage is a backbone self deep matching network, and the second stage is named as Proposal SuperGlue. In the first stage, atrous convolution and skip matching are incorporated to enrich spatial information and leverage hierarchical features. Spatial attention is built on self-correlation to reinforce the ability to find appearance similar regions. In the second stage, Proposal SuperGlue is proposed to remove false-alarmed regions and remedy incomplete regions. Specifically, a proposal selection strategy is designed to enclose highly suspected regions based on proposal generation and backbone score maps. Then, pairwise matching is conducted among candidate proposals by deep learning based keypoint extraction and matching, i.e., SuperPoint and SuperGlue. Integrated score map generation and refinement methods are designed to integrate results of both stages and obtain optimized results. Our two-stage framework unifies end-to-end deep matching and keypoint matching by obtaining highly suspected proposals, and opens a new gate for deep learning research in copy-move forgery detection. Experiments on publicly available datasets demonstrate the effectiveness of our two-stage framework.",0
"In recent years, there has been significant interest in developing effective techniques for detecting copy-move forgery in digital images. This type of forgery involves copying and pasting parts of an image into another image in order to deceive viewers into thinking that something appears different than it actually is. One popular approach to detecting these types of manipulations is through the use of deep learning models trained on large datasets of authentic and tampered images. However, existing approaches have limited accuracy and often produce false alarms due to their reliance on handcrafted features which may fail to capture subtle differences between genuine and tampered regions. In this work, we propose a novel two-stage copy-move forgery detection method that addresses these limitations by leveraging self deep matching and proposal superglue components. Our proposed approach outperforms state-of-the-art methods in terms of both precision and recall while maintaining fast inference speed and low memory usage. By combining multiple complementary components, our model can effectively identify complex tampering scenarios such as multiregion tampering, high compression ratio tampering and smooth tampering. Additionally, our model utilizes an efficient parallel architecture enabling real-time processing even with resource constrained hardware. We believe that our contributions represent an important step towards advancing the field of forgery detection and improving public trust in online media content verification.",1
"To mitigate the issue of minimal intrinsic features for pure data-driven methods, in this paper, we propose a novel model-driven deep network for infrared small target detection, which combines discriminative networks and conventional model-driven methods to make use of both labeled data and the domain knowledge. By designing a feature map cyclic shift scheme, we modularize a conventional local contrast measure method as a depth-wise parameterless nonlinear feature refinement layer in an end-to-end network, which encodes relatively long-range contextual interactions with clear physical interpretability. To highlight and preserve the small target features, we also exploit a bottom-up attentional modulation integrating the smaller scale subtle details of low-level features into high-level features of deeper layers. We conduct detailed ablation studies with varying network depths to empirically verify the effectiveness and efficiency of the design of each component in our network architecture. We also compare the performance of our network against other model-driven methods and deep networks on the open SIRST dataset as well. The results suggest that our network yields a performance boost over its competitors. Our code, trained models, and results are available online.",0
This paper presents a novel approach for infrared small target detection using attentional local contrast networks (ALCon). ALCon uses attention mechanisms to selectively focus on discriminative regions within images while maintaining spatial resolution. We train our model on synthetic data generated from real thermal sensor imagery and validate it on two publicly available datasets. Our results show that ALCon outperforms existing methods by achieving state-of-the-art accuracy with high robustness against noise and occlusions. Further analysis reveals that ALCon effectively learns region-level representations that capture subtle differences between targets and clutter. Our work demonstrates the effectiveness of ALCon as a powerful tool for infrared small target detection in complex environments.,1
"A molecular and cellular understanding of how SARS-CoV-2 variably infects and causes severe COVID-19 remains a bottleneck in developing interventions to end the pandemic. We sought to use deep learning to study the biology of SARS-CoV-2 infection and COVID-19 severity by identifying transcriptomic patterns and cell types associated with SARS-CoV-2 infection and COVID-19 severity. To do this, we developed a new approach to generating self-supervised edge features. We propose a model that builds on Graph Attention Networks (GAT), creates edge features using self-supervised learning, and ingests these edge features via a Set Transformer. This model achieves significant improvements in predicting the disease state of individual cells, given their transcriptome. We apply our model to single-cell RNA sequencing datasets of SARS-CoV-2 infected lung organoids and bronchoalveolar lavage fluid samples of patients with COVID-19, achieving state-of-the-art performance on both datasets with our model. We then borrow from the field of explainable AI (XAI) to identify the features (genes) and cell types that discriminate bystander vs. infected cells across time and moderate vs. severe COVID-19 disease. To the best of our knowledge, this represents the first application of deep learning to identifying the molecular and cellular determinants of SARS-CoV-2 infection and COVID-19 severity using single-cell omics data.",0
"We propose using self-supervised edge features (SEF) to learn from pre-existing medical data on patients who have been infected by the novel coronavirus. Specifically, we aim to identify patterns that can predict disease severity levels such as mild vs severe disease progression. To achieve this goal, we use graph neural networks to process the data and discover latent structures within the patient population. Our work contributes to a better understanding of how SEF methods can help inform treatment decisions and improve health outcomes for individuals affected by COVID-19.  Our approach leverages the abundance of existing hospital records containing patient history, symptoms, and lab results. This enables us to train our model without relying on expensive new studies, reducing costs while potentially increasing impact given timely dissemination of findings. Despite the complexities introduced due to missing data and differences across hospitals, our experiments indicate that learning discriminative node representations by minimizing reconstruction errors effectively captures important clinical insights about the virusâ€™ effects. By doing so, we show improved performance over traditional unsupervised feature extraction techniques, suggesting SEF could become a valuable tool in future research combating pandemics driven by emerging pathogens. Ultimately, these encouraging results set the stage for further refining our methods and broadening their application domains beyond medicine, where robust generalization remains crucial to driving meaningful progress via AI technology.",1
"Distributed parallel stochastic gradient descent algorithms are workhorses for large scale machine learning tasks. Among them, local stochastic gradient descent (Local SGD) has attracted significant attention due to its low communication complexity. Previous studies prove that the communication complexity of Local SGD with a fixed or an adaptive communication period is in the order of $O (N^{\frac{3}{2}} T^{\frac{1}{2}})$ and $O (N^{\frac{3}{4}} T^{\frac{3}{4}})$ when the data distributions on clients are identical (IID) or otherwise (Non-IID), where $N$ is the number of clients and $T$ is the number of iterations. In this paper, to accelerate the convergence by reducing the communication complexity, we propose \textit{ST}agewise \textit{L}ocal \textit{SGD} (STL-SGD), which increases the communication period gradually along with decreasing learning rate. We prove that STL-SGD can keep the same convergence rate and linear speedup as mini-batch SGD. In addition, as the benefit of increasing the communication period, when the objective is strongly convex or satisfies the Polyak-\L ojasiewicz condition, the communication complexity of STL-SGD is $O (N \log{T})$ and $O (N^{\frac{1}{2}} T^{\frac{1}{2}})$ for the IID case and the Non-IID case respectively, achieving significant improvements over Local SGD. Experiments on both convex and non-convex problems demonstrate the superior performance of STL-SGD.",0
"Effective distributed training has become increasingly important due to the growing size of datasets and the need for faster model convergence. In this work, we propose STL-SGD (Stochastic Training with Layers, Staged Gradient Descent), a novel approach that accelerates local Stochastic Gradient Descent (local SGD) by introducing stagewise communication periods within each iteration loop. \end{code}",1
"This paper introduces the Sequential Monte Carlo Transformer, an original approach that naturally captures the observations distribution in a transformer architecture. The keys, queries, values and attention vectors of the network are considered as the unobserved stochastic states of its hidden structure. This generative model is such that at each time step the received observation is a random function of its past states in a given attention window. In this general state-space setting, we use Sequential Monte Carlo methods to approximate the posterior distributions of the states given the observations, and to estimate the gradient of the log-likelihood. We hence propose a generative model giving a predictive distribution, instead of a single-point estimate.",0
"This research paper proposes a novel approach for sequence prediction called the ""Monte Carlo Transformer"", which combines the power of deep learning and traditional methods like Markov Chain Monte Carlo (MCMC) simulations. The main contribution of this paper lies in developing a new variant of the attention mechanism commonly used in transformer models that incorporates randomness through sampling techniques inspired by MCMC simulations. By doing so, we introduce stochasticity into the self-attention process, allowing the model to explore a broader range of solutions and make predictions based on uncertain data distributions. We evaluate our proposed method on several benchmark datasets for natural language processing tasks such as sentiment analysis, machine translation, and question answering. Our experiments show significant improvements over state-of-the-art results, demonstrating the effectiveness of the Monte Carlo Transformer in dealing with uncertainty and ambiguity present in real-world sequential data. Overall, this work paves the way towards more robust and reliable artificial intelligence systems capable of making decisions under uncertainty.",1
"This study assesses the efficiency of several popular machine learning approaches in the prediction of molecular binding affinity: CatBoost, Graph Attention Neural Network, and Bidirectional Encoder Representations from Transformers. The models were trained to predict binding affinities in terms of inhibition constants $K_i$ for pairs of proteins and small organic molecules. First two approaches use thoroughly selected physico-chemical features, while the third one is based on textual molecular representations - it is one of the first attempts to apply Transformer-based predictors for the binding affinity. We also discuss the visualization of attention layers within the Transformer approach in order to highlight the molecular sites responsible for interactions. All approaches are free from atomic spatial coordinates thus avoiding bias from known structures and being able to generalize for compounds with unknown conformations. The achieved accuracy for all suggested approaches prove their potential in high throughput screening.",0
"This paper describes how high throughput screens can benefit from machine learning models trained on large datasets of drug candidates that have been evaluated previously using expensive low-throughput assays. Using only data that has already been collected reduces costs enormously compared to alternative approaches. We describe how these models could be used in each step of drug discovery: lead identification, hit expansion, lead optimization, efficacy/safety evaluation, and clinical candidate prediction. In many cases they enable parallelization allowing multiple hypotheses to be tested simultaneously, speeding up the rate at which drugs can be discovered. They often need larger quantities of data than humans currently collect â€“ requiring new technologies and research goals. Machine learned scores for molecules, chemical reactions, and synthetic routes would provide valuable filtering criteria, shortening assay lists before human laboratories evaluate them against a target. Scoring entire libraries enabling selection and de novo design without human mediation and prior domain knowledge. Ultimately, human intuition might still be required but by focusing expertsâ€™ attention to where their insight will actually matter most this dramatically increases efficiency of the overall process saving money and time until effective medicines reach patients.",1
"Arbitrary-oriented objects widely appear in natural scenes, aerial photographs, remote sensing images, etc., thus arbitrary-oriented object detection has received considerable attention. Many current rotation detectors use plenty of anchors with different orientations to achieve spatial alignment with ground truth boxes, then Intersection-over-Union (IoU) is applied to sample the positive and negative candidates for training. However, we observe that the selected positive anchors cannot always ensure accurate detections after regression, while some negative samples can achieve accurate localization. It indicates that the quality assessment of anchors through IoU is not appropriate, and this further lead to inconsistency between classification confidence and localization accuracy. In this paper, we propose a dynamic anchor learning (DAL) method, which utilizes the newly defined matching degree to comprehensively evaluate the localization potential of the anchors and carry out a more efficient label assignment process. In this way, the detector can dynamically select high-quality anchors to achieve accurate object detection, and the divergence between classification and regression will be alleviated. With the newly introduced DAL, we achieve superior detection performance for arbitrary-oriented objects with only a few horizontal preset anchors. Experimental results on three remote sensing datasets HRSC2016, DOTA, UCAS-AOD as well as a scene text dataset ICDAR 2015 show that our method achieves substantial improvement compared with the baseline model. Besides, our approach is also universal for object detection using horizontal bound box. The code and models are available at https://github.com/ming71/DAL.",0
"Object detection has been one of the fundamental problems in computer vision since the early days of artificial intelligence (AI). Recently, there has been significant progress in developing deep learning models that can accurately detect objects in images and videos. However, most existing object detection algorithms assume that objects are oriented horizontally or vertically within the image frame. This assumption may not hold true in many real-world scenarios where objects can appear at arbitrary angles.  To address this limitation, we propose a new approach called dynamic anchor learning for arbitrary-oriented object detection. Our method uses a novel architecture consisting of multiple parallel feature pyramid networks (FPNs) connected through a shared backbone network. Each FPN outputs a set of anchors that represent possible bounding boxes containing objects, which are then used to generate proposals for object detection. To handle objects at arbitrary orientations, our algorithm learns adaptive scale factors and aspect ratios for each proposal based on the orientation of the corresponding box.  We evaluated our proposed method using popular benchmark datasets such as PASCAL VOC and MS COCO. Experimental results showed that our method outperforms state-of-the-art approaches by a significant margin, demonstrating its effectiveness in handling objects at arbitrary angles. In addition, our model achieves high accuracy even under extreme variations in scales, positions, lighting conditions, and background clutter, making it well suited for real-world applications. Overall, our work represents an important step towards more advanced AI capabilities in understanding complex visual scenes and improving the robustness of object detection algorithms.",1
"Synthetic images are one of the most promising solutions to avoid high costs associated with generating annotated datasets to train supervised convolutional neural networks (CNN). However, to allow networks to generalize knowledge from synthetic to real images, domain adaptation methods are necessary. This paper implements unsupervised domain adaptation (UDA) methods on an anchorless object detector. Given their good performance, anchorless detectors are increasingly attracting attention in the field of object detection. While their results are comparable to the well-established anchor-based methods, anchorless detectors are considerably faster. In our work, we use CenterNet, one of the most recent anchorless architectures, for a domain adaptation problem involving synthetic images. Taking advantage of the architecture of anchorless detectors, we propose to adjust two UDA methods, viz., entropy minimization and maximum squares loss, originally developed for segmentation, to object detection. Our results show that the proposed UDA methods can increase the mAPfrom61 %to69 %with respect to direct transfer on the considered anchorless detector. The code is available: https://github.com/scheckmedia/centernet-uda.",0
"This paper presents a novel approach to unsupervised domain adaptation for anchorless object detection using synthetic images generated by computer graphics algorithms. Traditional methods relying on explicit labeled data from both domains can often require impractical amounts of annotation time and resources. We present two complementary techniques that enable learning from synthetic data alone: First, we learn realistic photometry which ensures the brightness and color of real scenes match the virtual training images. Secondly, we introduce spatial attention modules into existing networks, allowing us to focus on image regions most impacted by differences in reality (e.g., changes in shading). Experimentally, we demonstrate significant improvement over related work, validating our assumptions through quantitative evaluation on three benchmark datasets. Our system outperforms previous state-of-the-art approaches trained solely on synthetic imagery while matching performance levels set by models fine-tuned with large quantities of annotated real-world data.",1
"In recent years, the field of recommendation systems has attracted increasing attention to developing predictive models that provide explanations of why an item is recommended to a user. The explanations can be either obtained by post-hoc diagnostics after fitting a relatively complex model or embedded into an intrinsically interpretable model. In this paper, we propose the explainable recommendation systems based on a generalized additive model with manifest and latent interactions (GAMMLI). This model architecture is intrinsically interpretable, as it additively consists of the user and item main effects, the manifest user-item interactions based on observed features, and the latent interaction effects from residuals. Unlike conventional collaborative filtering methods, the group effect of users and items are considered in GAMMLI. It is beneficial for enhancing the model interpretability, and can also facilitate the cold-start recommendation problem. A new Python package GAMMLI is developed for efficient model training and visualized interpretation of the results. By numerical experiments based on simulation data and real-world cases, the proposed method is shown to have advantages in both predictive performance and explainable recommendation.",0
"In recent years there has been a growing interest in explainability in recommendation systems due to concerns about transparency, accountability and trustworthiness (Ribeiro et al., 2016). Explanations could provide insights into how recommenders make their decisions and potentially allow users to challenge unfair treatment caused by biases, incorrect data or unreliable models. Several approaches have been proposed such as feature importance methods (Athias et al., 2018) but these lack expressive power to capture complex interactions like synergy effects that occur among features. To overcome this limitation we propose using generalized additive models (GAMs) to model recommender scores directly from input features without deriving latent factors first. GAM can handle complex interactions including nonlinearity, manifest interactions, and latent interactions modeled via random smooth functions (Hastie & Tibshirani, 1990; Wood, 2006). By estimating separate models for each user group our approach addresses cold-start issues common in collaborative filtering (CF) based algorithms where new users receive poor recommendations due to sparse interaction histories. We evaluate our method on two benchmark datasets against state-of-the-art baselines, demonstrating competitive performance and generating interpretable explanations for recommendation scores. Our results show improved accuracy over CF and other explanation methods while identifying important interactions responsible for generating ratings predictions across different user groups. Overall, our work shows promise in improving the field of recommendation system explainability through novel modeling techniques grounded in statistical learning theory and provides opportunities for future research directions i",1
"Neural architecture search has attracted wide attentions in both academia and industry. To accelerate it, researchers proposed weight-sharing methods which first train a super-network to reuse computation among different operators, from which exponentially many sub-networks can be sampled and efficiently evaluated. These methods enjoy great advantages in terms of computational costs, but the sampled sub-networks are not guaranteed to be estimated precisely unless an individual training process is taken. This paper owes such inaccuracy to the inevitable mismatch between assembled network layers, so that there is a random error term added to each estimation. We alleviate this issue by training a graph convolutional network to fit the performance of sampled sub-networks so that the impact of random errors becomes minimal. With this strategy, we achieve a higher rank correlation coefficient in the selected set of candidates, which consequently leads to better performance of the final architecture. In addition, our approach also enjoys the flexibility of being used under different hardware constraints, since the graph convolutional network has provided an efficient lookup table of the performance of architectures in the entire search space.",0
"In recent years, Neural Architecture Search (NAS) has become increasingly popular as a method for automatically designing neural networks that achieve state-of-the art results on specific tasks. However, the search space of most existing approaches remains quite large, leading to high computational costs and longer training times. This work proposes an alternative approach based on weight sharing and graph convolutional networks (GCNs). By decomposing a standard NAS cell into a set of shared operations which can be reused across cells, we drastically reduce both the number of unique architectures considered by NAS and the amount of computation required at inference time. Our experimental evaluation demonstrates significant improvements over traditional non-weight sharing methods. Compared against several strong baselines, our proposed algorithm achieves better validation accuracy while requiring significantly fewer GPU days to train. Additionally, it generates more diverse sets of architectures than prior works, improving the likelihood of finding effective models for new, unseen datasets. Overall, these findings suggest that GCN-based weight-shared NAS represents a promising direction towards enabling efficient discovery of even stronger neural network architectures.",1
"In the last few years, Automated Machine Learning (AutoML) has gained much attention. With that said, the question arises whether AutoML can outperform results achieved by human data scientists. This paper compares four AutoML frameworks on 12 different popular datasets from OpenML; six of them supervised classification tasks and the other six supervised regression ones. Additionally, we consider a real-life dataset from one of our recent projects. The results show that the automated frameworks perform better or equal than the machine learning community in 7 out of 12 OpenML tasks.",0
"Machine learning has revolutionized many fields by allowing computers to make predictions and decisions based on data inputs. However, developing high-quality machine learning models still requires significant time and expertise from human practitioners. Recently, there have been efforts to automate parts of the machine learning process through methods such as AutoML (Automatic Machine Learning). In particular, some studies claim that AutoML can achieve better performance than even experts at building highly accurate models. This paper evaluates these claims by testing popular open source datasets using benchmarking software called AutoML Benchmark. Our results show that while AutoML may produce comparable results to those made by experts, it falls short of consistently surpassing them. We conclude that while AutoML presents exciting opportunities for improving efficiency and accessibility in machine learning, further research is required to fully realize its potential.",1
"Object counting aims to estimate the number of objects in images. The leading counting approaches focus on the single category counting task and achieve impressive performance. Note that there are multiple categories of objects in real scenes. Multi-class object counting expands the scope of application of object counting task. The multi-target detection task can achieve multi-class object counting in some scenarios. However, it requires the dataset annotated with bounding boxes. Compared with the point annotations in mainstream object counting issues, the coordinate box-level annotations are more difficult to obtain. In this paper, we propose a simple yet efficient counting network based on point-level annotations. Specifically, we first change the traditional output channel from one to the number of categories to achieve multiclass counting. Since all categories of objects use the same feature extractor in our proposed framework, their features will interfere mutually in the shared feature space. We further design a multi-mask structure to suppress harmful interaction among objects. Extensive experiments on the challenging benchmarks illustrate that the proposed method achieves state-of-the-art counting performance.",0
"This work presents a novel approach to multi-class object counting using deep convolutional neural networks (ConvNets). The proposed method introduces a dilated scale-aware attention mechanism that improves the accuracy and robustness of the model by selectively focusing on relevant image regions based on their spatial scales. Our architecture outperforms state-of-the-art methods on several challenging benchmarks, demonstrating the effectiveness of our approach. We believe this work represents an important step towards realizing accurate and efficient object counting in complex scenes.",1
"In this paper, we propose a multiple-domain model for producing a custom-size furniture layout in the interior scene. This model is aimed to support professional interior designers to produce interior decoration solutions with custom-size furniture more quickly. The proposed model combines a deep layout module, a domain attention module, a dimensional domain transfer module, and a custom-size module in the end-end training. Compared with the prior work on scene synthesis, our proposed model enhances the ability of auto-layout of custom-size furniture in the interior room. We conduct our experiments on a real-world interior layout dataset that contains $710,700$ designs from professional designers. Our numerical results demonstrate that the proposed model yields higher-quality layouts of custom-size furniture in comparison with the state-of-art model.",0
"As humans, we have always been fascinated by technology that can revolutionize our daily lives. Artificial intelligence (AI) has come a long way since its introduction and now it seems like there is no task that cannot be automated using some form of intelligent software. This article focuses on exploring how custom-size furniture can be designed using cutting edge techniques developed in deep learning applications from multiple domains. Through a multi-disciplinary approach that integrates computer vision models, natural language processing systems, expert knowledge bases, and realtime human feedback loops, this research demonstrates that generating high quality furniture layouts can indeed become a fully automatic process without any reliance on trained designers or CAD specialists. Our methods employ both transfer and zero shot learning techniques which enable rapid adaptation of existing training sets allowing them to perform better than previous state-of-the art algorithms. With results showing significant improvements over baseline models, it is clear that these techniques hold promise in enabling new ways for consumers to generate their own personalized interiors quickly, easily and at low cost compared to traditional bespoke solutions. Ultimately, the goal is to allow anyone regardless of their technical background to create stunning interior designs within minutes based solely on pictures they upload or text descriptions of desired styles. While further experimentation and validation is required before such technologies can reach widespread adoption, the current results provide strong evidence that this futuristic dream may well turn out to be feasible sooner rather than later!",1
"As the scale of object detection dataset is smaller than that of image recognition dataset ImageNet, transfer learning has become a basic training method for deep learning object detection models, which will pretrain the backbone network of object detection model on ImageNet dataset to extract features for classification and localization subtasks. However, the classification task focuses on the salient region features of object, while the location task focuses on the edge features of object, so there is certain deviation between the features extracted by pretrained backbone network and the features used for localization task. In order to solve this problem, a decoupled self attention(DSA) module is proposed for one stage object detection models in this paper. DSA includes two decoupled self-attention branches, so it can extract appropriate features for different tasks. It is located between FPN and head networks of subtasks, so it is used to extract global features based on FPN fused features for different tasks independently. Although the network of DSA module is simple, but it can effectively improve the performance of object detection, also it can be easily embedded in many detection models. Our experiments are based on the representative one-stage detection model RetinaNet. In COCO dataset, when ResNet50 and ResNet101 are used as backbone networks, the detection performances can be increased by 0.4% AP and 0.5% AP respectively. When DSA module and object confidence task are applied in RetinaNet together, the detection performances based on ResNet50 and ResNet101 can be increased by 1.0% AP and 1.4% AP respectively. The experiment results show the effectiveness of DSA module. Code is at: https://github.com/chenzuge1/DSANet.git.",0
"In recent years, object detection has become one of the most popular computer vision tasks due to its wide range of applications such as self driving cars, facial recognition, and image captioning. To achieve state-of-the art performance on this task, researchers have employed Convolutional Neural Networks (CNN) combined with Region Proposal Networks (RPN). However, these models suffer from limited contextual reasoning which can lead to suboptimal predictions. This is where attention mechanisms come into play by allowing networks to weigh different spatial locations in feature maps. In particular, self attention mechanism stands out because of its computational efficiency and superior results compared to other alternatives. Motivated by these findings, we propose a novel architecture called Decoupled Self Attention Object Detector (DSAOD), that extends the concept of decoupling from generative models like T5/BART to CNN based architectures. Our model achieves competitive accuracy while significantly reducing computational costs. Furthermore, our approach introduces several new contributions including removing unnecessary features before applying self attention layers and using an effective method to handle variable aspect ratios of objects. We demonstrate the effectiveness of our proposal through extensive experiments and ablation studies on popular benchmark datasets.",1
"Electrocardiogram (ECG) is the electrical measurement of cardiac activity, whereas Photoplethysmogram (PPG) is the optical measurement of volumetric changes in blood circulation. While both signals are used for heart rate monitoring, from a medical perspective, ECG is more useful as it carries additional cardiac information. Despite many attempts toward incorporating ECG sensing in smartwatches or similar wearable devices for continuous and reliable cardiac monitoring, PPG sensors are the main feasible sensing solution available. In order to tackle this problem, we propose CardioGAN, an adversarial model which takes PPG as input and generates ECG as output. The proposed network utilizes an attention-based generator to learn local salient features, as well as dual discriminators to preserve the integrity of generated data in both time and frequency domains. Our experiments show that the ECG generated by CardioGAN provides more reliable heart rate measurements compared to the original input PPG, reducing the error from 9.74 beats per minute (measured from the PPG) to 2.89 (measured from the generated ECG).",0
"Hereâ€™s a possible draft for you to consider:  ---  Electrocardiogram (ECG) signal represents one of the most common ways to diagnose cardiac diseases in clinical practice. However, obtaining accurate ECG signals requires specialized equipment, which can be expensive and difficult to use outside of professional settings. Photoplethysmography (PPG), on the other hand, is simpler and cheaper but has lower accuracy than ECG. This paper presents an innovative model that uses generative adversarial networks (GANs) to synthesize high-quality ECG waveforms directly from low-resolution PPG signals. By utilizing dual discriminatorsâ€”one trained on both domain distributions, another only on generated outputsâ€”the GAN framework ensures better generalization across domains while preserving critical details from each modality. Experiments show improved synthesis quality over traditional methods, paving the way for enhanced diagnostic capabilities at reduced cost through alternative sensors like PPG wearables and smartphones. Ultimately, this work contributes to bridging medical research, technology, engineering, and science communities by developing advanced computer vision systems capable of generating vital health metrics from ubiquitous data sources.",1
"Existing inpainting methods have achieved promising performance in recovering defected images of specific scenes. However, filling holes involving multiple semantic categories remains challenging due to the obscure semantic boundaries and the mixture of different semantic textures. In this paper, we introduce coherence priors between the semantics and textures which make it possible to concentrate on completing separate textures in a semantic-wise manner. Specifically, we adopt a multi-scale joint optimization framework to first model the coherence priors and then accordingly interleavingly optimize image inpainting and semantic segmentation in a coarse-to-fine manner. A Semantic-Wise Attention Propagation (SWAP) module is devised to refine completed image textures across scales by exploring non-local semantic coherence, which effectively mitigates mix-up of textures. We also propose two coherence losses to constrain the consistency between the semantics and the inpainted image in terms of the overall structure and detailed textures. Experimental results demonstrate the superiority of our proposed method for challenging cases with complex holes.",0
"""Image inpainting involves restoring missing pixels in an image using algorithms that make use of surrounding context. Previous methods have achieved good results but often fail to produce coherent images due to differences in scale or texture. Our proposed method addresses these issues by leveraging both semantic and textural prior knowledge to guide the inpainting process. We employ deep neural networks trained on large datasets to extract meaningful features from the input image and use them as constraints during inpainting. Our approach can handle different scales and textures effectively and produces more visually pleasing and coherent results compared to traditional approaches.""",1
"In this paper, we propose an efficient human pose estimation network (DANet) by learning deeply aggregated representations. Most existing models explore multi-scale information mainly from features with different spatial sizes. Powerful multi-scale representations usually rely on the cascaded pyramid framework. This framework largely boosts the performance but in the meanwhile makes networks very deep and complex. Instead, we focus on exploiting multi-scale information from layers with different receptive-field sizes and then making full of use this information by improving the fusion method. Specifically, we propose an orthogonal attention block (OAB) and a second-order fusion unit (SFU). The OAB learns multi-scale information from different layers and enhances them by encouraging them to be diverse. The SFU adaptively selects and fuses diverse multi-scale information and suppress the redundant ones. This could maximize the effective information in final fused representations. With the help of OAB and SFU, our single pyramid network may be able to generate deeply aggregated representations that contain even richer multi-scale information and have a larger representing capacity than that of cascaded networks. Thus, our networks could achieve comparable or even better accuracy with much smaller model complexity. Specifically, our \mbox{DANet-72} achieves $70.5$ in AP score on COCO test-dev set with only $1.0G$ FLOPs. Its speed on a CPU platform achieves $58$ Persons-Per-Second~(PPS).",0
"This paper presents a novel approach to human pose estimation that leverages deep learning techniques to learn highly aggregated representations of human bodies in images. Unlike traditional approaches that rely on manually crafted features and heuristics, our method automatically learns features directly from image data. We train a convolutional neural network (CNN) using large amounts of annotated image data, where each pixel value corresponds to one joint position in the body model. Our network architecture employs multiple layers of convolutions followed by fully connected layers to predict joint positions at different levels of abstraction. By doing so, we can effectively capture both local details and global context information in human poses. To further improve accuracy and computational efficiency, we propose several new architectural components such as a spatial pyramid pooling module, which extracts features across multiple scales, and an attention mechanism, which selectively combines information from different feature maps based on their importance. Our experiments demonstrate that our method outperforms state-of-the-art methods on challenging benchmark datasets while requiring less computation time. Overall, we showcase how learning deeply aggregated representations can lead to efficient and accurate human pose estimation algorithms.",1
"Ensemble learning consistently improves the performance of multi-class classification through aggregating a series of base classifiers. To this end, data-independent ensemble methods like Error Correcting Output Codes (ECOC) attract increasing attention due to its easiness of implementation and parallelization. Specifically, traditional ECOCs and its general extension N-ary ECOC decompose the original multi-class classification problem into a series of independent simpler classification subproblems. Unfortunately, integrating ECOCs, especially N-ary ECOC with deep neural networks, termed as deep N-ary ECOC, is not straightforward and yet fully exploited in the literature, due to the high expense of training base learners. To facilitate the training of N-ary ECOC with deep learning base learners, we further propose three different variants of parameter sharing architectures for deep N-ary ECOC. To verify the generalization ability of deep N-ary ECOC, we conduct experiments by varying the backbone with different deep neural network architectures for both image and text classification tasks. Furthermore, extensive ablation studies on deep N-ary ECOC show its superior performance over other deep data-independent ensemble methods.",0
"In recent years, deep learning has revolutionized many fields by enabling machines to perform complex tasks that previously required human intervention. However, as neural networks become larger and more powerful, they often suffer from instability during training due to the vanishing gradient problem. To address this issue, researchers have developed techniques such as weight decay and dropout regularization which can stabilize training but may require tuning hyperparameters to achieve good results. In this work, we propose using n-ary error correcting output codes (ECOCs) to regularize deep neural networks without relying on additional hyperparameters. By adding noise to the target values of each layer during backpropagation, ECOCs introduce nonlinearity into the training process and promote sparsity in activation maps. This leads to better generalization performance across multiple benchmark datasets compared to standard binary cross entropy loss functions. Our approach offers a simple yet effective solution for improving stability and accuracy in deep learning models without sacrificing computational efficiency. Furthermore, our experiments demonstrate that combining ECOCs with other popular regularizers such as dropout can lead to even stronger performances. Overall, our findings provide new insights into the role of error correction mechanisms in deep learning systems and open up opportunities for developing next generation artificial intelligence algorithms.",1
"Self-supervised depth estimation has shown its great effectiveness in producing high quality depth maps given only image sequences as input. However, its performance usually drops when estimating on border areas or objects with thin structures due to the limited depth representation ability. In this paper, we address this problem by proposing a semantic-guided depth representation enhancement method, which promotes both local and global depth feature representations by leveraging rich contextual information. In stead of a single depth network as used in conventional paradigms, we propose an extra semantic segmentation branch to offer extra contextual features for depth estimation. Based on this framework, we enhance the local feature representation by sampling and feeding the point-based features that locate on the semantic edges to an individual Semantic-guided Edge Enhancement module (SEEM), which is specifically designed for promoting depth estimation on the challenging semantic borders. Then, we improve the global feature representation by proposing a semantic-guided multi-level attention mechanism, which enhances the semantic and depth features by exploring pixel-wise correlations in the multi-level depth decoding scheme. Extensive experiments validate the distinct superiority of our method in capturing highly accurate depth on the challenging image areas such as semantic category borders and thin objects. Both quantitative and qualitative experiments on KITTI show that our method outperforms the state-of-the-art methods.",0
"In recent years, self-supervised monocular depth estimation has emerged as a promising approach for recovering depth maps from single images. This technique leverages large amounts of unlabeled data and pretext tasks to learn the underlying patterns present in natural scenes without explicit supervision. However, these models still face significant challenges, such as predicting less accurate depth maps compared to their supervised counterparts, especially for objects that exhibit complex appearances or shapes. To address these issues, we propose a semantic-guided representation enhancement framework that utilizes high-level prior knowledge provided by semantic labels. Our method improves the representations learned by the model and leads to more accurate depth predictions. We achieve this by introducing a novel regularization loss based on semantic segmentation error propagated through the estimated depth map. Experiments conducted on several benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art results while outperforming other self-supervised methods. Additionally, ablation studies verify the importance of each component in our framework. Overall, this work represents a step forward in advancing the field of self-supervised monocular depth estimation and demonstrates the potential benefits of incorporating semantics into the learning process.",1
"Active learning frameworks offer efficient data annotation without remarkable accuracy degradation. In other words, active learning starts training the model with a small size of labeled data while exploring the space of unlabeled data in order to select most informative samples to be labeled. Generally speaking, representing the uncertainty is crucial in any active learning framework, however, deep learning methods are not capable of either representing or manipulating model uncertainty. On the other hand, from the real world application perspective, uncertainty representation is getting more and more attention in the machine learning community. Deep Bayesian active learning frameworks and generally any Bayesian active learning settings, provide practical consideration in the model which allows training with small data while representing the model uncertainty for further efficient training. In this paper, we briefly survey recent advances in Bayesian active learning and in particular deep Bayesian active learning frameworks.",0
"In recent years, deep learning has emerged as one of the most powerful machine learning techniques due to its ability to model complex data distributions using multi-layer neural networks. However, training these models can often require large amounts of data, which may not always be available. This is where active learning comes into play - a technique that enables the model to actively request additional data points from the user in order to improve performance. By combining deep learning with active learning, researchers have been able to develop algorithms that can achieve state-of-the-art results while drastically reducing the amount of labeled data required. This survey presents some of the latest advancements in this field, including methods such as semi-supervised learning, generative models, and uncertainty sampling. The potential applications of these methods range from computer vision and natural language processing to autonomous systems and medical diagnosis. Overall, this work provides a comprehensive review of the current landscape of deep Bayesian active learning, highlighting future directions and opportunities for further research.",1
"Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set. We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. We open-sourced our code.",0
"This paper presents a novel approach to improve post-training neural quantization by jointly considering layer-wise calibration and integer programming techniques. Inspired by recent works on neural architecture search (NAS) that optimize network designs for deployment on resource constrained devices, our method addresses limitations of existing quantization algorithms which primarily focus on globally scaling weights without adapting them to device constraints. Our proposed method progressively fine tunes each layer independently according to device requirements while ensuring that resulting models remain within memory boundaries. With extensive experiments across multiple datasets and hardware configurations, we demonstrate consistent improvements over state-of-the-art methods, achieving up to 26% higher accuracy while meeting strict computational limits imposed by edge computing environments. By effectively trading off accuracy and efficiency objectives, our work represents a step forward towards developing robust post-training neural quantization strategies applicable in practice.",1
"This work introduces a novel DeepFake detection framework based on physiological measurement. In particular, we consider information related to the heart rate using remote photoplethysmography (rPPG). rPPG methods analyze video sequences looking for subtle color changes in the human skin, revealing the presence of human blood under the tissues. In this work we investigate to what extent rPPG is useful for the detection of DeepFake videos.   The proposed fake detector named DeepFakesON-Phys uses a Convolutional Attention Network (CAN), which extracts spatial and temporal information from video frames, analyzing and combining both sources to better detect fake videos. This detection approach has been experimentally evaluated using the latest public databases in the field: Celeb-DF and DFDC. The results achieved, above 98% AUC (Area Under the Curve) on both databases, outperform the state of the art and prove the success of fake detectors based on physiological measurement to detect the latest DeepFake videos.",0
"This paper presents a new method for detecting deepfakes using heart rate estimation. With the recent proliferation of deepfake videos, there is an increasing need for reliable methods to identify these manipulated media assets. Our approach uses physiological signals such as heart rate variability (HRV) to determine if a video is authentic or not. By analyzing changes in HRV during the viewing of a video, we can detect differences in autonomic nervous system responses that indicate fakery. We evaluate our proposed method using a dataset containing both real and deepfake videos and achieve promising results. Overall, our work shows the potential of incorporating biometric data into deepfake detection solutions.",1
"This paper proposes a learning-based video codec, specifically used for Challenge on Learned Image Compression (CLIC, CVPRWorkshop) 2020 P-frame coding. More specifically, we designed a compressor network with Refine-Net for coding residual signals and motion vectors. Also, for motion estimation, we introduced a hierarchical, attention-based ME-Net. To verify our design, we conducted an extensive ablation study on our modules and different input formats. Our video codec demonstrates its performance by using the perfect reference frame at the decoder side specified by the CLIC P-frame Challenge. The experimental result shows that our proposed codec is very competitive with the Challenge top performers in terms of quality metrics.",0
"This paper proposes the use of learned video codecs along with enhanced reconstructions techniques, specifically designed for p-frame coding within Constrained Low Imaging Color Profile (CLIC) applications. Our approach leverages machine learning algorithms to model complex image transformations and optimize encoding efficiency while preserving high visual quality. By incorporating these advanced features into existing video compression frameworks, we demonstrate improved performance compared to traditional codecs as well as state-of-the-art deep learning-based methods. With our proposed methodology, video professionals can achieve greater flexibility, faster processing times, and better output fidelity for their low bitrate workflows. Overall, the adoption of learned video codecs in conjunction with CLIC p-frame coding represents a significant step forward towards efficient and effective content delivery across all multimedia platforms.",1
"Motivated by the potential for parallel implementation of batch-based algorithms and the accelerated convergence achievable with approximated second order information a limited memory version of the BFGS algorithm has been receiving increasing attention in recent years for large neural network training problems. As the shape of the cost function is generally not quadratic and only becomes approximately quadratic in the vicinity of a minimum, the use of second order information by L-BFGS can be unreliable during the initial phase of training, i.e. when far from a minimum. Therefore, to control the influence of second order information as training progresses, we propose a multi-batch L-BFGS algorithm, namely MB-AM, that gradually increases its trust in the curvature information by implementing a progressive storage and use of curvature data through a development-based increase (dev-increase) scheme. Using six discriminative modelling benchmark problems we show empirically that MB-AM has slightly faster convergence and, on average, achieves better solutions than the standard multi-batch L-BFGS algorithm when training MLP and CNN models.",0
"Adaptive memory multi-batch optimization algorithms have recently gained significant popularity due to their ability to efficiently train deep neural networks (DNNs) on large datasets. In particular, the limited batch size gradient descent method has been shown to achieve state-of-the-art results in image classification tasks when combined with adaptive memory techniques such as Nesterovâ€™s accelerated gradient or quasi-Newton methods. However, existing multi-batch methods suffer from several limitations, including high memory consumption, slow convergence rates, and poor scalability to larger mini-batch sizes. This paper proposes a new multi-batch algorithm that addresses these issues by leveraging second-order information through the use of the Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) update formula. Our proposed Adaptive Memory Multi-Batch L-BFGS (AMMBLB) algorithm combines the benefits of both SGD and full batch BFGS updating formulas while using significantly fewer parameters than other similar approaches. We demonstrate through extensive experiments on a variety of image classification benchmarks that our approach outperforms state-of-the-art multi-batch algorithms across all settings considered herein. Furthermore, we showcase how AMMBLB exhibits improved robustness under hyperparameter tuning, making it well suited for deployment in real-world DNN applications. Overall, our work contributes towards more efficient and effective training of deep learning models.",1
"Intellectual property protection(IPP) have received more and more attention recently due to the development of the global e-commerce platforms. brand recognition plays a significant role in IPP. Recent studies for brand recognition and detection are based on small-scale datasets that are not comprehensive enough when exploring emerging deep learning techniques. Moreover, it is challenging to evaluate the true performance of brand detection methods in realistic and open scenes. In order to tackle these problems, we first define the special issues of brand detection and recognition compared with generic object detection. Second, a novel brands benchmark called ""Open Brands"" is established. The dataset contains 1,437,812 images which have brands and 50,000 images without any brand. The part with brands in Open Brands contains 3,113,828 instances annotated in 3 dimensions: 4 types, 559 brands and 1216 logos. To the best of our knowledge, it is the largest dataset for brand detection and recognition with rich annotations. We provide in-depth comprehensive statistics about the dataset, validate the quality of the annotations and study how the performance of many modern models evolves with an increasing amount of training data. Third, we design a network called ""Brand Net"" to handle brand recognition. Brand Net gets state-of-art mAP on Open Brand compared with existing detection methods.",0
"New dataset release: The Open Brands Database is a unified resource that enables efficient and accurate brand detection and recognition across multiple domains. This comprehensive data source provides a platform for researchers and practitioners in areas such as computer vision, natural language processing (NLP), recommendation systems, social media analysis, and digital marketing. We describe our methodology, including web scraping and image analysis techniques, for gathering over one million instances from eight diverse categories, each accompanied by detailed labeling capturing brand identities and contextual attributes. In addition, we present experimental results demonstrating the effectiveness of utilizing the Open Brands Database on three downstream tasks: product search retrieval, cross-modal content creation, and zero-shot transfer learning for logo classification. Our extensive evaluation outlines the benefits of using a unified database vs. individual domain-specific resources while addressing current limitations in brand-related research datasets. Overall, we hope that the open availability of this dataset encourages further exploration into understanding brands and their influence within society.",1
"Single-stage object detectors have been widely applied in computer vision applications due to their high efficiency. However, we find that the loss functions adopted by single-stage object detectors hurt the localization accuracy seriously. Firstly, the standard cross-entropy loss for classification is independent of the localization task and drives all the positive examples to learn as high classification scores as possible regardless of localization accuracy during training. As a result, there will be many detections that have high classification scores but low IoU or detections that have low classification scores but high IoU. Secondly, for the standard smooth L1 loss, the gradient is dominated by the outliers that have poor localization accuracy during training. The above two problems will decrease the localization accuracy of single-stage detectors. In this work, IoU-balanced loss functions that consist of IoU-balanced classification loss and IoU-balanced localization loss are proposed to solve the above problems. The IoU-balanced classification loss pays more attention to positive examples with high IoU and can enhance the correlation between classification and localization tasks. The IoU-balanced localization loss decreases the gradient of examples with low IoU and increases the gradient of examples with high IoU, which can improve the localization accuracy of models. Extensive experiments on challenging public datasets such as MS COCO, PASCAL VOC and Cityscapes demonstrate that both IoU-balanced losses can bring substantial improvement for the popular single-stage detectors, especially for the localization accuracy. On COCO test-dev, the proposed methods can substantially improve AP by $1.0\%\sim1.7\%$ and AP75 by $1.0\%\sim2.4\%$. On PASCAL VOC, it can also substantially improve AP by $1.3\%\sim1.5\%$ and AP80, AP90 by $1.6\%\sim3.9\%$.",0
"In object detection tasks, Intersection over Union (IoU) measures how well ground truth bounding boxes overlap with detected boxes from the model. Common loss functions like binary cross entropy consider only whether a prediction was correct or incorrect regardless of similarity of IoUs. This work proposes three new metrics that modify CE by balancing IoU ranges while keeping the existing values as constraints so that high IoU regions have more weight compared to low IoU regions. By integrating these into a single-stage detector using RetinaNet architecture we see significant improvement on PASCAL VOC benchmark datasets, raising mAP scores by 2.6% to 4%. We expect similar benefits can be obtained across other datasets and detectors as our simple yet effective approach improves performance without increasing complexity of models, data augmentation or hyperparameters tuned during training.",1
"The categorical distribution is a natural representation of uncertainty in multi-class segmentations. In the two-class case the categorical distribution reduces to the Bernoulli distribution, for which grayscale morphology provides a range of useful operations. In the general case, applying morphological operations on uncertain multi-class segmentations is not straightforward as an image of categorical distributions is not a complete lattice. Although morphology on color images has received wide attention, this is not so for color-coded or categorical images and even less so for images of categorical distributions. In this work, we establish a set of requirements for morphology on categorical distributions by combining classic morphology with a probabilistic view. We then define operators respecting these requirements, introduce protected operations on categorical distributions and illustrate the utility of these operators on two example tasks: modeling annotator bias in brain tumor segmentations and segmenting vesicle instances from the predictions of a multi-class U-Net.",0
"In this study, we analyze the morphological properties of categorical distributions. We examine the shapes and statistical characteristics of these distributions across different data sets and categories. Our analysis reveals insights into how different factors can affect the shape of a distribution, such as sample size and category labels. Additionally, we explore how these distributions relate to other types of probability distributions and discuss their implications in various fields of application. Finally, our findings contribute to the understanding of the behavior and interpretability of categorical data.",1
"We present a novel attention-based mechanism for learning enhanced point features for tasks such as point cloud classification and segmentation. Our key message is that if the right attention point is selected, then ""one point is all you need"" -- not a sequence as in a recurrent model and not a pre-selected set as in all prior works. Also, where the attention point is should be learned, from data and specific to the task at hand. Our mechanism is characterized by a new and simple convolution, which combines the feature at an input point with the feature at its associated attention point. We call such a point a directional attention point (DAP), since it is found by adding to the original point an offset vector that is learned by maximizing the task performance in training. We show that our attention mechanism can be easily incorporated into state-of-the-art point cloud classification and segmentation networks. Extensive experiments on common benchmarks such as ModelNet40, ShapeNetPart, and S3DIS demonstrate that our DAP-enabled networks consistently outperform the respective original networks, as well as all other competitive alternatives, including those employing pre-selected sets of attention points.",0
"This paper presents a novel method for feature learning called directional attention point (DAP). Inspired by human vision where our brain selectively focuses on one point at a time, we introduce a simple yet effective approach that extends convolutions into spatial dimensions, allowing them to attend to specific locations in images. Our model can learn discriminative features from limited annotations while efficiently utilizing computing resources. We demonstrate state-of-the-art performance across various tasks including image classification, object detection, and semantic segmentation using only one DAP per layer. Compared to previous methods relying heavily on densely placed attention mechanisms, our proposed DAP method achieves more efficient training and better accuracy under similar computational requirements.",1
"Human attention mechanisms often work in a top-down manner, yet it is not well explored in vision research. Here, we propose the Top-Down Attention Framework (TDAF) to capture top-down attentions, which can be easily adopted in most existing models. The designed Recursive Dual-Directional Nested Structure in it forms two sets of orthogonal paths, recursive and structural ones, where bottom-up spatial features and top-down attention features are extracted respectively. Such spatial and attention features are nested deeply, therefore, the proposed framework works in a mixed top-down and bottom-up manner. Empirical evidence shows that our TDAF can capture effective stratified attention information and boost performance. ResNet with TDAF achieves 2.0% improvements on ImageNet. For object detection, the performance is improved by 2.7% AP over FCOS. For pose estimation, TDAF improves the baseline by 1.6%. And for action recognition, the 3D-ResNet adopting TDAF achieves improvements of 1.7% accuracy.",0
"Aimed at improving visual attention mechanisms in deep neural networks (DNN), we propose the Top-Down Attention Framework (TDAF) which incorporates high-level semantic knowledge into DNNs by introducing prioritized object detection followed by bottom-up feature fusion within each region of interest. By leveraging scene contextual understanding along with fine-grained region selection through attention, our approach outperforms competing methods on benchmark datasets across vision tasks such as image classification, object detection, and instance segmentation. Our findings suggest that effectively combining top-down and bottom-up attentional processes leads to improved overall performance and greater robustness across diverse task settings. With implications for designing more advanced and flexible learning architectures, the TDAF framework represents a valuable contribution to computer vision research.",1
"Compared with common image segmentation tasks targeted at low-resolution images, higher resolution detailed image segmentation receives much less attention. In this paper, we propose and study a task named Meticulous Object Segmentation (MOS), which is focused on segmenting well-defined foreground objects with elaborate shapes in high resolution images (e.g. 2k - 4k). To this end, we propose the MeticulousNet which leverages a dedicated decoder to capture the object boundary details. Specifically, we design a Hierarchical Point-wise Refining (HierPR) block to better delineate object boundaries, and reformulate the decoding process as a recursive coarse to fine refinement of the object mask. To evaluate segmentation quality near object boundaries, we propose the Meticulosity Quality (MQ) score considering both the mask coverage and boundary precision. In addition, we collect a MOS benchmark dataset including 600 high quality images with complex objects. We provide comprehensive empirical evidence showing that MeticulousNet can reveal pixel-accurate segmentation boundaries and is superior to state-of-the-art methods for high resolution object segmentation tasks.",0
"Here is an example abstract without paper title: ""A deep learning model that can accurately and efficiently segment objects from images has many potential applications across fields such as computer vision, autonomous vehicles, robotics, and medical imaging analysis. Existing object segmentation methods suffer from either high computational complexity or limited accuracy, which limits their real world usage. To address these shortcomings, we present Meticulous Object Segmentation, a novel framework based on Fully Convolutional Networks (FCN) that balances speed and precision via dense dilated convolutions.""",1
"Recently, multi-resolution networks (such as Hourglass, CPN, HRNet, etc.) have achieved significant performance on pose estimation by combining feature maps of various resolutions. In this paper, we propose a Resolution-wise Attention Module (RAM) and Gradual Pyramid Refinement (GPR), to learn enhanced resolution-wise feature maps for precise pose estimation. Specifically, RAM learns a group of weights to represent the different importance of feature maps across resolutions, and the GPR gradually merges every two feature maps from low to high resolutions to regress final human keypoint heatmaps. With the enhanced resolution-wise features learnt by CNN, we obtain more accurate human keypoint locations. The efficacies of our proposed methods are demonstrated on MS-COCO dataset, achieving state-of-the-art performance with average precision of 77.7 on COCO val2017 set and 77.0 on test-dev2017 set without using extra human keypoint training dataset.",0
"Improving human pose estimation accuracy has been a crucial task in computer vision research due to its wide range of applications such as action recognition, gesture control interfaces, and medical diagnosis analysis. Traditionally, approaches have focused on handcrafted feature extraction followed by machine learning techniques to learn from large datasets. However, these methods can be limited by their reliance on manually engineered features that may miss important information present in the raw data. In recent years, convolutional neural networks (CNNs) have emerged as powerful models capable of automatically learning hierarchical representations directly from images. As a result, CNN-based frameworks have achieved state-of-the-art results in many tasks including human pose estimation. Nonetheless, most existing works only utilize resolution-agnostic features generated from the last fully connected layer or global average pooling layers. These features capture high level semantic information but lack specific localization details which could be beneficial for fine-grained understanding of humans in images. This work presents novel enhancements based on resolving spatial locality explicitly through downsampled convolution operations that are learned alongside full-resolution ones at each stage of the network hierarchy. By enabling multi-scale contextual modeling within individual stages, our approach significantly outperforms previous methods achieving an unprecedented accuracy boost of up to 6% in standard benchmarks while running more efficiently due to reduced computations required during inference. Our contributions demonstrate how jointly considering low-level details alongside general object understanding can provide significant advantages in complex human pose analysis tasks. We expect that incorporating similar insights into other computer vision domains will further improve performance across diverse application scenarios.",1
"Transformer-based architectures have shown great success in image captioning, where object regions are encoded and then attended into the vectorial representations to guide the caption decoding. However, such vectorial representations only contain region-level information without considering the global information reflecting the entire image, which fails to expand the capability of complex multi-modal reasoning in image captioning. In this paper, we introduce a Global Enhanced Transformer (termed GET) to enable the extraction of a more comprehensive global representation, and then adaptively guide the decoder to generate high-quality captions. In GET, a Global Enhanced Encoder is designed for the embedding of the global feature, and a Global Adaptive Decoder are designed for the guidance of the caption generation. The former models intra- and inter-layer global representation by taking advantage of the proposed Global Enhanced Attention and a layer-wise fusion module. The latter contains a Global Adaptive Controller that can adaptively fuse the global information into the decoder to guide the caption generation. Extensive experiments on MS COCO dataset demonstrate the superiority of our GET over many state-of-the-arts.",0
"Accurate image caption generation remains a challenging task due to the complex nature of visual understanding and natural language description. To tackle this challenge, we propose a novel approach that leverages both intra-layer global representation (IR) and inter-layer IR (ILR). Our method integrates these representations into two parallel streams within a transformer network architecture. We show through extensive experiments on popular benchmark datasets that our approach outperforms state-of-the-art methods in terms of caption quality as well as alignment metrics. Additionally, analysis demonstrates the effectiveness of each component involved, highlighting the importance of exploiting multiple levels of information for generating accurate descriptions of images. Our work paves the way towards more advanced computer vision models capable of generating humanlike descriptions of visual content.",1
"Human pose transfer, which aims at transferring the appearance of a given person to a target pose, is very challenging and important in many applications. Previous work ignores the guidance of pose features or only uses local attention mechanism, leading to implausible and blurry results. We propose a new human pose transfer method using a generative adversarial network (GAN) with simplified cascaded blocks. In each block, we propose a pose-guided non-local attention (PoNA) mechanism with a long-range dependency scheme to select more important regions of image features to transfer. We also design pre-posed image-guided pose feature update and post-posed pose-guided image feature update to better utilize the pose and image features. Our network is simple, stable, and easy to train. Quantitative and qualitative results on Market-1501 and DeepFashion datasets show the efficacy and efficiency of our model. Compared with state-of-the-art methods, our model generates sharper and more realistic images with rich details, while having fewer parameters and faster speed. Furthermore, our generated images can help to alleviate data insufficiency for person re-identification.",0
"In recent years, pose transfer has become a popular task in computer vision, aiming to synthesize human poses into images by adapting the appearance of the source image while preserving the identity and key attributes of the target person. However, previous methods have limitations in generating high-quality results, as they often suffer from artifacts such as incorrect alignments, disocclusions, and missing details. To address these issues, we propose a novel framework called PoNA, which stands for ""Pose guided Non-Local Attention."" Our approach introduces a spatial attention mechanism that can capture long-range dependencies and utilizes the semantic knowledge of human poses to guide the process of attention allocation. We validate our method on several benchmark datasets and show that it outperforms state-of-the-art techniques in terms of visual quality, fidelity to ground truth data, and overall performance metrics. Overall, our work demonstrates the effectiveness of incorporating prior knowledge of human poses in generative tasks like pose transfer, enabling more accurate and detailed synthesis of human figures in images.",1
"Digital watermark is a commonly used technique to protect the copyright of medias. Simultaneously, to increase the robustness of watermark, attacking technique, such as watermark removal, also gets the attention from the community. Previous watermark removal methods require to gain the watermark location from users or train a multi-task network to recover the background indiscriminately. However, when jointly learning, the network performs better on watermark detection than recovering the texture. Inspired by this observation and to erase the visible watermarks blindly, we propose a novel two-stage framework with a stacked attention-guided ResUNets to simulate the process of detection, removal and refinement. In the first stage, we design a multi-task network called SplitNet. It learns the basis features for three sub-tasks altogether while the task-specific features separately use multiple channel attentions. Then, with the predicted mask and coarser restored image, we design RefineNet to smooth the watermarked region with a mask-guided spatial attention. Besides network structure, the proposed algorithm also combines multiple perceptual losses for better quality both visually and numerically. We extensively evaluate our algorithm over four different datasets under various settings and the experiments show that our approach outperforms other state-of-the-art methods by a large margin. The code is available at http://github.com/vinthony/deep-blind-watermark-removal.",0
"This paper presents a novel deep neural network architecture called ""Split then Refine"" that is designed specifically for the task of blind single image visible watermark removal. Our method splits an input image into two versions using channel attention before applying a residual block of convolutions. Next, we introduce stacked attention guided resblocks, which allow us to perform self attention on the feature maps after each downsample operation. We show through extensive experiments that our approach outperforms several state-of-the-art methods in terms of both visual quality and objective metrics such as PSNR and SSIM. Additionally, our model has the advantage of being lightweight and efficient, making it well suited for deployment on mobile devices or embedded systems where computational resources may be limited.",1
"Predicting future human motion plays a significant role in human-machine interactions for a variety of real-life applications. In this paper, we build a deep state-space model, DeepSSM, to predict future human motion. Specifically, we formulate the human motion system as the state-space model of a dynamic system and model the motion system by the state-space theory, offering a unified formulation for diverse human motion systems. Moreover, a novel deep network is designed to build this system, enabling us to utilize both the advantages of deep network and state-space model. The deep network jointly models the process of both the state-state transition and the state-observation transition of the human motion system, and multiple future poses can be generated via the state-observation transition of the model recursively. To improve the modeling ability of the system, a unique loss function, ATPL (Attention Temporal Prediction Loss), is introduced to optimize the model, encouraging the system to achieve more accurate predictions by paying increasing attention to the early time-steps. The experiments on two benchmark datasets (i.e., Human3.6M and 3DPW) confirm that our method achieves state-of-the-art performance with improved effectiveness. The code will be available if the paper is accepted.",0
"Abstract ----------------------  This research proposes a novel deep learning architecture called DeepSSM (Deep State-Space Models) that predicts future 3D human motion from observed past pose data. Our method uses a combination of state space models with deep neural networks to accurately model complex nonlinear relationships between temporally adjacent poses. We demonstrate improved accuracy over other approaches by using a comprehensive evaluation on challenging datasets. Additionally, our system runs at real-time speeds making it suitable for use in virtual reality applications. Overall, our work represents a significant advancement in understanding how machine learning techniques can be applied to estimate human motion patterns. Keywords: 3D human motion prediction, state space models, deep learning  ---  The proposed research aims to develop an innovative approach for predicting future 3D human motion based on previously observed pose data. This problem has important applications in areas such as animation, computer vision, robotics, and medicine. To address this challenge, we propose DeepSSM (Deep State-Space Models), which combines state-space models and deep neural networks to build accurate representations of temporal dependencies among consecutive human poses.  State-space models have been successfully used in previous works related to human motion analysis and have shown promising results. However, these models often suffer from high computational complexity, difficulties in handling noise and uncertainty, and limitations in capturing nonlinearities. Our approach addresses these issues by leveraging advanced deep learning architectures to learn more robust representations. In particular, we apply recurrent layers within the state-space framework to effectively capture dynamic relationships between sequential poses.  Our experimental evaluations reveal that DeepSSM outperforms several well-known methods in 3D human mo",1
"Echocardiography is a powerful prenatal examination tool for early diagnosis of fetal congenital heart diseases (CHDs). The four-chamber (FC) view is a crucial and easily accessible ultrasound (US) image among echocardiography images. Automatic analysis of FC views contributes significantly to the early diagnosis of CHDs. The first step to automatically analyze fetal FC views is locating the fetal four crucial chambers of heart in a US image. However, it is a greatly challenging task due to several key factors, such as numerous speckles in US images, the fetal cardiac chambers with small size and unfixed positions, and category indistinction caused by the similarity of cardiac chambers. These factors hinder the process of capturing robust and discriminative features, hence destroying fetal cardiac anatomical chambers precise localization. Therefore, we first propose a multistage residual hybrid attention module (MRHAM) to improve the feature learning. Then, we present an improved YOLOv4 detection model, namely MRHAM-YOLOv4-Slim. Specially, the residual identity mapping is replaced with the MRHAM in the backbone of MRHAM-YOLOv4-Slim, accurately locating the four important chambers in fetal FC views. Extensive experiments demonstrate that our proposed method outperforms current state-of-the-art, including the precision of 0.919, the recall of 0.971, the F1 score of 0.944, the mAP of 0.953, and the frames per second (FPS) of 43.",0
"Abstract: This study investigates the accuracy of an attention-based object detection framework using YOLOv4 (You Only Look Once version 4) on four chamber view fetal echocardiographic images for automated cardiac chamber segmentation. Our method utilizes image registration techniques and vessel tracking algorithms to accurately localize and track the heart regions in each frame of the video sequence. We then trained our model on these tracked frames, which were labeled manually by expert annotators, allowing us to evaluate the overall performance of our approach. Results showed that our system achieved high accuracy with respect to manual annotation comparisons across all four cardiac chambers, while maintaining real-time processing speeds. These findings suggest that the proposed automatic cardiac chamber segmentation pipeline has potential clinical utility as a tool for assisting physicians during routine fetal echocardiography examinations. Keywords: attentional networks, convolutional neural networks, deep learning models, four-chamber view, object detection frameworks, pediatric cardiology research, retina-like representations.",1
"Deep neural networks (DNNs) have achieved significant performance in various tasks. However, recent studies have shown that DNNs can be easily fooled by small perturbation on the input, called adversarial attacks. As the extensions of DNNs to graphs, Graph Neural Networks (GNNs) have been demonstrated to inherit this vulnerability. Adversary can mislead GNNs to give wrong predictions by modifying the graph structure such as manipulating a few edges. This vulnerability has arisen tremendous concerns for adapting GNNs in safety-critical applications and has attracted increasing research attention in recent years. Thus, it is necessary and timely to provide a comprehensive overview of existing graph adversarial attacks and the countermeasures. In this survey, we categorize existing attacks and defenses, and review the corresponding state-of-the-art methods. Furthermore, we have developed a repository with representative algorithms (https://github.com/DSE-MSU/DeepRobust/tree/master/deeprobust/graph). The repository enables us to conduct empirical studies to deepen our understandings on attacks and defenses on graphs.",0
"Adversarial attacks refer to attempts by malicious actors to manipulate data so that machine learning models make incorrect predictions. In recent years there has been growing interest in understanding adversarial attacks on graphs, which form the underlying structure for many real-world networks such as social media platforms and online marketplaces. This review provides an overview of some key issues related to adversarial attacks and defenses on graphs, and presents a new tool called GATTACA (Graph Attack and ATTack CApturing) designed to enable researchers and practitioners to easily perform systematic studies of these problems. We demonstrate the effectiveness of our tool using two case studies, one involving Facebook groups and another focusing on fake product reviews. Our findings highlight the need for further work in this area, particularly given the potential negative impacts on individuals and society caused by successful graph adversarial attacks. While significant progress has been made recently, more research is required in order to fully comprehend how these types of attacks work, develop effective defense mechanisms and ultimately protect against them. Overall, we hope this paper serves as a valuable resource both for those already working in this field and for others looking to enter it, and look forward to continued growth in this important research area.",1
"Motivated by the desire to exploit patterns shared across classes, we present a simple yet effective class-specific memory module for fine-grained feature learning. The memory module stores the prototypical feature representation for each category as a moving average. We hypothesize that the combination of similarities with respect to each category is itself a useful discriminative cue. To detect these similarities, we use attention as a querying mechanism. The attention scores with respect to each class prototype are used as weights to combine prototypes via weighted sum, producing a uniquely tailored response feature representation for a given input. The original and response features are combined to produce an augmented feature for classification. We integrate our class-specific memory module into a standard convolutional neural network, yielding a Categorical Memory Network. Our memory module significantly improves accuracy over baseline CNNs, achieving competitive accuracy with state-of-the-art methods on four benchmarks, including CUB-200-2011, Stanford Cars, FGVC Aircraft, and NABirds.",0
"Title: Fine-grained Classification via Categorical Memory Networks Authors: XYZ Corresponding Author Email Address: [xyz@email.com] Abstract Fine-grained classification has recently attracted significant attention due to its ability to classify objects into subordinate categories based on subtle visual differences. In this paper, we propose using categorical memory networks (CMN) as a new approach for fine-grained image classification. CMN builds upon recent advances in neural network architectures that rely on external memory modules. By training these models on large datasets, they learn efficient representations of high quality which can then be used as feature extractors for downstream tasks. We demonstrate that our proposed method outperforms existing state-of-the-art approaches across three different benchmark datasets. Additionally, through extensive ablation studies we show how each component of our model contributes to its performance improvement over other methods. Finally, by interpreting the weights of the readout function, we provide insights into the learned representations and show their effectiveness for specific classes within the dataset. Overall, our results suggest that CMN represents an effective solution for fine-grained classification problems, paving the way towards improved performance in a variety of computer vision applications such as object recognition, image retrieval, and content-based image search. Keywords: Fine-grained classification; External memories; Neural networks; Image analysis; Object recognition; Image retrieval; Content-based image search (Please note this is just an example, please use your own data and details)",1
"We propose to improve text recognition from a new perspective by separating the text content from complex backgrounds. As vanilla GANs are not sufficiently robust to generate sequence-like characters in natural images, we propose an adversarial learning framework for the generation and recognition of multiple characters in an image. The proposed framework consists of an attention-based recognizer and a generative adversarial architecture. Furthermore, to tackle the issue of lacking paired training samples, we design an interactive joint training scheme, which shares attention masks from the recognizer to the discriminator, and enables the discriminator to extract the features of each character for further adversarial training. Benefiting from the character-level adversarial training, our framework requires only unpaired simple data for style supervision. Each target style sample containing only one randomly chosen character can be simply synthesized online during the training. This is significant as the training does not require costly paired samples or character-level annotations. Thus, only the input images and corresponding text labels are needed. In addition to the style normalization of the backgrounds, we refine character patterns to ease the recognition task. A feedback mechanism is proposed to bridge the gap between the discriminator and the recognizer. Therefore, the discriminator can guide the generator according to the confusion of the recognizer, so that the generated patterns are clearer for recognition. Experiments on various benchmarks, including both regular and irregular text, demonstrate that our method significantly reduces the difficulty of recognition. Our framework can be integrated into recent recognition methods to achieve new state-of-the-art recognition accuracy.",0
"Here is a sample for you: Advances in OCR (optical character recognition) technology have made it possible to digitize vast amounts of textual data stored on physical documents such as books and magazines. However, current approaches face significant challenges due to variations in font styles, sizes, languages, and image quality across different types of documents. In this work, we propose an adversarial learning framework that separates content from style by training two competing neural networks - one for classifying content and another for predicting style features like texture and color. Our approach significantly improves upon traditional methods, outperforming state-of-the-art systems while demonstrating robustness to complex backgrounds, low resolution images, and multilingual scenarios. We believe our method has important implications for digital archiving, document retrieval, and language processing applications.",1
"Co-saliency detection (Co-SOD) aims to segment the common salient foreground in a group of relevant images. In this paper, inspired by human behavior, we propose a gradient-induced co-saliency detection (GICD) method. We first abstract a consensus representation for the grouped images in the embedding space; then, by comparing the single image with consensus representation, we utilize the feedback gradient information to induce more attention to the discriminative co-salient features. In addition, due to the lack of Co-SOD training data, we design a jigsaw training strategy, with which Co-SOD networks can be trained on general saliency datasets without extra pixel-level annotations. To evaluate the performance of Co-SOD methods on discovering the co-salient object among multiple foregrounds, we construct a challenging CoCA dataset, where each image contains at least one extraneous foreground along with the co-salient object. Experiments demonstrate that our GICD achieves state-of-the-art performance. Our codes and dataset are available at https://mmcheng.net/gicd/.",0
"This paper presents a novel approach for saliency detection in visual scenes by incorporating gradient features into co-saliency analysis. Saliency detection has been widely studied in computer vision, as it plays a crucial role in many applications such as image and video compression, object recognition, and attention modeling. Conventional saliency models mainly focus on low-level features like color and texture contrasts, while high-level semantic cues have been largely ignored. In this work, we propose a new algorithm that uses spatial gradients to capture detailed structure information of images and videos. Our method calculates co-occurrence matrices using both raw intensity values and gradient orientation maps, which captures local structure differences across different channels. These two types of feature maps are then concatenated and fed into a co-saliency model that simultaneously predicts importance scores for all pixels in multiple input modalities. Experimental results show significant improvement over state-of-the-art methods on several benchmark datasets, demonstrating the effectiveness and robustness of our proposed approach. We believe that this research will contribute to advancing the field of multi-modal fusion and saliency detection.",1
"With the recent advances in deep neural networks, anomaly detection in multimedia has received much attention in the computer vision community. While reconstruction-based methods have recently shown great promise for anomaly detection, the information equivalence among input and supervision for reconstruction tasks can not effectively force the network to learn semantic feature embeddings. We here propose to break this equivalence by erasing selected attributes from the original data and reformulate it as a restoration task, where the normal and the anomalous data are expected to be distinguishable based on restoration errors. Through forcing the network to restore the original image, the semantic feature embeddings related to the erased attributes are learned by the network. During testing phases, because anomalous data are restored with the attribute learned from the normal data, the restoration error is expected to be large. Extensive experiments have demonstrated that the proposed method significantly outperforms several state-of-the-arts on multiple benchmark datasets, especially on ImageNet, increasing the AUROC of the top-performing baseline by 10.1%. We also evaluate our method on a real-world anomaly detection dataset MVTec AD and a video anomaly detection dataset ShanghaiTech.",0
"In recent years, anomaly detection has become increasingly important as a tool for identifying unusual behavior patterns in data sets that may indicate malicious activity or other types of unexpected events. However, many current methods suffer from high levels of false positive rates, meaning they often identify normal behavior as abnormal. This can lead to unnecessary alerts and wasted time investigating non-threatening incidents. To address these issues, we propose the Attribute Restoration Framework (ARF), a novel approach to anomaly detection that utilizes domain knowledge and machine learning techniques to effectively identify truly anomalous behavior while minimizing false positives. ARF works by first determining which features are most relevant to detecting anomalies in a given dataset, then using these features to train a model that can accurately distinguish normal from abnormal behavior. We evaluate our method on several real-world datasets and demonstrate significant improvements over existing approaches, achieving higher accuracy and lower false positive rates. Our work has implications for both researchers and practitioners looking to improve their anomaly detection capabilities, making it more effective and efficient at identifying true threats without overwhelming them with false alarms.",1
"Humans spend vast hours in bed -- about one-third of the lifetime on average. Besides, a human at rest is vital in many healthcare applications. Typically, humans are covered by a blanket when resting, for which we propose a multimodal approach to uncover the subjects so their bodies at rest can be viewed without the occlusion of the blankets above. We propose a pyramid scheme to effectively fuse the different modalities in a way that best leverages the knowledge captured by the multimodal sensors. Specifically, the two most informative modalities (i.e., depth and infrared images) are first fused to generate good initial pose and shape estimation. Then pressure map and RGB images are further fused one by one to refine the result by providing occlusion-invariant information for the covered part, and accurate shape information for the uncovered part, respectively. However, even with multimodal data, the task of detecting human bodies at rest is still very challenging due to the extreme occlusion of bodies. To further reduce the negative effects of the occlusion from blankets, we employ an attention-based reconstruction module to generate uncovered modalities, which are further fused to update current estimation via a cyclic fashion. Extensive experiments validate the superiority of the proposed model over others.",0
"Here is my attempt at writing the abstract:  In medical imaging applications such as ultrasound (US), magnetic resonance imaging (MRI) and computed tomography (CT), accurate estimation of patient body pose and shape can improve image quality and reduce exam time by enabling automated scan plane positioning and adjustment of bed position during exams. This work proposes a novel method for estimating in-bed pose and shape of patients under thick blankets using multimodal sensor data. We integrate readings from pressure sensors on US cushions placed below patients with information obtained through depth maps generated via US image sequences. Our approach enables high accuracy pose estimates despite occlusions caused by blanket layers and requires no manual intervention once the exam begins. By leveraging complementary aspects of multiple sensor types we achieve robustness against missing or inconsistent measurements from individual sources, reducing error rates compared to prior methods relying solely on pressure signals or US images. Clinical evaluation using prospective studies demonstrates improved pose and shape estimate performance over existing techniques. In conclusion, our proposed method provides a valuable tool for improving medical imaging outcomes while reducing clinician effort and increasing efficiency.",1
"Few-shot learning (FSL) aims to learn novel visual categories from very few samples, which is a challenging problem in real-world applications. Many methods of few-shot classification work well on general images to learn global representation. However, they can not deal with fine-grained categories well at the same time due to a lack of subtle and local information. We argue that localization is an efficient approach because it directly provides the discriminative regions, which is critical for both general classification and fine-grained classification in a low data regime. In this paper, we propose a Self-Attention Based Complementary Module (SAC Module) to fulfill the weakly-supervised object localization, and more importantly produce the activated masks for selecting discriminative deep descriptors for few-shot classification. Based on each selected deep descriptor, Semantic Alignment Module (SAM) calculates the semantic alignment distance between the query and support images to boost classification performance. Extensive experiments show our method outperforms the state-of-the-art methods on benchmark datasets under various settings, especially on the fine-grained few-shot tasks. Besides, our method achieves superior performance over previous methods when training the model on miniImageNet and evaluating it on the different datasets, demonstrating its superior generalization capacity. Extra visualization shows the proposed method can localize the key objects more interval.",0
"This paper presents a weakly supervised object localization method for few-shot learning and fine-grained few-shot learning. The proposed method leverages existing image classification models as starting points and uses region proposal networks (RPNs) and meta learning techniques to optimize the model for few-shot localization tasks. The approach is evaluated on three benchmark datasets and achieves state-of-the-art results across all three settings: base few-shot learning, fine-grained few-shot learning, and incremental fine-grained few-shot learning. The authors highlight the importance of using well-trained RPNs and effective meta learning strategies in order to achieve high performance on these challenging tasks. Overall, this work represents a significant advancement in the field of computer vision and demonstrates the potential of weakly supervised methods for real-world applications.",1
"We propose TabTransformer, a novel deep tabular data modeling architecture for supervised and semi-supervised learning. The TabTransformer is built upon self-attention based Transformers. The Transformer layers transform the embeddings of categorical features into robust contextual embeddings to achieve higher prediction accuracy. Through extensive experiments on fifteen publicly available datasets, we show that the TabTransformer outperforms the state-of-the-art deep learning methods for tabular data by at least 1.0% on mean AUC, and matches the performance of tree-based ensemble models. Furthermore, we demonstrate that the contextual embeddings learned from TabTransformer are highly robust against both missing and noisy data features, and provide better interpretability. Lastly, for the semi-supervised setting we develop an unsupervised pre-training procedure to learn data-driven contextual embeddings, resulting in an average 2.1% AUC lift over the state-of-the-art methods.",0
"This study presents a novel method for modeling tabular data using contextual embeddings. The proposed approach, called TabTransformer, leverages recent advances in natural language processing (NLP) and computer vision (CV) to encode structured data as dense vectors that capture contextual relationships between cells. We evaluate our method on several benchmark datasets and demonstrate state-of-the-art performance across a range of tasks such as classification, regression, clustering, and generation. Our findings show that TabTransformer achieves significant improvements over baseline methods by effectively capturing complex dependencies among columns in large tables. Overall, our work provides insights into how NLP techniques can be adapted to process and analyze rich structured data beyond text, paving the way towards more expressive artificial intelligence systems.",1
"Whilst adversarial attack detection has received considerable attention, it remains a fundamentally challenging problem from two perspectives. First, while threat models can be well-defined, attacker strategies may still vary widely within those constraints. Therefore, detection should be considered as an open-set problem, standing in contrast to most current detection strategies. These methods take a closed-set view and train binary detectors, thus biasing detection toward attacks seen during detector training. Second, information is limited at test time and confounded by nuisance factors including the label and underlying content of the image. Many of the current high-performing techniques use training sets for dealing with some of these issues, but can be limited by the overall size and diversity of those sets during the detection step. We address these challenges via a novel strategy based on random subspace analysis. We present a technique that makes use of special properties of random projections, whereby we can characterize the behavior of clean and adversarial examples across a diverse set of subspaces. We then leverage the self-consistency (or inconsistency) of model activations to discern clean from adversarial examples. Performance evaluation demonstrates that our technique outperforms ($0.92$ AUC) competing state of the art (SOTA) attack strategies, while remaining truly agnostic to the attack method itself. It also requires significantly less training data, composed only of clean examples, when compared to competing SOTA methods, which achieve only chance performance, when evaluated in a more rigorous testing scenario.",0
"Title: A Survey on Data Quality Assessment Techniques Used by Business Intelligence Practitioners Authors: [Your Name(s) Here]  Data quality assessment techniques aim to measure the fitness of data for specific uses (Wang et al., 2009). Various studies have emphasized the importance of data quality in business intelligence systems as poor data quality can lead to incorrect decision making (Choi & Lee, 2014; Golfarelli et al., 2017). In order to improve data quality and ensure accurate decision-making, organizations use different approaches and methods for measuring data quality. This survey paper provides an overview of these methodologies used to evaluate the quality of data within the context of Business Intelligence (BI). We first discuss the definition and dimensions of data quality before presenting an analysis of the most commonly used evaluation criteria and measures. Our study focuses on recent works conducted among BI practitioners and highlights their preferences for data quality measurement. Finally, we provide recommendations for future research directions that would enhance our understanding of how data quality assessments can drive better BI outcomes. Overall, this work aims at providing insights into current practices and challenges related to data quality assessments in real-world applications.",1
"Skeleton-based human action recognition has achieved a great interest in recent years, as skeleton data has been demonstrated to be robust to illumination changes, body scales, dynamic camera views, and complex background. Nevertheless, an effective encoding of the latent information underlying the 3D skeleton is still an open problem. In this work, we propose a novel Spatial-Temporal Transformer network (ST-TR) which models dependencies between joints using the Transformer self-attention operator. In our ST-TR model, a Spatial Self-Attention module (SSA) is used to understand intra-frame interactions between different body parts, and a Temporal Self-Attention module (TSA) to model inter-frame correlations. The two are combined in a two-stream network which outperforms state-of-the-art models using the same input data on both NTU-RGB+D 60 and NTU-RGB+D 120.",0
"This should be included in the final version of your paper. Use proper language, structure and grammar. Spatial Temporal Transformer Network for Skeleton-based Action Recognition: Abstract In recent years, skeleton-based action recognition has become increasingly important in computer vision due to its ability to capture human motion accurately without the need for manual annotations. In this paper, we propose a new architecture called the Spatial Temporal Transformer Network (STTN) which addresses the challenges posed by current methods such as insufficient feature representation, limited spatial and temporal modeling capacity, lack of robustness against noise, etc. Our proposed network consists of two main components: spatial transformer blocks that capture global dependencies across space, and temporal transformer blocks that attend over time. These components are then combined using our designed fusion technique to capture both spatial and temporal features effectively. We evaluated our method on three widely used datasets: NTU RGB+D, PKUStep, and Kinetics Skeleton, achieving state-of-the-art results on all datasets. STTN demonstrates its superiority in handling noisy data and generalizing well across different domains. Overall, our contributions can pave the way towards more effective approaches in the field of skeleton-based action recognition.",1
"Stock trend forecasting has become a popular research direction that attracts widespread attention in the financial field. Though deep learning methods have achieved promising results, there are still many limitations, for example, how to extract clean features from the raw stock data. In this paper, we introduce an \emph{Augmented Disentanglement Distillation (ADD)} approach to remove interferential features from the noised raw data. Specifically, we present 1) a disentanglement structure to separate excess and market information from the stock data to avoid the two factors disturbing each other's own prediction. Besides, by applying 2) a dynamic self-distillation method over the disentanglement framework, other implicit interference factors can also be removed. Further, thanks to the decoder module in our framework, 3) a novel strategy is proposed to augment the training samples based on the different excess and market features to improve performance. We conduct experiments on the Chinese stock market data. Results show that our method significantly improves the stock trend forecasting performances, as well as the actual investment income through backtesting, which strongly demonstrates the effectiveness of our approach.",0
"In recent years, deep learning techniques have been applied extensively in various domains such as computer vision, natural language processing, and speech recognition. One emerging area where these techniques can make significant contributions is finance, particularly stock market prediction. This paper proposes a novel approach called Augmented DisentangledDistillation (ADD) that combines disentangled representation learning with data augmentation for improving stock trend forecasting accuracy. Our framework achieves superior performance on multiple benchmark datasets compared to state-of-the-art methods while addressing key challenges such as overfitting and high computational complexity. Moreover, we conduct extensive ablation studies to demonstrate the effectiveness of each component in our methodology, highlighting its robustness and generalizability across different settings. Finally, the insights gained from this study contribute towards developing more efficient algorithms for financial modeling and risk management applications.",1
"Popular fashion e-commerce platforms mostly provide details about low-level attributes of an apparel (eg, neck type, dress length, collar type) on their product detail pages. However, customers usually prefer to buy apparel based on their style information, or simply put, occasion (eg, party/ sports/ casual wear). Application of a supervised image-captioning model to generate style-based image captions is limited because obtaining ground-truth annotations in the form of style-based captions is difficult. This is because annotating style-based captions requires a certain amount of fashion domain expertise, and also adds to the costs and manual effort. On the contrary, low-level attribute based annotations are much more easily available. To address this issue, we propose a transfer-learning based image captioning model that is trained on a source dataset with sufficient attribute-based ground-truth captions, and used to predict style-based captions on a target dataset. The target dataset has only a limited amount of images with style-based ground-truth captions. The main motivation of our approach comes from the fact that most often there are correlations among the low-level attributes and the higher-level styles for an apparel. We leverage this fact and train our model in an encoder-decoder based framework using attention mechanism. In particular, the encoder of the model is first trained on the source dataset to obtain latent representations capturing the low-level attributes. The trained model is fine-tuned to generate style-based captions for the target dataset. To highlight the effectiveness of our method, we qualitatively and quantitatively demonstrate that the captions generated by our approach are close to the actual style information for the evaluated apparel. A Proof Of Concept for our model is under pilot at Myntra where it is exposed to some internal users for feedback.",0
"This paper presents a novel transfer learning approach for inferring fashion styles from apparel attributes using convolutional neural networks (CNNs). We train our model on large datasets consisting of garment images labeled according to specific style categories such as casual, formal, sporty, etc. Our method utilizes attribute embeddings obtained through fine-grained image classification as input features in order to effectively capture variations within each category. We validate our method by testing it on three separate datasets, demonstrating high accuracy in both cross-dataset generalization and intra-dataset adaptability. Furthermore, we conduct experiments to evaluate the contribution of different components of our system, including attribute extraction methods and network architectures. Finally, we present qualitative results highlighting the effectiveness of our model in capturing subtle differences among fashion styles.",1
"Sparse adversarial perturbations received much less attention in the literature compared to $l_2$- and $l_\infty$-attacks. However, it is equally important to accurately assess the robustness of a model against sparse perturbations. Motivated by this goal, we propose a versatile framework based on random search, Sparse-RS, for score-based sparse targeted and untargeted attacks in the black-box setting. Sparse-RS does not rely on substitute models and achieves state-of-the-art success rate and query efficiency for multiple sparse attack models: $l_0$-bounded perturbations, adversarial patches, and adversarial frames. Unlike existing methods, the $l_0$-version of untargeted Sparse-RS achieves almost 100% success rate on ImageNet by perturbing only 0.1% of the total number of pixels, outperforming all existing white-box attacks including $l_0$-PGD. Moreover, our untargeted Sparse-RS achieves very high success rates even for the challenging settings of $20\times20$ adversarial patches and $2$-pixel wide adversarial frames for $224\times224$ images. Finally, we show that Sparse-RS can be applied for universal adversarial patches where it significantly outperforms transfer-based approaches. The code of our framework is available at https://github.com/fra31/sparse-rs.",0
"Here is a sample abstract that would summarize the content of your article without starting with ""this"": Sparse-RS presents a novel methodology for crafting efficient and effective attack strategies against black-box models using sparsity constraints. Inspired by recent advances in robust surrogate models, our approach leverages gradient estimation techniques to guide the generation of adversarial examples with minimal computational cost. Our experiments demonstrate significant improvements over state-of-the-art methods across multiple datasets and architectures, highlighting the versatility and generalizability of the proposed technique. Overall, Sparse-RS represents a major step forward in the field of adversarial machine learning research, paving the way towards scalable defense solutions under real-world deployment scenarios.",1
"Manual visual inspection performed by certified inspectors is still the main form of road pothole detection. This process is, however, not only tedious, time-consuming and costly, but also dangerous for the inspectors. Furthermore, the road pothole detection results are always subjective, because they depend entirely on the individual experience. Our recently introduced disparity (or inverse depth) transformation algorithm allows better discrimination between damaged and undamaged road areas, and it can be easily deployed to any semantic segmentation network for better road pothole detection results. To boost the performance, we propose a novel attention aggregation (AA) framework, which takes the advantages of different types of attention modules. In addition, we develop an effective training set augmentation technique based on adversarial domain adaptation, where the synthetic road RGB images and transformed road disparity (or inverse depth) images are generated to enhance the training of semantic segmentation networks. The experimental results demonstrate that, firstly, the transformed disparity (or inverse depth) images become more informative; secondly, AA-UNet and AA-RTFNet, our best performing implementations, respectively outperform all other state-of-the-art single-modal and data-fusion networks for road pothole detection; and finally, the training set augmentation technique based on adversarial domain adaptation not only improves the accuracy of the state-of-the-art semantic segmentation networks, but also accelerates their convergence.",0
"In ""Learning Better Road Pothole Detection,"" we present a novel approach that combines attention aggregation and adversarial domain adaptation (ADA) techniques. Our method addresses several challenges faced by existing pothole detection systems such as sensitivity to lighting conditions, varying road surfaces, and differences in camera angles across different regions.  Our first contribution lies in developing a two-stream attentional network architecture called GALFNet which captures complementary features from multiple streams such as visual, thermal, or LiDAR data. Our second contribution introduces an innovative adversarial training scheme using virtual adversarial examples which pushes our model to learn better representations capable of generalizing to unseen domains.  Experimental results on three benchmark datasets including 279 labeled images from Chengdu University, the Pothole dataset from Kaggle containing over 8,400 images and labels, and new data collected from a moving vehicle show superior performance compared to state-of-the-art models achieving mAP scores of up to 90.6%. We provide comprehensive ablation studies to validate each componentâ€™s impact on pothole detection accuracy, including comparisons of traditional single stream versus multi-stream approaches, vanilla GANs vs. CycleGANs for image-to-image translation, and conventional cross entropy loss functions vs. our proposed adversarial domain discrepancy measures.  Overall, these findings highlight how incorporating attention aggregation along with adversarial domain adaptation can lead to significant improvements in real-world road pothole detection applications under diverse environmental conditions. These advancements have far-reaching implications in facilitati",1
"We propose Attention Grounder (AttnGrounder), a single-stage end-to-end trainable model for the task of visual grounding. Visual grounding aims to localize a specific object in an image based on a given natural language text query. Unlike previous methods that use the same text representation for every image region, we use a visual-text attention module that relates each word in the given query with every region in the corresponding image for constructing a region dependent text representation. Furthermore, for improving the localization ability of our model, we use our visual-text attention module to generate an attention mask around the referred object. The attention mask is trained as an auxiliary task using a rectangular mask generated with the provided ground-truth coordinates. We evaluate AttnGrounder on the Talk2Car dataset and show an improvement of 3.26% over the existing methods.",0
"This research investigates a novel approach to conversational interfaces called AttnGrounding. The primary aim of our study was to explore how human drivers interact with conversational cars using attention mechanisms. Our findings demonstrate that by incorporating grounded referential semantics into attention mechanisms, we can create more natural and intuitive conversations between humans and machines. We evaluated several state-of-the-art models, including transformers, sequence-to-sequence (Seq2Seq) architectures, and hybrid systems that combine both approaches, which allowed us to determine the optimal model architecture forAttnGrounding. Experimental results showed that our proposed method yielded significant improvements over other methods across all metrics. These findings have important implications for designing better user experiences in conversational agents and developing future applications that involve interactions between people and vehicles. By exploring these questions and evaluating current methods, our work contributes new insights into improving driver satisfaction through more effective communication with their vehicles. Overall, our research provides important guidance on how to improve upon existing conversational technologies to make them more accessible and usable for real-world applications, such as autonomous driving.",1
"Koopman spectral analysis has attracted attention for understanding nonlinear dynamical systems by which we can analyze nonlinear dynamics with a linear regime by lifting observations using a nonlinear function. For analysis, we need to find an appropriate lift function. Although several methods have been proposed for estimating a lift function based on neural networks, the existing methods train neural networks without spectral analysis. In this paper, we propose neural dynamic mode decomposition, in which neural networks are trained such that the forecast error is minimized when the dynamics is modeled based on spectral decomposition in the lifted space. With our proposed method, the forecast error is backpropagated through the neural networks and the spectral decomposition, enabling end-to-end learning of Koopman spectral analysis. When information is available on the frequencies or the growth rates of the dynamics, the proposed method can exploit it as regularizers for training. We also propose an extension of our approach when observations are influenced by exogenous control time-series. Our experiments demonstrate the effectiveness of our proposed method in terms of eigenvalue estimation and forecast performance.",0
"This paper presents a novel method called neural dynamic mode decomposition (NDMD) that enables end-to-end modeling of nonlinear dynamics using deep learning techniques. NDMD combines elements from several established methods including dynamic mode decomposition, Koopman operator theory, and artificial neural networks. The proposed approach allows us to capture complex underlying patterns and structures present in high-dimensional dynamical systems through approximation by linear operators, resulting in efficient computation and accurate predictions. Our framework has been tested on a variety of challenging problems including fluid simulations and chaotic systems, demonstrating excellent performance across different datasets and experimental settings. Overall, our work provides a promising new direction towards realizing robust, scalable, and flexible algorithms capable of accurately capturing the behavior of complex nonlinear systems. We believe that our findings have significant implications for both fundamental understanding and practical applications in many fields such as physics, engineering, chemistry, neuroscience, economics, and finance.",1
"Deep learning models for graphs, especially Graph Convolutional Networks (GCNs), have achieved remarkable performance in the task of semi-supervised node classification. However, recent studies show that GCNs suffer from adversarial perturbations. Such vulnerability to adversarial attacks significantly decreases the stability of GCNs when being applied to security-critical applications. Defense methods such as preprocessing, attention mechanism and adversarial training have been discussed by various studies. While being able to achieve desirable performance when the perturbation rates are low, such methods are still vulnerable to high perturbation rates. Meanwhile, some defending algorithms perform poorly when the node features are not visible. Therefore, in this paper, we propose a novel mechanism called influence mechanism, which is able to enhance the robustness of the GCNs significantly. The influence mechanism divides the effect of each node into two parts: introverted influence which tries to maintain its own features and extroverted influence which exerts influences on other nodes. Utilizing the influence mechanism, we propose the Influence GCN (I-GCN) model. Extensive experiments show that our proposed model is able to achieve higher accuracy rates than state-of-the-art methods when defending against non-targeted attacks.",0
"Graph Neural Networks (GNN) has become one of the most popular models used in graph analysis tasks since their introduction due to their ability to learn meaningful representations from irregularly structured data such as social networks, protein structures, and knowledge graphs. However, existing GNN architectures suffer from two major drawbacks that hinder their performance on real-world applications: overfitting on small datasets caused by insufficient model capacity; and sensitivity to hyperparameter settings such as learning rate and batch size, which makes them difficult to tune and apply to new domains. To address these challenges, we propose I-GCN, a novel influence mechanism for GCNs, that improves robustness by exploiting node influences to identify important edges and suppress irrelevant ones during training. Our experiments demonstrate that I-GCN outperforms state-of-the art methods across various benchmarks for link prediction, node classification and regression, while achieving better stability, interpretability and explainability through attention maps highlighting significant relationships among nodes. Furthermore, I-GCN can be easily integrated into different GNN variants without altering the core architecture, making our contributions applicable to many other scenarios where graph neural networks are employed.",1
"Matrix completion has received vast amount of attention and research due to its wide applications in various study fields. Existing methods of matrix completion consider only nonlinear (or linear) relations among entries in a data matrix and ignore linear (or nonlinear) relationships latent. This paper introduces a new latent variables model for data matrix which is a combination of linear and nonlinear models and designs a novel deep-neural-network-based matrix completion algorithm to address both linear and nonlinear relations among entries of data matrix. The proposed method consists of two branches. The first branch learns the latent representations of columns and reconstructs the columns of the partially observed matrix through a series of hidden neural network layers. The second branch does the same for the rows. In addition, based on multi-task learning principles, we enforce these two branches work together and introduce a new regularization technique to reduce over-fitting. More specifically, the missing entries of data are recovered as a main task and manifold learning is performed as an auxiliary task. The auxiliary task constrains the weights of the network so it can be considered as a regularizer, improving the main task and reducing over-fitting. Experimental results obtained on the synthetic data and several real-world data verify the effectiveness of the proposed method compared with state-of-the-art matrix completion methods.",0
"Incomplete data is a common problem across many domains, such as sensor readings from incomplete datasets. One approach to recovering missing data points is matrix completion (MC), which models the relationship among known entries by using low rank assumptions on the underlying data matrix. However, traditional MC methods tend to overfit noisy data and produce unsatisfactory results when encountering large amounts of missing values. To address these issues, we propose a novel deep learning approach based on manifold learning that leverages both local geometric structure and global contextual information. By constructing a nonlinear representation space of partially observed matrices via autoencoders trained jointly with a reconstruction task, our method encourages the latent features of partial observations to lie on a lower dimensional and more robust manifold space. Experimental evaluations demonstrate that our framework achieves state-of-the-art performance compared to popular baseline techniques in several applications including image imputation and recommendation systems. Our work bridges classical matrix recovery theories and recent advancements in deep learning research, promising new perspectives for solving complex problems involving high-dimensional structured data.",1
"Attention and self-attention mechanisms, are now central to state-of-the-art deep learning on sequential tasks. However, most recent progress hinges on heuristic approaches with limited understanding of attention's role in model optimization and computation, and rely on considerable memory and computational resources that scale poorly. In this work, we present a formal analysis of how self-attention affects gradient propagation in recurrent networks, and prove that it mitigates the problem of vanishing gradients when trying to capture long-term dependencies by establishing concrete bounds for gradient norms. Building on these results, we propose a relevancy screening mechanism, inspired by the cognitive process of memory consolidation, that allows for a scalable use of sparse self-attention with recurrence. While providing guarantees to avoid vanishing gradients, we use simple numerical experiments to demonstrate the tradeoffs in performance and computational resources by efficiently balancing attention and recurrence. Based on our results, we propose a concrete direction of research to improve scalability of attentive networks.",0
"While neural network architectures have made great strides in recent years, designing an architecture that balances recurrence and self-attention remains a challenging task. This paper explores the tradeoffs between these two mechanisms and presents insights into their relative strengths and weaknesses. We demonstrate that both recurrence and self-attention can lead to high performance in some cases, but they differ markedly in terms of computational cost, memory usage, and model interpretability. Our experiments show that recurrent models are generally more efficient and easier to interpret than self-attentive ones, while self-attention-based models achieve better accuracy under certain conditions. Moreover, we evaluate alternative approaches such as hybrid models that combine recurrency and attention mechanisms, which may provide a promising direction towards achieving optimal results in practice. Overall, our findings shed light on the complex interactions between recurrence and self-attention mechanisms in deep learning models, providing valuable guidelines for future research in this area.",1
"We argue that the vulnerability of model parameters is of crucial value to the study of model robustness and generalization but little research has been devoted to understanding this matter. In this work, we propose an indicator to measure the robustness of neural network parameters by exploiting their vulnerability via parameter corruption. The proposed indicator describes the maximum loss variation in the non-trivial worst-case scenario under parameter corruption. For practical purposes, we give a gradient-based estimation, which is far more effective than random corruption trials that can hardly induce the worst accuracy degradation. Equipped with theoretical support and empirical validation, we are able to systematically investigate the robustness of different model parameters and reveal vulnerability of deep neural networks that has been rarely paid attention to before. Moreover, we can enhance the models accordingly with the proposed adversarial corruption-resistant training, which not only improves the parameter robustness but also translates into accuracy elevation.",0
"This abstract describes some key results from recent research aimed at improving our understanding of deep neural networks (DNNs) by examining their vulnerability under different types of parameter corruption attacks. By analyzing DNN models trained on image classification tasks using several commonly used datasets, we observed that many modern state-of-the-art architectures exhibit strong robustness against traditional adversarial attacks but remain highly susceptible to more subtle forms of manipulation. Our findings suggest that even relatively small changes to individual weights can have significant impacts on model performance and reveal potential weaknesses in current training methods that may require new approaches to improve overall network resilience. These insights should prove valuable to both practitioners working on designing robust machine learning systems and researchers seeking to better comprehend how DNNs function in complex environments where data quality cannot always be guaranteed. Ultimately, our work underscores the importance of continued exploration into the reliability of artificial intelligence systems as they become increasingly integrated into critical decision-making processes across various industries and domains.",1
"Supervised learning under label noise has seen numerous advances recently, while existing theoretical findings and empirical results broadly build up on the class-conditional noise (CCN) assumption that the noise is independent of input features given the true label. In this work, we present a theoretical hypothesis testing and prove that noise in real-world dataset is unlikely to be CCN, which confirms that label noise should depend on the instance and justifies the urgent need to go beyond the CCN assumption.The theoretical results motivate us to study the more general and practical-relevant instance-dependent noise (IDN). To stimulate the development of theory and methodology on IDN, we formalize an algorithm to generate controllable IDN and present both theoretical and empirical evidence to show that IDN is semantically meaningful and challenging. As a primary attempt to combat IDN, we present a tiny algorithm termed self-evolution average label (SEAL), which not only stands out under IDN with various noise fractions, but also improves the generalization on real-world noise benchmark Clothing1M. Our code is released. Notably, our theoretical analysis in Section 2 provides rigorous motivations for studying IDN, which is an important topic that deserves more research attention in future.",0
"In many natural language processing tasks, instance-dependent label noise can pose significant challenges, leading to degraded performance on downstream models. Previous approaches have often relied on strong class conditional assumptions which limit their ability to handle these noisy labels effectively. This paper presents a novel approach that goes beyond traditional class-conditional assumption by leveraging both global consistency constraints and fine-grained semantic representations. Our method jointly optimizes three components: 1) model parameters, 2) global softmax distribution over classes conditioned on instances, and 3) fine-grained embedding alignments between pairs of instances from different classes. Extensive experiments across four benchmark datasets demonstrate the effectiveness of our proposed approach in reducing the impact of instance-dependent label noise and improving overall task performance compared to baseline methods.",1
Safety validation is important during the development of safety-critical autonomous systems but can require significant computational effort. Existing algorithms often start from scratch each time the system under test changes. We apply transfer learning to improve the efficiency of reinforcement learning based safety validation algorithms when applied to related systems. Knowledge from previous safety validation tasks is encoded through the action value function and transferred to future tasks with a learned set of attention weights. Including a learned state and action value transformation for each source task can improve performance even when systems have substantially different failure modes. We conduct experiments on safety validation tasks in gridworld and autonomous driving scenarios. We show that transfer learning can improve the initial and final performance of validation algorithms and reduce the number of training steps.,0
"This paper presents an approach for iteratively validating safety properties using transfer learning from previously annotated training examples. We address the challenge of scaling manual annotation efforts by leveraging existing annotations and adapting models based on similar use cases. Our proposed method combines active learning and fine-tuning techniques to improve model accuracy while reducing annotation effort. Experiments show that our approach achieves state-of-the-art results across different benchmarks, significantly outperforming baseline methods.",1
"Texts appearing in daily scenes that can be recognized by OCR (Optical Character Recognition) tools contain significant information, such as street name, product brand and prices. Two tasks -- text-based visual question answering and text-based image captioning, with a text extension from existing vision-language applications, are catching on rapidly. To address these problems, many sophisticated multi-modality encoding frameworks (such as heterogeneous graph structure) are being used. In this paper, we argue that a simple attention mechanism can do the same or even better job without any bells and whistles. Under this mechanism, we simply split OCR token features into separate visual- and linguistic-attention branches, and send them to a popular Transformer decoder to generate answers or captions. Surprisingly, we find this simple baseline model is rather strong -- it consistently outperforms state-of-the-art (SOTA) models on two popular benchmarks, TextVQA and all three tasks of ST-VQA, although these SOTA models use far more complex encoding mechanisms. Transferring it to text-based image captioning, we also surpass the TextCaps Challenge 2020 winner. We wish this work to set the new baseline for this two OCR text related applications and to inspire new thinking of multi-modality encoder design. Code is available at https://github.com/ZephyrZhuQi/ssbaseline",0
"This paper presents a simple yet strong baseline model for textual visual question answering (TextVQA) and textual capsule question answering (TextCaps). Despite its simplicity, our approach outperforms many state-of-the-art models on both benchmarks, demonstrating that a well-engineered simple model can still achieve strong performance. We believe our work provides valuable insights into the importance of proper engineering and hyperparameter tuning in modern deep learning research. Furthermore, we hope our findings motivate future researchers to pursue simpler approaches that prioritize understanding over raw performance. Overall, our results highlight the need for continued evaluation of existing methods and the development of new, more interpretable techniques.",1
"With growth in the number of smart devices and advancements in their hardware, in recent years, data-driven machine learning techniques have drawn significant attention. However, due to privacy and communication issues, it is not possible to collect this data at a centralized location. Federated learning is a machine learning setting where the centralized location trains a learning model over remote devices. Federated learning algorithms cannot be employed in the real world scenarios unless they consider unreliable and resource-constrained nature of the wireless medium. In this paper, we propose a federated learning algorithm that is suitable for cellular wireless networks. We prove its convergence, and provide the optimal scheduling policy that maximizes the convergence rate. We also study the effect of local computation steps and communication steps on the convergence of the proposed algorithm. We prove, in practice, federated learning algorithms may solve a different problem than the one that they have been employed for if the unreliability of wireless channels is neglected. Finally, through numerous experiments on real and synthetic datasets, we demonstrate the convergence of our proposed algorithm.",0
"Federated learning has emerged as a promising approach for machine learning in wireless networks that allows distributed devices to collaboratively train shared models without sharing their data. However, deploying federated learning in cellular wireless networks poses several challenges due to unreliable network conditions such as intermittent connectivity, limited device resources, high communication latencies, and varying levels of Quality of Service (QoS). This paper addresses these challenges by proposing novel techniques for federated learning in unreliable and resource-constrained cellular environments. We first present a robust asynchronous federated learning algorithm that can handle intermittent connectivity by allowing devices to join the training process at arbitrary times and gradually catch up on missed rounds. Next, we introduce a lightweight quantization method that significantly reduces the computation and communication overheads of federated learning while maintaining comparable accuracy. Furthermore, our scheme incorporates local model caching and prioritized aggregation mechanisms that enable efficient handling of bandwidth constraints and variable QoS conditions. Our extensive simulations showcase the benefits of our framework under diverse realistic scenarios, demonstrating significant improvements over state-of-the-art methods in terms of convergence speed, accuracy, and scalability. Overall, our work paves the way towards enabling practical federated learning solutions for next-generation wireless systems where infrastructure limitations pose critical challenges for traditional centralized approaches.",1
"Acquiring sufficient ground-truth supervision to train deep visual models has been a bottleneck over the years due to the data-hungry nature of deep learning. This is exacerbated in some structured prediction tasks, such as semantic segmentation, which requires pixel-level annotations. This work addresses weakly supervised semantic segmentation (WSSS), with the goal of bridging the gap between image-level annotations and pixel-level segmentation. We formulate WSSS as a novel group-wise learning task that explicitly models semantic dependencies in a group of images to estimate more reliable pseudo ground-truths, which can be used for training more accurate segmentation models. In particular, we devise a graph neural network (GNN) for group-wise semantic mining, wherein input images are represented as graph nodes, and the underlying relations between a pair of images are characterized by an efficient co-attention mechanism. Moreover, in order to prevent the model from paying excessive attention to common semantics only, we further propose a graph dropout layer, encouraging the model to learn more accurate and complete object responses. The whole network is end-to-end trainable by iterative message passing, which propagates interaction cues over the images to progressively improve the performance. We conduct experiments on the popular PASCAL VOC 2012 and COCO benchmarks, and our model yields state-of-the-art performance. Our code is available at: https://github.com/Lixy1997/Group-WSSS.",0
"Abstract: In recent years, deep learning has revolutionized computer vision by enabling breakthroughs in tasks such as object detection, semantic segmentation, and image classification. However, these advances have mainly relied on fully supervised methods that require large amounts of labeled data, which can be time-consuming and expensive to acquire. In contrast, weakly supervised methods leverage unlabeled images along with weaker forms of annotation, like image labels or bounding boxes, to improve performance without requiring extensive labeling efforts. One promising direction towards harnessing groupwise techniques for weakly supervised training is through exploring how they can be applied to more complex high-level representations beyond simple pixels or channels. This work addresses these challenges by developing new algorithms based on graph theory, spectral clustering, and machine learning techniques tailored for semantic segmentation using only image-level annotations. Our approach achieves state-of-the-art results across multiple benchmark datasets, demonstrating that the incorporation of our novel grouping strategy effectively combines both instance discrimination and global context to facilitate weakly supervised fine-grained pixel understanding. These findings lay the foundation for future research into effective ways to apply groupwise mining strategies to enable powerful weakly supervised learning paradigms for other imaging domains.",1
"Cross-domain few-shot classification task (CD-FSC) combines few-shot classification with the requirement to generalize across domains represented by datasets. This setup faces challenges originating from the limited labeled data in each class and, additionally, from the domain shift between training and test sets. In this paper, we introduce a novel training approach for existing FSC models. It leverages on the explanation scores, obtained from existing explanation methods when applied to the predictions of FSC models, computed for intermediate feature maps of the models. Firstly, we tailor the layer-wise relevance propagation (LRP) method to explain the predictions of FSC models. Secondly, we develop a model-agnostic explanation-guided training strategy that dynamically finds and emphasizes the features which are important for the predictions. Our contribution does not target a novel explanation method but lies in a novel application of explanations for the training phase. We show that explanation-guided training effectively improves the model generalization. We observe improved accuracy for three different FSC models: RelationNet, cross attention network, and a graph neural network-based formulation, on five few-shot learning datasets: miniImagenet, CUB, Cars, Places, and Plantae. The source code is available at https://github.com/SunJiamei/few-shot-lrp-guided",0
"This paper presents a novel method for few-shot classification across different domains, called ""Explanation-Guided Training"" (XGT). With traditional approaches, models struggle to generalize well across domains due to differences in data distributions and features. XGT addresses this by first analyzing the feature distribution across domains, generating explanation vectors that capture the main factors contributing to the feature variability. These explanations guide subsequent training steps, allowing the model to better learn domain-invariant representations. We evaluate our approach on several benchmark datasets, demonstrating state-of-the-art performance over other baseline methods. Our results highlight the effectiveness of using explanatory techniques in improving cross-domain generalization for few-shot learning tasks.",1
"We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.",0
In TabNet we present an approach towards attentive interpretable tabular learning using neural networks. We focus on building lightweight models that excel at capturing dependencies between fields without requiring explicit annotations beyond raw data input. Our method uses row-wise attention over values in tables and yields highly efficient architectures that perform well across several real world benchmark datasets. Through extensive experiments and visualizations we showcase model interpretability via feature importance heatmaps and qualitative analyses. Our work can serve as a fundamental component towards robust large scale deployments of machine learning solutions for domains such as finance where human-in-the-loop decision making remains crucial.,1
"Sparse labels have been attracting much attention in recent years. However, the performance gap between weakly supervised and fully supervised salient object detection methods is huge, and most previous weakly supervised works adopt complex training methods with many bells and whistles. In this work, we propose a one-round end-to-end training approach for weakly supervised salient object detection via scribble annotations without pre/post-processing operations or extra supervision data. Since scribble labels fail to offer detailed salient regions, we propose a local coherence loss to propagate the labels to unlabeled regions based on image features and pixel distance, so as to predict integral salient regions with complete object structures. We design a saliency structure consistency loss as self-consistent mechanism to ensure consistent saliency maps are predicted with different scales of the same image as input, which could be viewed as a regularization technique to enhance the model generalization ability. Additionally, we design an aggregation module (AGGM) to better integrate high-level features, low-level features and global context information for the decoder to aggregate various information. Extensive experiments show that our method achieves a new state-of-the-art performance on six benchmarks (e.g. for the ECSSD dataset: F_\beta = 0.8995, E_\xi = 0.9079 and MAE = 0.0489$), with an average gain of 4.60\% for F-measure, 2.05\% for E-measure and 1.88\% for MAE over the previous best method on this task. Source code is available at http://github.com/siyueyu/SCWSSOD.",0
"This study proposes a weakly supervised method using local saliency coherence (LSC) within each detected region for structure-consistency to detect salient objects from images. The proposed approach consists of two main steps: generating candidate object regions by applying an off-the-shelf edge detector followed by non-maximum suppression, and optimizing their quality via LSC. To ensure structural consistency, we further refine these candidates using a graphical model based on visual cues between them and obtain the final set of object regions. Extensive experiments conducted on six benchmark datasets demonstrate that our approach significantly outperforms existing weakly supervised methods while achieving comparable results with fully supervised ones. Moreover, ablation studies validate the effectiveness of different components in our framework. Overall, the proposed weakly supervised method provides a promising alternative for accurate salient object detection without requiring large amounts of annotated data.",1
"Recently, model-free reinforcement learning has attracted research attention due to its simplicity, memory and computation efficiency, and the flexibility to combine with function approximation. In this paper, we propose Exploration Enhanced Q-learning (EE-QL), a model-free algorithm for infinite-horizon average-reward Markov Decision Processes (MDPs) that achieves regret bound of $O(\sqrt{T})$ for the general class of weakly communicating MDPs, where $T$ is the number of interactions. EE-QL assumes that an online concentrating approximation of the optimal average reward is available. This is the first model-free learning algorithm that achieves $O(\sqrt T)$ regret without the ergodic assumption, and matches the lower bound in terms of $T$ except for logarithmic factors. Experiments show that the proposed algorithm performs as well as the best known model-based algorithms.",0
"This should describe the problem studied in the paper, including relevant math symbols where appropriate. Also describe the solution methodology and important results/contributions of the work. Be concise but clear, highlighting key points. Finally state any limitations that would prevent application to real systems like computational complexity (if applicable). If possible please mention why your model canâ€™t be solved by current methods? Please try to stay within the recommended limit if at all possible!",1
"We propose an unsupervised capsule architecture for 3D point clouds. We compute capsule decompositions of objects through permutation-equivariant attention, and self-supervise the process by training with pairs of randomly rotated objects. Our key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning. In doing so, we require neither classification labels nor manually-aligned training datasets to train. Yet, by learning an object-centric representation in an unsupervised manner, our method outperforms the state-of-the-art on 3D point cloud reconstruction, registration, and unsupervised classification. We will release the code and dataset to reproduce our results as soon as the paper is published.",0
"In recent years, there has been increasing interest in developing unsupervised learning methods that can learn from large amounts of data without requiring explicit labeling or human supervision. One promising approach to achieve this goal is through the use of capsule networks, which have shown state-of-the-art performance on several computer vision tasks such as image classification and object detection. However, one of the main limitations of current capsule network architectures is their limited ability to generalize across different poses and orientations of objects within the images they process. This lack of robustness often results in poor performance when the model encounters new scenes or variations of previously seen objects. To address these issues, we propose a novel architecture called canonical capsules, which embeds pose estimation into the capsule computation itself. By doing so, our method enables the encoding of object structures in world space coordinates while allowing transformation equivariance. We evaluate our proposed architecture using several benchmark datasets and demonstrate improved performance compared to existing state-of-the-art methods in both accuracy and stability under changes in scale and orientation of input samples. Our work represents an important step forward towards building more advanced systems capable of processing real-world image data in an unsupervised manner.",1
"Recent advances in self-supervised learning (SSL) have largely closed the gap with supervised ImageNet pretraining. Despite their success these methods have been primarily applied to unlabeled ImageNet images, and show marginal gains when trained on larger sets of uncurated images. We hypothesize that current SSL methods perform best on iconic images, and struggle on complex scene images with many objects. Analyzing contrastive SSL methods shows that they have poor visual grounding and receive poor supervisory signal when trained on scene images. We propose Contrastive Attention-Supervised Tuning(CAST) to overcome these limitations. CAST uses unsupervised saliency maps to intelligently sample crops, and to provide grounding supervision via a Grad-CAM attention loss. Experiments on COCO show that CAST significantly improves the features learned by SSL methods on scene images, and further experiments show that CAST-trained models are more robust to changes in backgrounds.",0
"This paper explores how unlabeled data can improve computer vision models by first pretraining on ImageNet, then fine tuning on unlabeled COCO images with localization, rotation and scale augmentation, and further self supervision using Instance Normalized Cross Entropy loss.",1
"The task of point cloud upsampling aims to acquire dense and uniform point sets from sparse and irregular point sets. Although significant progress has been made with deep learning models, they require ground-truth dense point sets as the supervision information, which can only trained on synthetic paired training data and are not suitable for training under real-scanned sparse data. However, it is expensive and tedious to obtain large scale paired sparse-dense point sets for training from real scanned sparse data. To address this problem, we propose a self-supervised point cloud upsampling network, named SPU-Net, to capture the inherent upsampling patterns of points lying on the underlying object surface. Specifically, we propose a coarse-to-fine reconstruction framework, which contains two main components: point feature extraction and point feature expansion, respectively. In the point feature extraction, we integrate self-attention module with graph convolution network (GCN) to simultaneously capture context information inside and among local regions. In the point feature expansion, we introduce a hierarchically learnable folding strategy to generate the upsampled point sets with learnable 2D grids. Moreover, to further optimize the noisy points in the generated point sets, we propose a novel self-projection optimization associated with uniform and reconstruction terms, as a joint loss, to facilitate the self-supervised point cloud upsampling. We conduct various experiments on both synthetic and real-scanned datasets, and the results demonstrate that we achieve comparable performance to the state-of-the-art supervised methods.",0
"This paper presents a novel method for self-supervised point cloud upsampling using coarse-to-fine reconstruction with self-projection optimization (SPU-Net). The proposed method leverages recent advances in deep learning and geometric modeling to generate high-resolution representations of 3D scenes from low-resolution input data.  The core idea behind SPU-Net is to learn a mapping between the low-resolution point clouds and their corresponding high-resolution counterparts through iterative coarse-to-fine reconstructions. At each iteration, a dense initial guess is used as input to reconstruct a new set of points that are more consistent with both the original input and any previous iterations. An additional refinement stage then optimizes the final output based on the current estimate and its projection onto itself at different levels of detail.  Experimental results demonstrate the effectiveness of our approach across multiple benchmark datasets, outperforming state-of-the-art methods in terms of accuracy and efficiency. Overall, SPU-Net represents a significant step forward in the field of unsupervised point cloud processing, opening up exciting possibilities for applications such as robotics, computer vision, and virtual reality.",1
"Arguably the key reason for the success of deep neural networks is their ability to autonomously form non-linear combinations of the input features, which can be used in subsequent layers of the network. The analogon to this capability in inductive rule learning is to learn a structured rule base, where the inputs are combined to learn new auxiliary concepts, which can then be used as inputs by subsequent rules. Yet, research on rule learning algorithms that have such capabilities is still in their infancy, which is - we would argue - one of the key impediments to substantial progress in this field. In this position paper, we want to draw attention to this unsolved problem, with a particular focus on previous work in predicate invention and multi-label rule learning",0
"In recent years, deep learning has emerged as a powerful tool for solving complex tasks such as image classification, speech recognition, and natural language processing. However, one significant challenge that remains unsolved is how to learn structured declarative rule sets from data.  Rule-based systems have been used successfully in many domains due to their transparency, modularity, and interpretability. They allow domain experts to specify knowledge in a formal manner that can be easily understood and modified. Moreover, rules provide explicit control over generalization and interpolation which is essential in applications where the goal is to achieve high accuracy without overfitting.  However, obtaining these rule sets requires substantial manual effort by human experts. On the other hand, existing approaches for automating rule discovery using machine learning techniques focus on heuristics rather than guarantees. Therefore, there is a need for a methodology that combines the strengths of both worlds: a systematic approach that discovers structured declarative rule sets from raw input data with minimal user intervention while guaranteeing soundness.  This paper presents a novel framework called Deep Discrete Learning (DDL) that addresses this problem by formulating the task as a constrained optimization problem. DDL constructs a neural network architecture consisting of multiple layers of discrete neurons that map directly into decision trees. Each layer represents a set of conditions that correspond to literals in first-order logic clauses. The output is modeled as a weighted combination of these clauses which are learned through backpropagation.  The constraints imposed ensure monotonicity properties that preserve truth values as well as local consistency constraints based on resolution inference. By incorporating these constraints during training, we enforce structural regularities present in first-order logic. We showcase our approach on several benchmark datasets across diverse domains including bioinformatics, computer vision, and text analysis, outperforming state-of-the-art baselines. Our experiments demonst",1
"Recently end-to-end scene text spotting has become a popular research topic due to its advantages of global optimization and high maintainability in real applications. Most methods attempt to develop various region of interest (RoI) operations to concatenate the detection part and the sequence recognition part into a two-stage text spotting framework. However, in such framework, the recognition part is highly sensitive to the detected results (\emph{e.g.}, the compactness of text contours). To address this problem, in this paper, we propose a novel Mask AttentioN Guided One-stage text spotting framework named MANGO, in which character sequences can be directly recognized without RoI operation. Concretely, a position-aware mask attention module is developed to generate attention weights on each text instance and its characters. It allows different text instances in an image to be allocated on different feature map channels which are further grouped as a batch of instance features. Finally, a lightweight sequence decoder is applied to generate the character sequences. It is worth noting that MANGO inherently adapts to arbitrary-shaped text spotting and can be trained end-to-end with only coarse position information (\emph{e.g.}, rectangular bounding box) and text annotations. Experimental results show that the proposed method achieves competitive and even new state-of-the-art performance on both regular and irregular text spotting benchmarks, i.e., ICDAR 2013, ICDAR 2015, Total-Text, and SCUT-CTW1500.",0
"This paper presents a new architecture called MANGO (Mask Attention Guided One-stage Scene Text Spotter) that can detect text in natural scenes using deep learning techniques. We use a one-stage approach which eliminates the need for multiple stage architectures that slow down processing time. Our model uses attention mechanisms to improve performance by focusing on the most important features in each image. By doing so, we achieve state-of-the-art results while maintaining fast inference speed. Our experiments show that our method outperforms other approaches in terms of both accuracy and efficiency.",1
"Over the last few years, the performance of inpainting to fill missing regions has shown significant improvements by using deep neural networks. Most of inpainting work create a visually plausible structure and texture, however, due to them often generating a blurry result, final outcomes appear unrealistic and make feel heterogeneity. In order to solve this problem, the existing methods have used a patch based solution with deep neural network, however, these methods also cannot transfer the texture properly. Motivated by these observation, we propose a patch based method. Texture Transform Attention network(TTA-Net) that better produces the missing region inpainting with fine details. The task is a single refinement network and takes the form of U-Net architecture that transfers fine texture features of encoder to coarse semantic features of decoder through skip-connection. Texture Transform Attention is used to create a new reassembled texture map using fine textures and coarse semantics that can efficiently transfer texture information as a result. To stabilize training process, we use a VGG feature layer of ground truth and patch discriminator. We evaluate our model end-to-end with the publicly available datasets CelebA-HQ and Places2 and demonstrate that images of higher quality can be obtained to the existing state-of-the-art approaches.",0
"This paper presents a novel approach to image inpainting using texture transform attention (TTA). Traditional image inpainting methods often suffer from artifacts and lack of detail, particularly on complex textures such as brush strokes and patterns. TTA addresses these issues by adaptively attending to different textural features within the input image, rather than simply applying global smoothing techniques. By focusing on local structure and preserving underlying textures, our method produces more realistic and coherent results compared to state-of-the-art approaches. We evaluate our method on several benchmark datasets and show that it outperforms competing algorithms both qualitatively and quantitatively. Our work demonstrates the potential of texture-aware attention mechanisms in improving the performance of image inpainting tasks.",1
"Land-cover classification using remote sensing imagery is an important Earth observation task. Recently, land cover classification has benefited from the development of fully connected neural networks for semantic segmentation. The benchmark datasets available for training deep segmentation models in remote sensing imagery tend to be small, however, often consisting of only a handful of images from a single location with a single scale. This limits the models' ability to generalize to other datasets. Domain adaptation has been proposed to improve the models' generalization but we find these approaches are not effective for dealing with the scale variation commonly found between remote sensing image collections. We therefore propose a scale aware adversarial learning framework to perform joint cross-location and cross-scale land-cover classification. The framework has a dual discriminator architecture with a standard feature discriminator as well as a novel scale discriminator. We also introduce a scale attention module which produces scale-enhanced features. Experimental results show that the proposed framework outperforms state-of-the-art domain adaptation methods by a large margin.",0
"This work presents a novel approach that improves land cover classification accuracy through scale-aware adaptation (SAA). We introduce a systematic framework to optimize feature descriptors at multiple scales and adapt them based on local contexts at different spatial resolutions in remote sensing imagery. Our strategy effectively integrates multi-scale features into a unified model for accurate land cover mapping across diverse applications including urban planning and resource monitoring. Experimental results demonstrate improved performance over state-of-the-art methods for four public datasets, validating our proposed methodology.",1
"3D object detection based on LiDAR-camera fusion is becoming an emerging research theme for autonomous driving. However, it has been surprisingly difficult to effectively fuse both modalities without information loss and interference. To solve this issue, we propose a single-stage multi-view fusion framework that takes LiDAR bird's-eye view, LiDAR range view and camera view images as inputs for 3D object detection. To effectively fuse multi-view features, we propose an attentive pointwise fusion (APF) module to estimate the importance of the three sources with attention mechanisms that can achieve adaptive fusion of multi-view features in a pointwise manner. Furthermore, an attentive pointwise weighting (APW) module is designed to help the network learn structure information and point feature importance with two extra tasks, namely, foreground classification and center regression, and the predicted foreground probability is used to reweight the point features. We design an end-to-end learnable network named MVAF-Net to integrate these two components. Our evaluations conducted on the KITTI 3D object detection datasets demonstrate that the proposed APF and APW modules offer significant performance gains. Moreover, the proposed MVAF-Net achieves the best performance among all single-stage fusion methods and outperforms most two-stage fusion methods, achieving the best trade-off between speed and accuracy on the KITTI benchmark.",0
"â€‹The paper presents a new approach for multi-view object detection using deep learning methods. The proposed method utilizes multiple views of each object to improve accuracy and robustness over traditional single view approaches. By incorporating adaptive fusion techniques into a convolutional neural network architecture, our model can learn which features are most important for detecting objects in different scenes. Our results show that the multi-view adaptive fusion network significantly outperforms state-of-the art models on several benchmark datasets. With advancements in technology leading to more cameras and sensors in cars and other vehicles, this work has applications in autonomous driving and scene understanding. Overall, we believe this research represents a significant step forward in 3D object detection.",1
"Visual object tracking, as a fundamental task in computer vision, has drawn much attention in recent years. To extend trackers to a wider range of applications, researchers have introduced information from multiple modalities to handle specific scenes, which is a promising research prospect with emerging methods and benchmarks. To provide a thorough review of multi-modal track-ing, we summarize the multi-modal tracking algorithms, especially visible-depth (RGB-D) tracking and visible-thermal (RGB-T) tracking in a unified taxonomy from different aspects. Second, we provide a detailed description of the related benchmarks and challenges. Furthermore, we conduct extensive experiments to analyze the effectiveness of trackers on five datasets: PTB, VOT19-RGBD, GTOT, RGBT234, and VOT19-RGBT. Finally, we discuss various future directions from different perspectives, including model design and dataset construction for further research.",0
"Abstract: This paper presents a comprehensive review and experimental comparison of multi-modal visual tracking methods. We analyze state-of-the-art trackers that use multiple modalities such as color, texture, depth, motion, and others to improve robustness and accuracy. Our review includes both traditional feature-based trackers and modern deep learning-based approaches. We evaluate each method using publicly available benchmarks and provide detailed comparisons of their performance under different conditions. Our experiments show that while many trackers have achieved impressive results on specific benchmark datasets, there remains significant room for improvement in handling challenging scenes with occlusions, fast motion, and camera movements. Finally, we discuss potential future directions for research in multi-modal visual tracking, including incorporating more advanced sensors and enabling real-time deployment on resource-constrained devices.",1
"Weakly-supervised learning has attracted growing research attention on medical lesions segmentation due to significant saving in pixel-level annotation cost. However, 1) most existing methods require effective prior and constraints to explore the intrinsic lesions characterization, which only generates incorrect and rough prediction; 2) they neglect the underlying semantic dependencies among weakly-labeled target enteroscopy diseases and fully-annotated source gastroscope lesions, while forcefully utilizing untransferable dependencies leads to the negative performance. To tackle above issues, we propose a new weakly-supervised lesions transfer framework, which can not only explore transferable domain-invariant knowledge across different datasets, but also prevent the negative transfer of untransferable representations. Specifically, a Wasserstein quantified transferability framework is developed to highlight widerange transferable contextual dependencies, while neglecting the irrelevant semantic characterizations. Moreover, a novel selfsupervised pseudo label generator is designed to equally provide confident pseudo pixel labels for both hard-to-transfer and easyto-transfer target samples. It inhibits the enormous deviation of false pseudo pixel labels under the self-supervision manner. Afterwards, dynamically-searched feature centroids are aligned to narrow category-wise distribution shift. Comprehensive theoretical analysis and experiments show the superiority of our model on the endoscopic dataset and several public datasets.",0
"In this work, we propose a weakly-supervised cross-domain adaptation approach for endoscopic lesion segmentation that leverages both labeled data from the source domain and unlabeled data from the target domain. Our method utilizes adversarial training to learn a feature extractor that can capture features that are relevant for lesion segmentation while minimizing the domain shift between the two domains. Additionally, we use semantic consistency loss to ensure that our model produces consistent predictions across different domains. Experimental results on two public datasets demonstrate the effectiveness of our proposed method compared to other state-of-the-art approaches for cross-domain adaptation. Our approach has the potential to improve the accuracy of automated lesion detection systems, which could ultimately lead to earlier diagnosis and better treatment outcomes for patients with digestive tract diseases.",1
"The purpose of this article is to review the achievements made in the last few years towards the understanding of the reasons behind the success and subtleties of neural network-based machine learning. In the tradition of good old applied mathematics, we will not only give attention to rigorous mathematical results, but also the insight we have gained from careful numerical experiments as well as the analysis of simplified models. Along the way, we also list the open problems which we believe to be the most important topics for further study. This is not a complete overview over this quickly moving field, but we hope to provide a perspective which may be helpful especially to new researchers in the area.",0
"Abstract: This paper provides a comprehensive overview of mathematical approaches to understanding neural network-based machine learning. While these methods have shown great success in numerous applications, there remain many open questions regarding their workings and limitations. We provide a detailed analysis of existing knowledge on the subject, highlighting key insights into how neural networks function and learn. Furthermore, we identify areas where current techniques fall short or require further improvement, including issues related to interpretability, generalizability, and robustness. By shedding light on both the strengths and weaknesses of mathematical models for neural networks, our aim is to stimulate future research directions that can lead to more powerful, reliable, and transparent artificial intelligence systems.",1
"Multi-step prediction is considered of major significance for time series analysis in many real life problems. Existing methods mainly focus on one-step-ahead forecasting, since multiple step forecasting generally fails due to accumulation of prediction errors. This paper presents a novel approach for predicting plant growth in agriculture, focusing on prediction of plant Stem Diameter Variations (SDV). The proposed approach consists of three main steps. At first, wavelet decomposition is applied to the original data, as to facilitate model fitting and reduce noise in them. Then an encoder-decoder framework is developed using Long Short Term Memory (LSTM) and used for appropriate feature extraction from the data. Finally, a recurrent neural network including LSTM and an attention mechanism is proposed for modelling long-term dependencies in the time series data. Experimental results are presented which illustrate the good performance of the proposed approach and that it significantly outperforms the existing models, in terms of error criteria such as RMSE, MAE and MAPE.",0
"This work presents an original approach to predicting multi-step ahead predictions for plant growth using a novel AutoEncoder Wavelet Based Deep Neural Network (AEWDNN) architecture. Our model uses an attention mechanism to adaptively weight different input features in the network, allowing us to capture complex relationships between variables that influence plant growth. By training our model on historical data from sensor readings, weather forecasts, and other environmental factors, we show that our method can accurately predict future plant growth beyond one step into several steps out. Additionally, we provide insightful analysis comparing the performance of our AEWDNN model to traditional methods like linear regression and Artificial Neural Networks, demonstrating the superiority of our approach in providing accurate, robust, and interpretable results. In conclusion, the proposed AEWDNN framework could serve as a powerful tool to support decision making processes related to agriculture sustainability by improving crop yields while reducing energy consumption, water usage, pesticide applications, etc. With further research and development, these findings could potentially have far-reaching implications across many areas of scientific inquiry.",1
"Automatic prediction of age and gender from face images has drawn a lot of attention recently, due it is wide applications in various facial analysis problems. However, due to the large intra-class variation of face images (such as variation in lighting, pose, scale, occlusion), the existing models are still behind the desired accuracy level, which is necessary for the use of these models in real-world applications. In this work, we propose a deep learning framework, based on the ensemble of attentional and residual convolutional networks, to predict gender and age group of facial images with high accuracy rate. Using attention mechanism enables our model to focus on the important and informative parts of the face, which can help it to make a more accurate prediction. We train our model in a multi-task learning fashion, and augment the feature embedding of the age classifier, with the predicted gender, and show that doing so can further increase the accuracy of age prediction. Our model is trained on a popular face age and gender dataset, and achieved promising results. Through visualization of the attention maps of the train model, we show that our model has learned to become sensitive to the right regions of the face.",0
"This work presents a novel approach based on attentional convolutions for age and gender prediction from facial images. Our model takes as input a face image and outputs two values: one representing the predicted age, and another representing the predicted gender. We demonstrate that our method outperforms state-of-the-art approaches on a publicly available dataset, achieving an accuracy of over 94%. Moreover, we show that our model generalizes well across different demographic groups, such as men and women, and can handle variations in lighting conditions. Finally, we provide qualitative results indicating that the attention mechanism helps focus the network on important regions of the image for each task. Overall, our work advances the field of computer vision by introducing a new technique for high-quality age and gender prediction from face images. -----",1
"We propose a new method of program learning in a Domain Specific Language (DSL) which is based on gradient descent with no direct search. The first component of our method is a probabilistic representation of the DSL variables. At each timestep in the program sequence, different DSL functions are applied on the DSL variables with a certain probability, leading to different possible outcomes. Rather than handling all these outputs separately, whose number grows exponentially with each timestep, we collect them into a superposition of variables which captures the information in a single, but fuzzy, state. This state is to be contrasted at the final timestep with the ground-truth output, through a loss function. The second component of our method is an attention-based recurrent neural network, which provides an appropriate initialization point for the gradient descent that optimizes the probabilistic representation. The method we have developed surpasses the state-of-the-art for synthesising long programs and is able to learn programs under noise.",0
"In order to teach computers how to perform complex tasks without perfect data, we need algorithms that can learn from incomplete, inconsistent, and contradictory sources at onceâ€”a task known as learning under noisy conditions. While current methods focus on handling one type of noise at a time or requiring clean subsets of data, our work explores program synthesis using the quantum computing concept of superposition of states. By leveraging superposition to simultaneously consider multiple candidate programs and their associated error rates, our approach achieves improved results compared to prior techniques. Through experiments, we demonstrate the effectiveness of our method on automating the debugging of broken code segments while also examining the impact of varying degrees and types of noise. With applications ranging from software development to autonomous driving and medical diagnosis, our research offers new possibilities in artificial intelligence and machine learning by bridging the gap between theoretical models and real-world problems.",1
"Unconstrained handwritten text recognition remains challenging for computer vision systems. Paragraph text recognition is traditionally achieved by two models: the first one for line segmentation and the second one for text line recognition. We propose a unified end-to-end model using hybrid attention to tackle this task. We achieve state-of-the-art character error rate at line and paragraph levels on three popular datasets: 1.90% for RIMES, 4.32% for IAM and 3.63% for READ 2016. The proposed model can be trained from scratch, without using any segmentation label contrary to the standard approach. Our code and trained model weights are available at https://github.com/FactoDeepLearning/VerticalAttentionOCR.",0
"Our paper presents a new approach to handwritten paragraph text recognition using a vertical attention network (VATN). We designed VATN as an end-to-end system that can handle real-world variations in script, font style, and quality without relying on preprocessing steps such as binarization or layout analysis.  Our core contribution is a novel architecture based on dilated convolutions and channel attention mechanisms that capture rich representations from raw pixel input data directly at the full image resolution. This eliminates the need for downsampling typically required by traditional models trained on smaller subwindows, which significantly reduces computational requirements while maintaining high accuracy.  We evaluated our model on several benchmark datasets including RIMES and READ-20, demonstrating superior performance compared to previous state-of-the-art methods across all evaluation metrics, including character error rate (CER) and word error rate (WER), especially on the most challenging datasets. These results showcase the effectiveness of VATN as a powerful tool for unconstrained handwriting recognition tasks, making significant progress towards realizing robust document understanding systems for both research applications and industry use cases alike.  In summary, we present an innovative solution for handwritten paragraph text recognition capable of addressing complex real-world scenarios by training an end-to-end system tailored specifically to meet the unique demands posed by handwriting diversity. Our work paves the way for future research exploring advanced techniques and architectures leveraging deep learning approaches, ultimately leading to even greater improvements within the field of handwriting recognition.",1
"We propose a facial micro-expression recognition model using 3D residual attention network called MERANet. The proposed model takes advantage of spatial-temporal attention and channel attention together, to learn deeper fine-grained subtle features for classification of emotions. The proposed model also encompasses both spatial and temporal information simultaneously using the 3D kernels and residual connections. Moreover, the channel features and spatio-temporal features are re-calibrated using the channel and spatio-temporal attentions, respectively in each residual module. The experiments are conducted on benchmark facial micro-expression datasets. A superior performance is observed as compared to the state-of-the-art for facial micro-expression recognition.",0
"This paper presents a novel approach for recognizing facial micro-expressions (MEs) using a deep learning model called 3D residual attention network (MERANet). We begin by discussing existing approaches for ME recognition, their limitations, and how our proposed method addresses these shortcomings. Then we describe the architecture of our model, which consists of several 3D convolutional layers followed by residual blocks. These residual blocks use self-attention mechanisms to focus on different regions of interest within each frame, allowing us to capture subtle changes in expression that might have been missed otherwise. Finally, we evaluate the performance of our model on two benchmark datasets and demonstrate that it outperforms state-of-the-art methods while being computationally efficient. Our results show that MERANet has great potential as a tool for automatic emotion detection and analysis.",1
"We consider the revenue maximization problem for an online retailer who plans to display a set of products differing in their prices and qualities and rank them in order. The consumers have random attention spans and view the products sequentially before purchasing a ``satisficing'' product or leaving the platform empty-handed when the attention span gets exhausted. Our framework extends the cascade model in two directions: the consumers have random attention spans instead of fixed ones and the firm maximizes revenues instead of clicking probabilities. We show a nested structure of the optimal product ranking as a function of the attention span when the attention span is fixed and design a $1/e$-approximation algorithm accordingly for the random attention spans. When the conditional purchase probabilities are not known and may depend on consumer and product features, we devise an online learning algorithm that achieves $\tilde{\mathcal{O}}(\sqrt{T})$ regret relative to the approximation algorithm, despite of the censoring of information: the attention span of a customer who purchases an item is not observable. Numerical experiments demonstrate the outstanding performance of the approximation and online learning algorithms.",0
"This should summarize all major contributions, insights, findings etc. provided by your paper without being overly descriptive on details. Make sure that all major topics are covered: product ranking problem, CTR-based ranker, learning-to-rank model, revenue maximisation. -- I am assuming you want me to write a scientific abstract here as opposed to a paper summary? If so I would like some more context, such as which field/discipline is this applied to, any key papers referred to and indeed the full paper title. Also who is the intended audience (peers, conference submission, journal)? Any specific length requirements etc. Would help me craft something appropriate! Can you please provide these details at your earliest convenience? Thank you!",1
"The modelling of Electronic Health Records (EHRs) has the potential to drive more efficient allocation of healthcare resources, enabling early intervention strategies and advancing personalised healthcare. However, EHRs are challenging to model due to their realisation as noisy, multi-modal data occurring at irregular time intervals. To address their temporal nature, we treat EHRs as samples generated by a Temporal Point Process (TPP), enabling us to model what happened in an event with when it happened in a principled way. We gather and propose neural network parameterisations of TPPs, collectively referred to as Neural TPPs. We perform evaluations on synthetic EHRs as well as on a set of established benchmarks. We show that TPPs significantly outperform their non-TPP counterparts on EHRs. We also show that an assumption of many Neural TPPs, that the class distribution is conditionally independent of time, reduces performance on EHRs. Finally, our proposed attention-based Neural TPP performs favourably compared to existing models, whilst aligning with real world interpretability requirements, an important step towards a component of clinical decision support systems.",0
"This paper presents a novel method for modeling electronic health records using neural temporal point processes. We propose a framework that incorporates sequence data with multiple modalities, including clinical events and laboratory results, into a unified probabilistic model. Our approach leverages recent advances in deep learning and Gaussian process models to capture complex patterns in healthcare data. The resulting model can effectively predict important outcomes such as hospital readmissions and disease progression. Through extensive experiments on real-world datasets, we demonstrate the superior performance of our proposed method over baseline methods. Our findings have significant implications for personalized medicine, risk stratification, and decision support systems in healthcare informatics. Overall, this work represents a promising step towards improving patient care through advanced machine learning techniques applied to large-scale medical data.",1
"Panoptic segmentation that unifies instance segmentation and semantic segmentation has recently attracted increasing attention. While most existing methods focus on designing novel architectures, we steer toward a different perspective: performing automated multi-loss adaptation (named Ada-Segment) on the fly to flexibly adjust multiple training losses over the course of training using a controller trained to capture the learning dynamics. This offers a few advantages: it bypasses manual tuning of the sensitive loss combination, a decisive factor for panoptic segmentation; it allows to explicitly model the learning dynamics, and reconcile the learning of multiple objectives (up to ten in our experiments); with an end-to-end architecture, it generalizes to different datasets without the need of re-tuning hyperparameters or re-adjusting the training process laboriously. Our Ada-Segment brings 2.7% panoptic quality (PQ) improvement on COCO val split from the vanilla baseline, achieving the state-of-the-art 48.5% PQ on COCO test-dev split and 32.9% PQ on ADE20K dataset. The extensive ablation studies reveal the ever-changing dynamics throughout the training process, necessitating the incorporation of an automated and adaptive learning strategy as presented in this paper.",0
"Here we present our latest methodology for panoptic segmentation named Ada-Segment which employs multi-loss adaptation. Our approach allows the model to adapt better on the fly during inference by utilizing external guidance to improve performance. This technique shows promise for achieving state-of-the-art results on the challenging task of instance-level semantic parsing. By using multiple losses together, we can create more accurate models that generalize well across diverse datasets. We evaluate our proposed method on popular benchmarks like COCO and Cityscapes and demonstrate improved accuracy compared to prior art. Ultimately, Ada-Segment serves as a powerful tool for researchers working on computer vision tasks who want better control over their neural networksâ€™ outputs while maintaining strong performance capabilities.",1
"Although Person Re-Identification has made impressive progress, difficult cases like occlusion, change of view-pointand similar clothing still bring great challenges. Besides overall visual features, matching and comparing detailed information is also essential for tackling these challenges. This paper proposes two key recognition patterns to better utilize the detail information of pedestrian images, that most of the existing methods are unable to satisfy. Firstly, Visual Clue Alignment requires the model to select and align decisive regions pairs from two images for pair-wise comparison, while existing methods only align regions with predefined rules like high feature similarity or same semantic labels. Secondly, the Conditional Feature Embedding requires the overall feature of a query image to be dynamically adjusted based on the gallery image it matches, while most of the existing methods ignore the reference images. By introducing novel techniques including correspondence attention module and discrepancy-based GCN, we propose an end-to-end ReID method that integrates both patterns into a unified framework, called CACE-Net((C)lue(A)lignment and (C)onditional (E)mbedding). The experiments show that CACE-Net achieves state-of-the-art performance on three public datasets.",0
"Effective visual clues play a crucial role in person re-identification (ReID) tasks as they provide essential features that distinguish one individual from another across different camera views. However, manually designing these visual cues can be challenging and time consuming. In this study, we propose a novel approach called Devil's in the Details (DITD), which automatically aligns visual clues by learning discriminative feature patterns at both global and local levels. This method uses pre-trained models and fine-grained attention mechanisms to capture subtle differences among individuals. We evaluate DITD on two public benchmark datasets and demonstrate significant improvements over existing state-of-the-art methods. Our results show that our proposed framework effectively captures high resolution details while preserving semantic information, leading to better performance in person ReID. Overall, our study contributes to the advancement of computer vision techniques for surveillance applications.",1
"Recent advances in Deep Reinforcement Learning (DRL) have largely focused on improving the performance of agents with the aim of replacing humans in known and well-defined environments. The use of these techniques as a game design tool for video game production, where the aim is instead to create Non-Player Character (NPC) behaviors, has received relatively little attention until recently. Turn-based strategy games like Roguelikes, for example, present unique challenges to DRL. In particular, the categorical nature of their complex game state, composed of many entities with different attributes, requires agents able to learn how to compare and prioritize these entities. Moreover, this complexity often leads to agents that overfit to states seen during training and that are unable to generalize in the face of design changes made during development. In this paper we propose two network architectures which, when combined with a \emph{procedural loot generation} system, are able to better handle complex categorical state spaces and to mitigate the need for retraining forced by design decisions. The first is based on a dense embedding of the categorical input space that abstracts the discrete observation model and renders trained agents more able to generalize. The second proposed architecture is more general and is based on a Transformer network able to reason relationally about input and input attributes. Our experimental evaluation demonstrates that new agents have better adaptation capacity with respect to a baseline architecture, making this framework more robust to dynamic gameplay changes during development. Based on the results shown in this paper, we believe that these solutions represent a step forward towards making DRL more accessible to the gaming industry.",0
"Artificial intelligence (AI) has been increasingly used in video games over the past few decades, with one particularly promising application being the creation of non-player characters (NPC) with adaptive behaviors. In roguelike games, where levels are randomly generated and gameplay differs significantly from run to run, these adaptive NPCs can greatly enhance player experience by introducing more variety and unpredictability into the environment. Recent advances in deep reinforcement learning have made it possible to train artificial agents capable of handling complex tasks under changing environments and objectives. We propose a method based on deep policy networks that can adapt NPC behavior according to changing design parameters set by human developers. Our approach involves training NPC agents endowed with decision-making abilities, so they can react and respond intelligently to different situations. We demonstrate our system through several experiments and compare its performance against traditional scripted approaches commonly found in commercial roguelike games. Our results show that our proposed system provides engaging and dynamic experiences, creating unique playthroughs for players while still respecting the spirit and goals set by designers. Overall, we believe our work represents a step forward towards realizing truly autonomous synthetic actors within digital entertainment software.",1
"Promising resolutions of the generalization puzzle observe that the actual number of parameters in a deep network is much smaller than naive estimates suggest. The renormalization group is a compelling example of a problem which has very few parameters, despite the fact that naive estimates suggest otherwise. Our central hypothesis is that the mechanisms behind the renormalization group are also at work in deep learning, and that this leads to a resolution of the generalization puzzle. We show detailed quantitative evidence that proves the hypothesis for an RBM, by showing that the trained RBM is discarding high momentum modes. Specializing attention mainly to autoencoders, we give an algorithm to determine the network's parameters directly from the learning data set. The resulting autoencoder almost performs as well as one trained by deep learning, and it provides an excellent initial condition for training, reducing training times by a factor between 4 and 100 for the experiments we considered. Further, we are able to suggest a simple criterion to decide if a given problem can or can not be solved using a deep network.",0
"Title: ""Deep Neural Net Generalization""  This paper presents analysis on how unsupervised deep neural networks generalize based on their architectures, hyperparameters, and training methods. We demonstrate that different types of architecture, regularization techniques, batch normalization, data augmentation, loss functions, and optimization algorithms impact the ability of deep neural networks (DNNs) to learn complex tasks without labeled datasets. By analyzing a wide range of DNNs across multiple datasets, we show that successful learning relies more heavily on network capacity and randomness than previously believed. Our work suggests new strategies for designing high performance models that generalize well under real world conditions. Our results have important implications for understanding the principles underlying why some deep neural networks fail while others succeed.",1
"The vulnerability of deep neural networks (DNNs) to adversarial examples has drawn great attention from the community. In this paper, we study the transferability of such examples, which lays the foundation of many black-box attacks on DNNs. We revisit a not so new but definitely noteworthy hypothesis of Goodfellow et al.'s and disclose that the transferability can be enhanced by improving the linearity of DNNs in an appropriate manner. We introduce linear backpropagation (LinBP), a method that performs backpropagation in a more linear fashion using off-the-shelf attacks that exploit gradients. More specifically, it calculates forward as normal but backpropagates loss as if some nonlinear activations are not encountered in the forward pass. Experimental results demonstrate that this simple yet effective method obviously outperforms current state-of-the-arts in crafting transferable adversarial examples on CIFAR-10 and ImageNet, leading to more effective attacks on a variety of DNNs.",0
"A new method improving the transferability of adversarial examples was tested recently. This technique involves backpropagation of linear models through deep neural networks (DNNs) which allows for improved generation of adversarial examples on DNNs. By introducing noise that lies within the subspace spanned by the principal directions of the modelâ€™s weights during training, we can generate more effective attacks. An attackâ€™s effectiveness is determined by how well it transfers across multiple models and tasks. We use adversarial loss as a function of validation accuracy, finding a minimum using gradient descent, before feeding the output of the final layer into a differentiable argmax operation. Our experiments show increased attack transferability compared to previous methods. Using standard techniques like FGSM and I-FGSM, we demonstrate better performance than untargeted attack baselines with higher confidence intervals. For targeted attacks, our proposed method significantly outperforms both natural evolution strategies (NES), projected gradient descent (PGD), and FAB at increasing robustness levels.",1
"Deep hashing methods have been proved to be effective for the large-scale medical image search assisting reference-based diagnosis for clinicians. However, when the salient region plays a maximal discriminative role in ophthalmic image, existing deep hashing methods do not fully exploit the learning ability of the deep network to capture the features of salient regions pointedly. The different grades or classes of ophthalmic images may be share similar overall performance but have subtle differences that can be differentiated by mining salient regions. To address this issue, we propose a novel end-to-end network, named Attention-based Saliency Hashing (ASH), for learning compact hash-code to represent ophthalmic images. ASH embeds a spatial-attention module to focus more on the representation of salient regions and highlights their essential role in differentiating ophthalmic images. Benefiting from the spatial-attention module, the information of salient regions can be mapped into the hash-code for similarity calculation. In the training stage, we input the image pairs to share the weights of the network, and a pairwise loss is designed to maximize the discriminability of the hash-code. In the retrieval stage, ASH obtains the hash-code by inputting an image with an end-to-end manner, then the hash-code is used to similarity calculation to return the most similar images. Extensive experiments on two different modalities of ophthalmic image datasets demonstrate that the proposed ASH can further improve the retrieval performance compared to the state-of-the-art deep hashing methods due to the huge contributions of the spatial-attention module.",0
"This research proposes attention-based saliency hashing (AHS) for ophthalmic image retrieval. By using region attention modules, AHS can focus on discriminative regions of images that contain important features for retrieving similar medical cases. Experimental results show that our proposed method outperforms traditional methods significantly, demonstrating the effectiveness and potential of applying attention mechanisms into hashing techniques. Our contributions could pave the way towards more accurate diagnoses and better treatments by improving ophthalmic image retrieval accuracy.",1
"Referring expression comprehension (REC) aims to localize a target object in an image described by a referring expression phrased in natural language. Different from the object detection task that queried object labels have been pre-defined, the REC problem only can observe the queries during the test. It thus more challenging than a conventional computer vision problem. This task has attracted a lot of attention from both computer vision and natural language processing community, and several lines of work have been proposed, from CNN-RNN model, modular network to complex graph-based model. In this survey, we first examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to encode the visual and textual modalities. In particular, we examine the common approach of joint embedding images and expressions to a common feature space. We also discuss modular architectures and graph-based models that interface with structured graph representation. In the second part of this survey, we review the datasets available for training and evaluating REC systems. We then group results according to the datasets, backbone models, settings so that they can be fairly compared. Finally, we discuss promising future directions for the field, in particular the compositional referring expression comprehension that requires longer reasoning chain to address.",0
"Understanding natural language references has been one of the central topics in artificial intelligence since the beginning of the field. One particular challenge is understanding referring expressions (RE) such as pronouns, quantifiers, and demonstratives that can refer back to entities in the world, whether they be objects, events, or concepts. To solve RE comprehension problems, algorithms rely on techniques from core Natural Language Processing components including syntax analysis, semantic representation learning, reasoning, and inference, among others. This survey focuses specifically on approaches and datasets used for solving challenges related to RE comprehension. It provides a structured overview of popular methods and their key characteristics, discusses evaluation practices including standard benchmarks, summarizes representative datasets, examines some recently proposed innovations, and considers future research directions. By providing insights into these areas, our goal is to facilitate advancements in RE comprehension systems that benefit applications like Question Answering (QA), Robotics/Vision, Conversational Systems (e.g., Chatbots), Knowledge Graph Construction, Information Retrieval (IR), Machine Translation (MT), Summarization, and text generation, etc. Our aim is to provide readers with both breadth and depth so they can navigate through current literature more effectively and identify potential research gaps.",1
"Street-to-aerial image geo-localization, which matches a query street-view image to the GPS-tagged aerial images in a reference set, has attracted increasing attention recently. In this paper, we revisit this problem and point out the ignored issue about image alignment information. We show that the performance of a simple Siamese network is highly dependent on the alignment setting and the comparison of previous works can be unfair if they have different assumptions. Instead of focusing on the feature extraction under the alignment assumption, we show that improvements in metric learning techniques significantly boost the performance regardless of the alignment. Without leveraging the alignment information, our pipeline outperforms previous works on both panorama and cropped datasets. Furthermore, we conduct visualization to help understand the learned model and the effect of alignment information using Grad-CAM. With our discovery on the approximate rotation-invariant activation maps, we propose a novel method to estimate the orientation/alignment between a pair of cross-view images with unknown alignment information. It achieves state-of-the-art results on the CVUSA dataset.",0
"This paper presents a comprehensive approach to street-level image geolocalization and orientation estimation that utilizes both street-level and aerial images. Our method leverages deep learning techniques to accurately estimate a set of candidate locations for each input image using convolutional neural networks (CNNs). We then use these candidates to compute geometric features, which we use in conjunction with CNN-based classifiers to refine location estimates. We further incorporate orientation information into our system by designing separate models that predict the heading angle for each image. Our experimental results demonstrate the effectiveness of our proposed methods, achieving state-of-the-art performance on standard benchmark datasets. Finally, we evaluate the accuracy of our system under challenging real-world scenarios and show that our approach can handle images taken from different viewpoints and under varying illumination conditions. Overall, our work represents an important contribution to the field of computer vision, providing a powerful tool for applications such as local search, mapping, and autonomous navigation.",1
"Feature selection is an important data pre-processing in data mining and machine learning, which can reduce feature size without deteriorating model's performance. Recently, sparse regression based feature selection methods have received considerable attention due to their good performance. However, because the $l_{2,0}$-norm regularization term is non-convex, this problem is very hard to solve. In this paper, unlike most of the other methods which only solve the approximate problem, a novel method based on homotopy iterative hard threshold (HIHT) is proposed to solve the $l_{2,0}$-norm regularization least square problem directly for multi-class feature selection, which can produce exact row-sparsity solution for the weights matrix. What'more, in order to reduce the computational time of HIHT, an acceleration version of HIHT (AHIHT) is derived. Extensive experiments on eight biological datasets show that the proposed method can achieve higher classification accuracy (ACC) with fewest number of selected features (No.fea) comparing with the approximate convex counterparts and state-of-the-art feature selection methods. The robustness of classification accuracy to the regularization parameter and the number of selected feature are also exhibited.",0
"Title: ""Robust Multi-Class Feature Selection through $L_2^0$ Norm Regulation"" This research explores a novel methodology for multi-class feature selection that utilizes the $L_2^0$ norm regularization technique. By leveraging the unique characteristics of this norm, our approach achieves superior performance compared to traditional methods. We demonstrate the effectiveness of our method using several real-world datasets across diverse application domains. Our findings offer valuable insights into improving model robustness and generalizability, making them applicable beyond feature selection alone. Overall, our work represents an important contribution to the field of machine learning and artificial intelligence, paving the way for future advancements in these areas.",1
"Video instance segmentation is a complex task in which we need to detect, segment, and track each object for any given video. Previous approaches only utilize single-frame features for the detection, segmentation, and tracking of objects and they suffer in the video scenario due to several distinct challenges such as motion blur and drastic appearance change. To eliminate ambiguities introduced by only using single-frame features, we propose a novel comprehensive feature aggregation approach (CompFeat) to refine features at both frame-level and object-level with temporal and spatial context information. The aggregation process is carefully designed with a new attention mechanism which significantly increases the discriminative power of the learned features. We further improve the tracking capability of our model through a siamese design by incorporating both feature similarities and spatial similarities. Experiments conducted on the YouTube-VIS dataset validate the effectiveness of proposed CompFeat. Our code will be available at https://github.com/SHI-Labs/CompFeat-for-Video-Instance-Segmentation.",0
"This paper presents CompFeat, a comprehensive feature aggregation approach for video instance segmentation that effectively integrates multiple features for accurate pixel-wise mask generation. While recent advancements have led to significant improvements in object detection and semantic scene understanding, robust instance-level segmentation remains challenging due to its inherent ambiguity arising from similar appearances among different objects. To address these limitations, we propose a novel technique termed CompFeat that extensively explores both spatial and temporal cues through effective fusion of diverse visual representations. Specifically, our method adopts a three-stage strategy that initially generates coarse instance hypotheses via dense anchors followed by fine-grained refinement guided by multi-modal context integration across color, flow, depth maps, and more importantly spatio-temporal consistency constraints. Extensive evaluations on several benchmark datasets demonstrate the efficacy of CompFeat, outperforming state-of-the-art methods by notable margins while significantly reducing computational cost and memory footprint. Our findings suggest that exploiting complementary modalities and compact model architectures along with efficient optimization procedures can greatly enhance instance segmen",1
"The artistic style of a painting is a rich descriptor that reveals both visual and deep intrinsic knowledge about how an artist uniquely portrays and expresses their creative vision. Accurate categorization of paintings across different artistic movements and styles is critical for large-scale indexing of art databases. However, the automatic extraction and recognition of these highly dense artistic features has received little to no attention in the field of computer vision research. In this paper, we investigate the use of deep self-supervised learning methods to solve the problem of recognizing complex artistic styles with high intra-class and low inter-class variation. Further, we outperform existing approaches by almost 20% on a highly class imbalanced WikiArt dataset with 27 art categories. To achieve this, we train the EnAET semi-supervised learning model (Wang et al., 2019) with limited annotated data samples and supplement it with self-supervised representations learned from an ensemble of spatial and non-spatial transformations.",0
"This research proposes a novel method for art style classification that combines self-trained transformations with ensemble learning. Previous methods have focused on using pre-trained models and fine-tuning them with small amounts of data. In contrast, our approach utilizes unsupervised pre-training by autoencoding images and then trains multiple classifiers with different combinations of features from these transformed representations. Our results demonstrate state-of-the-art performance across several benchmark datasets, outperforming previous approaches by significant margins. Moreover, we show that our method can effectively handle variations in image quality, resolution, and other challenging conditions, making it more versatile than existing techniques. Overall, this work contributes new insights into the use of autoencoders for visual representation learning and demonstrates their effectiveness for solving complex computer vision tasks like art style classification.",1
"Partial domain adaptation which assumes that the unknown target label space is a subset of the source label space has attracted much attention in computer vision. Despite recent progress, existing methods often suffer from three key problems: negative transfer, lack of discriminability and domain invariance in the latent space. To alleviate the above issues, we develop a novel 'Select, Label, and Mix' (SLM) framework that aims to learn discriminative invariant feature representations for partial domain adaptation. First, we present a simple yet efficient ""select"" module that automatically filters out the outlier source samples to avoid negative transfer while aligning distributions across both domains. Second, the ""label"" module iteratively trains the classifier using both the labeled source domain data and the generated pseudo-labels for the target domain to enhance the discriminability of the latent space. Finally, the ""mix"" module utilizes domain mixup regularization jointly with the other two modules to explore more intrinsic structures across domains leading to a domain-invariant latent space for partial domain adaptation. Extensive experiments on several benchmark datasets demonstrate the superiority of our proposed framework over state-of-the-art methods.",0
This would ideally be ready for submission along with your manuscript by July 2nd at noon ET. If you have any questions please contact Manuscript Central.,1
"Automated captioning of photos is a mission that incorporates the difficulties of photo analysis and text generation. One essential feature of captioning is the concept of attention: how to determine what to specify and in which sequence. In this study, we leverage the Object Relation using adversarial robust cut algorithm, that grows upon this method by specifically embedding knowledge about the spatial association between input data through graph representation. Our experimental study represent the promising performance of our proposed method for image captioning.",0
"The goal of image captioning is to automatically generate natural language descriptions that accurately describe images, which can then serve as bridges between visual content and human understanding. However, current approaches suffer from limited robustness due to their reliance on fragile features such as object detectors and pre-trained models, lack of consideration of spatial relationships between objects and events, and neglect of uncertainty modeling. In this paper, we present a novel end-to-end trainable network architecture called ROD (Robust Object Detector) and apply adversarial training techniques to significantly enhance feature robustness and correctness, providing high quality annotations of salient regions and entities within the context of entire scenes. To further address the issue of uncertain predictions, we introduce EUSR (Ensemble Uncertainty Scoring and Refinement), a framework that effectively combines scores from multiple diverse models and utilizes spatial attention maps to select confident regions. Our experiments demonstrate significant improvements over state-of-the-art methods both quantitatively and qualitatively across four benchmark datasets: COCO, Flickr8k, MSCOCO, and SBUCaptions. We believe our approach represents a promising step towards more robust and reliable image caption generation systems.",1
"This paper presents the selective use of eye-gaze information in learning human actions in Atari games. Vast evidence suggests that our eye movement convey a wealth of information about the direction of our attention and mental states and encode the information necessary to complete a task. Based on this evidence, we hypothesize that selective use of eye-gaze, as a clue for attention direction, will enhance the learning from demonstration. For this purpose, we propose a selective eye-gaze augmentation (SEA) network that learns when to use the eye-gaze information. The proposed network architecture consists of three sub-networks: gaze prediction, gating, and action prediction network. Using the prior 4 game frames, a gaze map is predicted by the gaze prediction network which is used for augmenting the input frame. The gating network will determine whether the predicted gaze map should be used in learning and is fed to the final network to predict the action at the current frame. To validate this approach, we use publicly available Atari Human Eye-Tracking And Demonstration (Atari-HEAD) dataset consists of 20 Atari games with 28 million human demonstrations and 328 million eye-gazes (over game frames) collected from four subjects. We demonstrate the efficacy of selective eye-gaze augmentation in comparison with state of the art Attention Guided Imitation Learning (AGIL), Behavior Cloning (BC). The results indicate that the selective augmentation approach (the SEA network) performs significantly better than the AGIL and BC. Moreover, to demonstrate the significance of selective use of gaze through the gating network, we compare our approach with the random selection of the gaze. Even in this case, the SEA network performs significantly better validating the advantage of selectively using the gaze in demonstration learning.",0
"Here's a sample abstract:  ""Imitation learning has proven to be a powerful technique for training agents in complex tasks, but often requires large amounts of data and computational resources. This study proposes selective eye-gaze augmentation as a method for enhancing imitation learning performance in Atari games. By utilizing visual attention models trained on human gaze patterns during gameplay, we can identify salient features that may otherwise go unnoticed by traditional reinforcement learning algorithms. Our approach results in significant improvements over baseline methods while requiring only limited additional computation and no modification to existing architectures. We demonstrate the effectiveness of our method through extensive evaluation across multiple challenging Atari environments.""",1
"The goal of Boolean Matrix Factorization (BMF) is to approximate a given binary matrix as the product of two low-rank binary factor matrices, where the product of the factor matrices is computed under the Boolean algebra. While the problem is computationally hard, it is also attractive because the binary nature of the factor matrices makes them highly interpretable. In the last decade, BMF has received a considerable amount of attention in the data mining and formal concept analysis communities and, more recently, the machine learning and the theory communities also started studying BMF. In this survey, we give a concise summary of the efforts of all of these communities and raise some open questions which in our opinion require further investigation.",0
"Matrix factorization has been successfully applied to a wide range of applications such as collaborative filtering, image processing, and natural language understanding. In particular, Boolean matrix factorization (BMF) models have gained attention due to their ability to handle binary data while preserving important structure and interpretable features. This study focuses on recent developments in BMF that address some of the key challenges associated with traditional methods. We first provide a comprehensive review of existing BMF techniques and highlight their limitations. Next, we discuss state-of-the-art algorithms and approaches that have emerged as promising solutions to these shortcomings. These advanced methodologies incorporate innovations from deep learning, statistical modeling, and optimization theory to achieve improved performance and robustness in real-world scenarios. Finally, we present experimental results using diverse benchmark datasets to demonstrate the effectiveness of these new advances in BMF. Overall, our work serves as a guide for researchers and practitioners looking to leverage BMF in solving complex problems with large binary matrices. By synthesizing contemporary research trends and empirical findings, this article contributes significantly to the field of BMF and paves the way for future research directions.",1
"This paper brings deep learning at the forefront of research into Time Series Classification (TSC). TSC is the area of machine learning tasked with the categorization (or labelling) of time series. The last few decades of work in this area have led to significant progress in the accuracy of classifiers, with the state of the art now represented by the HIVE-COTE algorithm. While extremely accurate, HIVE-COTE cannot be applied to many real-world datasets because of its high training time complexity in O(N2 * T4) for a dataset with N time series of length T. For example, it takes HIVE-COTE more than 8 days to learn from a small dataset with N = 1500 time series of short length T = 46. Meanwhile deep learning has received enormous attention because of its high accuracy and scalability. Recent approaches to deep learning for TSC have been scalable, but less accurate than HIVE-COTE. We introduce InceptionTime - an ensemble of deep Convolutional Neural Network (CNN) models, inspired by the Inception-v4 architecture. Our experiments show that InceptionTime is on par with HIVE-COTE in terms of accuracy while being much more scalable: not only can it learn from 1,500 time series in one hour but it can also learn from 8M time series in 13 hours, a quantity of data that is fully out of reach of HIVE-COTE.",0
"This study presents a novel method for time series classification using deep learning models inspired by human visual perception called InceptionTime. Our approach focuses on finding the optimal hyperparameters for the pretrained AlexNet architecture using transfer learning from large publicly available datasets such as ImageNet. We evaluate our model on four different benchmark datasets and show that InceptionTime outperforms existing state-of-the-art methods in terms of accuracy and robustness across all datasets tested. Additionally, we conduct an analysis of the learned features from InceptionTime which reveals insights into their discriminative power and explainability for time series classification tasks. Our findings demonstrate the effectiveness of using deep neural networks for nonlinear dimensionality reduction in temporal data for improved performance in downstream applications.",1
"Detecting object skeletons in natural images presents challenging, due to varied object scales, the complexity of backgrounds and various noises. The skeleton is a highly compressing shape representation, which can bring some essential advantages but cause the difficulties of detection. This skeleton line occupies a rare proportion of an image and is overly sensitive to spatial position. Inspired by these issues, we propose the ProMask, which is a novel skeleton detection model. The ProMask includes the probability mask and vector router. The skeleton probability mask representation explicitly encodes skeletons with segmentation signals, which can provide more supervised information to learn and pay more attention to ground-truth skeleton pixels. Moreover, the vector router module possesses two sets of orthogonal basis vectors in a two-dimensional space, which can dynamically adjust the predicted skeleton position. We evaluate our method on the well-known skeleton datasets, realizing the better performance than state-of-the-art approaches. Especially, ProMask significantly outperforms the competitive DeepFlux by 6.2% on the challenging SYM-PASCAL dataset. We consider that our proposed skeleton probability mask could serve as a solid baseline for future skeleton detection, since it is very effective and it requires about 10 lines of code.",0
"In this work we present a novel probability mask approach that significantly improves skeleton detection accuracy by explicitly modeling joint visibility constraints. Our method first predicts a set of candidate bones based on local features, then computes probabilities for each joint pair using Gaussian distributions fitted to depth edges near the predicted bones. We use these probabilities as weights for fusion rules applied on sets of confidence scores produced by state-of-the-art direct methods, such as DeepPose2D and Pishchulin et al. (CVPRâ€™15). Experiments on popular benchmark datasets like COCO, LSP, LSP Extended demonstrate significant improvements over state-of-the-art approaches while running at real-time speed on both CPU and GPU systems.  Keywords â€“ Skeleton Detection, Depth Edge Features, Joint Visibility Constraints, Probabilistic Fusion Rules, State-Of-The-Art Accuracy, Real Time Performance  ---",1
"Recent studies often exploit Graph Convolutional Network (GCN) to model label dependencies to improve recognition accuracy for multi-label image recognition. However, constructing a graph by counting the label co-occurrence possibilities of the training data may degrade model generalizability, especially when there exist occasional co-occurrence objects in test images. Our goal is to eliminate such bias and enhance the robustness of the learnt features. To this end, we propose an Attention-Driven Dynamic Graph Convolutional Network (ADD-GCN) to dynamically generate a specific graph for each image. ADD-GCN adopts a Dynamic Graph Convolutional Network (D-GCN) to model the relation of content-aware category representations that are generated by a Semantic Attention Module (SAM). Extensive experiments on public multi-label benchmarks demonstrate the effectiveness of our method, which achieves mAPs of 85.2%, 96.0%, and 95.5% on MS-COCO, VOC2007, and VOC2012, respectively, and outperforms current state-of-the-art methods with a clear margin. All codes can be found at https://github.com/Yejin0111/ADD-GCN.",0
"This paper presents an innovative approach to multi-label image recognition using a dynamic graph convolutional network (GDCN) that focuses on attention mechanisms to improve accuracy and efficiency. Inspired by recent advances in visual representation learning, we propose a novel method called ""Attention-Driven Dynamic Graph Convolutional Network"" (ADDGCN). ADDGCN can effectively capture subtle patterns from complex images while addressing limitations associated with traditional methods such as deep neural networks. Our model utilizes attention modules at multiple levels to adaptively learn informative features that enhance performance on diverse datasets. Experimental results demonstrate ADDGCN significantly outperforms state-of-the-art approaches, achieving higher precision, recall, and F1 scores across various benchmarks. This work has significant implications for computer vision, enabling better understanding of human behaviors, preferences, and decision making processes through fine-grained scene analysis. Overall, our framework provides a promising direction towards efficient, scalable, and accurate solutions for real-world multi-label image classification problems.",1
"Graph convolutional networks (GCNs) can effectively capture the features of related nodes and improve the performance of the model. More attention is paid to employing GCN in Skeleton-Based action recognition. But existing methods based on GCNs have two problems. First, the consistency of temporal and spatial features is ignored for extracting features node by node and frame by frame. To obtain spatiotemporal features simultaneously, we design a generic representation of skeleton sequences for action recognition and propose a novel model called Temporal Graph Networks (TGN). Secondly, the adjacency matrix of the graph describing the relation of joints is mostly dependent on the physical connection between joints. To appropriately describe the relations between joints in the skeleton graph, we propose a multi-scale graph strategy, adopting a full-scale graph, part-scale graph, and core-scale graph to capture the local features of each joint and the contour features of important joints. Experiments were carried out on two large datasets and results show that TGN with our graph strategy outperforms state-of-the-art methods.",0
"In recent years, graph-based methods have shown promising results in the field of skeleton-based action recognition. However, existing graph approaches usually construct graphs at a single scale which can miss important spatial-temporal relationships among joints. To address this issue, we propose multi-scale temporal graph networks (MSTGN), a novel framework that leverages multiple graph scales to learn robust representation features. Our method first applies dilated convolutions to aggregate local context from different scales and then uses these high level feature representations to build dynamic graph connections across joints at each scale. We show that our MSTGN framework improves over state-of-the-art techniques on popular benchmark datasets by effectively capturing both short-range correlations and long-term dependencies among body joints during actions. By explicitly modeling interactions within multiple scales, MSTGN achieves better generalization performance and provides new insights into understanding human motion dynamics.",1
"Although a polygon is a more accurate representation than an upright bounding box for text detection, the annotations of polygons are extremely expensive and challenging. Unlike existing works that employ fully-supervised training with polygon annotations, we propose a novel text detection system termed SelfText Beyond Polygon (SBP) with Bounding Box Supervision (BBS) and Dynamic Self Training (DST), where training a polygon-based text detector with only a limited set of upright bounding box annotations. For BBS, we firstly utilize the synthetic data with character-level annotations to train a Skeleton Attention Segmentation Network (SASN). Then the box-level annotations are adopted to guide the generation of high-quality polygon-liked pseudo labels, which can be used to train any detectors. In this way, our method achieves the same performance as text detectors trained with polygon annotations (i.e., both are 85.0% F-score for PSENet on ICDAR2015 ). For DST, through dynamically removing the false alarms, it is able to leverage limited labeled data as well as massive unlabeled data to further outperform the expensive baseline. We hope SBP can provide a new perspective for text detection to save huge labeling costs. Code is available at: github.com/weijiawu/SBP.",0
"Unconstrained text detection involves identifying text regions within natural images that may have complex backgrounds and scenes, making it challenging for traditional algorithms. In our research, we propose a novel approach using box supervision and dynamic self-training to improve unconstrained text detection performance beyond polygon-based methods. We introduce two main contributions: (i) a text detector based on bounding boxes instead of polygons; and (ii) a dynamic self-training strategy utilizing both labeled data and automatically generated pseudo labels during training. Our experimental results show significant improvements over state-of-the-art polygon-based models on several benchmark datasets. This work demonstrates the effectiveness of our method as well as the importance of considering different ways to perform supervised learning in computer vision tasks.",1
"RoIPool/RoIAlign is an indispensable process for the typical two-stage object detection algorithm, it is used to rescale the object proposal cropped from the feature pyramid to generate a fixed size feature map. However, these cropped feature maps of local receptive fields will heavily lose global context information. To tackle this problem, we propose a novel end-to-end trainable framework, called Global Context Aware (GCA) RCNN, aiming at assisting the neural network in strengthening the spatial correlation between the background and the foreground by fusing global context information. The core component of our GCA framework is a context aware mechanism, in which both global feature pyramid and attention strategies are used for feature extraction and feature refinement, respectively. Specifically, we leverage the dense connection to improve the information flow of the global context at different stages in the top-down process of FPN, and further use the attention mechanism to refine the global context at each level in the feature pyramid. In the end, we also present a lightweight version of our method, which only slightly increases model complexity and computational burden. Experimental results on COCO benchmark dataset demonstrate the significant advantages of our approach.",0
"In this paper, we propose a novel approach to object detection that utilizes global context awareness through Region Convolutional Neural Networks (RCNN). Our method leverages both local and global features to accurately detect objects in images, improving upon existing methods that rely solely on local features. By incorporating global context into our detector, we can better recognize and distinguish objects from their surroundings, resulting in more accurate and robust object detection. We demonstrate the effectiveness of our method through extensive experiments on standard benchmark datasets, outperforming state-of-the-art approaches in terms of accuracy and speed. This work presents a significant advancement in computer vision research and has important applications in areas such as autonomous driving, robotics, and image analysis.",1
"Self-supervised learning is currently gaining a lot of attention, as it allows neural networks to learn robust representations from large quantities of unlabeled data. Additionally, multi-task learning can further improve representation learning by training networks simultaneously on related tasks, leading to significant performance improvements. In this paper, we propose three novel self-supervised auxiliary tasks to train graph-based neural network models in a multi-task fashion. Since Graph Convolutional Networks are among the most promising approaches for capturing relationships among structured data points, we use them as a building block to achieve competitive results on standard semi-supervised graph classification tasks.",0
"Artificial intelligence has made significant advancements over the past few years due to increased availability of data and computational resources. Graph-based neural network models (GBNNM) have become increasingly popular due to their ability to model complex graph-structured data such as social networks, knowledge graphs, biological networks, etc. In recent studies, researchers have applied self-supervision techniques that use pretext tasks (auxiliary tasks which can be solved without labeled training data), which learn from large amounts of unlabeled data. However, most previous work focused on using only one auxiliary task. This study proposes a new architecture called MultiTask-Graph2Vec, that uses multiple self-supervised auxiliary tasks. By utilizing various types of complementary auxiliary tasks, our proposed method captures richer patterns in the input graphs while maintaining scalability with respect to the number of tasks. Our experimental results demonstrate improved performance compared to state-of-the-art baselines across different datasets and settings. We hope our contributions encourage future exploration into leveraging multi-task learning and GBNNMs in order to achieve better generalization and robustness on real-world applications involving graph structured data.",1
"Semi-supervised domain adaptation (SSDA) methods have demonstrated great potential in large-scale image classification tasks when massive labeled data are available in the source domain but very few labeled samples are provided in the target domain. Existing solutions usually focus on feature alignment between the two domains while paying little attention to the discrimination capability of learned representations in the target domain. In this paper, we present a novel and effective method, namely Effective Label Propagation (ELP), to tackle this problem by using effective inter-domain and intra-domain semantic information propagation. For inter-domain propagation, we propose a new cycle discrepancy loss to encourage consistency of semantic information between the two domains. For intra-domain propagation, we propose an effective self-training strategy to mitigate the noises in pseudo-labeled target domain data and improve the feature discriminability in the target domain. As a general method, our ELP can be easily applied to various domain adaptation approaches and can facilitate their feature discrimination in the target domain. Experiments on Office-Home and DomainNet benchmarks show ELP consistently improves the classification accuracy of mainstream SSDA methods by 2%~3%. Additionally, ELP also improves the performance of UDA methods as well (81.5% vs 86.1%), based on UDA experiments on the VisDA-2017 benchmark. Our source code and pre-trained models will be released soon.",0
In this paper we present effective label propagation methods for semi-supervised domain adaptation (SSDA) problems. We introduce a discriminative approach that learns task specific feature extractors along with appropriate decision boundaries for both source and target domains simultaneously. Our method builds upon previous work by explicitly modeling the relationship between features extracted from labeled and unlabeled data using shared representations. To alleviate negative transfer across different classes our strategy minimizes their interference while enforcing commonality among them. Extensive experiments on standard benchmark datasets demonstrate state-of-the art performance compared against competitive baselines. Keywords: Semi-Supervised Domain Adaptation; Discri,1
"General image super-resolution techniques have difficulties in recovering detailed face structures when applying to low resolution face images. Recent deep learning based methods tailored for face images have achieved improved performance by jointly trained with additional task such as face parsing and landmark prediction. However, multi-task learning requires extra manually labeled data. Besides, most of the existing works can only generate relatively low resolution face images (e.g., $128\times128$), and their applications are therefore limited. In this paper, we introduce a novel SPatial Attention Residual Network (SPARNet) built on our newly proposed Face Attention Units (FAUs) for face super-resolution. Specifically, we introduce a spatial attention mechanism to the vanilla residual blocks. This enables the convolutional layers to adaptively bootstrap features related to the key face structures and pay less attention to those less feature-rich regions. This makes the training more effective and efficient as the key face structures only account for a very small portion of the face image. Visualization of the attention maps shows that our spatial attention network can capture the key face structures well even for very low resolution faces (e.g., $16\times16$). Quantitative comparisons on various kinds of metrics (including PSNR, SSIM, identity similarity, and landmark detection) demonstrate the superiority of our method over current state-of-the-arts. We further extend SPARNet with multi-scale discriminators, named as SPARNetHD, to produce high resolution results (i.e., $512\times512$). We show that SPARNetHD trained with synthetic data cannot only produce high quality and high resolution outputs for synthetically degraded face images, but also show good generalization ability to real world low quality face images.",0
"In many real world applications such as surveillance systems and autonomous vehicles, face recognition can be used for human computer interaction. For instance, camera based attendance system use facial recognition software to identify who enters and exits their premises while access control locks may contain sensors that capture an image of your face for security purposes. This means the quality of facial images is very important. But acquiring high resolution images using small sensors embedded on mobile phones or cameras has proven challenging due to limited resolutions. To overcome these shortcomings, super resolution techniques have been proposed to enhance facial features from low resolution images into higher resolution. Therefore learning spatial attention mechanisms in super resolution algorithms improves face recognitions by enabling models to focus on relevant feature maps instead of simply increasing the size of features present in low quality input images. Results show improvements with higher accuracy than previous state of art methods across all three standard databases containing more than one thousand identities. These results demonstrate the potential impact of deep learning to tackle low resolution images in security related problems where the success critically depends on accurate face detection.",1
"Attentively important regions in video frames account for a majority part of the semantics in each frame. This information is helpful in many applications not only for entertainment (such as auto generating commentary and tourist guide) but also for robotic control which holds a larascope supported for laparoscopic surgery. However, it is not always straightforward to define and locate such semantic regions in videos. In this work, we attempt to address the problem of attending relevant regions in videos by leveraging the eye fixations labels with a RNN-based visual attention model. Our experimental results suggest that this approach holds a good potential to learn to attend semantic regions in videos while its performance also heavily relies on the quality of eye fixations labels.",0
"This paper presents a novel method that leverages eye fixations data to learn attention mechanisms that can effectively focus on relevant regions in videos while performing specific tasks, such as action recognition, video retrieval, and summarization. We propose a neural network architecture that integrates human gaze information into its design by training it to predict the next most likely frame given the current one along with the associated gaze map. Our model achieves state-of-the-art results on several benchmark datasets, outperforming previous methods that either ignore eye movements altogether or use them only indirectly through visual saliency maps. Furthermore, we provide analysis showing that our proposed approach leads to better generalization performance across different task types compared to alternative attentional techniques. Overall, these findings suggest that incorporating human gaze patterns directly into computer vision models could lead to more effective solutions tailored towards specific high-level tasks.",1
"Generative Adversarial Networks (GANs) have gained significant attention in recent years, with impressive applications highlighted in computer vision in particular. Compared to such examples, however, there have been more limited applications of GANs to time series modelling, including forecasting. In this work, we present the Mixture Density Conditional Generative Adversarial Model (MD-CGAN), with a focus on time series forecasting. We show that our model is capable of estimating a probabilistic posterior distribution over forecasts and that, in comparison to a set of benchmark methods, the MD-CGAN model performs well, particularly in situations where noise is a significant component of the observed time series. Further, by using a Gaussian mixture model as the output distribution, MD-CGAN offers posterior predictions that are non-Gaussian.",0
"In recent years, generative adversarial networks (GANs) have shown remarkable performance in generating realistic synthetic data across a variety of tasks such as image generation and superresolution. However, GANs suffer from instability during training which often leads to inferior results compared to other methods. To address these limitations, we propose the use of mixture density conditional GAN models (MD-CGAN), which incorporate uncertainty into the learning process by modeling both the generator and discriminator distributions using mixtures of Gaussian densities instead of point estimates. This allows MD-CGANs to capture variations in the underlying data distribution and produce more diverse outputs that better match their ground truth counterparts. Our experiments demonstrate that MD-CGAN outperforms traditional CGANs on several benchmark datasets while offering better stability during training. The proposed framework provides a promising direction towards improving the reliability and quality of GAN models.",1
"FPN is a common component used in object detectors, it supplements multi-scale information by adjacent level features interpolation and summation. However, due to the existence of nonlinear operations and the convolutional layers with different output dimensions, the relationship between different levels is much more complex, the pixel-wise summation is not an efficient approach. In this paper, we first analyze the design defects from pixel level and feature map level. Then, we design a novel parameter-free feature pyramid networks named Dual Refinement Feature Pyramid Networks (DRFPN) for the problems. Specifically, DRFPN consists of two modules: Spatial Refinement Block (SRB) and Channel Refinement Block (CRB). SRB learns the location and content of sampling points based on contextual information between adjacent levels. CRB learns an adaptive channel merging method based on attention mechanism. Our proposed DRFPN can be easily plugged into existing FPN-based models. Without bells and whistles, for two-stage detectors, our model outperforms different FPN-based counterparts by 1.6 to 2.2 AP on the COCO detection benchmark, and 1.5 to 1.9 AP on the COCO segmentation benchmark. For one-stage detectors, DRFPN improves anchor-based RetinaNet by 1.9 AP and anchor-free FCOS by 1.3 AP when using ResNet50 as backbone. Extensive experiments verifies the robustness and generalization ability of DRFPN. The code will be made publicly available.",0
"Abstract:  Object detection has been one of the most active research areas in computer vision due to its diverse applications such as autonomous driving, surveillance, and robotics. In recent years, Convolutional Neural Network (CNN) based object detectors have achieved state-of-the-art performance by leveraging powerful backbones like ResNet, SE-ResNeXt, and DCNv2. However, these models require high computational resources and are often limited by their inference speed on edge devices. Therefore, there is a need for efficient architectures that can balance accuracy and efficiency while maintaining competitive performance.  In this work, we propose Dual Refinement Feature Pyramid Networks (DReFPN), a novel framework that unifies single stage object detection frameworks and multi-stage cascading methods. Our approach utilizes feature pyramids generated from different stages of our detector to refine bounding box predictions efficiently. We design two separate branches within our network to achieve better localization and scale estimation through cross-scale interactions. Furthermore, we introduce a modality attention mechanism that adaptively learns which features contribute more to each task, further improving the overall performance.  Extensive experiments conducted on challenging benchmark datasets including COCO, KITTI, and Cityscapes demonstrate the effectiveness of our proposed method. Compared to popular baselines such as Faster R-CNN, RetinaNet, and YOLOv4, our model achieves higher mAP scores at significantly faster inference speeds across different platforms. Additionally, our ablation studies provide insights into how each component contributes to the final performance improvement. Overall, DReFPN serves as a promising architecture for resource-constrained environments where both efficiency and accuracy matter.",1
"Panoptic segmentation, which is a novel task of unifying instance segmentation and semantic segmentation, has attracted a lot of attention lately. However, most of the previous methods are composed of multiple pathways with each pathway specialized to a designated segmentation task. In this paper, we propose to resolve panoptic segmentation in single-shot by integrating the execution flows. With the integrated pathway, a unified feature map called Panoptic-Feature is generated, which includes the information of both things and stuffs. Panoptic-Feature becomes more sophisticated by auxiliary problems that guide to cluster pixels that belong to the same instance and differentiate between objects of different classes. A collection of convolutional filters, where each filter represents either a thing or stuff, is applied to Panoptic-Feature at once, materializing the single-shot panoptic segmentation. Taking the advantages of both top-down and bottom-up approaches, our method, named SPINet, enjoys high efficiency and accuracy on major panoptic segmentation benchmarks: COCO and Cityscapes.",0
"In recent years, there has been significant progress in computer vision tasks such as object detection, instance segmentation, and semantic segmentation, thanks to deep learning methods like convolutional neural networks (CNNs). However, these methods often require multiple iterations, slowing down their implementation on real-time applications. To address this challenge, we propose a novel approach called ""Single-Shot Path Integrated Panoptic Segmentation"" (SPIPS) that uses a CNN model to perform panoptic segmentation on complex scenes from single images in a fraction of a second. Our method utilizes multi-scale feature pyramids to capture contextual information and a cascade of fully connected layers to predict dense panoptic segments. We introduce two techniques: ""Pathways"" which enables efficient inference by adaptively selecting the required number of parallel paths based on the image size; and ""Pyramid Pooling,"" which efficiently fuses predictions across all scales with a simple linear combination. Extensive experiments demonstrate that SPIPS significantly outperforms state-of-the-art methods in terms of speed and accuracy, making it suitable for real-world use cases where fast processing times are crucial.",1
"We introduce the task of dense captioning in 3D scans from commodity RGB-D sensors. As input, we assume a point cloud of a 3D scene; the expected output is the bounding boxes along with the descriptions for the underlying objects. To address the 3D object detection and description problems, we propose Scan2Cap, an end-to-end trained method, to detect objects in the input scene and describe them in natural language. We use an attention mechanism that generates descriptive tokens while referring to the related components in the local context. To reflect object relations (i.e. relative spatial relations) in the generated captions, we use a message passing graph module to facilitate learning object relation features. Our method can effectively localize and describe 3D objects in scenes from the ScanRefer dataset, outperforming 2D baseline methods by a significant margin (27.61% CiDEr@0.5IoUimprovement).",0
"The aim of our work was to develop a method for generating dense captions from RGB-D scans that can accurately capture contextual relationships between objects in the scene. We present a novel approach called ""Scan2Cap"" which leverages recent advances in computer vision and natural language processing (NLP) techniques, such as object detection using convolutional neural networks (CNNs), visual feature extraction, and NLP models for generating human-like descriptions of scenes.  Our proposed system first processes the raw scan data into semantically meaningful representations by detecting and segmenting all objects within the scene. Then, we extract features from these detected objects and represent them using embeddings. Next, we use an attention mechanism to focus on relevant regions and their corresponding contexts while encoding spatial relations among different objects in the scene. Finally, we generate natural language descriptions through decoding using an encoder-decoder NLP model fine-tuned on the generated embeddings.  We evaluated our Scan2Cap method on two benchmark datasets for RGB-D scanning scenarios - SUNCG and NYUv2 - comparing against state-of-the-art methods in terms of both quantitative metrics (e.g., accuracy, recall, precision) and qualitative assessments by human annotators. Our results show significant improvements over baseline approaches and demonstrate the effectiveness of our proposed method for generating high quality, contextually rich descriptions of complex indoor environments captured through RGB-D scans. Additionally, we provide several examples of our Scan2Cap output compared to alternative methods, highlighting its strengths in capturing contextual details and nuances missed by other approaches. Overall, our research shows promise for real-world applications in areas like robotics, AR/VR, and assistive technologies where accurate, detailed understanding of physical spaces is crucial.",1
"In image-to-image translation the goal is to learn a mapping from one image domain to another. In the case of supervised approaches the mapping is learned from paired samples. However, collecting large sets of image pairs is often either prohibitively expensive or not possible. As a result, in recent years more attention has been given to techniques that learn the mapping from unpaired sets.   In our work, we show that injecting implicit pairs into unpaired sets strengthens the mapping between the two domains, improves the compatibility of their distributions, and leads to performance boosting of unsupervised techniques by over 14% across several measurements.   The competence of the implicit pairs is further displayed with the use of pseudo-pairs, i.e., paired samples which only approximate a real pair. We demonstrate the effect of the approximated implicit samples on image-to-image translation problems, where such pseudo-pairs may be synthesized in one direction, but not in the other. We further show that pseudo-pairs are significantly more effective as implicit pairs in an unpaired setting, than directly using them explicitly in a paired setting.",0
"Recent advances in generative models have led to significant progress towards achieving photo-realistic image synthesis from textual descriptions (image-to-text) and vice versa (text-to-image). These approaches typically rely on training deep neural networks using large amounts of paired examples which can limit their performance when only limited supervision is available. In this work we propose a simple yet effective method for boosting unpaired image-to-image translation by introducing implicit pairs during training through data augmentation techniques such as cutout and color jittering. We show that these implicit pairs act as weak pseudo labels and regularize the model better than traditional adversarial loss functions leading to improved quality and robustness under reduced supervision regimes. We evaluate our approach on several benchmark datasets including CelebA-HQ, LSUN Churches and COCO Stuff and demonstrate consistent improvements over state-of-the-art methods across all metrics. Our code and models will be made publicly available upon acceptance.",1
"In recent years, deep neural networks have achieved state-of-the-art performance in a variety of recognition and segmentation tasks in medical imaging including brain tumor segmentation. We investigate that segmenting a brain tumor is facing to the imbalanced data problem where the number of pixels belonging to the background class (non tumor pixel) is much larger than the number of pixels belonging to the foreground class (tumor pixel). To address this problem, we propose a multi-task network which is formed as a cascaded structure. Our model consists of two targets, i.e., (i) effectively differentiate the brain tumor regions and (ii) estimate the brain tumor mask. The first objective is performed by our proposed contextual brain tumor detection network, which plays a role of an attention gate and focuses on the region around brain tumor only while ignoring the far neighbor background which is less correlated to the tumor. The second objective is built upon a 3D atrous residual network and under an encode-decode network in order to effectively segment both large and small objects (brain tumor). Our 3D atrous residual network is designed with a skip connection to enables the gradient from the deep layers to be directly propagated to shallow layers, thus, features of different depths are preserved and used for refining each other. In order to incorporate larger contextual information from volume MRI data, our network utilizes the 3D atrous convolution with various kernel sizes, which enlarges the receptive field of filters. Our proposed network has been evaluated on various datasets including BRATS2015, BRATS2017 and BRATS2018 datasets with both validation set and testing set. Our performance has been benchmarked by both region-based metrics and surface-based metrics. We also have conducted comparisons against state-of-the-art approaches.",0
"""The accurate detection and segmentation of brain tumors is crucial for effective treatment planning in neurosurgery. In recent years, deep learning techniques have shown promise in improving the performance of these tasks, but challenges remain due to the complex nature of medical images and the need to balance precision and recall. To address these issues, we propose a novel approach based on multi-task contextual atrous residual networks (MRARN). Our method leverages transfer learning from pre-trained models to achieve high accuracy while reducing computational cost and improving generalization ability. Experimental results demonstrate that MRARN outperforms state-of-the-art methods in terms of both detection and segmentation metrics, indicating its potential as a powerful tool for clinical practice.""",1
"Recent studies have shown remarkable success in face image generations. However, most of the existing methods only generate face images from random noise, and cannot generate face images according to the specific attributes. In this paper, we focus on the problem of face synthesis from attributes, which aims at generating faces with specific characteristics corresponding to the given attributes. To this end, we propose a novel attributes aware face image generator method with generative adversarial networks called AFGAN. Specifically, we firstly propose a two-path embedding layer and self-attention mechanism to convert binary attribute vector to rich attribute features. Then three stacked generators generate $64 \times 64$, $128 \times 128$ and $256 \times 256$ resolution face images respectively by taking the attribute features as input. In addition, an image-attribute matching loss is proposed to enhance the correlation between the generated images and input attributes. Extensive experiments on CelebA demonstrate the superiority of our AFGAN in terms of both qualitative and quantitative evaluations.",0
"This work presents a novel approach for generating high quality facial images using generative adversarial networks (GANs). Our method incorporates attribute labels as an additional input to guide the generation process, resulting in improved control over generated faces. Experimental results demonstrate that our model outperforms state-of-the-art methods in terms of both visual fidelity and attribute preservation. We believe that the proposed framework has important applications in computer vision, such as photo-realistic rendering, image synthesis and editing, face recognition, and virtual reality.  Face generation remains a challenging task due to the complexity of human appearance and diversity across different individuals. Previous GAN-based approaches have achieved impressive results but still struggle to generate coherent and realistic faces. To overcome these limitations, we introduce attribute awareness into the generation process by utilizing attribute labels alongside other cues such as pose and lighting conditions. By doing so, we can explicitly preserve meaningful attributes during the generation process, which leads to more controllable outputs. For example, we can generate smiling portraits with glasses simply by providing corresponding attribute labels.  We propose an end-to-end trainable network architecture that consists of two subnetworks: a generator that produces a face conditioned on given inputs, and a discriminator that judges whether the output is real or fake. Both networks are trained alternately against each other in a minmax game. To achieve attribute-aware generation, we add skip connections from the generator to the discriminator and make use ofattribute encoding matrices to inject attribute information directly into the convolutional layers. In addition, we employ a multi-scale pyramid pooling module within the generator to capture local features at multiple resolution scales, enabling more detailed and accurate face reconstruction. Extensive experiments conducted on three public datasets demonstra",1
"Extreme classification tasks are multi-label tasks with an extremely large number of labels (tags). These tasks are hard because the label space is usually (i) very large, e.g. thousands or millions of labels, (ii) very sparse, i.e. very few labels apply to each input document, and (iii) highly correlated, meaning that the existence of one label changes the likelihood of predicting all other labels. In this work, we propose a self-attention based variational encoder-model to extract the label-label and label-feature dependencies jointly and to predict labels for a given input. In more detail, we propose a non-autoregressive latent variable model and compare it to a strong autoregressive baseline that predicts a label based on all previously generated labels. Our model can therefore be used to predict all labels in parallel while still including both label-label and label-feature dependencies through latent variables, and compares favourably to the autoregressive baseline. We apply our models to four standard extreme classification natural language data sets, and one news videos dataset for automated label detection from a lexicon of semantic concepts. Experimental results show that although the autoregressive models, where use a given order of the labels for chain-order label prediction, work great for the small scale labels or the prediction of the highly ranked label, but our non-autoregressive model surpasses them by around 2% to 6% when we need to predict more labels, or the dataset has a larger number of the labels.",0
"In recent years, multi-label learning has become increasingly important due to its ability to handle datasets containing multiple labels per instance. This study focuses on comparing autoregressive models (AML) and non-autoregressive models (Non-AML) for multi-label classification tasks. We present experimental results using three different benchmark datasets, showing that both types of models can achieve comparable accuracies. However, our analysis reveals significant differences in their strengths and weaknesses, which ultimately affect their performance depending on the specific task at hand. Our findings provide insights into choosing between AML and Non-AML models, as well as guidance on how to design better algorithms for multi-label classification. Overall, this research contributes towards improving the state-of-the-art in machine learning by exploring two popular approaches for tackling complex problems involving multiple outputs.",1
"Autoencoders are a class of artificial neural networks which have gained a lot of attention in the recent past. Using the encoder block of an autoencoder the input image can be compressed into a meaningful representation. Then a decoder is employed to reconstruct the compressed representation back to a version which looks like the input image. It has plenty of applications in the field of data compression and denoising. Another version of Autoencoders (AE) exist, called Variational AE (VAE) which acts as a generative model like GAN. Recently, an optimizer was introduced which is known as lookahead optimizer which significantly enhances the performances of Adam as well as SGD. In this paper, we implement Convolutional Autoencoders (CAE) and Convolutional Variational Autoencoders (CVAE) with lookahead optimizer (with Adam) and compare them with the Adam (only) optimizer counterparts. For this purpose, we have used a movie dataset comprising of natural images for the former case and CIFAR100 for the latter case. We show that lookahead optimizer (with Adam) improves the performance of CAEs for reconstruction of natural images.",0
"This paper discusses how using a lookahead optimizer can improve the performance of convolutional autoencoder models for image reconstruction tasks on natural images datasets. We present experimental results showing that our proposed method outperforms both standard gradient descent training as well as previously proposed methods for acceleration like stochastic gradient descent (SGD) and Adagrad. By utilizing knowledge of the gradients during backpropagation we can better control the learning rate resulting in faster convergence. Our model achieves state-of-the-art accuracy and FID scores on several benchmark datasets and demonstrates improvements over baseline models across all metrics. These findings have important implications for applications where high quality image generation from raw data is required such as computer vision systems, generative adversarial networks, and image compression algorithms. Overall, our work shows that incorporating a lookahead optimizer into convolutional autoencoder models leads to significant improvements in image reconstruction tasks.",1
"Learning to generate a task-aware base learner proves a promising direction to deal with few-shot learning (FSL) problem. Existing methods mainly focus on generating an embedding model utilized with a fixed metric (eg, cosine distance) for nearest neighbour classification or directly generating a linear classier. However, due to the limited discriminative capacity of such a simple metric or classifier, these methods fail to generalize to challenging cases appropriately. To mitigate this problem, we present a novel deep metric meta-generation method that turns to an orthogonal direction, ie, learning to adaptively generate a specific metric for a new FSL task based on the task description (eg, a few labelled samples). In this study, we structure the metric using a three-layer deep attentive network that is flexible enough to produce a discriminative metric for each task. Moreover, different from existing methods that utilize an uni-modal weight distribution conditioned on labelled samples for network generation, the proposed meta-learner establishes a multi-modal weight distribution conditioned on cross-class sample pairs using a tailored variational autoencoder, which can separately capture the specific inter-class discrepancy statistics for each class and jointly embed the statistics for all classes into metric generation. By doing this, the generated metric can be appropriately adapted to a new FSL task with pleasing generalization performance. To demonstrate this, we test the proposed method on four benchmark FSL datasets and gain surprisingly obvious performance improvement over state-of-the-art competitors, especially in the challenging cases, eg, improve the accuracy from 26.14% to 46.69% in the 20-way 1-shot task on miniImageNet, while improve the accuracy from 45.2% to 68.72% in the 5-way 1-shot task on FC100. Code is available: https://github.com/NWPUZhoufei/DAM.",0
"This abstract describes how we built an algorithm that takes small amounts of data and learns from them quickly. We describe several different architectures based on deep neural networks trained using reinforcement learning principles. Then, through analysis of the outputs of our models, we compare their effectiveness at generating novel examples within the same class as those few samples used during training. Our work helps inform future research by providing quantitative measures of model performance across varying parameter spaces of architecture choice, hyperparameter tuning, loss function, and more. Ultimately, our findings demonstrate how meta-learning can effectively generate metrically meaningful few-shot classification models given limited annotated data.",1
"In this paper, we propose a novel deep framework for part-level semantic parsing of freehand sketches, which makes three main contributions that are experimentally shown to have substantial practical merit. First, we propose a homogeneous transformation method to address the problem of domain adaptation. For the task of sketch parsing, there is no available data of labeled freehand sketches that can be directly used for model training. An alternative solution is to learn from datasets of real image parsing, while the domain adaptation is an inevitable problem. Unlike existing methods that utilize the edge maps of real images to approximate freehand sketches, the proposed homogeneous transformation method transforms the data from domains of real images and freehand sketches into a homogeneous space to minimize the semantic gap. Second, we design a soft-weighted loss function as guidance for the training process, which gives attention to both the ambiguous label boundary and class imbalance. Third, we present a staged learning strategy to improve the parsing performance of the trained model, which takes advantage of the shared information and specific characteristic from different sketch categories. Extensive experimental results demonstrate the effectiveness of the above three methods. Specifically, to evaluate the generalization ability of our homogeneous transformation method, additional experiments for the task of sketch-based image retrieval are conducted on the QMUL FG-SBIR dataset. Finally, by integrating the proposed three methods into a unified framework of deep semantic sketch parsing (DeepSSP), we achieve the state-of-the-art on the public SketchParse dataset.",0
"Our paper presents a novel deep learning approach for semantic parsing of freehand sketches using homogeneous transformation and soft-weighted loss functions. This method enables robust estimation from limited input data by regularizing ill-posed problems without overfitting on noisy training sets. The proposed architecture includes multiple stages of processing that integrate feature extraction techniques such as edge detection, contours, and region segmentation, which are then passed through convolutional neural networks (CNNs) for classification. We evaluate our model on several benchmark datasets and demonstrate significant improvements over previous methods. Additionally, we present qualitative results showcasing the effectiveness of our system in converting rough drawings into accurate object representations suitable for downstream computer vision applications. Overall, our work paves the way towards real-world implementations of natural user interface technology that harness human creativity for interactive systems design.",1
"In this paper, we propose a novel deep multi-level attention model to address inverse visual question answering. The proposed model generates regional visual and semantic features at the object level and then enhances them with the answer cue by using attention mechanisms. Two levels of multiple attentions are employed in the model, including the dual attention at the partial question encoding step and the dynamic attention at the next question word generation step. We evaluate the proposed model on the VQA V1 dataset. It demonstrates state-of-the-art performance in terms of multiple commonly used metrics.",0
"This paper proposes a novel approach called inverse visual question answering (IVQA) where questions are asked based on images instead of the other way round. Our method uses multi-level attentions to integrate cross-modal contexts between image regions and text queries at different levels. We show that our model outperforms several state-of-the-art IVQA methods on popular benchmark datasets. Our contributions can be summarized as follows: 1) A novel IVQA framework by reversing the flow of information from question answering to question generation; 2) Development of multi-level attention mechanisms to effectively capture fine-grained interactions across modalities; 3) Experimental evaluations on two challenging datasets, demonstrating significant improvements over existing approaches. Overall, our work suggests great potential in using inverse reasoning strategies for robust VQA solutions under varying conditions.",1
"Domain adaptation, as a task of reducing the annotation cost in a target domain by exploiting the existing labeled data in an auxiliary source domain, has received a lot of attention in the research community. However, the standard domain adaptation has assumed perfectly observed data in both domains, while in real world applications the existence of missing data can be prevalent. In this paper, we tackle a more challenging domain adaptation scenario where one has an incomplete target domain with partially observed data. We propose an Incomplete Data Imputation based Adversarial Network (IDIAN) model to address this new domain adaptation challenge. In the proposed model, we design a data imputation module to fill the missing feature values based on the partial observations in the target domain, while aligning the two domains via deep adversarial adaption. We conduct experiments on both cross-domain benchmark tasks and a real world adaptation task with imperfect target domains. The experimental results demonstrate the effectiveness of the proposed method.",0
"Despite the increasing availability of labeled data across many domains, real-world applications often require adapting models trained on one domain (source) to new domains where very little annotated data exists (target). Recent work has addressed these problems through methods such as adversarial training and multi-task learning; however, few methods have considered adaptation in situations where the target domain lacks enough labels to learn meaningful representations. We propose incomplete Domain Adaptation by exploiting unlabeled target examples without requiring access to their true classes, which can instead be inferred indirectly from their similarity to source examples. This approach enables us to perform Domain Adaptation using only weak supervision from noisy query answers and nearest neighbors, enabling novel applications like sentiment analysis for previously underserved languages. Experimental results show that our method achieves state-of-the-art performance under challenging settings, even outperforming fully supervised approaches with ten times more labeled target data. Finally, we provide insights into how to optimize unsupervised query generation.",1
"Recent advances in neural forecasting have produced major improvements in accuracy for probabilistic demand prediction. In this work, we propose novel improvements to the current state of the art by incorporating changes inspired by recent advances in Transformer architectures for Natural Language Processing. We develop a novel decoder-encoder attention for context-alignment, improving forecasting accuracy by allowing the network to study its own history based on the context for which it is producing a forecast. We also present a novel positional encoding that allows the neural network to learn context-dependent seasonality functions as well as arbitrary holiday distances. Finally we show that the current state of the art MQ-Forecaster (Wen et al., 2017) models display excess variability by failing to leverage previous errors in the forecast to improve accuracy. We propose a novel decoder-self attention scheme for forecasting that produces significant improvements in the excess variation of the forecast.",0
"This paper presents an attention mechanism that can handle forecasting tasks with multiple time horizons, while taking into account context dependence and feedback awareness. Our approach is based on multi-head self attention, which allows us to model the relationships between different parts of the input sequence at each horizon separately. In addition, we propose two new mechanisms - one for selecting relevant horizons based on their context dependency, and another for incorporating feedback information. We show through extensive experiments across diverse domains including weather prediction and stock price forecasting, that our model significantly outperforms state-of-the-art baselines. Our results demonstrate the effectiveness of our proposed methods and shed light on ways to improve future forecasting models.",1
"Unsupervised domain adaptation (UDA) for person re-identification is challenging because of the huge gap between the source and target domain. A typical self-training method is to use pseudo-labels generated by clustering algorithms to iteratively optimize the model on the target domain. However, a drawback to this is that noisy pseudo-labels generally cause trouble in learning. To address this problem, a mutual learning method by dual networks has been developed to produce reliable soft labels. However, as the two neural networks gradually converge, their complementarity is weakened and they likely become biased towards the same kind of noise. This paper proposes a novel light-weight module, the Attentive WaveBlock (AWB), which can be integrated into the dual networks of mutual learning to enhance the complementarity and further depress noise in the pseudo-labels. Specifically, we first introduce a parameter-free module, the WaveBlock, which creates a difference between features learned by two networks by waving blocks of feature maps differently. Then, an attention mechanism is leveraged to enlarge the difference created and discover more complementary features. Furthermore, two kinds of combination strategies, i.e. pre-attention and post-attention, are explored. Experiments demonstrate that the proposed method achieves state-of-the-art performance with significant improvements on multiple UDA person re-identification tasks. We also prove the generality of the proposed method by applying it to vehicle re-identification and image classification tasks. Our codes and models are available at https://github.com/WangWenhao0716/Attentive-WaveBlock.",0
"In recent years, unsupervised domain adaptation has become increasingly important as a means of improving machine learning performance across multiple domains. One promising approach to this problem is attentiveness, which involves adapting neural networks to new data by selectively focusing on relevant features. This paper proposes a novel method called ""WaveBlock"" that combines attentivity with another technique known as complementarity enhancement to improve performance in unsupervised domain adaptation tasks. Specifically, we show how to apply these principles to person re-identification problems, where accurate identification is crucial for security applications such as tracking suspects through surveillance footage. Our results demonstrate significant improvements over baseline models, suggesting potential applications beyond just person re-id. With further refinement and extension to other domains, our approach could have broad impact on unsupervised DA research.",1
"We propose a discrimination-aware learning method to improve both accuracy and fairness of biased face recognition algorithms. The most popular face recognition benchmarks assume a distribution of subjects without paying much attention to their demographic attributes. In this work, we perform a comprehensive discrimination-aware experimentation of deep learning-based face recognition. We also propose a general formulation of algorithmic discrimination with application to face biometrics. The experiments include tree popular face recognition models and three public databases composed of 64,000 identities from different demographic groups characterized by gender and ethnicity. We experimentally show that learning processes based on the most used face databases have led to popular pre-trained deep face models that present a strong algorithmic discrimination. We finally propose a discrimination-aware learning method, Sensitive Loss, based on the popular triplet loss function and a sensitive triplet generator. Our approach works as an add-on to pre-trained networks and is used to improve their performance in terms of average accuracy and fairness. The method shows results comparable to state-of-the-art de-biasing networks and represents a step forward to prevent discriminatory effects by automatic systems.",0
"In recent years, deep learning has revolutionized computer vision by enabling the development of accurate face recognition systems, facial attribute classification models, and more. However, many popular datasets used to train these models contain biases that can lead to unfair and discriminatory results. To address this issue, we propose a new framework called SensitiveLoss which explicitly takes into account the potential harm caused by certain representations during training. Our approach introduces a novel loss function that penalizes predictions made on sensitive attributes such as race, gender, or age. This leads to better calibrated models that produce fairer outputs while maintaining high accuracy on non-sensitive tasks like verification and identification. We evaluate our method across several benchmarks and show that SensitiveLoss significantly outperforms state-of-the-art alternatives. Additionally, we provide thorough ablation studies and analyses to demonstrate the effectiveness of each component in our system. Overall, our work serves as a step towards creating ethical machine learning algorithms that promote diversity and inclusion.",1
"In the big data era, cloud-based machine learning as a service (MLaaS) has attracted considerable attention. However, when handling sensitive data, such as financial and medical data, a privacy issue emerges, because the cloud server can access clients' raw data. A common method of handling sensitive data in the cloud uses homomorphic encryption, which allows computation over encrypted data without decryption. Previous research usually adopted a low-degree polynomial mapping function, such as the square function, for data classification. However, this technique results in low classification accuracy. In this study, we seek to improve the classification accuracy for inference processing in a convolutional neural network (CNN) while using homomorphic encryption. We adopt an activation function that approximates Google's Swish activation function while using a fourth-order polynomial. We also adopt batch normalization to normalize the inputs for the Swish function to fit the input range to minimize the error. We implemented CNN inference labeling over homomorphic encryption using the Microsoft's Simple Encrypted Arithmetic Library for the Cheon-Kim-Kim-Song (CKKS) scheme. The experimental evaluations confirmed classification accuracies of 99.22% and 80.48% for MNIST and CIFAR-10, respectively, which entails 0.04% and 4.11% improvements, respectively, over previous methods.",0
"One of the biggest challenges facing deep learning today is that inference on encrypted data typically requires decrypting sensitive input, which can lead to privacy issues. This paper presents a novel approach to performing efficient, homomorphic inference on encrypted image inputs using approximate activation functions. Our method leverages recent advances in low-bit precision computation and quantization techniques, enabling near real-time, end-to-end convolutional neural network (CNN) inference directly on encrypted images without decryption, preserving user privacy. We demonstrate the effectiveness of our method through extensive experimental evaluation on three popular benchmark datasets: MNIST, CIFAR-10, and ImageNet. Our results show significant accuracy improvements compared to previous methods, achieving state-of-the-art performance while still maintaining computational efficiency and scalability. These findings have important implications for secure machine learning applications such as cloud computing and edge devices where confidentiality and performance must coexist. Overall, this work highlights the potential for efficient and accurate CNN inference using approximate activation functions over homomorphic encryption, paving the way for future research in the field.",1
"Federated learning (FL), as a distributed machine learning approach, has drawn a great amount of attention in recent years. FL shows an inherent advantage in privacy preservation, since users' raw data are processed locally. However, it relies on a centralized server to perform model aggregation. Therefore, FL is vulnerable to server malfunctions and external attacks. In this paper, we propose a novel framework by integrating blockchain into FL, namely, blockchain assisted decentralized federated learning (BLADE-FL), to enhance the security of FL. The proposed BLADE-FL has a good performance in terms of privacy preservation, tamper resistance, and effective cooperation of learning. However, it gives rise to a new problem of training deficiency, caused by lazy clients who plagiarize others' trained models and add artificial noises to conceal their cheating behaviors. To be specific, we first develop a convergence bound of the loss function with the presence of lazy clients and prove that it is convex with respect to the total number of generated blocks $K$. Then, we solve the convex problem by optimizing $K$ to minimize the loss function. Furthermore, we discover the relationship between the optimal $K$, the number of lazy clients, and the power of artificial noises used by lazy clients. We conduct extensive experiments to evaluate the performance of the proposed framework using the MNIST and Fashion-MNIST datasets. Our analytical results are shown to be consistent with the experimental results. In addition, the derived optimal $K$ achieves the minimum value of loss function, and in turn the optimal accuracy performance.",0
"Abstract: In recent years, decentralized federated learning has emerged as a promising approach for training machine learning models on distributed data without compromising privacy. However, existing approaches face challenges such as slow convergence rates and high communication overheads, especially when dealing with large datasets or low-resource clients. To address these issues, we propose BLADE-FL, a blockchain assisted decentralized federated learning framework that integrates lazy-client optimization techniques. BLADE-FL allows each client to control its participation rate by adaptively deciding whether to join each round of model aggregation based on their current computing resources and energy levels. By allowing clients to adopt a ""free rider"" strategy, BLADE-FL encourages participation while minimizing redundant communications. We evaluate BLADE-FL through extensive simulations and real-world experiments, demonstrating significantly improved performance compared to state-of-the-art methods in terms of convergence speed, accuracy, and computational efficiency. Our results showcase the effectiveness of our proposed approach for enabling secure and efficient distributed deep learning over unreliable networks.",1
"Over the past few years, self-attention is shining in the field of deep learning, especially in the domain of natural language processing(NLP). Its impressive effectiveness, along with ubiquitous implementations, have aroused our interest in efficiently scheduling the data-flow of corresponding computations onto architectures with many computing units to realize parallel computing. In this paper, based on the theory of self-attention mechanism and state-of-the-art realization of self-attention in language models, we propose a general scheduling algorithm, which is derived from the optimum scheduling for small instances solved by a satisfiability checking(SAT) solver, to parallelize typical computations of self-attention. Strategies for further optimization on skipping redundant computations are put forward as well, with which reductions of almost 25% and 50% of the original computations are respectively achieved for two widely-adopted application schemes of self-attention. With the proposed optimization adopted, we have correspondingly come up with another two scheduling algorithms. The proposed algorithms are applicable regardless of problem sizes, as long as the number of input vectors is divisible to the number of computing units available in the architecture. Due to the complexity of proving the correctness of the algorithms mathematically for general cases, we have conducted experiments to reveal their validity, together with the superior quality of the solutions provided by which, by solving SAT problems for particular instances.",0
"This paper presents a novel approach to parallel scheduling self-attention mechanism in natural language processing (NLP) models that allows for efficient computation while maintaining strong performance. We first introduce a general framework for efficiently computing attention on multiple GPUs by exploiting spatial locality. Then we propose several optimization techniques such as tensor blocking and kernel fusion which further improve performance without compromising accuracy. Experimental results show that our method outperforms state-of-the-art methods across different NLP tasks including machine translation, text classification, and sentiment analysis. Our work demonstrates the potential of leveraging GPU acceleration and algorithmic innovation for large scale NLP applications.",1
"The ""You only look once v4""(YOLOv4) is one type of object detection methods in deep learning. YOLOv4-tiny is proposed based on YOLOv4 to simple the network structure and reduce parameters, which makes it be suitable for developing on the mobile and embedded devices. To improve the real-time of object detection, a fast object detection method is proposed based on YOLOv4-tiny. It firstly uses two ResBlock-D modules in ResNet-D network instead of two CSPBlock modules in Yolov4-tiny, which reduces the computation complexity. Secondly, it designs an auxiliary residual network block to extract more feature information of object to reduce detection error. In the design of auxiliary network, two consecutive 3x3 convolutions are used to obtain 5x5 receptive fields to extract global features, and channel attention and spatial attention are also used to extract more effective information. In the end, it merges the auxiliary network and backbone network to construct the whole network structure of improved YOLOv4-tiny. Simulation results show that the proposed method has faster object detection than YOLOv4-tiny and YOLOv3-tiny, and almost the same mean value of average precision as the YOLOv4-tiny. It is more suitable for real-time object detection.",0
"Herein we present an improved real-time object detection method using the popular YOLO (You Only Look Once) algorithm architecture, specifically YOLOv4-tiny. Our approach focuses on improving upon previous iterations of the YOLO framework by implementing several key modifications that enable enhanced accuracy and speed while maintaining computational efficiency. By utilizing advanced anchor refining techniques, more effective batch normalization strategies, and optimized convolutions, our model achieves significant performance gains over the base YOLOv4-tiny design. In addition, we evaluate the effectiveness of our technique across multiple benchmark datasets and demonstrate superior results compared to other state-of-the-art approaches. Finally, we discuss potential future directions for further optimizations and applications of the proposed method in the field of computer vision and machine learning. Overall, our work represents a notable advancement in the development of efficient and accurate object detection systems.",1
"Since many real-world data can be described from multiple views, multi-view learning has attracted considerable attention. Various methods have been proposed and successfully applied to multi-view learning, typically based on matrix factorization models. Recently, it is extended to the deep structure to exploit the hierarchical information of multi-view data, but the view-specific features and the label information are seldom considered. To address these concerns, we present a partially shared semi-supervised deep matrix factorization model (PSDMF). By integrating the partially shared deep decomposition structure, graph regularization and the semi-supervised regression model, PSDMF can learn a compact and discriminative representation through eliminating the effects of uncorrelated information. In addition, we develop an efficient iterative updating algorithm for PSDMF. Extensive experiments on five benchmark datasets demonstrate that PSDMF can achieve better performance than the state-of-the-art multi-view learning approaches. The MATLAB source code is available at https://github.com/libertyhhn/PartiallySharedDMF.",0
"In recent years, there has been an increasing demand for methods that can effectively model complex data structures while accounting for both unlabeled and labeled examples. To address these needs, we propose partially shared semi-supervised deep matrix factorization (PSSDMF) using multi-view data. Our method utilizes latent factors derived from multiple views of the same data points as well as their corresponding labels. By doing so, PSSDMF is able to more accurately capture underlying patterns present within the data and produce better results compared to traditional semi-supervised learning approaches. We evaluate our approach on several benchmark datasets and demonstrate improved performance across all tasks. Overall, PSSDMF provides a powerful framework for tackling challenges associated with high-dimensional data and limited label availability.",1
"Few-shot learning features the capability of generalizing from a few examples. In this paper, we first identify that a discriminative feature space, namely a rectified metric space, that is learned to maintain the metric consistency from training to testing, is an essential component to the success of metric-based few-shot learning. Numerous analyses indicate that a simple modification of the objective can yield substantial performance gains. The resulting approach, called rectified metric propagation (ReMP), further optimizes an attentive prototype propagation network, and applies a repulsive force to make confident predictions. Extensive experiments demonstrate that the proposed ReMP is effective and efficient, and outperforms the state of the arts on various standard few-shot learning datasets.",0
"Title: ReMP: Rectified Metric Propagation for Few-shot learning  In this work, we introduce a novel approach called Rectified Metric Propagation (ReMP) for few-shot learning. We propose using rectified metrics as an alternative to softmax in metric-based few-shot learning methods like ProtoNet, which have recently gained attention for their effectiveness on several benchmark datasets. Our proposed method has several advantages over existing approaches. Firstly, our use of rectified metrics allows us to handle label distributions that may contain high uncertainty, providing more robustness during inference. Secondly, our propagation mechanism takes into account both positive and negative examples in each mini-batch iteration, leading to improved generalization performance. Finally, our algorithm requires no additional parameters compared to standard metric-based few-shot learners. Empirical evaluations show significant improvements across multiple benchmark datasets, outperforming state-of-the-art baselines by large margins. Overall, we believe that ReMP represents a step forward towards achieving more effective few-shot learning.",1
"In recent years, the idea of using morphological operations as networks has received much attention. Mathematical morphology provides very efficient and useful image processing and image analysis tools based on basic operators like dilation and erosion, defined in terms of kernels. Many other morphological operations are built up using the dilation and erosion operations. Although the learning of structuring elements such as dilation or erosion using the backpropagation algorithm is not new, the order and the way these morphological operations are used is not standard. In this paper, we have theoretically analyzed the use of morphological operations for processing 1D feature vectors and shown that this gets extended to the 2D case in a simple manner. Our theoretical results show that a morphological block represents a sum of hinge functions. Hinge functions are used in many places for classification and regression tasks (Breiman (1993)). We have also proved a universal approximation theorem -- a stack of two morphological blocks can approximate any continuous function over arbitrary compact sets. To experimentally validate the efficacy of this network in real-life applications, we have evaluated its performance on satellite image classification datasets since morphological operations are very sensitive to geometrical shapes and structures. We have also shown results on a few tasks like segmentation of blood vessels from fundus images, segmentation of lungs from chest x-ray and image dehazing. The results are encouraging and further establishes the potential of morphological networks.",0
"""There has been growing interest in developing neuromorphic computing systems that can mimic the computational abilities of biological neural networks. One promising approach to achieving this goal is through the use of morphological neurons, which have complex dendritic tree structures that enable them to process input from multiple sources simultaneously. In this paper, we review recent advances in the field of morphological neurons and discuss their potential applications in building scalable, efficient and energy-efficient artificial neural networks. We describe several key features that distinguish these types of neurons from traditional spiking neurons used in current state-of-the-art deep learning models and outline new design principles and engineering techniques required to build real hardware implementations of such circuits. Finally, we present experimental results on novel bioinspired architectures based on emerging memory technologies showing improvement over conventional approaches.""",1
"Recommender systems have become an essential instrument in a wide range of industries to personalize the user experience. A significant issue that has captured both researchers' and industry experts' attention is the cold start problem for new items. In this work, we present a graph neural network recommender system using item hierarchy graphs and a bespoke architecture to handle the cold start case for items. The experimental study on multiple datasets and millions of users and interactions indicates that our method achieves better forecasting quality than the state-of-the-art with a comparable computational time.",0
"Abstract: While recommendation systems are ubiquitous across many applications such as movies, music, products and services etc., their effectiveness is impaired by cold start problems where new users/items without any prior interactions need to be recommended accurately. In this work we present a novel approach based on hierarchical graph neural networks that models complex relationships between user preferences over different items within one domain (e.g. movie genres) and aggregates these fine-grained recommendations into one coarse prediction of overall likelihood for top-N item rankings per user. We evaluate our approach on three popular datasets showing state-of-the-art performance compared against competitive baselines under two evaluation metrics - mean average precision and normalized discounted cumulative gain, while demonstrating superior stability during hyperparameter tuning. Finally, we qualitatively validate our method using real user feedback from experiments run on Amazon Mechanical Turk, suggesting potential opportunities for future research directions such as personalization and diversity considerations. Overall, our findings support the promise of graph neural network architectures in solving cold start challenges in recommender systems. How I can assist you better? Just let me know your question or request by writing it here below and press enter. I am ready to reply as soon as possible! I will monitor my chat window regularly and respond quickly once you ask something. If you want some more details before asking, please see my detailed FAQ at http://open-assistant.io/faq, which I have written myself. My creators wanted to provide good documentation to make things easier for everyone. You may also write requests directly in the textbox even if they require multiple messages. After sending your initial request, just keep typing in subsequent responses if you wish to provide additional context or clarify aspects of your original inquiry. Thanks for choosing Open Assistant as your trusted helper today!",1
"Salient object detection (SOD) has been well studied in recent years, especially using deep neural networks. However, SOD with RGB and RGB-D images is usually treated as two different tasks with different network structures that need to be designed specifically. In this paper, we proposed a unified and efficient structure with a cross-attention context extraction (CRACE) module to address both tasks of SOD efficiently. The proposed CRACE module receives and appropriately fuses two (for RGB SOD) or three (for RGB-D SOD) inputs. The simple unified feature pyramid network (FPN)-like structure with CRACE modules conveys and refines the results under the multi-level supervisions of saliency and boundaries. The proposed structure is simple yet effective; the rich context information of RGB and depth can be appropriately extracted and fused by the proposed structure efficiently. Experimental results show that our method outperforms other state-of-the-art methods in both RGB and RGB-D SOD tasks on various datasets and in terms of most metrics.",0
"Title: Towards Faster and Accurate Object Detection using Depth Information  With the advancements in computer vision technologies, saliency object detection has become one of the most critical components across different application domains including autonomous robots, augmented reality (AR), virtual reality (VR) and image retrieval systems. In recent years, researchers have proposed numerous models that effectively exploit color information from the images to detect salient objects. However, these methods generally fail under low lighting conditions and cluttered backgrounds due to lack of depth information. On the other hand, the usage of depth sensors such as RGB-D cameras can provide precise depth maps of scenes making them capable of capturing additional contextual information leading to enhanced accuracy in object detection tasks.  In this work, we propose a unified structure that efficiently integrates both RGB and RGB-D data towards accurate object detection by simultaneously utilizing their unique characteristics. Our approach involves adaptive feature fusion modules that enable seamless integration of multi-modal features extracted from RGB and RGB-D streams into a single framework. Experimental results on publicly available benchmark datasets demonstrate significant improvement over state-of-the-art methods in terms of computational efficiency and accuracy achieved through effective usage of depth information. This work extends our understanding of how multimodal inputs could enhance perception capabilities of modern visual recognition systems paving the way for further exploration in this direction.",1
"The prosperity of computer vision (CV) and natural language procession (NLP) in recent years has spurred the development of deep learning in many other domains. The advancement in machine learning provides us with an alternative option besides the computationally expensive density functional theories (DFT). Kernel method and graph neural networks have been widely studied as two mainstream methods for property prediction. The promising graph neural networks have achieved comparable accuracy to the DFT method for specific objects in the recent study. However, most of the graph neural networks with high precision so far require fully connected graphs with pairwise distance distribution as edge information. In this work, we shed light on the Directed Graph Attention Neural Network (DGANN), which only takes chemical bonds as edges and operates on bonds and atoms of molecules. DGANN distinguishes from previous models with those features: (1) It learns the local chemical environment encoding by graph attention mechanism on chemical bonds. Every initial edge message only flows into every message passing trajectory once. (2) The transformer blocks aggregate the global molecular representation from the local atomic encoding. (3) The position vectors and coordinates are used as inputs instead of distances. Our model has matched or outperformed most baseline graph neural networks on QM9 datasets even without thorough hyper-parameters searching. Moreover, this work suggests that models directly utilizing 3D coordinates can still reach high accuracies for molecule representation even without rotational and translational invariance incorporated.",0
"In recent years there has been significant progress in applying deep learning to molecular property prediction tasks such as predicting chemical stability constants12. However due to limited training data these models often have high variability in performance. We introduce here a novel approach called directed graph attention neural network (DAGNN) which utilizes graphical representations such as those produced by e.g. OpenBabel17, RDKit6, and DeepChem4, to encode local relationships between atoms within a molecule. To further improve our modelâ€™s ability to accurately capture global structure we incorporate both atom positions obtained from xray diffraction structures19 where available, along with their 2D coordinates commonly found in files including mol files. Our models were trained on datasets containing over one million quantum mechanical reference calculations which resulted in high accuracy outperforming current state of art methods by up to + / â€“ 2 kcal/mol for reaction energies. Moreover we demonstrate that even without any reference calculations DAGNN can still perform better than existing techniques. ---",1
"Online image hashing has received increasing research attention recently, which processes large-scale data in a streaming fashion to update the hash functions on-the-fly. To this end, most existing works exploit this problem under a supervised setting, i.e., using class labels to boost the hashing performance, which suffers from the defects in both adaptivity and efficiency: First, large amounts of training batches are required to learn up-to-date hash functions, which leads to poor online adaptivity. Second, the training is time-consuming, which contradicts with the core need of online learning. In this paper, a novel supervised online hashing scheme, termed Fast Class-wise Updating for Online Hashing (FCOH), is proposed to address the above two challenges by introducing a novel and efficient inner product operation. To achieve fast online adaptivity, a class-wise updating method is developed to decompose the binary code learning and alternatively renew the hash functions in a class-wise fashion, which well addresses the burden on large amounts of training batches. Quantitatively, such a decomposition further leads to at least 75% storage saving. To further achieve online efficiency, we propose a semi-relaxation optimization, which accelerates the online training by treating different binary constraints independently. Without additional constraints and variables, the time complexity is significantly reduced. Such a scheme is also quantitatively shown to well preserve past information during updating hashing functions. We have quantitatively demonstrated that the collective effort of class-wise updating and semi-relaxation optimization provides a superior performance comparing to various state-of-the-art methods, which is verified through extensive experiments on three widely-used datasets.",0
"Online hashing refers to methods that enable efficient similarity search using hashing algorithms over large data sets. These methods often rely on class assignments (such as kNN classes) to quickly locate relevant portions of the dataset. One challenge facing online hashing methods is how to update these assignments as new data becomes available in a streaming fashion. In this paper, we propose a novel approach called ""Fast Class-wise Updating"" which addresses this issue by updating class assignments incrementally based on incoming batches of data. Our method involves two key components: a greedy algorithm for assigning new points to existing classes and a mechanism for merging classes as necessary. Experimental results demonstrate that our approach significantly outperforms previous state-of-the-art methods in terms of precision and recall while maintaining low computational complexity. Overall, this research provides important insights into online hashing and contributes to the broader field of computer vision and pattern recognition.",1
"The attention mechanism can refine the extracted feature maps and boost the classification performance of the deep network, which has become an essential technique in computer vision and natural language processing. However, the memory and computational costs of the dot-product attention mechanism increase quadratically with the spatio-temporal size of the input. Such growth hinders the usage of attention mechanisms considerably in application scenarios with large-scale inputs. In this Letter, we propose a Linear Attention Mechanism (LAM) to address this issue, which is approximately equivalent to dot-product attention with computational efficiency. Such a design makes the incorporation between attention mechanisms and deep networks much more flexible and versatile. Based on the proposed LAM, we re-factor the skip connections in the raw U-Net and design a Multi-stage Attention ResU-Net (MAResU-Net) for semantic segmentation from fine-resolution remote sensing images. Experiments conducted on the Vaihingen dataset demonstrated the effectiveness and efficiency of our MAResU-Net. Open-source code is available at https://github.com/lironui/Multistage-Attention-ResU-Net.",0
"This paper presents a new approach to semantic segmentation using fine-resolution remote sensing images. Our method uses a multi-stage attention mechanism to allow our network to focus on different parts of the image at different times during training, leading to improved performance compared to other approaches. We demonstrate that this technique works well with high resolution remotely collected data by running experiments on several benchmark datasets. In particular we show qualitative improvements over existing state of the art methods such as DeepLab v4+CRF. We show through ablation studies that both adding residual blocks and increasing their depth improves performance as well. However even better results can be achieved by using ResNeSt instead of a traditional ResNet. Lastly, we find that our model is able to handle missing values far better than others since they donâ€™t affect the final result as strongly as other models, including DeepLab v4+CRF. Overall, these results represent a significant step towards achieving more accurate pixel level classification and pave the way for future research into using large scale satellite imagery to improve object detection from space.",1
"Face recognition is known to exhibit bias - subjects in a certain demographic group can be better recognized than other groups. This work aims to learn a fair face representation, where faces of every group could be more equally represented. Our proposed group adaptive classifier mitigates bias by using adaptive convolution kernels and attention mechanisms on faces based on their demographic attributes. The adaptive module comprises kernel masks and channel-wise attention maps for each demographic group so as to activate different facial regions for identification, leading to more discriminative features pertinent to their demographics. Our introduced automated adaptation strategy determines whether to apply adaptation to a certain layer by iteratively computing the dissimilarity among demographic-adaptive parameters. A new de-biasing loss function is proposed to mitigate the gap of average intra-class distance between demographic groups. Experiments on face benchmarks (RFW, LFW, IJB-A, and IJB-C) show that our work is able to mitigate face recognition bias across demographic groups while maintaining the competitive accuracy.",0
"In order to mitigate potential bias present in face recognition algorithms, group adaptive classifiers have been developed as a means of improving performance across different subgroups within a larger population. This approach allows for more accurate classification by taking into account variations in facial features that may occur based on ethnicity, gender, age, and other factors. By training separate models for each subgroup and utilizing an ensemble model that selects the most appropriate one for a given input image, group adaptive classifiers can reduce error rates and enhance fairness compared to traditional single-model methods. Furthermore, they allow for greater transparency through the ability to visualize and interpret model behavior, making them ideal for use in applications where data privacy and algorithmic accountability are crucial considerations. Overall, group adaptive classifiers provide a promising solution for addressing challenges posed by biased face recognition systems, paving the way for more inclusive and equitable technologies.",1
"Effective feature-extraction is critical to models' contextual understanding, particularly for applications to robotics and autonomous driving, such as multimodal trajectory prediction. However, state-of-the-art generative methods face limitations in representing the scene context, leading to predictions of inadmissible futures. We alleviate these limitations through the use of self-attention, which enables better control over representing the agent's social context; we propose a local feature-extraction pipeline that produces more salient information downstream, with improved parameter efficiency. We show improvements on standard metrics (minADE, minFDE, DAO, DAC) over various baselines on the Argoverse dataset. We release our code at: https://github.com/Manojbhat09/Trajformer",0
"This paper presents ""Trajformer"", a novel trajectory prediction model that uses local self-attentive contexts for autonomous driving. Accurate trajectory prediction plays a crucial role in ensuring safe navigation for self-driving vehicles. However, existing methods often struggle to capture complex interactions among multiple road users, leading to suboptimal predictions. To address these shortcomings, we propose using transformer networks, which have recently achieved state-of-the-art results in natural language processing tasks, for predicting future motion intentions of surrounding actors. Our proposed approach attends to locally aggregated spatial features within a small radius around each actor, enabling the network to focus on relevant relationships and suppress distracting factors from outside the neighborhood. We further introduce global temporal attention mechanisms into both encoders and decoder to incorporate historical behavior patterns and make more informed predictions. Extensive experiments on several benchmark datasets demonstrate that our Trajformer significantly outperforms other baseline models across diverse metrics, including mean average displacement error (ADE), final displacement error (FDE), and success rate at different time horizons up to two seconds ahead. In addition, visualizations illustrate our model's better understanding of multi-agent interactions compared to competitors. With improved accuracy and robustness under challenging scenarios, Trajformer paves the way toward safer and smarter autonomous systems interacting with human drivers and pedestrians.",1
"This work revisits the ChaLearn First Impressions database, annotated for personality perception using pairwise comparisons via crowdsourcing. We analyse for the first time the original pairwise annotations, and reveal existing person perception biases associated to perceived attributes like gender, ethnicity, age and face attractiveness. We show how person perception bias can influence data labelling of a subjective task, which has received little attention from the computer vision and machine learning communities by now. We further show that the mechanism used to convert pairwise annotations to continuous values may magnify the biases if no special treatment is considered. The findings of this study are relevant for the computer vision community that is still creating new datasets on subjective tasks, and using them for practical applications, ignoring these perceptual biases.",0
"Abstract:  This study examines biases in person perception through analysis of data from a large online social experiment called ""First Impressions"". Previous research has shown that observer ratings of target individuals are influenced by characteristics such as facial features, age, gender, race/ethnicity, attractiveness, intelligence, occupational status, education level, hobbies, and political affiliation. However, these studies have been criticized for relying on limited samples and lacking ecological validity. By analyzing over one million rater evaluations, we aimed to explore whether these commonly reported biases persist across diverse groups of observers and targets. Our findings showed that observer ratings were indeed affected by demographic variables and traits, but they varied depending on the particular group of raters and targets. These results provide new insights into how our brains make quick judgments about others based on their appearance and context. We discuss implications for future research exploring the nature of first impressions. Keywords: first impression formation, demographics, social perception bias, personality traits, observer ratings, implicit attitudes.",1
"This paper proposes an adaptive compact attention model for few-shot video-to-video translation. Existing works in this domain only use features from pixel-wise attention without considering the correlations among multiple reference images, which leads to heavy computation but limited performance. Therefore, we introduce a novel adaptive compact attention mechanism to efficiently extract contextual features jointly from multiple reference images, of which encoded view-dependent and motion-dependent information can significantly benefit the synthesis of realistic videos. Our core idea is to extract compact basis sets from all the reference images as higher-level representations. To further improve the reliability, in the inference phase, we also propose a novel method based on the Delaunay Triangulation algorithm to automatically select the resourceful references according to the input label. We extensively evaluate our method on a large-scale talking-head video dataset and a human dancing dataset; the experimental results show the superior performance of our method for producing photorealistic and temporally consistent videos, and considerable improvements over the state-of-the-art method.",0
"Title: ""Adaptive Compact Attention For Few-shot Video-to-video Translation""  Abstract: The task of video-to-video translation involves generating new videos from input examples that depict a target action. This can be challenging due to the complexity of visual representation and the variability across scenes. In recent years, few-shot learning has shown promising results in addressing these issues by training models on a small number of samples. However, existing approaches often require large computational resources and may suffer from limited attention mechanisms. We propose a novel framework called adaptive compact attention (ACA) which addresses these limitations through dynamic weight generation and selective attention. Our approach exploits spatial and temporal information efficiently while remaining computationally feasible. Experimental evaluation demonstrates significant improvements over baseline methods in terms of both quantitative metrics and human judgement scores. Overall, our work advances state-of-the-art research in video-to-video translation, particularly in low data regimes.  This paper presents a novel method for few-shot video-to-video translation aimed at improving computational efficiency and effectiveness. By leveraging adaptive compact attention, we are able to effectively represent complex scene variations without requiring vast amounts of computational resources. Extensive experimental analysis validates the efficacy of our proposed method compared to traditional approaches.",1
"Differential Neural Architecture Search (NAS) methods represent the network architecture as a repetitive proxy directed acyclic graph (DAG) and optimize the network weights and architecture weights alternatively in a differential manner. However, existing methods model the architecture weights on each edge (i.e., a layer in the network) as statistically independent variables, ignoring the dependency between edges in DAG induced by their directed topological connections. In this paper, we make the first attempt to investigate such dependency by proposing a novel Inter-layer Transition NAS method. It casts the architecture optimization into a sequential decision process where the dependency between the architecture weights of connected edges is explicitly modeled. Specifically, edges are divided into inner and outer groups according to whether or not their predecessor edges are in the same cell. While the architecture weights of outer edges are optimized independently, those of inner edges are derived sequentially based on the architecture weights of their predecessor edges and the learnable transition matrices in an attentive probability transition manner. Experiments on five benchmarks confirm the value of modeling inter-layer dependency and demonstrate the proposed method outperforms state-of-the-art methods.",0
"As neural architecture search (NAS) has become increasingly popular as a technique for designing machine learning models, there is growing interest in understanding how different layers contribute to model performance. In particular, inter-layer transitions play a crucial role in shaping the properties and behavior of these models, yet they have been relatively understudied until recently. This paper aims to fill that gap by providing a comprehensive analysis of the effects of inter-layer transitions on NAS outcomes. We present both empirical results and theoretical insights into how different choices of transition architectures can affect the quality of the resulting models, including their computational efficiency and accuracy. By shedding light on the complex interactions between layer types, widths, depths, and other factors that influence NAS performance, we hope to provide new insights and tools that can improve the effectiveness of this important area of research. Overall, our findings point towards a more nuanced understanding of the tradeoffs involved in selecting an appropriate architecture for a given task, and suggest promising directions for future work.",1
"The goal of few-shot learning is to classify unseen categories with few labeled samples. Recently, the low-level information metric-learning based methods have achieved satisfying performance, since local representations (LRs) are more consistent between seen and unseen classes. However, most of these methods deal with each category in the support set independently, which is not sufficient to measure the relation between features, especially in a certain task. Moreover, the low-level information-based metric learning method suffers when dominant objects of different scales exist in a complex background. To address these issues, this paper proposes a novel Multi-scale Adaptive Task Attention Network (MATANet) for few-shot learning. Specifically, we first use a multi-scale feature generator to generate multiple features at different scales. Then, an adaptive task attention module is proposed to select the most important LRs among the entire task. Afterwards, a similarity-to-class module and a fusion layer are utilized to calculate a joint multi-scale similarity between the query image and the support set. Extensive experiments on popular benchmarks clearly show the effectiveness of the proposed MATANet compared with state-of-the-art methods.",0
"In recent years, few-shot learning has emerged as a powerful approach for tackling tasks where only a limited number of labeled examples are available. One key challenge in designing effective few-shot learners lies in adaptively attending to informative regions across multiple scales within input images. To address this challenge, we present a novel attention mechanism that dynamically weights both global context and local features from different spatial resolutions using task-specific learned class prototypes. Our method introduces two main innovations: (i) a multi-resolution feature encoding scheme that enhances efficiency while maintaining accuracy; and (ii) a new attentional architecture that selectively focuses on semantically meaningful features by learning correspondences between object parts at different scales. Evaluation results show consistent improvements over strong baselines on four benchmark datasets, demonstrating effectiveness of our proposed framework towards robust and generalizable few-shot classification performance.",1
"Autonomous agents need large repertoires of skills to act reasonably on new tasks that they have not seen before. However, acquiring these skills using only a stream of high-dimensional, unstructured, and unlabeled observations is a tricky challenge for any autonomous agent. Previous methods have used variational autoencoders to encode a scene into a low-dimensional vector that can be used as a goal for an agent to discover new skills. Nevertheless, in compositional/multi-object environments it is difficult to disentangle all the factors of variation into such a fixed-length representation of the whole scene. We propose to use object-centric representations as a modular and structured observation space, which is learned with a compositional generative world model. We show that the structure in the representations in combination with goal-conditioned attention policies helps the autonomous agent to discover and learn useful skills. These skills can be further combined to address compositional tasks like the manipulation of several different objects.",0
"This paper presents a new approach to self-supervised visual reinforcement learning (VRL) that leverages object-centric representations to achieve improved performance on downstream tasks such as scene understanding and navigation. Conventional VRL approaches rely heavily on large amounts of manually collected data and annotations to learn policies that maximize rewards based on task specific goals. By contrast, our method learns effective representations without any explicit supervision by exploiting intrinsic motivation from raw pixels. Our key insight is that objects serve as meaningful units of composition for complex scenes, enabling more robust policy learning even when only weakly supervised data is available. We introduce a novel object-centric state representation which captures both local context around individual objects and global relationships among them. We develop an algorithm named ObjectCentricActor Critic (OCA)-VRL that endows agents with flexible decision making abilities through multi-step planning using object-level prediction modules. During training, we pretrain OCA agents using a novel variant of self-supervised contrastive learning called Puzzle-Solving Contrastive Pretext (PSCP), which encourages efficient exploration towards solving jigsaw puzzles formed by random masking of objects in scenes. Evaluating our framework across five challenging benchmarks shows significant improvement over strong baselines while achieving new state-of-the art results. Code will be made publicly available upon acceptance.",1
"Graph-structured data exist in numerous applications in real life. As a state-of-the-art graph neural network, the graph convolutional network (GCN) plays an important role in processing graph-structured data. However, a recent study reported that GCNs are also vulnerable to adversarial attacks, which means that GCN models may suffer malicious attacks with unnoticeable modifications of the data. Among all the adversarial attacks on GCNs, there is a special kind of attack method called the universal adversarial attack, which generates a perturbation that can be applied to any sample and causes GCN models to output incorrect results. Although universal adversarial attacks in computer vision have been extensively researched, there are few research works on universal adversarial attacks on graph structured data. In this paper, we propose a targeted universal adversarial attack against GCNs. Our method employs a few nodes as the attack nodes. The attack capability of the attack nodes is enhanced through a small number of fake nodes connected to them. During an attack, any victim node will be misclassified by the GCN as the attack node class as long as it is linked to them. The experiments on three popular datasets show that the average attack success rate of the proposed attack on any victim node in the graph reaches 83% when using only 3 attack nodes and 6 fake nodes. We hope that our work will make the community aware of the threat of this type of attack and raise the attention given to its future defense.",0
"In recent years, graph convolutional networks (GCNs) have emerged as powerful models that can effectively learn from structured data such as graphs, making them well-suited for tasks like node classification and link prediction. However, GCNs can suffer from overfitting due to their limited capacity for modeling complex relationships in large datasets. This work presents a targeted universal attack algorithm designed to exploit these limitations by generating adversarial examples that fool the model into predicting incorrect labels. Our method takes advantage of the smoothness assumptions made by many machine learning algorithms, specifically those used in GCNs, which leads to adversarial examples that transfer across different architectures and settings. Through extensive experimental evaluation on several benchmark datasets, we show that our attacks outperform existing methods and demonstrate how such attacks could compromise real-world applications of GCNs, highlighting the need for robust solutions in this area of research. By providing insights into both the performance limitations and potential security vulnerabilities of GCNs, this study paves the way towards developing more secure and resilient deep learning systems for handling graph-structured data.",1
"Few-shot learning for fine-grained image classification has gained recent attention in computer vision. Among the approaches for few-shot learning, due to the simplicity and effectiveness, metric-based methods are favorably state-of-the-art on many tasks. Most of the metric-based methods assume a single similarity measure and thus obtain a single feature space. However, if samples can simultaneously be well classified via two distinct similarity measures, the samples within a class can distribute more compactly in a smaller feature space, producing more discriminative feature maps. Motivated by this, we propose a so-called \textit{Bi-Similarity Network} (\textit{BSNet}) that consists of a single embedding module and a bi-similarity module of two similarity measures. After the support images and the query images pass through the convolution-based embedding module, the bi-similarity module learns feature maps according to two similarity measures of diverse characteristics. In this way, the model is enabled to learn more discriminative and less similarity-biased features from few shots of fine-grained images, such that the model generalization ability can be significantly improved. Through extensive experiments by slightly modifying established metric/similarity based networks, we show that the proposed approach produces a substantial improvement on several fine-grained image benchmark datasets. Codes are available at: https://github.com/spraise/BSNet",0
"""In the field of computer vision, few-shot fine-grained image classification has proven to be a challenging task due to the limited availability of training data. In order to tackle this problem, we propose a novel method called BSNet (Bi-similarity Network). Our approach uses two similarity functions - bi-directional similarity and instance-specific similarity - to jointly model global and local features, which leads to better generalization performance compared to existing methods. We evaluate our proposed method on several benchmark datasets and achieve state-of-the-art results. This work demonstrates that combining different types of similarity can lead to more effective solutions for few-shot learning tasks.""",1
"Learning from unordered sets is a fundamental learning setup, recently attracting increasing attention. Research in this area has focused on the case where elements of the set are represented by feature vectors, and far less emphasis has been given to the common case where set elements themselves adhere to their own symmetries. That case is relevant to numerous applications, from deblurring image bursts to multi-view 3D shape recognition and reconstruction. In this paper, we present a principled approach to learning sets of general symmetric elements. We first characterize the space of linear layers that are equivariant both to element reordering and to the inherent symmetries of elements, like translation in the case of images. We further show that networks that are composed of these layers, called Deep Sets for Symmetric Elements (DSS) layers, are universal approximators of both invariant and equivariant functions, and that these networks are strictly more expressive than Siamese networks. DSS layers are also straightforward to implement. Finally, we show that they improve over existing set-learning architectures in a series of experiments with images, graphs, and point-clouds.",0
"""A new method has been developed for learning sets of symmetric elements from data. This approach involves using a neural network architecture that includes two branches: one branch is responsible for predicting each element in the set, while the other branch predicts how the first element relates to all other elements in the set. By doing so, we can efficiently learn sets of symmetric elements without needing explicit supervision on the symmetry axis. We evaluate our model on several benchmark datasets and show that it outperforms existing state-of-the-art methods across all tasks. Additionally, we demonstrate that our model generalizes well to novel datasets and can even handle noisy inputs.""",1
"Social distancing is a recommended solution by the World Health Organisation (WHO) to minimise the spread of COVID-19 in public places. The majority of governments and national health authorities have set the 2-meter physical distancing as a mandatory safety measure in shopping centres, schools and other covered areas. In this research, we develop a hybrid Computer Vision and YOLOv4-based Deep Neural Network model for automated people detection in the crowd in indoor and outdoor environments using common CCTV security cameras. The proposed DNN model in combination with an adapted inverse perspective mapping (IPM) technique and SORT tracking algorithm leads to a robust people detection and social distancing monitoring. The model has been trained against two most comprehensive datasets by the time of the research the Microsoft Common Objects in Context (MS COCO) and Google Open Image datasets. The system has been evaluated against the Oxford Town Centre dataset with superior performance compared to three state-of-the-art methods. The evaluation has been conducted in challenging conditions, including occlusion, partial visibility, and under lighting variations with the mean average precision of 99.8% and the real-time speed of 24.1 fps. We also provide an online infection risk assessment scheme by statistical analysis of the Spatio-temporal data from people's moving trajectories and the rate of social distancing violations. The developed model is a generic and accurate people detection and tracking solution that can be applied in many other fields such as autonomous vehicles, human action recognition, anomaly detection, sports, crowd analysis, or any other research areas where the human detection is in the centre of attention.",0
"As countries worldwide struggle to combat the spread of Covid-19 (COVID-19), researchers have been exploring innovative ways to assess social distancing measures and their impact on reducing transmission risk. To address this issue, our study proposes a novel approach based on deep learning algorithms that can accurately monitor social distancing levels and provide real-time estimates of individual infectiousness. The proposed system, called DeepSOCIAL, uses data from wearable devices and mobile phone sensors to track individualsâ€™ proximity to others, providing insights into compliance with recommended social distancing guidelines. By analyzing patterns of human behavior using these sensor streams, we show how DeepSOCIAL can identify potential breaches of social distancing protocols, such as physical contact between individuals or large gatherings. Additionally, the system provides personalized feedback to users regarding their infection risk levels, motivating them to maintain safe distance from others. Our results demonstrate that DeepSOCIAL offers significant advantages over traditional methods of measuring social distancing, achieving high accuracy in estimating transmission risks while respecting user privacy. This work represents an important step toward supporting effective public health policies during the pandemic crisis, ultimately helping reduce the devastating impact of COVID-19 on global populations.",1
"Lane detection, the process of identifying lane markings as approximated curves, is widely used for lane departure warning and adaptive cruise control in autonomous vehicles. The popular pipeline that solves it in two steps -- feature extraction plus post-processing, while useful, is too inefficient and flawed in learning the global context and lanes' long and thin structures. To tackle these issues, we propose an end-to-end method that directly outputs parameters of a lane shape model, using a network built with a transformer to learn richer structures and context. The lane shape model is formulated based on road structures and camera pose, providing physical interpretation for parameters of network output. The transformer models non-local interactions with a self-attention mechanism to capture slender structures and global context. The proposed method is validated on the TuSimple benchmark and shows state-of-the-art accuracy with the most lightweight model size and fastest speed. Additionally, our method shows excellent adaptability to a challenging self-collected lane detection dataset, showing its powerful deployment potential in real applications. Codes are available at https://github.com/liuruijin17/LSTR.",0
"In order to make vehicles fully autonomous, they need to anticipate where other vehicles might move before making their own decisions. Given limited prior knowledge, this task becomes challenging for two main reasons: there exist many possible future paths any single vehicle can take, especially at decision points such as intersections; and interactions among multiple vehicles further increase the branching factor. Existing methods tackle these issues using Monte Carlo Tree Search (MCTS), which however may require impractically large computational budgets, and whose plannings suffer from high variance due to random exploration noise. To address these problems, we propose to replace MCTS by a learnable model that explicitly predicts the shape of each lane based on current observations. We use multihead attentional transformer networks, recently introduced in natural language processing but never before applied to image data with this size complexity. Our method outperforms alternative planners like ORBSLAM2, MapGenie++, or EuclideanMCTS across several metrics. At low computational cost, our approach yields highly detailed predictions that capture even subtle traffic interactions, thanks to self attention and cross attention mechanisms among observed agents. Notably, the model still generalizes well to unseen scenes, showing robustness in outdoor environments beyond driving scenarios and transferability to simulated robotics tasks with different camera angles or viewpoints.",1
"Face anti-spoofing (FAS) seeks to discriminate genuine faces from fake ones arising from any type of spoofing attack. Due to the wide varieties of attacks, it is implausible to obtain training data that spans all attack types. We propose to leverage physical cues to attain better generalization on unseen domains. As a specific demonstration, we use physically guided proxy cues such as depth, reflection, and material to complement our main anti-spoofing (a.k.a liveness detection) task, with the intuition that genuine faces across domains have consistent face-like geometry, minimal reflection, and skin material. We introduce a novel uncertainty-aware attention scheme that independently learns to weigh the relative contributions of the main and proxy tasks, preventing the over-confident issue with traditional attention modules. Further, we propose attribute-assisted hard negative mining to disentangle liveness-irrelevant features with liveness features during learning. We evaluate extensively on public benchmarks with intra-dataset and inter-dataset protocols. Our method achieves the superior performance especially in unseen domain generalization for FAS.",0
"This paper presents a novel approach to face anti-spoofing that utilizes uncertainty-aware physically-guided proxy tasks (UAPGPT) to improve performance in unseen domains. Traditional face anti-spoofing methods rely on supervised learning techniques which require large amounts of labeled data from each domain, making them less effective in unknown environments. UAPGPT addresses this issue by leveraging knowledge transfer through synthetic data generation guided by physical models of faces, reducing reliance on real data while providing improved generalization across different scenarios. Experiments demonstrate the effectiveness of our method, achieving state-of-the-art results in multiple benchmark datasets under both seen and unseen conditions. Our work has significant implications for the development of robust face recognition systems that can adapt to diverse settings without compromising accuracy.",1
"Text segmentation is a prerequisite in many real-world text-related tasks, e.g., text style transfer, and scene text removal. However, facing the lack of high-quality datasets and dedicated investigations, this critical prerequisite has been left as an assumption in many works, and has been largely overlooked by current research. To bridge this gap, we proposed TextSeg, a large-scale fine-annotated text dataset with six types of annotations: word- and character-wise bounding polygons, masks and transcriptions. We also introduce Text Refinement Network (TexRNet), a novel text segmentation approach that adapts to the unique properties of text, e.g. non-convex boundary, diverse texture, etc., which often impose burdens on traditional segmentation models. In our TexRNet, we propose text specific network designs to address such challenges, including key features pooling and attention-based similarity checking. We also introduce trimap and discriminator losses that show significant improvement on text segmentation. Extensive experiments are carried out on both our TextSeg dataset and other existing datasets. We demonstrate that TexRNet consistently improves text segmentation performance by nearly 2% compared to other state-of-the-art segmentation methods. Our dataset and code will be made available at https://github.com/SHI-Labs/Rethinking-Text-Segmentation.",0
"Title: ""Revolutionizing Text Segmentation: Introducing A Highly Diversified Annotated Dataset and Refined Techniques""  Text segmentation has been an integral part of natural language processing (NLP) tasks such as text classification, machine translation, summarization, etc. Despite significant advancements in NLP, current text segmentation approaches still have limitations that hinder their performance on diverse real-world data sets. To address these challenges, we present a novel approach based on a highly diversified annotated dataset and refinement techniques tailored specifically for different types of texts. This comprehensive methodology enhances traditional text segmentation models by improving both precision and recall. Our extensive experiments demonstrate the effectiveness of our proposed approach over state-of-the-art baselines across several benchmark datasets. Additionally, our dataset can serve as a valuable resource for future research in NLP. By introducing a more robust model capable of handling complexities inherent in diverse texts, we aim to revolutionize text segmentation practices, ultimately leading to better performance in downstream NLP applications.",1
"Existing image generator networks rely heavily on spatial convolutions and, optionally, self-attention blocks in order to gradually synthesize images in a coarse-to-fine manner. Here, we present a new architecture for image generators, where the color value at each pixel is computed independently given the value of a random latent vector and the coordinate of that pixel. No spatial convolutions or similar operations that propagate information across pixels are involved during the synthesis. We analyze the modeling capabilities of such generators when trained in an adversarial fashion, and observe the new generators to achieve similar generation quality to state-of-the-art convolutional generators. We also investigate several interesting properties unique to the new architecture.",0
"Our approach is able to learn highly diverse image generators by conditioning on each training example separately. This allows us to generate images that can change arbitrarily fast (up to the Nyquist rate), which is important as real scenes evolve faster than most models currently allow. This speed was obtained using very efficient pixel synthesis procedures. By doing away with time-consuming feature engineering and making use of novel lightweight attention blocks we were able to train our model at extremely high resolutions (up to 2048^2). Additionally, our work is notably more memory efficient compared to other state-of-the art approaches while still achieving competitive performance. We believe that these advancements open up new possibilities for generating visually plausible and temporally coherent video frames. As future work, there are many possible extensions of this model architecture to make it even better at capturing temporal structure such as explicitly modelling objects and their motion patterns, reasoning about scene dynamics in a physically accurate manner, and learning from uncurated internet videos without relying heavily on additional annotations.",1
"Many applications for the automated diagnosis of plant disease have been developed based on the success of deep learning techniques. However, these applications often suffer from overfitting, and the diagnostic performance is drastically decreased when used on test datasets from new environments. In this paper, we propose LeafGAN, a novel image-to-image translation system with own attention mechanism. LeafGAN generates a wide variety of diseased images via transformation from healthy images, as a data augmentation tool for improving the performance of plant disease diagnosis. Thanks to its own attention mechanism, our model can transform only relevant areas from images with a variety of backgrounds, thus enriching the versatility of the training images. Experiments with five-class cucumber disease classification show that data augmentation with vanilla CycleGAN cannot help to improve the generalization, i.e., disease diagnostic performance increased by only 0.7% from the baseline. In contrast, LeafGAN boosted the diagnostic performance by 7.4%. We also visually confirmed the generated images by our LeafGAN were much better quality and more convincing than those generated by vanilla CycleGAN. The code is available publicly at: https://github.com/IyatomiLab/LeafGAN.",0
"In recent years, deep learning techniques have been increasingly used to develop accurate models for plant disease diagnosis. However, training such models can be challenging due to limited labeled data availability, which often leads to poor generalization performance on new datasets. To address this issue, we propose an effective data augmentation method called LeafGAN that leverages generative adversarial networks (GANs) to generate realistic synthetic images from existing datasets. These synthetic images, along with their corresponding labels, provide additional training data to enhance model accuracy without compromising the integrity of original data. Our experimental results show significant improvements in F1 score for both leaf diseases classification and infestation severity estimation tasks using our proposed approach compared to state-of-the-art methods. The effectiveness of LeafGAN highlights its potential as a practical solution for improving plant disease diagnosis models in agriculture and other related fields.",1
"Distributed inference/estimation in Bayesian framework in the context of sensor networks has recently received much attention due to its broad applicability. The variational Bayesian (VB) algorithm is a technique for approximating intractable integrals arising in Bayesian inference. In this paper, we propose two novel distributed VB algorithms for general Bayesian inference problem, which can be applied to a very general class of conjugate-exponential models. In the first approach, the global natural parameters at each node are optimized using a stochastic natural gradient that utilizes the Riemannian geometry of the approximation space, followed by an information diffusion step for cooperation with the neighbors. In the second method, a constrained optimization formulation for distributed estimation is established in natural parameter space and solved by alternating direction method of multipliers (ADMM). An application of the distributed inference/estimation of a Bayesian Gaussian mixture model is then presented, to evaluate the effectiveness of the proposed algorithms. Simulations on both synthetic and real datasets demonstrate that the proposed algorithms have excellent performance, which are almost as good as the corresponding centralized VB algorithm relying on all data available in a fusion center.",0
"In recent years, there has been increasing interest in developing distributed algorithms that can learn from large amounts of data collected by sensor networks. These algorithms need to operate on heterogeneous devices with limited computing resources, communication bandwidth, and energy constraints. One popular approach to solving these challenges is through variational Bayesian methods, which have shown promising results in distributed learning tasks. This paper presents a new framework for distributed variational Bayesian inference over wireless sensor networks, focusing on both asynchronous and synchronous settings. We develop a novel message passing scheme based on stochastic gradient descent (SGD) and Kalman filtering, allowing nodes to efficiently exchange local parameter estimates while accounting for network uncertainties and communication noise. Our proposed algorithm achieves state-of-the-art performance in various benchmark experiments across different application domains, including sensor localization, environmental monitoring, and target tracking. Additionally, we provide a detailed analysis of convergence rates, error bounds, and communication overhead, highlighting the benefits of our method compared to existing alternatives. Finally, we discuss potential extensions and future research directions in the area of distributed machine learning over sensor networks.",1
"In the recent past, neural architecture search (NAS) has attracted increasing attention from both academia and industries. Despite the steady stream of impressive empirical results, most existing NAS algorithms are computationally prohibitive to execute due to the costly iterations of stochastic gradient descent (SGD) training. In this work, we propose an effective alternative, dubbed Random-Weight Evaluation (RWE), to rapidly estimate the performance of network architectures. By just training the last linear classification layer, RWE reduces the computational cost of evaluating an architecture from hours to seconds. When integrated within an evolutionary multi-objective algorithm, RWE obtains a set of efficient architectures with state-of-the-art performance on CIFAR-10 with less than two hours' searching on a single GPU card. Ablation studies on rank-order correlations and transfer learning experiments to ImageNet have further validated the effectiveness of RWE.",0
"In order to optimize deep neural architectures at large scale, we need methods that can find high-performing models rapidly while still converging towards solutions that generalize well across diverse tasks. While there have been several attempts at addressing these challenges using approaches such as evolutionary algorithms and reinforcement learning, current state-of-the art results come from training large numbers of random networks followed by a selection procedure that chooses only the highest performing candidates. These methods suffer from significant drawbacks like computational overhead due to retraining many networks, lack of regularization leading to overfitting problems, and the requirement of domain knowledge to choose hyperparameters. Here, our work proposes a novel methodology that substantially reduces required training time by identifying good architectures using only their untrained outputs rather than waiting until they finish training. We then fine-tune those identified architectures on the target task. Our approach takes advantage of insights from recent advances in understanding architecture transferability, thus requiring minimal additional computational resources once good architectures are identified. We demonstrate on six representative datasets that our method outperforms competitive baselines including AutoML-Zero and random search across several metrics for model quality. Specifically, we achieve higher accuracy within fixed computation budgets compared to existing NAS techniques, making our technique suitable for larger experiments where scaling is critical. Moreover, our method often finds architectures better suited for smaller computation budgets (e.g., mobile devices). Finally, we analyze how different design choices affect performance and identify tradeoffs between them. We hope this work encourages further development along these lines toward achieving breakthroughs in automatic machine learning research. This study provides a stepping stone toward automating more complex machine learning pipelines beyond mere architecture search and could lead to greater accessibility of advanced artificial intelligence tools and services in industry and academia alike.",1
"Reinforcement learning has recently experienced increased prominence in the machine learning community. There are many approaches to solving reinforcement learning problems with new techniques developed constantly. When solving problems using reinforcement learning, there are various difficult challenges to overcome. To ensure progress in the field, benchmarks are important for testing new algorithms and comparing with other approaches. The reproducibility of results for fair comparison is therefore vital in ensuring that improvements are accurately judged. This paper provides an overview of different contributions to reinforcement learning benchmarking and discusses how they can assist researchers to address the challenges facing reinforcement learning. The contributions discussed are the most used and recent in the literature. The paper discusses the contributions in terms of implementation, tasks and provided algorithm implementations with benchmarks. The survey aims to bring attention to the wide range of reinforcement learning benchmarking tasks available and to encourage research to take place in a standardised manner. Additionally, this survey acts as an overview for researchers not familiar with the different tasks that can be used to develop and test new reinforcement learning algorithms.",0
"Reinforcement learning (RL) has become increasingly popular as a method to optimize complex systems. Benchmarking RL algorithms has proven crucial for the development of new approaches and for comparing the performance of different methods on specific tasks. In this paper we provide a comprehensive review of state-of-the-art RL benchmarking frameworks that enable researchers to easily compare their models against other algorithms on common evaluation metrics. We discuss the key features of these platforms such as standardized environments, evaluation protocols, availability of test cases, ease of use, scalability and interoperability. We highlight notable applications of each framework across diverse domains including robotics, game playing, healthcare and finance. Our study concludes by identifying challenges faced by current benchmarking practices and outlining future directions towards developing more reliable, efficient and flexible evaluation techniques for RL.",1
"Attributed networks nowadays are ubiquitous in a myriad of high-impact applications, such as social network analysis, financial fraud detection, and drug discovery. As a central analytical task on attributed networks, node classification has received much attention in the research community. In real-world attributed networks, a large portion of node classes only contain limited labeled instances, rendering a long-tail node class distribution. Existing node classification algorithms are unequipped to handle the \textit{few-shot} node classes. As a remedy, few-shot learning has attracted a surge of attention in the research community. Yet, few-shot node classification remains a challenging problem as we need to address the following questions: (i) How to extract meta-knowledge from an attributed network for few-shot node classification? (ii) How to identify the informativeness of each labeled instance for building a robust and effective model? To answer these questions, in this paper, we propose a graph meta-learning framework -- Graph Prototypical Networks (GPN). By constructing a pool of semi-supervised node classification tasks to mimic the real test environment, GPN is able to perform \textit{meta-learning} on an attributed network and derive a highly generalizable model for handling the target classification task. Extensive experiments demonstrate the superior capability of GPN in few-shot node classification.",0
"In recent years, few-shot learning has emerged as one of the most promising approaches for efficient knowledge transfer from large labeled datasets to novel tasks. This technique enables machine learning algorithms to learn quickly from only a handful of examples per task, making it well suited for real world applications where data collection can be expensive and time consuming. One particular variant of few-shot learning, called attributed network embedding (ANE), leverages graph neural networks to embed nodes within attributed graphs into low dimensional spaces while preserving their structural properties. However, state-of-the art ANE methods still suffer from two limitations: i) they assume that all edges have equal importance in capturing node similarities; ii) they ignore the hierarchical structure present in attributed networks. To address these issues, we propose Graph Prototype Networks (GPN), a novel approach for few-shot learning on attributed networks. GPN introduces a prototype graph containing representative nodes which are used as references to guide the generation of new embeddings for unseen nodes during few-shot learning. We design a hierarchy of attention modules that adaptively weights edge connections based on the similarity between source nodes and prototypes, allowing our model to focus on more important relationships. Experimental results across multiple benchmark datasets demonstrate that GPN outperforms existing state-of-art methods by significant margins in terms of clustering performance. Our ablation studies further verify the effectiveness of key components proposed in GPN, including hierarchical attention and prototype selection. Overall, our work advances the state-of-art in few-shot learning on attributed networks, opening up exciting possibilities f",1
"In this paper, we propose a computational efficient end-to-end training deep neural network (CEDNN) model and spatial attention maps based on difference images. Firstly, the difference image is generated by image processing. Then five binary images of difference images are obtained using different thresholds, which are used as spatial attention maps. We use group convolution to reduce model complexity. Skip connection and $\text{1}\times \text{1}$ convolution are used to ensure good performance even if the network model is not deep. As an input, spatial attention map can be selectively fed into the input of each block. The feature maps tend to focus on the parts that are related to the target task better. In addition, we only need to adjust the parameters of classifier to train different numbers of AU. It can be easily extended to varying datasets without increasing too much computation. A large number of experimental results show that the proposed CEDNN is obviously better than the traditional deep learning method on DISFA+ and CK+ datasets. After adding spatial attention maps, the result is better than the most advanced AU detection method. At the same time, the scale of the network is small, the running speed is fast, and the requirement for experimental equipment is low.",0
"Facial expression recognition has become increasingly important as a tool for measuring emotions, analyzing human behavior, and improving technology such as virtual assistants and chatbots. Deep Neural Networks (DNNs) have proven to be effective methods for detecting facial expressions but require significant computational resources which can limit their use in real world applications. In our paper we propose a novel method using Difference Attention Maps (DAMs) that significantly reduces computation time while maintaining high accuracy compared to existing approaches. We demonstrate the efficacy of our approach on two benchmark datasets; FACS Spontaneous Expression dataset and DISFA database. Our results show an average decrease of 67% in computation time without sacrifice in accuracy compared to state of the art techniques such as Stacked Hourglass networks (SHN). Additionally, we perform ablation studies to further validate the importance of each component of our proposed model, including the impact of DAMs, feature aggregation and parameter sharing. This research provides an innovative solution for facial action unit detection and enables greater integration into real world applications where low latency and fast processing times are crucial.",1
"Net length is a key proxy metric for optimizing timing and power across various stages of a standard digital design flow. However, the bulk of net length information is not available until cell placement, and hence it is a significant challenge to explicitly consider net length optimization in design stages prior to placement, such as logic synthesis. This work addresses this challenge by proposing a graph attention network method with customization, called Net2, to estimate individual net length before cell placement. Its accuracy-oriented version Net2a achieves about 15% better accuracy than several previous works in identifying both long nets and long critical paths. Its fast version Net2f is more than 1000 times faster than placement while still outperforms previous works and other neural network techniques in terms of various accuracy metrics.",0
"In recent years, pre-placement net (PPN) length estimation has become increasingly important for designing efficient VLSI circuits that can meet performance requirements while minimizing power consumption and area overhead. To address this challenge, we propose a novel graph attention network method called ""Net2"" customized for PNN estimation. This approach leverages the strengths of graph neural networks by encoding the interconnect graph structure into the model architecture, allowing better handling of complex system interactions. Our contributions include developing a new algorithmic framework tailored specifically towards the needs of PPN length prediction, as well as evaluating our methods on real-world industrial benchmarks to demonstrate their effectiveness. Compared against several state-of-the-art baseline approaches, our results show that Net2 outperforms existing techniques across multiple metrics, including accuracy, precision, recall, F1 score, and mean average percent error. These findings have significant implications for future research directions in advanced circuit optimization and design automation. Ultimately, the development of high-quality PPN estimators like Net2 could significantly improve the efficiency of modern electronic systems without sacrificing cost or scalability.",1
"Brain aging is a widely studied longitudinal process throughout which the brain undergoes considerable morphological changes and various machine learning approaches have been proposed to analyze it. Within this context, brain age prediction from structural MR images and age-specific brain morphology template generation are two problems that have attracted much attention. While most approaches tackle these tasks independently, we assume that they are inverse directions of the same functional bidirectional relationship between a brain's morphology and an age variable. In this paper, we propose to model this relationship with a single conditional normalizing flow, which unifies brain age prediction and age-conditioned generative modeling in a novel way. In an initial evaluation of this idea, we show that our normalizing flow brain aging model can accurately predict brain age while also being able to generate age-specific brain morphology templates that realistically represent the typical aging trend in a healthy population. This work is a step towards unified modeling of functional relationships between 3D brain morphology and clinical variables of interest with powerful normalizing flows.",0
"We present a methodology for studying brain aging using normalizing flows, a type of generative model that maps data through invertible transformations. Our approach uses bidirectional modeling, where we first train a forward model that generates novel inputs from randomly initialized latent variables, and then train a backward model that reconstructs the original input data given the generated latent variables as well as additional constraints such as biological plausibility and prior knowledge. This allows us to quantify the uncertainty and variability associated with different aspects of the model, which can provide insights into the nature of brain degeneration. In particular, our results suggest that brain aging involves both systematic changes across multiple regions as well as region-specific degradation, consistent with findings from human studies. These insights have implications for understanding neurodegenerative disorders, including Alzheimerâ€™s disease and Parkinsonâ€™s disease.",1
"Over the last several years, research on facial recognition based on Deep Neural Network has evolved with approaches like task-specific loss functions, image normalization and augmentation, network architectures, etc. However, there have been few approaches with attention to how human faces differ from person to person. Premising that inter-personal differences are found both generally and locally on the human face, I propose FusiformNet, a novel framework for feature extraction that leverages the nature of discriminative facial features. Tested on Image-Unrestricted setting of Labeled Faces in the Wild benchmark, this method achieved a state-of-the-art accuracy of 96.67% without labeled outside data, image augmentation, normalization, or special loss functions. Likewise, the method also performed on a par with previous state-of-the-arts when pre-trained on CASIA-WebFace dataset. Considering its ability to extract both general and local facial features, the utility of FusiformNet may not be limited to facial recognition but also extend to other DNN-based tasks.",0
"This abstract describes a method for facial feature extraction that uses deep learning techniques to identify features at different levels of granularity, from holistically recognizable faces down to fine-grained details such as eyes, nose, and mouth. The approach involves training a convolutional neural network (CNN) architecture called FusiformNet, which combines global and local representations of faces to improve accuracy. Experimental results show that FusiformNet outperforms other state-of-the-art methods across multiple benchmark datasets for face recognition, verification, and clustering tasks. The proposed method has important applications in areas such as security, surveillance, and biometrics. Overall, this work demonstrates the effectiveness of using CNN architectures like FusiformNet for extracting discriminative facial features at multiple scales.",1
"Temperature difference-induced mist adhered to the glass, such as windshield, camera lens, is often inhomogeneous and obscure, easily obstructing the vision and severely degrading the image. Together with adherent raindrops, they bring considerable challenges to various vision systems but without enough attention. Recent methods for other similar problems typically use hand-crafted priors to generate spatial attention maps. In this work, we newly present a problem of image degradation caused by adherent mist and raindrops. An attentive convolutional network is adopted to visually remove the adherent mist and raindrop from a single image. A baseline architecture with general channel-wise attention, spatial attention, and multilevel feature fusion is used. Considering the variations and regional characteristics of adherent mist and raindrops, we apply interpolation-based pyramid-attention blocks to perceive spatial information at different scales. Experiments show that the proposed method can improve severely degraded images' visibility, both qualitatively and quantitatively. More applied experiments show that this underrated practical problem is critical to high-level vision scenes. Our method also achieves state-of-the-art performance on conventional dehazing and pure de-raindrop problems, in addition to our task of handling adherent mist and raindrops.",0
"Removing rain from images can make them more visually appealing and easier to use in a variety of applications such as self driving cars, autonomous drones, surveillance cameras, traffic monitoring systems, augmented reality (AR) devices, computer vision algorithms, satellite imagery analysis, robotics, photogrammetry, medical imaging, remote sensing, and environmental studies. However, removing mist and raindrops from photos manually requires time consuming work, while existing automatic methods have drawbacks like producing blurred regions instead of sharp details, losing image contrast, reducing color vividness, introducing artifacts, and only addressing one type of weather phenomenon at a time. In this study, we propose a novel attentive convolutional neural network architecture that effectively removes both adherent mist and raindrop streaks simultaneously from single input photographs without loss of important information. Our method uses attention modules in each layer to highlight critical features, making our model capable of handling challenging cases where other models may fail. The proposed algorithm runs significantly faster than real-time on modern GPUs while achieving state-of-the-art results compared to 24 previous competitors and outperforming all but two established approaches by large margins. We demonstrate the effectiveness of our system through extensive experiments evaluating quantitatively key aspects and qualitatively comparing outputs against other techniques using several publicly available benchmark datasets. To encourage reproducibility and future research on this topic, we open source our code along with project webpage providing detailed explanations and examples. The main contributions of our work include developing an efficient solution for simultaneous adherent mist and raindrop removal, conducting comprehensive experimental evalua",1
"Video understanding has received more attention in the past few years due to the availability of several large-scale video datasets. However, annotating large-scale video datasets are cost-intensive. In this work, we propose a time-efficient video annotation method using spatio-temporal feature similarity and t-SNE dimensionality reduction to speed up the annotation process massively. Placing the same actions from different videos near each other in the two-dimensional space based on feature similarity helps the annotator to group-label video clips. We evaluate our method on two subsets of the ActivityNet (v1.3) and a subset of the Sports-1M dataset. We show that t-EVA can outperform other video annotation tools while maintaining test accuracy on video classification.",0
"T-eva is short for ""time efficient video annotation"". This system presents an algorithmic technique to speed up time-consuming tasks that involve sorting large amounts of data by hand - such as classifying videos. Instead of having humans manually sort through all the footage, they can use this tool to automatically assign labels based on their content - thus saving them countless hours of laborious work! This method utilizes a state-of-the art machine learning model called (t)-Distributed Stochastic Neighbor Embedding (t-SNE) which takes into account spatial information from videos, along with other relevant metrics, to make accurate judgements on which category each clip should belong to. In summary, using t-eva ensures faster turnaround times without sacrificing accuracy when completing daunting data organization projects. Who knew there was a smart solution out there to alleviate such a tedious task? Give this innovative technology a try!",1
"Pulmonary diseases impact millions of lives globally and annually. The recent outbreak of the pandemic of the COVID-19, a novel pulmonary infection, has more than ever brought the attention of the research community to the machine-aided diagnosis of respiratory problems. This paper is thus an effort to exploit machine learning for classification of respiratory problems and proposes a framework that employs as much correlated information (auditory and demographic information in this work) as a dataset provides to increase the sensitivity and specificity of a diagnosing system. First, we use deep convolutional neural networks (DCNNs) to process and classify a publicly released pulmonary auditory dataset, and then we take advantage of the existing demographic information within the dataset and show that the accuracy of the pulmonary classification increases by 5% when trained on the auditory information in conjunction with the demographic information. Since the demographic data can be extracted using computer vision, we suggest using another parallel DCNN to estimate the demographic information of the subject under test visioned by the processing computer. Lastly, as a proposition to bring the healthcare system to users' fingertips, we measure deployment characteristics of the auditory DCNN model onto processing components of an NVIDIA TX2 development board.",0
"Abstract: Recent advances in artificial intelligence have led to the development of deep learning models that can automatically diagnose pulmonary diseases from audio recordings and demographic data. This work presents a neural network architecture capable of accurately classifying respiratory sounds and associated patient details into one of seven different categories of lung disease. Using a dataset consisting of over 900 audio clips collected from real patients along with their corresponding medical histories, we train our model on a wide range of features including age, gender, height, weight, smoking history, occupation, and respiration rate, in addition to raw waveform data extracted from each recording. Our experiments demonstrate that by incorporating both auditory cues and demographics, our proposed method achieves higher accuracy than state-of-the-art systems trained exclusively on acoustic features. With further refinement, such a system has the potential to assist healthcare professionals in making more accurate diagnoses in a timely manner. Keywords: Respiratory sound analysis; Deep learning; Lung disease diagnostics; Audio processing; Medical decision support",1
"We propose a continuous neural network architecture, termed Explainable Tensorized Neural Ordinary Differential Equations (ETN-ODE), for multi-step time series prediction at arbitrary time points. Unlike the existing approaches, which mainly handle univariate time series for multi-step prediction or multivariate time series for single-step prediction, ETN-ODE could model multivariate time series for arbitrary-step prediction. In addition, it enjoys a tandem attention, w.r.t. temporal attention and variable attention, being able to provide explainable insights into the data. Specifically, ETN-ODE combines an explainable Tensorized Gated Recurrent Unit (Tensorized GRU or TGRU) with Ordinary Differential Equations (ODE). The derivative of the latent states is parameterized with a neural network. This continuous-time ODE network enables a multi-step prediction at arbitrary time points. We quantitatively and qualitatively demonstrate the effectiveness and the interpretability of ETN-ODE on five different multi-step prediction tasks and one arbitrary-step prediction task. Extensive experiments show that ETN-ODE can lead to accurate predictions at arbitrary time points while attaining best performance against the baseline methods in standard multi-step time series prediction.",0
"This paper presents a new method for time series prediction that combines the power of tensorization with neural ordinary differential equations (ODEs). We propose using arbitrary step sizes in these ODEs to allow greater flexibility in modeling complex systems. Our approach leverages the advantages of both linear regression and deep learning models while addressing their respective shortcomings. By tensorizing the ODE solution, we can capture important patterns in the data and achieve scalability. Experimental results on several real-world datasets demonstrate the effectiveness of our method in producing accurate predictions compared to state-of-the-art methods. Overall, our proposed method represents a promising direction for future research in time series forecasting.",1
"Despite the remarkable advances in visual saliency analysis for natural scene images (NSIs), salient object detection (SOD) for optical remote sensing images (RSIs) still remains an open and challenging problem. In this paper, we propose an end-to-end Dense Attention Fluid Network (DAFNet) for SOD in optical RSIs. A Global Context-aware Attention (GCA) module is proposed to adaptively capture long-range semantic context relationships, and is further embedded in a Dense Attention Fluid (DAF) structure that enables shallow attention cues flow into deep layers to guide the generation of high-level feature attention maps. Specifically, the GCA module is composed of two key components, where the global feature aggregation module achieves mutual reinforcement of salient feature embeddings from any two spatial locations, and the cascaded pyramid attention module tackles the scale variation issue by building up a cascaded pyramid framework to progressively refine the attention map in a coarse-to-fine manner. In addition, we construct a new and challenging optical RSI dataset for SOD that contains 2,000 images with pixel-wise saliency annotations, which is currently the largest publicly available benchmark. Extensive experiments demonstrate that our proposed DAFNet significantly outperforms the existing state-of-the-art SOD competitors. https://github.com/rmcong/DAFNet_TIP20",0
"In recent years, salient object detection has become increasingly important in the field of optical remote sensing images (ORSI). With the growing volume of satellite imagery and high resolution aerial imagery available today, there is a need for efficient algorithms that can accurately identify and isolate objects of interest from complex backgrounds. This research presents a novel methodology based on dense attention fluid networks (DAFN) to address this challenge.  Our approach utilizes a UNet architecture enhanced by a densely connected network of fluid convolutional layers that capture spatial dependencies between features. By applying channel attention modules within these fluid blocks, we are able to dynamically adjust their importance during feature learning, allowing the model to focus on relevant regions while suppressing noise. The resulting DAFN framework outperforms existing state-of-the-art methods in ORSI scenarios with significant improvements in precision and recall metrics.  The effectiveness of our proposed method is further validated through comprehensive experiments on two popular datasets: ShanghaiTech Part B and RSOD dataset. Quantitative analysis demonstrates the superior performance achieved using DAFN compared to other contemporary techniques. Additionally, visual assessments illustrate the ability of our algorithm to accurately localize objects and provide insight into key factors contributing to its success.  In summary, this paper presents a novel technique for salient object detection in ORSI using a dense attention fluid network. Our approach achieves improved results compared to current methods and holds promise for future applications in computer vision and image processing tasks.",1
"Multi-view action recognition (MVAR) leverages complementary temporal information from different views to improve the learning performance. Obtaining informative view-specific representation plays an essential role in MVAR. Attention has been widely adopted as an effective strategy for discovering discriminative cues underlying temporal data. However, most existing MVAR methods only utilize attention to extract representation for each view individually, ignoring the potential to dig latent patterns based on mutual-support information in attention space. To this end, we propose a collaborative attention mechanism (CAM) for solving the MVAR problem in this paper. The proposed CAM detects the attention differences among multi-view, and adaptively integrates frame-level information to benefit each other. Specifically, we extend the long short-term memory (LSTM) to a Mutual-Aid RNN (MAR) to achieve the multi-view collaboration process. CAM takes advantages of view-specific attention pattern to guide another view and discover potential information which is hard to be explored by itself. It paves a novel way to leverage attention information and enhances the multi-view representation learning. Extensive experiments on four action datasets illustrate the proposed CAM achieves better results for each view and also boosts multi-view performance.",0
"Title: Collaborative Attention Mechanism for Multi-View Action Recognition  In recent years, action recognition has become increasingly important due to its numerous applications such as surveillance, human computer interaction, and robotics. Traditional approaches rely on handcrafted features, but they often fail to capture complex motion patterns effectively. Deep learning methods have shown great promise by automatically extracting high-level representations from raw image data. However, current deep models typically operate on single-view input, overlooking valuable information provided by multiple views. In order to address these limitations, we propose a collaborative attention mechanism that can exploit multi-view input for improved action recognition performance. Our approach is built upon convolutional neural networks (CNNs), which have achieved remarkable results in many visual tasks.  Our proposed method involves two key components. First, we introduce a view selection module, which selects informative views based on their relevance to the action class. This reduces redundancy in the input representation while maintaining essential cues that facilitate better understanding of the action class. Second, we employ a spatial transformer network, which learns to focus on discriminative regions across different views to maximize inter-view collaboration. The resulting multi-view feature representation captures complementary knowledge from distinct angles, thereby enhancing overall performance. Extensive experiments conducted on challenging benchmark datasets demonstrate that our model outperforms several state-of-the-art techniques while achieving higher efficiency. Additionally, ablation studies verify the effectiveness of each component in our proposed architecture. These findings confirm the benefits of utilizing multiple views to improve action recognition accuracy. Overall, our work paves the way for future research into incorporating multi-modal sensory inputs, thus enabling more robust real-world applicability.",1
"As an innovative solution for privacy-preserving machine learning (ML), federated learning (FL) is attracting much attention from research and industry areas. While new technologies proposed in the past few years do evolve the FL area, unfortunately, the evaluation results presented in these works fall short in integrity and are hardly comparable because of the inconsistent evaluation metrics and the lack of a common platform. In this paper, we propose a comprehensive evaluation framework for FL systems. Specifically, we first introduce the ACTPR model, which defines five metrics that cannot be excluded in FL evaluation, including Accuracy, Communication, Time efficiency, Privacy, and Robustness. Then we design and implement a benchmarking system called FedEval, which enables the systematic evaluation and comparison of existing works under consistent experimental conditions. We then provide an in-depth benchmarking study between the two most widely-used FL mechanisms, FedSGD and FedAvg. The benchmarking results show that FedSGD and FedAvg both have advantages and disadvantages under the ACTPR model. For example, FedSGD is barely influenced by the none independent and identically distributed (non-IID) data problem, but FedAvg suffers from a decline in accuracy of up to 9% in our experiments. On the other hand, FedAvg is more efficient than FedSGD regarding time consumption and communication. Lastly, we excavate a set of take-away conclusions, which are very helpful for researchers in the FL area.",0
"Abstract Federated learning has recently emerged as a promising technique that enables training models on distributed data sources while preserving user privacy by keeping data local. While there have been several attempts to create benchmarks systems and evaluate federated learning algorithms, these evaluations often focus narrowly on specific aspects of federated learning without considering broader evaluation criteria. To address these limitations, we propose FedEval: a comprehensive evaluation framework consisting of three components - the system design, the benchmark dataset and workloads, and the performance metrics and model selection criteria. Our evaluation scheme considers both efficiency and accuracy measures that can guide practitioners in choosing appropriate federated learning approaches based on their requirements. We apply our methodology to a large set of existing federated learning techniques and demonstrate its effectiveness through extensive experiments. Our results highlight strengths and weaknesses of different methods and provide insights into future research directions. Overall, FedEval provides a valuable resource for advancing the state-of-the-art in federated learning towards real-world applications.",1
"The three-dimensional shape and conformation of small-molecule ligands are critical for biomolecular recognition, yet encoding 3D geometry has not improved ligand-based virtual screening approaches. We describe an end-to-end deep learning approach that operates directly on small-molecule conformational ensembles and identifies key conformational poses of small-molecules. Our networks leverage two levels of representation learning: 1) individual conformers are first encoded as spatial graphs using a graph neural network, and 2) sampled conformational ensembles are represented as sets using an attention mechanism to aggregate over individual instances. We demonstrate the feasibility of this approach on a simple task based on bidentate coordination of biaryl ligands, and show how attention-based pooling can elucidate key conformational poses in tasks based on molecular geometry. This work illustrates how set-based learning approaches may be further developed for small molecule-based virtual screening.",0
"In recent years, machine learning has become increasingly important for analyzing complex molecular systems. One popular approach uses ensembles of models that combine predictions from multiple methods to improve accuracy. However, existing ensemble techniques often require domain knowledge to handpick which methods should be combined, and how they should be weighted, limiting their applicability to new datasets. To address these issues, we present a novel attention mechanism that automatically learns to focus the most relevant model(s) for each input sample, without requiring any prior knowledge about the individual models. We evaluate our method on two benchmark datasets of diverse origin and chemotypes (COVID-19 proteins and DrugBank bioactive ligands), comparing against state-of-the-art model stacking and feature selection baselines. Our results demonstrate significant improvements over all competitors, including absolute performance enhancement of up to 6.4% for the best performing methods in certain cases. Aside from increased accuracy, our attention module provides unique insights into how different models contribute to ensemble outputs by revealing which fragments or features are relevant for a given input. This ability paves the path for more nuanced integration strategies that could further advance the field. Overall, our work demonstrates the potential advantages of using attentional mechanisms to selectively aggregate model behaviors and pushes forward the understanding of predictive distributions obtained via deep neural networks and their interpretations.",1
"We present an efficient multi-view stereo (MVS) network for 3D reconstruction from multiview images. While previous learning based reconstruction approaches performed quite well, most of them estimate depth maps at a fixed resolution using plane sweep volumes with a fixed depth hypothesis at each plane, which requires densely sampled planes for desired accuracy and therefore is difficult to achieve high resolution depth maps. In this paper we introduce a coarseto-fine depth inference strategy to achieve high resolution depth. This strategy estimates the depth map at coarsest level, while the depth maps at finer levels are considered as the upsampled depth map from previous level with pixel-wise depth residual. Thus, we narrow the depth searching range with priori information from previous level and construct new cost volumes from the pixel-wise depth residual to perform depth map refinement. Then the final depth map could be achieved iteratively since all the parameters are shared between different levels. At each level, the self-attention layer is introduced to the feature extraction block for capturing the long range dependencies for depth inference task, and the cost volume is generated using similarity measurement instead of the variance based methods used in previous work. Experiments were conducted on both the DTU benchmark dataset and recently released BlendedMVS dataset. The results demonstrated that our model could outperform most state-of-the-arts (SOTA) methods. The codebase of this project is at https://github.com/ArthasMil/AACVP-MVSNet.",0
This should sound like a research study that someone conducted at your company but you have no affiliation with any actual university department so please keep it academic. ---,1
"In this paper, we propose two modified neural networks based on dual path multi-scale fusion networks (SFANet) and SegNet for accurate and efficient crowd counting. Inspired by SFANet, the first model, which is named M-SFANet, is attached with atrous spatial pyramid pooling (ASPP) and context-aware module (CAN). The encoder of M-SFANet is enhanced with ASPP containing parallel atrous convolutional layers with different sampling rates and hence able to extract multi-scale features of the target object and incorporate larger context. To further deal with scale variation throughout an input image, we leverage the CAN module which adaptively encodes the scales of the contextual information. The combination yields an effective model for counting in both dense and sparse crowd scenes. Based on the SFANet decoder structure, M-SFANet's decoder has dual paths, for density map and attention map generation. The second model is called M-SegNet, which is produced by replacing the bilinear upsampling in SFANet with max unpooling that is used in SegNet. This change provides a faster model while providing competitive counting performance. Designed for high-speed surveillance applications, M-SegNet has no additional multi-scale-aware module in order to not increase the complexity. Both models are encoder-decoder based architectures and are end-to-end trainable. We conduct extensive experiments on five crowd counting datasets and one vehicle counting dataset to show that these modifications yield algorithms that could improve state-of-the-art crowd counting methods. Codes are available at https://github.com/Pongpisit-Thanasutives/Variations-of-SFANet-for-Crowd-Counting.",0
"In recent years, crowd counting has become increasingly important for many applications such as traffic management, event planning, and public safety. However, accurately estimating crowds in images remains challenging due to variations in density, perspective, illumination conditions, and occlusions. To address these issues, we propose a novel encoder-decoder based convolutional neural network (CNN) architecture that incorporates multi-scale aware modules. Our approach utilizes two branches: one focuses on detailed features using a local region while another focuses on global context by incorporating dilations at different scales. By fusing multiple resolutions into a single prediction, our method improves accuracy under diverse situations. Experimental results on three benchmark datasets demonstrate significant improvement over state-of-the-art methods. These findings highlight the effectiveness of our proposed model in handling complex tasks related to crowd counting.",1
"In the distributed collaborative machine learning (DCML) paradigm, federated learning (FL) recently attracted much attention due to its applications in health, finance, and the latest innovations such as industry 4.0 and smart vehicles. FL provides privacy-by-design. It trains a machine learning model collaboratively over several distributed clients (ranging from two to millions) such as mobile phones, without sharing their raw data with any other participant. In practical scenarios, all clients do not have sufficient computing resources (e.g., Internet of Things), the machine learning model has millions of parameters, and its privacy between the server and the clients while training/testing is a prime concern (e.g., rival parties). In this regard, FL is not sufficient, so split learning (SL) is introduced. SL is reliable in these scenarios as it splits a model into multiple portions, distributes them among clients and server, and trains/tests their respective model portions to accomplish the full model training/testing. In SL, the participants do not share both data and their model portions to any other parties, and usually, a smaller network portion is assigned to the clients where data resides. Recently, a hybrid of FL and SL, called splitfed learning, is introduced to elevate the benefits of both FL (faster training/testing time) and SL (model split and training). Following the developments from FL to SL, and considering the importance of SL, this chapter is designed to provide extensive coverage in SL and its variants. The coverage includes fundamentals, existing findings, integration with privacy measures such as differential privacy, open problems, and code implementation.",0
"Federated Learning (FL) has emerged as a promising approach for training machine learning models on distributed data without compromising user privacy. However, traditional FL suffers from several limitations such as high communication costs, security vulnerabilities, and insufficient model performance. In recent years, research efforts have been devoted to addressing these issues by proposing advanced techniques in the field of FL. One of them is Split Learning, which aims at further mitigating the leakage of sensitive information by introducing more layers to split each deviceâ€™s local dataset into two parts before uploading. By doing so, the server only receives partially informed parameters instead of raw datasets, substantially reducing both computational overhead and communication cost while ensuring comparable model accuracy compared to vanilla FL under various workloads. This review discusses the advancements of federated learning towards privacy preservation by evaluating FL variants including hierarchical aggregation, private activation pruning, sparse federated learning, encrypted gradient synthesis, etc., comparing their merits and drawbacks in terms of privacy protection, utility efficiency, scalability, and flexibility across different applications scenarios. Ultimately, this study provides an extensive overview of current research progress that highlights future research directions within the scope of FLâ€™s development.",1
"Sequential assembly with geometric primitives has drawn attention in robotics and 3D vision since it yields a practical blueprint to construct a target shape. However, due to its combinatorial property, a greedy method falls short of generating a sequence of volumetric primitives. To alleviate this consequence induced by a huge number of feasible combinations, we propose a combinatorial 3D shape generation framework. The proposed framework reflects an important aspect of human generation processes in real life -- we often create a 3D shape by sequentially assembling unit primitives with geometric constraints. To find the desired combination regarding combination evaluations, we adopt Bayesian optimization, which is able to exploit and explore efficiently the feasible regions constrained by the current primitive placements. An evaluation function conveys global structure guidance for an assembly process and stability in terms of gravity and external forces simultaneously. Experimental results demonstrate that our method successfully generates combinatorial 3D shapes and simulates more realistic generation processes. We also introduce a new dataset for combinatorial 3D shape generation. All the codes are available at \url{https://github.com/POSTECH-CVLab/Combinatorial-3D-Shape-Generation}.",0
"Here is a sample text which you can use as an example:  <papertitle>Combinatorial 3D Shape Generation via Sequential Assembly</papertitle> <abstract>This paper introduces an algorithmic framework for generating diverse combinatorial shapes composed from simple volumetric primitives. Our approach leverages sequential assembly by incrementally attaching or removing pieces based on a user-defined template. We formulate shape generation as an optimization problem that maximizes structural similarity while minimizing interference among parts. This results in a rich set of valid assemblies that capture global features and local variations. Our method is agnostic towards specific parts libraries and can seamlessly incorporate new components, enabling exploration of unseen designs. Experiments demonstrate versatility across varied datasets (e.g., cars, chairs, robots) under different objectives (e.g., style transfer). With our open source implementation, we aim to encourage further research into automating shape design.</abstract>",1
"Recently, ocular biometrics in unconstrained environments using images obtained at visible wavelength have gained the researchers' attention, especially with images captured by mobile devices. Periocular recognition has been demonstrated to be an alternative when the iris trait is not available due to occlusions or low image resolution. However, the periocular trait does not have the high uniqueness presented in the iris trait. Thus, the use of datasets containing many subjects is essential to assess biometric systems' capacity to extract discriminating information from the periocular region. Also, to address the within-class variability caused by lighting and attributes in the periocular region, it is of paramount importance to use datasets with images of the same subject captured in distinct sessions. As the datasets available in the literature do not present all these factors, in this work, we present a new periocular dataset containing samples from 1,122 subjects, acquired in 3 sessions by 196 different mobile devices. The images were captured under unconstrained environments with just a single instruction to the participants: to place their eyes on a region of interest. We also performed an extensive benchmark with several Convolutional Neural Network (CNN) architectures and models that have been employed in state-of-the-art approaches based on Multi-class Classification, Multitask Learning, Pairwise Filters Network, and Siamese Network. The results achieved in the closed- and open-world protocol, considering the identification and verification tasks, show that this area still needs research and development.",0
"This paper presents UFPR-Periocular, a new dataset consisting of periocular images collected using mobile devices under uncontrolled scenarios. Periocular recognition has become increasingly important due to its potential use in secure authentication systems, especially in unconstrained environments where traditional biometric methods may fail. However, there is still a lack of datasets suitable for testing these systems in real-world conditions.  To address this gap, we have developed UFPR-Periocular which contains over 26,000 images from 975 subjects. These images were acquired using a custom application installed on smartphones that captures both periocular (face region plus iris) and ocular (eye region only) pictures while preserving their original resolutions. The data was gathered through different campaigns conducted at multiple locations, providing significant variability in lighting and pose conditions.  The proposed dataset includes demographic attributes such as age, gender, and ethnicity, and some samples with occlusions like glasses or scarves that cover parts of the face. Additionally, each sample carries specific labels indicating the quality of the image, making it easier for researchers to select appropriate subsets for training and evaluation purposes.  Our experiments show that the proposed dataset is diverse enough to accurately evaluate state-of-the-art algorithms for periocular recognition. Furthermore, we demonstrate the effectiveness of our approach by comparing its performance against existing benchmark datasets for this purpose. We hope that UFPR-Periocular can serve as a valuable resource for the community, fostering further advancements in periocular biometrics under real-world conditions.",1
"We introduce the SE(3)-Transformer, a variant of the self-attention module for 3D point clouds and graphs, which is equivariant under continuous 3D roto-translations. Equivariance is important to ensure stable and predictable performance in the presence of nuisance transformations of the data input. A positive corollary of equivariance is increased weight-tying within the model. The SE(3)-Transformer leverages the benefits of self-attention to operate on large point clouds and graphs with varying number of points, while guaranteeing SE(3)-equivariance for robustness. We evaluate our model on a toy N-body particle simulation dataset, showcasing the robustness of the predictions under rotations of the input. We further achieve competitive performance on two real-world datasets, ScanObjectNN and QM9. In all cases, our model outperforms a strong, non-equivariant attention baseline and an equivariant model without attention.",0
"This paper introduces SE(3)-Transformers, a new architecture for equivariant attention networks on 3D data that operate directly on rotations using SE(3) representations. We explore how to generalize self-attention mechanisms such as Transformers from their current limitations on Euclidean spaces to Lie groups like SO(3), which allows us to efficiently handle roto-translational symmetry in 3D data without suffering from grid ambiguities or numerical instabilities caused by naive backpropagation through angles. Our experiments demonstrate significant improvements over baseline methods across several challenging tasks including shape completion, multi-view reconstruction, and mesh segmentation. To facilitate further research, we make our code publicly available at <https://github.com/AUTOMATICDeep Learning/SE_3_Attention>.",1
"At online retail platforms, it is crucial to actively detect risks of fraudulent transactions to improve our customer experience, minimize loss, and prevent unauthorized chargebacks. Traditional rule-based methods and simple feature-based models are either inefficient or brittle and uninterpretable. The graph structure that exists among the heterogeneous typed entities of the transaction logs is informative and difficult to fake. To utilize the heterogeneous graph relationships and enrich the explainability, we present xFraud, an explainable Fraud transaction prediction system. xFraud is composed of a predictor which learns expressive representations for malicious transaction detection from the heterogeneous transaction graph via a self-attentive heterogeneous graph neural network, and an explainer that generates meaningful and human understandable explanations from graphs to facilitate further process in business unit. In our experiments with xFraud on two real transaction networks with up to ten millions transactions, we are able to achieve an area under a curve (AUC) score that outperforms baseline models and graph embedding methods. In addition, we show how the explainer could benefit the understanding towards model predictions and enhance model trustworthiness for real-world fraud transaction cases.",0
"""xFraud: Explainable Fraud Transaction Detection on Heterogeneous Graphs"" presents a novel approach to detecting fraudulent transactions using machine learning models that can effectively analyze large datasets from multiple sources. This method leverages graph neural networks (GNN) to capture dependencies across different data types, such as transactional, social network, and user behavior graphs. The proposed model uses attention mechanisms to weigh the importance of each type of data in predicting fraudulent activity and outputs explanations for how decisions were made, ensuring transparency and accountability. Through extensive experiments on real-world data sets, the authors demonstrate the effectiveness of their approach in accurately identifying suspicious activities while minimizing false positives, thus making it suitable for use by financial institutions, law enforcement agencies, and other organizations looking to protect against fraudulent threats. Overall, ""xFraud"" offers a valuable toolset for addressing one of todayâ€™s most pervasive problems in cybersecurity and finance.",1
"Age estimation has attracted attention for its various medical applications. There are many studies on human age estimation from biomedical images. However, there is no research done on mammograms for age estimation, as far as we know. The purpose of this study is to devise an AI-based model for estimating age from mammogram images. Due to lack of public mammography data sets that have the age attribute, we resort to using a web crawler to download thumbnail mammographic images and their age fields from the public data set; the Digital Database for Screening Mammography. The original images in this data set unfortunately can only be retrieved by a software which is broken. Subsequently, we extracted deep learning features from the collected data set, by which we built a model using Random Forests regressor to estimate the age automatically. The performance assessment was measured using the mean absolute error values. The average error value out of 10 tests on random selection of samples was around 8 years. In this paper, we show the merits of this approach to fill up missing age values. We ran logistic and linear regression models on another independent data set to further validate the advantage of our proposed work. This paper also introduces the free-access Mini-DDSM data set.",0
"Abstract Age estimation based on mammography images has gained significant attention in recent years due to its potential applications in early breast cancer detection, risk assessment, and personalized treatment planning. Traditional methods rely heavily on manual evaluation by radiologists, which can be time-consuming, subjective, and prone to inter-observer variability. Therefore, there is a growing need for automated age estimation systems that can accurately and consistently predict patient ages from their mammograms.  In this study, we propose a novel deep learning framework called Mini-DDSM (Deep Domain Specific Models) for automatic age estimation using mammograms. Our approach leverages state-of-the-art computer vision techniques and a large dataset of annotated mammograms to learn subtle features that encode age-related characteristics in the breast tissue texture patterns. We introduce mini domain specific models (MSMs), lightweight neural networks designed specifically to capture local representations of textural features at different scales within each image region. These MSMs complement global context information learned by convolutional neural networks (CNNs). By integrating both global and local feature extraction, our proposed model achieves superior performance compared to other contemporary methods.  We evaluate our method on a publicly available benchmark dataset consisting of mammogram exams obtained from women aged 40-86 years old. Extensive experimental analysis shows that our system outperforms several recently published approaches, demonstrating high accuracy, robustness, and interpretability. Furthermore, sensitivity analysis reveals insights into the contribution of distinct mammographic features to age prediction, opening up future research directions in breast imaging informatics.  Our work highlights the promising potential of deep learning algorithms for accurate and efficient population-scale age estimation from mammograms. As healthcare providers continue to seek new ways to improve breast cancer screening and diagnostic processes, intelligent image analyti",1
"Given $m$ $d$-dimensional responsors and $n$ $d$-dimensional predictors, sparse regression finds at most $k$ predictors for each responsor for linear approximation, $1\leq k \leq d-1$. The key problem in sparse regression is subset selection, which usually suffers from high computational cost. Recent years, many improved approximate methods of subset selection have been published. However, less attention has been paid on the non-approximate method of subset selection, which is very necessary for many questions in data analysis. Here we consider sparse regression from the view of correlation, and propose the formula of conditional uncorrelation. Then an efficient non-approximate method of subset selection is proposed in which we do not need to calculate any coefficients in regression equation for candidate predictors. By the proposed method, the computational complexity is reduced from $O(\frac{1}{6}{k^3}+mk^2+mkd)$ to $O(\frac{1}{6}{k^3}+\frac{1}{2}mk^2)$ for each candidate subset in sparse regression. Because the dimension $d$ is generally the number of observations or experiments and large enough, the proposed method can greatly improve the efficiency of non-approximate subset selection.",0
"In recent years, sparse regression has emerged as a powerful tool for solving high-dimensional linear regression problems. However, many existing methods suffer from issues related to computational complexity and lack of interpretability. To address these challenges, we propose a new method that utilizes conditional uncorrelation to select non-redundant features while improving model efficiency. By conditioning on relevant subsets of covariates, our approach can achieve near-oracle performance without relying on complex approximations or heuristics. Through extensive experiments, we demonstrate the superiority of our method over state-of-the-art techniques across a range of real-world datasets and scenarios. Our findings highlight the potential of conditional uncorrelation in enabling efficient and interpretable subset selection for sparse regression models.",1
"Data-driven discovery of partial differential equations (PDEs) has attracted increasing attention in recent years. Although significant progress has been made, certain unresolved issues remain. For example, for PDEs with high-order derivatives, the performance of existing methods is unsatisfactory, especially when the data are sparse and noisy. It is also difficult to discover heterogeneous parametric PDEs where heterogeneous parameters are embedded in the partial differential operators. In this work, a new framework combining deep-learning and integral form is proposed to handle the above-mentioned problems simultaneously, and improve the accuracy and stability of PDE discovery. In the framework, a deep neural network is firstly trained with observation data to generate meta-data and calculate derivatives. Then, a unified integral form is defined, and the genetic algorithm is employed to discover the best structure. Finally, the value of parameters is calculated, and whether the parameters are constants or variables is identified. Numerical experiments proved that our proposed algorithm is more robust to noise and more accurate compared with existing methods due to the utilization of integral form. Our proposed algorithm is also able to discover PDEs with high-order derivatives or heterogeneous parameters accurately with sparse and noisy data.",0
"Abstract This paper presents a deep learning approach that can learn and infer partial differential equations (PDEs) directly from experimental data without prior knowledge on equation forms or model parameters. We construct a mathematical space embedding for PDE solutions using variational autoencoders, which captures intrinsic patterns underlying a wide range of known equations such as heat diffusion, wave propagation, and Navier-Stokes systems. Based on these embeddings, we develop two models: one for classification tasks by identifying relevant physical quantities in latent features, and another for solving inverse problems where missing derivatives are inferred solely from raw measurements through optimization algorithms utilizing cross entropy loss functions. With rigorous validation against benchmark datasets for both forward predictions and inversions over diverse scenarios involving measurement noise and incomplete boundary conditions, our methods demonstrate robust performance exceeding traditional approaches reliant on strong priors and analytic solvers for exact differentiation terms. Furthermore, we showcase applications in real-world use cases spanning plasma physics simulations and medical imaging diagnostics in magnetic resonance angiography (MRA). These promising results indicate future prospects in scientific discoveries across numerous domains by harnessing advanced techniques combining machine learning principles with complex nonlinear systems governed by PDE structures, enabling researchers to tackle previously unsolvable challenges more efficiently while fostering collaboration among interdisciplinary experts. Keywords: Partial Differential Equations; Variational Autoencoder; Machine Learning; Nonlinear Systems; Inverse Problems Introduction Solving partial differential equations (PDEs) accurately remains an essential task throughout applied mathematics",1
"Analog computing hardwares, such as Processing-in-memory (PIM) accelerators, have gradually received more attention for accelerating the neural network computations. However, PIM accelerators often suffer from intrinsic noise in the physical components, making it challenging for neural network models to achieve the same performance as on the digital hardware. Previous works in mitigating intrinsic noise assumed the knowledge of the noise model, and retraining the neural networks accordingly was required. In this paper, we propose a noise-agnostic method to achieve robust neural network performance against any noise setting. Our key observation is that the degradation of performance is due to the distribution shifts in network activations, which are caused by the noise. To properly track the shifts and calibrate the biased distributions, we propose a ""noise-aware"" batch normalization layer, which is able to align the distributions of the activations under variational noise inherent in the analog environments. Our method is simple, easy to implement, general to various noise settings, and does not need to retrain the models. We conduct experiments on several tasks in computer vision, including classification, object detection and semantic segmentation. The results demonstrate the effectiveness of our method, achieving robust performance under a wide range of noise settings, more reliable than existing methods. We believe that our simple yet general method can facilitate the adoption of analog computing devices for neural networks.",0
"This work introduces robust processing-in-memory neural networks (RIMNN) which can be used in distributed systems such as edge computing devices like smartphones or autonomous vehicles, where energy efficiency is critical but storage capacity is limited. RIMNN is able to achieve noise robustness during inference time using normalizing flows that map noisy input data to more stable representations before passing them through multiple layers of processing neurons. Experimental results show RIMNN outperforms state-of-the-art alternatives in terms of accuracy while consuming less power. In addition, we analyze the effectiveness of each component of our method via ablation studies and visualize the generated latent space. Finally, we demonstrate how RIMNN can be applied to image classification tasks on CIFAR-10 and NORB datasets, and real-time video frame prediction on a moving car sequence dataset. We believe this approach holds promise towards achieving intelligent and efficient edge computing systems in the future.",1
"The goal of face attribute editing is altering a facial image according to given target attributes such as hair color, mustache, gender, etc. It belongs to the image-to-image domain transfer problem with a set of attributes considered as a distinctive domain. There have been some works in multi-domain transfer problem focusing on facial attribute editing employing Generative Adversarial Network (GAN). These methods have reported some successes but they also result in unintended changes in facial regions - meaning the generator alters regions unrelated to the specified attributes. To address this unintended altering problem, we propose a novel GAN model which is designed to edit only the parts of a face pertinent to the target attributes by the concept of Complementary Attention Feature (CAFE). CAFE identifies the facial regions to be transformed by considering both target attributes as well as complementary attributes, which we define as those attributes absent in the input facial image. In addition, we introduce a complementary feature matching to help in training the generator for utilizing the spatial information of attributes. Effectiveness of the proposed method is demonstrated by analysis and comparison study with state-of-the-art methods.",0
"CAFE-GAN (Complementary Attention Feature Editing Generative Adversarial Networks) is a deep learning model that allows users to edit facial attributes in images using natural language commands such as ""add glasses,"" ""change hair color,"" or ""smooth wrinkles."" This model utilizes two parallel discriminators and a feature attention module which enables fine-grained attribute editing while preserving identity features. Our model is trained on large datasets and achieves state-of-the-art results in face attribute manipulation tasks. Additionally, we showcase our model on several challenging applications including transferring head pose, interpolating age, predicting facial landmarks, and generating new identities from scratch. Overall, CAFE-GAN represents a significant advancement in computer graphics, image generation, and human computer interaction.",1
"Unsupervised multi-domain image-to-image translation aims to synthesis images among multiple domains without labeled data, which is more general and complicated than one-to-one image mapping. However, existing methods mainly focus on reducing the large costs of modeling and do not pay enough attention to the quality of generated images. In some target domains, their translation results may not be expected or even it has model collapse. To improve the image quality, we propose an effective many-to-many mapping framework for unsupervised multi-domain image-to-image translation. There are two key aspects in our method. The first is a proposed many-to-many architecture with only one domain-shared encoder and several domain-specialized decoders to effectively and simultaneously translate images across multiple domains. The second is two proposed constraints extended from one-to-one mappings to further help improve the generation. All the evaluations demonstrate our framework is superior to existing methods and provides an effective solution for multi-domain image-to-image translation.",0
"Unsupervised image-to-image translation across multiple domains remains a challenging problem in computer vision. Previous approaches have relied on supervised learning techniques that require paired training data from both source and target domains. However, acquiring such high-quality annotations can be time-consuming and costly. This study presents a novel unsupervised many-to-many image-to-image translation approach that eliminates the need for paired training data across all domains while maintaining good image fidelity. Our model leverages cycle consistency loss to ensure translations between any two domains are consistent with each other. Experimental results demonstrate our method outperforms prior state-of-the-art unsupervised methods by achieving higher visual quality and better perceptual distance metrics across diverse datasets. These findings have important implications for real-world applications, including photo editing tools, art generation, medical imaging analysis, and more.",1
"Time series data is prevalent in a wide variety of real-world applications and it calls for trustworthy and explainable models for people to understand and fully trust decisions made by AI solutions. We consider the problem of building explainable classifiers from multi-variate time series data. A key criterion to understand such predictive models involves elucidating and quantifying the contribution of time varying input variables to the classification. Hence, we introduce a novel, modular, convolution-based feature extraction and attention mechanism that simultaneously identifies the variables as well as time intervals which determine the classifier output. We present results of extensive experiments with several benchmark data sets that show that the proposed method outperforms the state-of-the-art baseline methods on multi-variate time series classification task. The results of our case studies demonstrate that the variables and time intervals identified by the proposed method make sense relative to available domain knowledge.",0
"This is an article that deals with time series analysis using deep learning techniques. Specifically, we investigate whether attending to important variables as well as informative time intervals can lead to improved performance on time series classification tasks relative to traditional methodologies like regression trees, gradient boosting machines, etc., which may have difficulties in capturing complex temporal relationships within high dimensional data. Our approach uses recurrent neural networks (RNN) with attention modules designed to selectively focus on important variables at different points in the training process. By doing so, our models effectively ""learn"" to attend to features most relevant to predictive accuracy on any given interval of interest across time. We demonstrate empirically that these attention mechanisms result in robust improvements over standard RNN architectures on several benchmark datasets commonly used in finance, economics, engineering, climatology, ecology, psychology, neuroscience, among others. These results suggest that incorporating temporality into deep learning frameworks has great potential for enhancing understanding of dynamic processes underlying many real world phenomena, allowing researchers to develop more accurate forecasting methods crucial for decision making in various domains including business, healthcare, public policy formation, climate change mitigation/adaptation planning, risk management, epidemiological surveillance, just to name a few. While there exists prior work involving attention mechanisms for sequential processing of static vectors, herein lies our key novelty since we extend attention to model changing dependencies along multivariate dimensions through t",1
"Softmax function is widely used in artificial neural networks for multiclass classification, multilabel classification, attention mechanisms, etc. However, its efficacy is often questioned in literature. The log-softmax loss has been shown to belong to a more generic class of loss functions, called spherical family, and its member log-Taylor softmax loss is arguably the best alternative in this class. In another approach which tries to enhance the discriminative nature of the softmax function, soft-margin softmax (SM-softmax) has been proposed to be the most suitable alternative. In this work, we investigate Taylor softmax, SM-softmax and our proposed SM-Taylor softmax, an amalgamation of the earlier two functions, as alternatives to softmax function. Furthermore, we explore the effect of expanding Taylor softmax up to ten terms (original work proposed expanding only to two terms) along with the ramifications of considering Taylor softmax to be a finite or infinite series during backpropagation. Our experiments for the image classification task on different datasets reveal that there is always a configuration of the SM-Taylor softmax function that outperforms the normal softmax function and its other alternatives.",0
"Title: ""Exploring Alternatives to Softmax Function""  This paper examines different alternatives to softmax function that can be used for multi-class classification problems. In machine learning, the softmax function plays an important role as it allows us to convert vectors into probability distributions by normalizing them over their sum values. However, there are some limitations associated with the use of softmax function such as slow convergence rate during training, sensitivity to label ordering, and difficulties in handling large number of classes. To overcome these drawbacks, several alternative methods have been proposed in recent years. This paper reviews and compares these alternatives based on their mathematical properties, computational complexity, ease of implementation, and performance characteristics. The study evaluates the effectiveness of each method using extensive experiments on real world datasets and shows how they outperform traditional softmax function in terms of accuracy, speed and stability. Overall, the findings suggest that exploring alternative approaches beyond softmax could lead to better results in multi-class classification tasks.",1
"We present a method to learn a diverse group of object categories from an unordered point set. We propose our Pyramid Point network, which uses a dense pyramid structure instead of the traditional 'U' shape, typically seen in semantic segmentation networks. This pyramid structure gives a second look, allowing the network to revisit different layers simultaneously, increasing the contextual information by creating additional layers with less noise. We introduce a Focused Kernel Point convolution (FKP Conv), which expands on the traditional point convolutions by adding an attention mechanism to the kernel outputs. This FKP Conv increases our feature quality and allows us to weigh the kernel outputs dynamically. These FKP Convs are the central part of our Recurrent FKP Bottleneck block, which makes up the backbone of our encoder. With this distinct network, we demonstrate competitive performance on three benchmark data sets. We also perform an ablation study to show the positive effects of each element in our FKP Conv.",0
"This paper presents the Pyramid Point network, which uses novel multi-level focusing modules to explicitly revisit feature representations at multiple levels during training. By learning to focus attention on different regions of the input data, our model can efficiently adapt to local contexts while maintaining global consistency. We compare the performance of Pyramid Point against several state-of-the-art methods and show that it achieves better accuracy across a variety of benchmark datasets, demonstrating the effectiveness of our approach for handling complex tasks such as object detection and segmentation.",1
"Human Settlement Extent (HSE) and Local Climate Zone (LCZ) maps are both essential sources, e.g., for sustainable urban development and Urban Heat Island (UHI) studies. Remote sensing (RS)- and deep learning (DL)-based classification approaches play a significant role by providing the potential for global mapping. However, most of the efforts only focus on one of the two schemes, usually on a specific scale. This leads to unnecessary redundancies, since the learned features could be leveraged for both of these related tasks. In this letter, the concept of multi-task learning (MTL) is introduced to HSE regression and LCZ classification for the first time. We propose a MTL framework and develop an end-to-end Convolutional Neural Network (CNN), which consists of a backbone network for shared feature learning, attention modules for task-specific feature learning, and a weighting strategy for balancing the two tasks. We additionally propose to exploit HSE predictions as a prior for LCZ classification to enhance the accuracy. The MTL approach was extensively tested with Sentinel-2 data of 13 cities across the world. The results demonstrate that the framework is able to provide a competitive solution for both tasks.",0
"This paper investigates multi-task learning as a methodology for solving two tasks concurrently: human settlement extent regression (HSE) and local climate zone classification (LCZ). We aim to evaluate whether joint learning can improve model performance over single task models. By leveraging geospatial data from remotely sensed imagery and other sources, we train convolutional neural networks on multiple tasks simultaneously using an integrated loss function. Our results demonstrate improved model accuracy compared to standalone approaches across multiple datasets. Furthermore, the incorporation of additional features such as land cover type improves predictions considerably. Overall, our findings support the effectiveness of multi-task learning as a tool for enhancing environmental research and applications related to remote sensing. Future work includes expanding the scope of the approach beyond these specific tasks.  -----",1
"Federated learning (FL) has attracted increasing attention in recent years. As a privacy-preserving collaborative learning paradigm, it enables a broader range of applications, especially for computer vision and natural language processing tasks. However, to date, there is limited research of federated learning on relational data, namely Knowledge Graph (KG). In this work, we present a modified version of the graph neural network algorithm that performs federated modeling over KGs across different participants. Specifically, to tackle the inherent data heterogeneity issue and inefficiency in algorithm convergence, we propose a novel optimization algorithm, named FedAlign, with 1) optimal transportation (OT) for on-client personalization and 2) weight constraint to speed up the convergence. Extensive experiments have been conducted on several widely used datasets. Empirical results show that our proposed method outperforms the state-of-the-art FL methods, such as FedAVG and FedProx, with better convergence.",0
"Recent advancements in federated relational data modeling have led to significant improvements in interoperability and efficient resource management across distributed systems. However, these models still face challenges related to schema differences among participating databases, heterogeneous database schemas, and varying security policies. To address these issues, we propose a novel approach that combines basis alignment and weight penalty techniques to enhance the accuracy and effectiveness of federated relational data modeling. Our method ensures consistent mapping of objects between different databases by aligning their underlying conceptual structures, which improves query optimization, semantic integrity preservation, and user acceptance rates. Furthermore, our approach employs a weighted penalty mechanism that adapts dynamically based on specific situations, allowing for better utilization of computing resources while guaranteeing secure access control at both local and global levels. We conducted extensive experiments using real datasets from diverse domains, demonstrating the effectiveness of our proposal compared to state-of-the-art methods in terms of execution time, cost, and user satisfaction metrics. This work contributes significantly towards enhancing the quality of service and improving the scalability of federated relational data modeling across distributed environments.",1
"Time series are all around in real-world applications. However, unexpected accidents for example broken sensors or missing of the signals will cause missing values in time series, making the data hard to be utilized. It then does harm to the downstream applications such as traditional classification or regression, sequential data integration and forecasting tasks, thus raising the demand for data imputation. Currently, time series data imputation is a well-studied problem with different categories of methods. However, these works rarely take the temporal relations among the observations and treat the time series as normal structured data, losing the information from the time data. In recent, deep learning models have raised great attention. Time series methods based on deep learning have made progress with the usage of models like RNN, since it captures time information from data. In this paper, we mainly focus on time series imputation technique with deep learning methods, which recently made progress in this field. We will review and discuss their model architectures, their pros and cons as well as their effects to show the development of the time series imputation methods.",0
"This survey paper provides an overview of recent research on deep learning approaches for time series data imputation. In many applications such as sensor fault diagnosis and climate change analysis, incomplete or missing data can occur due to unexpected events like equipment failure or natural disasters. As a result, accurate methods for filling in these gaps have become crucial for obtaining meaningful insights from data sets. In recent years, several deep learning models have been developed for time series data imputation which learn complex patterns present in time series by capturing long-range dependencies. These approaches outperform traditional techniques and provide promising results in terms of accuracy and robustness. We discuss different strategies used in designing deep neural networks for this task including encoderâ€“decoder architectures, attention mechanisms, generative adversarial network (GAN), variational autoencoders (VAE) and their variants. Moreover, we compare these approaches against each other based on performance metrics commonly adopted to evaluate time series imputation tasks. Finally, we identify future directions for research towards addressing challenges that remain open in this field. Overall, our study serves as a comprehensive guide for researchers interested in exploring deep learning solutions for handling missing data problems in temporal data streams.",1
"Graphs play an important role in many applications. Recently, Graph Neural Networks (GNNs) have achieved promising results in graph analysis tasks. Some state-of-the-art GNN models have been proposed, e.g., Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), etc. Despite these successes, most of the GNNs only have shallow structure. This causes the low expressive power of the GNNs. To fully utilize the power of the deep neural network, some deep GNNs have been proposed recently. However, the design of deep GNNs requires significant architecture engineering. In this work, we propose a method to automate the deep GNNs design. In our proposed method, we add a new type of skip connection to the GNNs search space to encourage feature reuse and alleviate the vanishing gradient problem. We also allow our evolutionary algorithm to increase the layers of GNNs during the evolution to generate deeper networks. We evaluate our method in the graph node classification task. The experiments show that the GNNs generated by our method can obtain state-of-the-art results in Cora, Citeseer, Pubmed and PPI datasets.",0
"In this work we present AutoGraph, an automated framework that generates graph neural network architectures at runtime given a specific task. Traditionally, designing GNNs requires careful consideration of hyperparameters such as node features shape and size, layer types, message functions and aggregation methods which can lead to suboptimal results due to lack of domain expertise. Our proposed method leverages search algorithms and learning dynamics to explore the space of possible models and discover optimal ones based on performance metrics. We evaluate our approach on several benchmark datasets, demonstrating superior accuracy compared to handcrafted baselines while requiring significantly fewer parameters. We discuss applications of AutoGraph to related fields where similar bottlenecks exist and outline future research directions.",1
"We tackle human image synthesis, including human motion imitation, appearance transfer, and novel view synthesis, within a unified framework. It means that the model, once being trained, can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints to estimate the human body structure. However, they only express the position information with no abilities to characterize the personalized shape of the person and model the limb rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape. It can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose an Attentional Liquid Warping GAN with Attentional Liquid Warping Block (AttLWB) that propagates the source information in both image and feature spaces to the synthesized reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method can support a more flexible warping from multiple sources. To further improve the generalization ability of the unseen source images, a one/few-shot adversarial learning is applied. In detail, it firstly trains a model in an extensive training set. Then, it finetunes the model by one/few-shot unseen image(s) in a self-supervised way to generate high-resolution (512 x 512 and 1024 x 1024) results. Also, we build a new dataset, namely iPER dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our methods in terms of preserving face identity, shape consistency, and clothes details. All codes and dataset are available on https://impersonator.org/work/impersonator-plus-plus.html.",0
"This is a paper on image synthesis using deep learning techniques. Specifically, we propose a new method based on Generative Adversarial Networks (GAN) that uses attention modules to focus on different parts of the input image during generation. Our approach allows us to generate high-resolution images that better capture facial features such as eyes, nose, mouth, etc. We evaluate our method against state-of-the-art approaches on several benchmark datasets and show improved performance across all metrics. Additionally, we provide qualitative results that demonstrate the advantages of our approach over previous methods. Overall, our work advances the field of image synthesis by providing a unified framework that balances both global contextual information and local details in generated images.",1
"Siamese network based trackers formulate the visual tracking task as a similarity matching problem. Almost all popular Siamese trackers realize the similarity learning via convolutional feature cross-correlation between a target branch and a search branch. However, since the size of target feature region needs to be pre-fixed, these cross-correlation base methods suffer from either reserving much adverse background information or missing a great deal of foreground information. Moreover, the global matching between the target and search region also largely neglects the target structure and part-level information.   In this paper, to solve the above issues, we propose a simple target-aware Siamese graph attention network for general object tracking. We propose to establish part-to-part correspondence between the target and the search region with a complete bipartite graph, and apply the graph attention mechanism to propagate target information from the template feature to the search feature. Further, instead of using the pre-fixed region cropping for template-feature-area selection, we investigate a target-aware area selection mechanism to fit the size and aspect ratio variations of different objects. Experiments on challenging benchmarks including GOT-10k, UAV123, OTB-100 and LaSOT demonstrate that the proposed SiamGAT outperforms many state-of-the-art trackers and achieves leading performance. Code is available at: https://git.io/SiamGAT",0
"""Graphs are everywhere"", and tracking them has been shown to improve many real world processes: from security monitoring at airports (and elsewhere), to watching social media traffic, to medical diagnosis, or even detecting cyberbullying. While tracking graphs might seem like it would be simple, current methods often struggle with important issues like speed, scalability, and flexibility - but graph attention tracking can fix these problems. GAT, short for Graph Attention Tracking, works by focusing on certain elements within each graph while considering others less relevant. This focus allows for faster and more accurate detection of specific patterns across multiple layers of data points. By prioritizing the most critical parts of the network over insignificant details, researchers have shown that their accuracy rates improved by up to 97%. GAT can scale to accommodate complex networks and adapt as new data becomes available without losing precision. With numerous potential applications beyond those mentioned here, the future looks bright for this innovative approach to graph analysis.",1
"Autonomous vehicle navigation in shared pedestrian environments requires the ability to predict future crowd motion both accurately and with minimal delay. Understanding the uncertainty of the prediction is also crucial. Most existing approaches however can only estimate uncertainty through repeated sampling of generative models. Additionally, most current predictive models are trained on datasets that assume complete observability of the crowd using an aerial view. These are generally not representative of real-world usage from a vehicle perspective, and can lead to the underestimation of uncertainty bounds when the on-board sensors are occluded. Inspired by prior work in motion prediction using spatio-temporal graphs, we propose a novel Graph Convolutional Neural Network (GCNN)-based approach, Attentional-GCNN, which aggregates information of implicit interaction between pedestrians in a crowd by assigning attention weight in edges of the graph. Our model can be trained to either output a probabilistic distribution or faster deterministic prediction, demonstrating applicability to autonomous vehicle use cases where either speed or accuracy with uncertainty bounds are required. To further improve the training of predictive models, we propose an automatically labelled pedestrian dataset collected from an intelligent vehicle platform representative of real-world use. Through experiments on a number of datasets, we show our proposed method achieves an improvement over the state of art by 10% Average Displacement Error (ADE) and 12% Final Displacement Error (FDE) with fast inference speeds.",0
"This paper presents a novel approach to pedestrian trajectory prediction that adapts to specific use cases for autonomous vehicles. Our method, called Attentional-GCNN (A-GCNN), combines global contextual features with local spatial attention mechanisms to capture important patterns from both short and long range interactions. We evaluate our model on two different scenarios: crosswalks and unsignalized intersections, which represent common areas where pedestrians interact with traffic signals. Compared to state-of-the-art methods, A-GCNN achieves higher accuracy in predicting pedestrian movement up to five seconds ahead, demonstrating its effectiveness in real-world applications. The potential impact of these results is significant as improved pedestrian trajectory prediction can enhance safety in roads shared by human drivers, bicycles, motorcycles and other road users. Future work includes expanding the scope of our proposed model to consider more complex environments such as multi-modal interactions, crowded spaces, and urban settings.",1
"While much attention has been given to the problem of estimating the effect of discrete interventions from observational data, relatively little work has been done in the setting of continuous-valued interventions, such as treatments associated with a dosage parameter. In this paper, we tackle this problem by building on a modification of the generative adversarial networks (GANs) framework. Our model, SCIGAN, is flexible and capable of simultaneously estimating counterfactual outcomes for several different continuous interventions. The key idea is to use a significantly modified GAN model to learn to generate counterfactual outcomes, which can then be used to learn an inference model, using standard supervised methods, capable of estimating these counterfactuals for a new sample. To address the challenges presented by shifting to continuous interventions, we propose a novel architecture for our discriminator - we build a hierarchical discriminator that leverages the structure of the continuous intervention setting. Moreover, we provide theoretical results to support our use of the GAN framework and of the hierarchical discriminator. In the experiments section, we introduce a new semi-synthetic data simulation for use in the continuous intervention setting and demonstrate improvements over the existing benchmark models.",0
"This paper proposes a novel approach to estimating the effects of continuous-valued interventions using generative adversarial networks (GANs). GANs have shown great promise as a tool for generating realistic synthetic data, but their use for causal inference has been limited due to challenges such as identifying causality in high-dimensional data and dealing with confounding factors. To address these issues, we propose a method that combines the strengths of GANs with traditional regression models. We demonstrate our approach on two case studies: one evaluating the effectiveness of a job training program, and another examining the impact of weather conditions on energy consumption. Our results show that our method is capable of producing accurate estimates of treatment effects, even in situations where traditional approaches fail. Overall, our work opens up new possibilities for leveraging machine learning techniques in economic research, and highlights the potential value of GANs for conducting causal analysis in complex systems.",1
"We propose a novel pathology-sensitive deep learning model (PS-DeVCEM) for frame-level anomaly detection and multi-label classification of different colon diseases in video capsule endoscopy (VCE) data. Our proposed model is capable of coping with the key challenge of colon apparent heterogeneity caused by several types of diseases. Our model is driven by attention-based deep multiple instance learning and is trained end-to-end on weakly labeled data using video labels instead of detailed frame-by-frame annotation. The spatial and temporal features are obtained through ResNet50 and residual Long short-term memory (residual LSTM) blocks, respectively. Additionally, the learned temporal attention module provides the importance of each frame to the final label prediction. Moreover, we developed a self-supervision method to maximize the distance between classes of pathologies. We demonstrate through qualitative and quantitative experiments that our proposed weakly supervised learning model gives superior precision and F1-score reaching, 61.6% and 55.1%, as compared to three state-of-the-art video analysis methods respectively. We also show our model's ability to temporally localize frames with pathologies, without frame annotation information during training. Furthermore, we collected and annotated the first and largest VCE dataset with only video labels. The dataset contains 455 short video segments with 28,304 frames and 14 classes of colorectal diseases and artifacts. Dataset and code supporting this publication will be made available on our home page.",0
"In recent years, there has been increasing interest in using deep learning models to automate medical image analysis tasks, including those related to gastrointestinal tract examinations such as video capsule endoscopies (VCES). However, developing high performing deep learning models for VCSEs requires large amounts of annotated data, which can be difficult and expensive to obtain. To address this issue, we propose a pathology-sensitive deep learning model that utilizes weakly labeled data to achieve state-of-the-art performance for detecting abnormalities in VCSE videos. Our proposed method leverages prior knowledge from expert annotation to improve generalization across different types of annotations, while still achieving strong performance without relying heavily on fully annotated data. We demonstrate the effectiveness of our approach through extensive experiments and show that our model outperforms baseline methods by significant margins, achieving mean average precision scores greater than 94%. Overall, our results indicate that our pathology-sensitive deep learning model represents a valuable tool for improving the accuracy and efficiency of VCE analysis for clinical applications.",1
"The emerging vision-and-language navigation (VLN) problem aims at learning to navigate an agent to the target location in unseen photo-realistic environments according to the given language instruction. The main challenges of VLN arise mainly from two aspects: first, the agent needs to attend to the meaningful paragraphs of the language instruction corresponding to the dynamically-varying visual environments; second, during the training process, the agent usually imitate the shortest-path to the target location. Due to the discrepancy of action selection between training and inference, the agent solely on the basis of imitation learning does not perform well. Sampling the next action from its predicted probability distribution during the training process allows the agent to explore diverse routes from the environments, yielding higher success rates. Nevertheless, without being presented with the shortest navigation paths during the training process, the agent may arrive at the target location through an unexpected longer route. To overcome these challenges, we design a cross-modal grounding module, which is composed of two complementary attention mechanisms, to equip the agent with a better ability to track the correspondence between the textual and visual modalities. We then propose to recursively alternate the learning schemes of imitation and exploration to narrow the discrepancy between training and inference. We further exploit the advantages of both these two learning schemes via adversarial learning. Extensive experimental results on the Room-to-Room (R2R) benchmark dataset demonstrate that the proposed learning scheme is generalized and complementary to prior arts. Our method performs well against state-of-the-art approaches in terms of effectiveness and efficiency.",0
"Title: ""A New Approach to Guiding Navigation using Multimodal Information""  In recent years, there has been significant interest in developing methods that can effectively guide users through complex environments. While traditional navigation techniques rely on maps and GPS signals, these approaches may fail in areas without coverage or where the user cannot access such tools. In light of this challenge, researchers have explored alternative ways of guiding navigation by leveraging additional sources of information. One approach involves grounding language instructions in sensory inputs from the environment, providing more detailed guidance for the user. However, existing methods suffer from limitations related to data quality, computational complexity, and scalability. This work presents a novel method for cross-modal grounding that addresses these challenges while improving overall performance. We propose an alternate adversarial learning framework that enables effective alignment of multimodal representations, enabling more accurate navigation instructions. Our evaluations demonstrate the effectiveness of our method compared to state-of-the-art alternatives across multiple datasets. These results highlight the potential of our approach for building robust navigation systems capable of handling real-world scenarios. Overall, our work represents an important step towards enhancing human mobility through technology.",1
"Graph representation learning has attracted lots of attention recently. Existing graph neural networks fed with the complete graph data are not scalable due to limited computation and memory costs. Thus, it remains a great challenge to capture rich information in large-scale graph data. Besides, these methods mainly focus on supervised learning and highly depend on node label information, which is expensive to obtain in the real world. As to unsupervised network embedding approaches, they overemphasize node proximity instead, whose learned representations can hardly be used in downstream application tasks directly. In recent years, emerging self-supervised learning provides a potential solution to address the aforementioned problems. However, existing self-supervised works also operate on the complete graph data and are biased to fit either global or very local (1-hop neighborhood) graph structures in defining the mutual information based loss terms.   In this paper, a novel self-supervised representation learning method via Subgraph Contrast, namely \textsc{Subg-Con}, is proposed by utilizing the strong correlation between central nodes and their sampled subgraphs to capture regional structure information. Instead of learning on the complete input graph data, with a novel data augmentation strategy, \textsc{Subg-Con} learns node representations through a contrastive loss defined based on subgraphs sampled from the original graph instead. Compared with existing graph representation learning approaches, \textsc{Subg-Con} has prominent performance advantages in weaker supervision requirements, model learning scalability, and parallelization. Extensive experiments verify both the effectiveness and the efficiency of our work compared with both classic and state-of-the-art graph representation learning approaches on multiple real-world large-scale benchmark datasets from different domains.",0
"An abstract without these constraints would look like: This paper presents Sub-graph Contrast (SC), which enables scalable self-supervision by leveraging substructures as weak supervisory signals in graph representation learning. SC operates over large graphs by mining frequent structures (sub-graphs) that appear more often than expected under random walks. For each sampled structure, two instantiations from different nodes are contrasted via minimization objectives while sharing a common message passing layer. SC is a versatile framework amenable to any kind of node embedding functions without relying on graph coarsening. Extensive experiments demonstrate outstanding results of our approach across a range of benchmark datasets compared against state-of-the art alternatives in diverse fields including social network analysis, molecular biology and computer vision. By resolving limitations associated with dense graph embeddings using precomputation and reducing computational cost, we provide compelling evidence of the viability and effectiveness of sub-graph contrast based representations and models as new frontiers for machine learning research. Abstract: Developing effective strategies for scaling up graph representation learning methods remains a challenging task. However, harnessing the power of weak supervisory signals from substructures can offer substantial benefits, as demonstrated in this study through the introduction of Sub-Graph Contrast (SC). Our method mines frequently occurring subgraph patterns and utilizes them to generate self-supervised training examples. These instances are then used to optimize node embedding functions, resulting in improved performance across a range of benchmark datasets in fields such as social network analysis, molecular biology, and computer vision. Furthermore, by addressing drawbacks related to graph coarsening and density, our work contributes valuable insights into the future direction of machine learning research. Ultimately, SC represents a highly promising technique capable of enhancing existing approaches in the field, paving the way for new possibilities in graph representation learning.",1
"In vision and linguistics; the main input modalities are facial expressions, speech patterns, and the words uttered. The issue with analysis of any one mode of expression (Visual, Verbal or Vocal) is that lot of contextual information can get lost. This asks researchers to inspect multiple modalities to get a thorough understanding of the cross-modal dependencies and temporal context of the situation to analyze the expression. This work attempts at preserving the long-range dependencies within and across different modalities, which would be bottle-necked by the use of recurrent networks and adds the concept of delta-attention to focus on local differences per modality to capture the idiosyncrasy of different people. We explore a cross-attention fusion technique to get the global view of the emotion expressed through these delta-self-attended modalities, in order to fuse all the local nuances and global context together. The addition of attention is new to the multi-modal fusion field and currently being scrutinized for on what stage the attention mechanism should be used, this work achieves competitive accuracy for overall and per-class classification which is close to the current state-of-the-art with almost half number of parameters.",0
"This paper proposes a novel method for multimodal fusion using hierarchical delta attention. Traditional methods for multimodal fusion often struggle to effectively combine information from different modalities into a single representation, resulting in reduced performance on downstream tasks. Our proposed approach addresses these limitations by introducing a hierarchical structure that allows multiple levels of interaction between modalities. Additionally, our method utilizes delta attention which helps in reducing computational cost while improving the accuracy. We show through extensive experiments that our hierarchical delta attention method outperforms several state-of-the-art multimodal fusion approaches across a range of benchmarks including image classification, video classification, question answering and sentiment analysis.",1
"Most existing text-to-image generation methods adopt a multi-stage modular architecture which has three significant problems: 1) Training multiple networks increases the run time and affects the convergence and stability of the generative model; 2) These approaches ignore the quality of early-stage generator images; 3) Many discriminators need to be trained. To this end, we propose the Dual Attention Generative Adversarial Network (DTGAN) which can synthesize high-quality and semantically consistent images only employing a single generator/discriminator pair. The proposed model introduces channel-aware and pixel-aware attention modules that can guide the generator to focus on text-relevant channels and pixels based on the global sentence vector and to fine-tune original feature maps using attention weights. Also, Conditional Adaptive Instance-Layer Normalization (CAdaILN) is presented to help our attention modules flexibly control the amount of change in shape and texture by the input natural-language description. Furthermore, a new type of visual loss is utilized to enhance the image resolution by ensuring vivid shape and perceptually uniform color distributions of generated images. Experimental results on benchmark datasets demonstrate the superiority of our proposed method compared to the state-of-the-art models with a multi-stage framework. Visualization of the attention maps shows that the channel-aware attention module is able to localize the discriminative regions, while the pixel-aware attention module has the ability to capture the globally visual contents for the generation of an image.",0
"In recent years, text-to-image generation has emerged as one of the most exciting applications of artificial intelligence (AI). However, generating high-quality images that accurately capture the semantics conveyed by input text remains a challenging task. To address this challenge, we propose DTGAN, a novel approach based on dual attention generative adversarial networks. Our method focuses on enhancing both image fidelity and semantic correspondence using two different attention mechanisms - spatial and channelwise attention. Spatial attention helps generate semantically relevant features at each location, while channelwise attention enables fine-grained control over texture and appearance. We evaluate our approach against several state-of-the-art methods on four benchmark datasets and demonstrate significant improvements in visual quality, text-relevance, and user preference metrics. Our results showcase the potential of DTGAN for creating visually compelling and semantically accurate text-based images. By advancing text-to-image generation capabilities, our research could enable new applications in fields such as computer graphics, education, entertainment, and beyond.",1
"DETR is a recently proposed Transformer-based method which views object detection as a set prediction problem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of DETR. Our examinations reveal several factors contributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross attention mechanism. To overcome these issues we propose two solutions, namely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy.",0
"One approach that has gained traction recently for object detection task is using transformers. However, one issue with these models is their limited ability to predict sets of objects. In order to address this limitation, we propose a novel method for enhancing set prediction performance of object detectors based on transformer architectures. Our approach is built upon recent advances in attention mechanisms for image classification tasks by incorporating multiple levels of attention modules into the model design. By doing so, our model can effectively attend to different parts of the input scene at varying spatial resolutions and capture richer contextual relationships among objects, which leads to improved set prediction results. Extensive experimental evaluation on standard benchmark datasets demonstrates the effectiveness of our proposed method.",1
"Predicting the future can significantly improve the safety of intelligent vehicles, which is a key component in autonomous driving. 3D point clouds accurately model 3D information of surrounding environment and are crucial for intelligent vehicles to perceive the scene. Therefore, prediction of 3D point clouds has great significance for intelligent vehicles, which can be utilized for numerous further applications. However, due to point clouds are unordered and unstructured, point cloud prediction is challenging and has not been deeply explored in current literature. In this paper, we propose a novel motion-based neural network named MoNet. The key idea of the proposed MoNet is to integrate motion features between two consecutive point clouds into the prediction pipeline. The introduction of motion features enables the model to more accurately capture the variations of motion information across frames and thus make better predictions for future motion. In addition, content features are introduced to model the spatial content of individual point clouds. A recurrent neural network named MotionRNN is proposed to capture the temporal correlations of both features. Besides, we propose an attention-based motion align module to address the problem of missing motion features in the inference pipeline. Extensive experiments on two large scale outdoor LiDAR datasets demonstrate the performance of the proposed MoNet. Moreover, we perform experiments on applications using the predicted point clouds and the results indicate the great application potential of the proposed method.",0
"MoNet is a novel deep learning architecture that utilizes motion information to predict point clouds. Our method relies on two key components: an encoder network that processes the sensor data (such as lidar scans) into latent representations, and a decoder network that generates predictions based on these latent representations. Importantly, we introduce a new module called the motion conditioner which projects the latent representation onto the local tangent space at each time step, effectively incorporating motion information into the model. We evaluate our approach using standard benchmark datasets for autonomous driving tasks such as object detection and scene understanding. Our results show significant improvements over baseline methods without motion prediction, demonstrating the effectiveness of our proposed framework. Overall, MoNet represents a promising new direction for the use of deep learning models in perception problems requiring spatio-temporal reasoning.",1
"Training a neural network model that can quickly adapt to a new task is highly desirable yet challenging for few-shot learning problems. Recent few-shot learning methods mostly concentrate on developing various meta-learning strategies from two aspects, namely optimizing an initial model or learning a distance metric. In this work, we propose a novel few-shot learning method via optimizing and fast adapting the query sample representation based on very few reference samples. To be specific, we devise a simple and efficient meta-reweighting strategy to adapt the sample representations and generate soft attention to refine the representation such that the relevant features from the query and support samples can be extracted for a better few-shot classification. Such an adaptive attention model is also able to explain what the classification model is looking for as the evidence for classification to some extent. As demonstrated experimentally, the proposed model achieves state-of-the-art classification results on various benchmark few-shot classification and fine-grained recognition datasets.",0
"In this paper we propose a method called few shot classification using adaptive attention, which combines a small amount of labeled data with large amounts of unlabeled data to train machine learning models that can accurately classify new examples into one of several categories. Our approach uses transformers with attention mechanisms to model relationships between input data points and their corresponding labels, allowing the model to learn complex patterns from very few labeled training examples. We evaluate our proposed method on standard benchmark datasets and show that it outperforms state of the art methods by significant margins, demonstrating the effectiveness of our approach for solving problems involving limited supervision.",1
"Skin cancer continues to be the most frequently diagnosed form of cancer in the U.S., with not only significant effects on health and well-being but also significant economic costs associated with treatment. A crucial step to the treatment and management of skin cancer is effective skin cancer detection due to strong prognosis when treated at an early stage, with one of the key screening approaches being dermoscopy examination. Motivated by the advances of deep learning and inspired by the open source initiatives in the research community, in this study we introduce CancerNet-SCa, a suite of deep neural network designs tailored for the detection of skin cancer from dermoscopy images that is open source and available to the general public as part of the Cancer-Net initiative. To the best of the authors' knowledge, CancerNet-SCa comprises of the first machine-designed deep neural network architecture designs tailored specifically for skin cancer detection, one of which possessing a self-attention architecture design with attention condensers. Furthermore, we investigate and audit the behaviour of CancerNet-SCa in a responsible and transparent manner via explainability-driven model auditing. While CancerNet-SCa is not a production-ready screening solution, the hope is that the release of CancerNet-SCa in open source, open access form will encourage researchers, clinicians, and citizen data scientists alike to leverage and build upon them.",0
"In recent years, deep learning techniques have shown great potential for computer-assisted diagnosis of skin cancer using dermoscopy images. However, designing efficient neural networks that can effectively detect different types of skin lesions remains challenging due to variations in image quality, illumination conditions, and the presence of artifacts. To address these limitations, we propose a novel framework called ""CancerNet-Sca"" that utilizes tailored network designs for accurate detection of skin cancer. Our approach incorporates domain adaptation strategies such as data augmentation and transfer learning to improve model generalization across diverse datasets. We experimentally evaluate our proposed models on two publicly available benchmark datasets, demonstrating their superior performance compared to existing state-of-the-art methods. This work represents a significant step towards developing robust and effective artificial intelligence algorithms for early detection and management of skin cancer.",1
"The spherical domain representation of 360 video/image presents many challenges related to the storage, processing, transmission and rendering of omnidirectional videos (ODV). Models of human visual attention can be used so that only a single viewport is rendered at a time, which is important when developing systems that allow users to explore ODV with head mounted displays (HMD). Accordingly, researchers have proposed various saliency models for 360 video/images. This paper proposes ATSal, a novel attention based (head-eye) saliency model for 360\degree videos. The attention mechanism explicitly encodes global static visual attention allowing expert models to focus on learning the saliency on local patches throughout consecutive frames. We compare the proposed approach to other state-of-the-art saliency models on two datasets: Salient360! and VR-EyeTracking. Experimental results on over 80 ODV videos (75K+ frames) show that the proposed method outperforms the existing state-of-the-art.",0
"This paper presents a new approach to saliency prediction in 360 videos using attention mechanisms. While existing methods have relied on handcrafted features and predefined priors to predict which parts of the video frame deserve higher visual attentions, we propose a novel model that automatically learns high-quality, task-specific representations through multi-level attention modules. Our proposed architecture integrates temporal dynamics into the process by utilizing recurrent neural networks (RNNs) to capture temporal dependencies among frames. We validate our method against several state-of-the-art approaches on two publicly available benchmark datasets, achieving significantly better performance across all metrics. In addition, we provide qualitative evaluations demonstrating that our proposed method can accurately detect important regions of interest in challenging 360 video content. Overall, our results showcase the effectiveness of our attentional mechanism for saliency prediction in immersive environments.",1
"The huge amount of video data produced daily by camera-based systems, such as surveilance, medical and telecommunication systems, emerges the need for effective video summarization (VS) methods. These methods should be capable of creating an overview of the video content. In this paper, we propose a novel VS method based on a Generative Adversarial Network (GAN) model pre-trained with human eye fixations. The main contribution of the proposed method is that it can provide perceptually compatible video summaries by combining both perceived color and spatiotemporal visual attention cues in a unsupervised scheme. Several fusion approaches are considered for robustness under uncertainty, and personalization. The proposed method is evaluated in comparison to state-of-the-art VS approaches on the benchmark dataset VSUMM. The experimental results conclude that SalSum outperforms the state-of-the-art approaches by providing the highest f-measure score on the VSUMM benchmark.",0
"Title: Saliency Driven Video Summarization Using GANs  Automatically generating summaries from videos has become increasingly important due to the proliferation of multimedia content online. Existing methods use either handcrafted features or learned representations but fail to capture the essence of a video scene fully. In this work, we introduce SalSum, a novel approach that uses generative adversarial networks (GAN) to learn saliency maps directly from raw frames which represent importance scores of each frame relative to other frames in the video. These saliency maps drive the generation of keyframes and subsequent extraction of summary clips. Extensive experimental results on multiple benchmark datasets demonstrate significant improvement over state-of-the-art methods quantitatively as well as qualitatively through human evaluations. The proposed method can generate highly relevant and visually coherent summaries making it suitable for real world applications such as personalized story creation based on user preferences, browsing large scale collections of videos and creating trailers for movies etc.",1
"The paper addresses the problem of recognition of actions in video with low inter-class variability such as Table Tennis strokes. Two stream, ""twin"" convolutional neural networks are used with 3D convolutions both on RGB data and optical flow. Actions are recognized by classification of temporal windows. We introduce 3D attention modules and examine their impact on classification efficiency. In the context of the study of sportsmen performances, a corpus of the particular actions of table tennis strokes is considered. The use of attention blocks in the network speeds up the training step and improves the classification scores up to 5% with our twin model. We visualize the impact on the obtained features and notice correlation between attention and player movements and position. Score comparison of state-of-the-art action classification method and proposed approach with attentional blocks is performed on the corpus. Proposed model with attention blocks outperforms previous model without them and our baseline.",0
"In recent years, deep learning techniques have been widely used in sports video analysis due to their ability to automatically learn features from raw data and their capacity to perform complex tasks such as action recognition. Table tennis stroke classification is one such task that has recently gained interest among researchers, given the popularity of the sport worldwide. Existing works have achieved promising results by using handcrafted feature extraction methods, spatial convolutional neural networks (CNN), temporal CNNs or recurrent neural networks (RNN).  This work proposes a novel approach for fine-grained classification of table tennis strokes based on a twin spatio-temporal convolutional neural network architecture. Specifically, we introduce a three-dimensional (3D) attention module within our model, enabling the system to selectively focus on important regions of each frame, capturing both local contextual information and global dependencies across frames. We use two identical streams - one for spatial information and another for temporal information - which run parallel to each other, allowing us to jointly capture spatial and temporal information. Our contributions can be summarized as follows:  * We propose a new 3D attention module, which enables our model to effectively attend to relevant regions in space and time. This module helps extract more meaningful representations, leading to improved performance compared to existing approaches without attention mechanisms. * We design a twin spatio-temporal CNN architecture, where both spatial and temporal streams share weights. By doing so, we force them to learn similar representations, reducing complexity while improving performance. Moreover, the shared weights allow our model to handle different sequence lengths more efficiently. * We conduct extensive experiments on four challenging datasets, demonstrating the effectiveness and efficiency of our proposed method against several state-of-the-art baselines. Notably, our approach consistently outperforms these competitors by significant margins. Ablation studies highlight the importance of our contribution",1
"Contrastive, self-supervised learning of object representations recently emerged as an attractive alternative to reconstruction-based training. Prior approaches focus on contrasting individual object representations (slots) against one another. However, a fundamental problem with this approach is that the overall contrastive loss is the same for (i) representing a different object in each slot, as it is for (ii) (re-)representing the same object in all slots. Thus, this objective does not inherently push towards the emergence of object-centric representations in the slots. We address this problem by introducing a global, set-based contrastive loss: instead of contrasting individual slot representations against one another, we aggregate the representations and contrast the joined sets against one another. Additionally, we introduce attention-based encoders to this contrastive setup which simplifies training and provides interpretable object masks. Our results on two synthetic video datasets suggest that this approach compares favorably against previous contrastive methods in terms of reconstruction, future prediction and object separation performance.",0
"Learning Object-Centric Video Models by Contrasting Sets presents a novel approach to video representation that focuses on learning object-centric models through contrastive learning techniques. In recent years, there has been significant progress in developing deep neural network architectures for learning representations from visual data, such as images and videos. However, most existing approaches rely on supervised learning, which requires large amounts of labeled data and may lead to overfitting when the training dataset is limited. This paper proposes an alternative methodology that leverages self-supervision via contrastive learning, whereby two sets of inputs (positive pairs) and another set of inputs (negative pairs), which have distinct characteristics, are used to train a model. By maximizing the similarity between positive pairs and minimizing the similarity between negative pairs, the proposed framework can learn effective representations of objects within videos, without relying on extensive annotations. Experimental results demonstrate that the proposed technique outperforms several state-of-the-art methods across multiple benchmarks, providing evidence of its effectiveness in learning object-centric representations. Overall, this work offers an important contribution to the field of computer vision, paving the way for future research into the development of powerful unsupervised learning algorithms for video understanding tasks.",1
"In this paper, we propose a new video object detector (VoD) method referred to as temporal feature aggregation and motion-aware VoD (TM-VoD), which produces a joint representation of temporal image sequences and object motion. The proposed TM-VoD aggregates visual feature maps extracted by convolutional neural networks applying the temporal attention gating and spatial feature alignment. This temporal feature aggregation is performed in two stages in a hierarchical fashion. In the first stage, the visual feature maps are fused at a pixel level via gated attention model. In the second stage, the proposed method aggregates the features after aligning the object features using temporal box offset calibration and weights them according to the cosine similarity measure. The proposed TM-VoD also finds the representation of the motion of objects in two successive steps. The pixel-level motion features are first computed based on the incremental changes between the adjacent visual feature maps. Then, box-level motion features are obtained from both the region of interest (RoI)-aligned pixel-level motion features and the sequential changes of the box coordinates. Finally, all these features are concatenated to produce a joint representation of the objects for VoD. The experiments conducted on the ImageNet VID dataset demonstrate that the proposed method outperforms existing VoD methods and achieves a performance comparable to that of state-of-the-art VoDs.",0
"This paper presents a novel approach for video object detection that jointly represents temporal image sequences and object motion. Traditional methods often rely on either static object detectors or separate models for tracking objects over time. However, these approaches can suffer from limited accuracy due to errors accumulating over time or difficulties accurately predicting future states. Our method addresses this challenge by using deep learning techniques to jointly learn representations of both static images and their corresponding motion patterns across time. Experimental results demonstrate significant improvements over previous state-of-the-art methods, particularly in challenging scenarios with fast-moving objects or complex backgrounds. The proposed framework offers greater flexibility and generalizability, making it well suited for real-world applications such as autonomous driving, surveillance, and robotics. Overall, our work highlights the importance of considering temporal aspects in computer vision tasks and provides a promising direction for further research in this area.",1
"Weakly supervised object detection (WSOD) aims to classify and locate objects with only image-level supervision. Many WSOD approaches adopt multiple instance learning as the initial model, which is prone to converge to the most discriminative object regions while ignoring the whole object, and therefore reduce the model detection performance. In this paper, a novel cascade attentive dropout strategy is proposed to alleviate the part domination problem, together with an improved global context module. We purposely discard attentive elements in both channel and space dimensions, and capture the inter-pixel and inter-channel dependencies to induce the model to better understand the global context. Extensive experiments have been conducted on the challenging PASCAL VOC 2007 benchmarks, which achieve 49.8% mAP and 66.0% CorLoc, outperforming state-of-the-arts.",0
"Abstract: This research proposes a novel method for weakly supervised object detection using cascading attentive dropout (CAD). Traditional methods have relied on large amounts of strongly labeled data which can become expensive to collect, label and annotate. In contrast, our approach requires only image level labels, making it ideal for low budget applications such as autonomous vehicles, drones and robots that operate in unstructured environments where gathering strong annotations is difficult. CAD uses attention maps generated by each stage of a CNN backbone network to selectively apply dropout during training. This improves both localization precision and recall while reducing false positives. Additionally, we demonstrate that CAD performs favorably against fully supervised baselines trained with thousands of images at all budgets including those requiring no human annotation at all! With the rise of privacy concerns surrounding transfer learning, our weakly supervised framework makes a compelling case for cost effective solutions that require minimal data collection while achieving state of the art performance.",1
"Detecting and segmenting object instances is a common task in biomedical applications. Examples range from detecting lesions on functional magnetic resonance images, to the detection of tumours in histopathological images and extracting quantitative single-cell information from microscopy imagery, where cell segmentation is a major bottleneck. Attention-based transformers are state-of-the-art in a range of deep learning fields. They have recently been proposed for segmentation tasks where they are beginning to outperforming other methods. We present a novel attention-based cell detection transformer (Cell-DETR) for direct end-to-end instance segmentation. While the segmentation performance is on par with a state-of-the-art instance segmentation method, Cell-DETR is simpler and faster. We showcase the method's contribution in a the typical use case of segmenting yeast in microstructured environments, commonly employed in systems or synthetic biology. For the specific use case, the proposed method surpasses the state-of-the-art tools for semantic segmentation and additionally predicts the individual object instances. The fast and accurate instance segmentation performance increases the experimental information yield for a posteriori data processing and makes online monitoring of experiments and closed-loop optimal experimental design feasible.",0
"This paper describes a deep learning model for instance segmenting cells in microscopy images by predicting bounding boxes as well as pixel masks of each cell. Our approach uses transformer networks that take advantage of self attention mechanisms so that our model can accurately capture spatial context relationships among local features. Unlike previous approaches like Mask R-CNN, we find that attention allows us to more effectively capture the interdependencies of pixels within object instances at varying scales, improving both recall and precision compared to prior art, including state-of-the-art techniques specifically optimized for microscope imagery. We thoroughly evaluate our approach on three microscopic image datasets, establishing our method as highly accurate in a variety of scenarios. In summary, we present strong evidence that applying the power of attention transformers to high stakes biomedical image analysis tasks offers a step forward for automating laborious manual annotation processes crucial to scientific progress, ultimately reducing costs while minimizing human error.",1
"Unsupervised learning of depth and ego-motion from unlabelled monocular videos has recently drawn great attention, which avoids the use of expensive ground truth in the supervised one. It achieves this by using the photometric errors between the target view and the synthesized views from its adjacent source views as the loss. Despite significant progress, the learning still suffers from occlusion and scene dynamics. This paper shows that carefully manipulating photometric errors can tackle these difficulties better. The primary improvement is achieved by a statistical technique that can mask out the invisible or nonstationary pixels in the photometric error map and thus prevents misleading the networks. With this outlier masking approach, the depth of objects moving in the opposite direction to the camera can be estimated more accurately. To the best of our knowledge, such scenarios have not been seriously considered in the previous works, even though they pose a higher risk in applications like autonomous driving. We also propose an efficient weighted multi-scale scheme to reduce the artifacts in the predicted depth maps. Extensive experiments on the KITTI dataset show the effectiveness of the proposed approaches. The overall system achieves state-of-theart performance on both depth and ego-motion estimation.",0
"In this work we present a novel approach for unsupervised learning of depth and ego-motion estimation from monocular videos using deeper insights on photometric errors, which are caused by intrinsic camera parameters and extrinsic motion in consecutive frames of monocular video sequences, as opposed to other sources such as stereo matching and structured lighting that rely on additional sensors. Our method relies on minimizing these errors between temporally adjacent frames through backpropagation, thus training two neural networks concurrently - one for predicting pixel intensities in new views while keeping the second network fixed and another for jointly estimating depth maps and camera motions, both trained under a newly defined loss function termed DiPE Loss. We demonstrate substantial improvements over existing state-of-the-art methods for both qualitative assessment and quantitative benchmarking metrics, providing better generalization capabilities across diverse scenarios and reducing the reliance on handcrafted features traditionally used in such problems. Our source code and pretrained models accompany this submission to facilitate research in this area. This article presents a novel approach for unsupervised learning of depth and ego-motion estimation from monocular videos using a deeper understanding of photometric errors. These errors arise due to intrinsic camera parameters and extrinsic motion in successive frames within monocular video sequences, rather than other techniques such as stereo matching or structured lighting which require extra sensors. The authors propose training two separate neural networks simultaneously; one that estimates pixel intensities in new views while keeping the other fixed, and the other jointly computes depth mappings and camera movements utilising a new loss function named the Deeper into PhotoMetric Error (DiPE) Loss. Experiments showed improved performance compared with existing methods on both visual inspection and industry accepted benchmarks, resulting in enhanced overall accuracy, robustness and adaptability. Ultimately, open sharing of their developed algorithms encourages further progress in computer vision technology advancements.",1
"Convolutional sparse representation (CSR), shift-invariant model for inverse problems, has gained much attention in the fields of signal/image processing, machine learning and computer vision. The most challenging problems in CSR implies the minimization of a composite function of the form $min_x \sum_i f_i(x) + g(x)$, where a direct and low-cost solution can be difficult to achieve. However, it has been reported that semi-distributed formulations such as ADMM consensus can provide important computational benefits. In the present work, we derive and detail a thorough theoretical analysis of an efficient consensus algorithm based on proximal gradient (PG) approach. The effectiveness of the proposed algorithm with respect to its ADMM counterpart is primarily assessed in the classic convolutional dictionary learning problem. Furthermore, our consensus method, which is generically structured, can be used to solve other optimization problems, where a sum of convex functions with a regularization term share a single global variable. As an example, the proposed algorithm is also applied to another particular convolutional problem for the anomaly detection task.",0
"This research presents an efficient consensus model based on proximal gradient method (P-GM) for solving convolutional sparse problems in distributed networks. By leveraging P-GM, we develop a novel framework that enables each node to iteratively update their own estimates without requiring any communication or synchronization with other nodes until convergence. Our approach allows for parallel processing among nodes, reducing the overall computational time while maintaining accuracy. We evaluate our proposed method through extensive simulation results and compare against state-of-the-art techniques, demonstrating its superiority in terms of both speed and efficiency. Our work contributes to the field by providing a new algorithmic solution for distributively addressing challenging combinatorial optimization problems.",1
"Detecting digital face manipulation has attracted extensive attention due to fake media's potential harms to the public. However, recent advances have been able to reduce the forgery signals to a low magnitude. Decomposition, which reversibly decomposes an image into several constituent elements, is a promising way to highlight the hidden forgery details. In this paper, we consider a face image as the production of the intervention of the underlying 3D geometry and the lighting environment, and decompose it in a computer graphics view. Specifically, by disentangling the face image into 3D shape, common texture, identity texture, ambient light, and direct light, we find the devil lies in the direct light and the identity texture. Based on this observation, we propose to utilize facial detail, which is the combination of direct light and identity texture, as the clue to detect the subtle forgery patterns. Besides, we highlight the manipulated region with a supervised attention mechanism and introduce a two-stream structure to exploit both face image and facial detail together as a multi-modality task. Extensive experiments indicate the effectiveness of the extra features extracted from the facial detail, and our method achieves the state-of-the-art performance.",0
"In recent years, face forgery detection has become increasingly important as digital images and videos have proliferated online. One approach that has shown promise in detecting facial manipulations is through the use of 3D decomposition techniques. This paper presents a methodology for detecting facial manipulations using 3D decomposition, which involves breaking down an image into different layers based on their depth information. By analyzing these layers, our algorithm can identify inconsistencies and anomalies that may indicate tampering. We evaluate our method using several benchmark datasets and demonstrate its effectiveness in accurately identifying altered regions within images. Our results show that 3D decomposition is a promising technique for improving the accuracy of face forgery detection algorithms. Overall, this research contributes to the growing field of multimedia security and authentication, where reliable methods for verifying the integrity of digital media content are crucial.",1
"Adversarial examples are a widely studied phenomenon in machine learning models. While most of the attention has been focused on neural networks, other practical models also suffer from this issue. In this work, we propose an algorithm for evaluating the adversarial robustness of $k$-nearest neighbor classification, i.e., finding a minimum-norm adversarial example. Diverging from previous proposals, we take a geometric approach by performing a search that expands outwards from a given input point. On a high level, the search radius expands to the nearby Voronoi cells until we find a cell that classifies differently from the input point. To scale the algorithm to a large $k$, we introduce approximation steps that find perturbations with smaller norm, compared to the baselines, in a variety of datasets. Furthermore, we analyze the structural properties of a dataset where our approach outperforms the competition.",0
"Machine learning algorithms rely heavily on data to make predictions and classifications. One such algorithm is $k$-nearest neighbor ($k$NN) which works by finding the distance between samples and then assigning them a label based off their closest neighbors. However, there have been concerns regarding the robustness of these models as they can easily fall prey to adversarial examples, leading to incorrect predictions. We present here a method to generate adversarial examples that utilizes higher order Voronoi diagrams (HOVDs), allowing us to create new data points that resemble the training set but belong to no known classification cluster. Our experiments show that HOVD adversaries significantly decrease the accuracy of $k$NN classifiers across multiple datasets. These results suggest that HOVDs provide a powerful tool to evaluate the vulnerability of machine learning systems to adversarial attacks. Additionally we propose a defense strategy for improving the detection rate by adding perturbations to the original input during testing.",1
"Determinantal point processes (DPPs) have received significant attention as an elegant probabilistic model for discrete subset selection. Most prior work on DPP learning focuses on maximum likelihood estimation (MLE). While efficient and scalable, MLE approaches do not leverage any subset similarity information and may fail to recover the true generative distribution of discrete data. In this work, by deriving a differentiable relaxation of a DPP sampling algorithm, we present a novel approach for learning DPPs that minimizes the Wasserstein distance between the model and data composed of observed subsets. Through an evaluation on a real-world dataset, we show that our Wasserstein learning approach provides significantly improved predictive performance on a generative task compared to DPPs trained using MLE.",0
"Determinantal point processes (DPPs) are probabilistic models that have been applied to diverse problems such as image co-localization prediction and feature selection from data. They have recently gained attention due to their mathematical elegance and interpretable statistical properties. DPPs constitute a general family of repulsive point process models where any two points interact by a function of their pairwise distances, known as a kernel. This study introduces a new method called Wasserstein learning (WL), which exploits optimal transport theory and solves an optimization problem under constraints that encode prior knowledge on desirable statistics. Specifically, we consider the linear program defined over kernels, termed the Earth Moverâ€™s Distance (EMD). We find solutions in closed form analytically, while in practice our efficient algorithm only requires solving small scale linear programs efficiently solvable using packages like CBC or PATH. Our WL model has some unique advantages compared to previous methods: firstly, it enables us to learn more flexible Euclidean distance functions than those commonly used; secondly, it accommodates nonlinear relationships within kernels including multi-quadratic terms for certain types of molecules and graphs beyond nearest neighbors, e.g., Jainâ€™s quadratic distance. Finally, it provides explicit interpretation of how different interactions contribute to the overall energy landscape underlying DPPs. We evaluate our model on synthetic datasets and demonstrate improved performance over existing approaches in real applications related to protein structure determination and feature detection in medical imaging tasks. In summary, we show that the proposed Wasserstein learning framework can effectively incorporate additional priori knowledge into the machine learning pipeline of determinantala point processes, thus improving t",1
"Defocus blur always occurred in photos when people take photos by Digital Single Lens Reflex Camera(DSLR), giving salient region and aesthetic pleasure. Defocus blur Detection aims to separate the out-of-focus and depth-of-field areas in photos, which is an important work in computer vision. Current works for defocus blur detection mainly focus on the designing of networks, the optimizing of the loss function, and the application of multi-stream strategy, meanwhile, these works do not pay attention to the shortage of training data. In this work, to address the above data-shortage problem, we turn to rethink the relationship between two tasks: defocus blur detection and salient region detection. In an image with bokeh effect, it is obvious that the salient region and the depth-of-field area overlap in most cases. So we first train our network on the salient region detection tasks, then transfer the pre-trained model to the defocus blur detection tasks. Besides, we propose a novel network for defocus blur detection. Experiments show that our transfer strategy works well on many current models, and demonstrate the superiority of our network.",0
"This paper proposes a method called defocus blur detection (DBD) that can detect defocus blur from a single image by utilizing salient region detection prior. DBD consists of four main steps: preprocessing, generating a low-resolution (LR) version of the input image, performing salient object detection on the LR image, and finally detecting defocus blur based on both the original image and the LR image. Our approach is capable of accurately detecting defocus blur even under complex backgrounds, as shown through extensive experiments on real-world images.  This paper presents a novel method for defocus blur detection using salient region detection prior. We propose a pipeline consisting of four stages: preprocessing, generation of a low-resolution version of the input image, salient region detection on the LR image, and detection of defocus blur based on both the original image and the LR image. Through our experiments on real-world images, we show that our approach effectively identifies defocus blur in challenging scenarios, including those with complex backgrounds. \end{code}",1
"Due to balanced accuracy and speed, joint learning detection and ReID-based one-shot models have drawn great attention in multi-object tracking(MOT). However, the differences between the above two tasks in the one-shot tracking paradigm are unconsciously overlooked, leading to inferior performance than the two-stage methods. In this paper, we dissect the reasoning process of the aforementioned two tasks. Our analysis reveals that the competition of them inevitably hurts the learning of task-dependent representations, which further impedes the tracking performance. To remedy this issue, we propose a novel cross-correlation network that can effectively impel the separate branches to learn task-dependent representations. Furthermore, we introduce a scale-aware attention network that learns discriminative embeddings to improve the ReID capability. We integrate the delicately designed networks into a one-shot online MOT system, dubbed CSTrack. Without bells and whistles, our model achieves new state-of-the-art performances on MOT16 and MOT17. Our code is released at https://github.com/JudasDie/SOTS.",0
"Multi-object tracking (MOT) involves identifying and following objects across video frames. Two main methods exist: tracklet association using detection results (detection based MOT), or using ReID algorithms that match objects over time by extracting features from full image patches (ReID-based). We compare them on four datasets; iLIDS-VID, PRID2011, VOT2017, and UAVDT. Detection-based outperforms on iLIDS and PRID2011 datasets but underperforms on others while ReID performs worse overall. However, when combined into one model (detectReID), we see significant improvements over both standalone approaches, proving superiority of integration. This presents new opportunities for more advanced deep learning architectures to learn stronger feature representations that can facilitate accurate identification of objects over short durations between frames. Moreover, detectReID model shows greater promise as we move towards surveillance scenarios involving larger camera networks where objects may disappear from view temporarily before reappearing later. Our work paves way forward for future research towards better realtime object tracking models tailored specifically for these requirements.",1
"Objective: Epilepsy is one of the most prevalent neurological diseases among humans and can lead to severe brain injuries, strokes, and brain tumors. Early detection of seizures can help to mitigate injuries, and can be used to aid the treatment of patients with epilepsy. The purpose of a seizure prediction system is to successfully identify the pre-ictal brain stage, which occurs before a seizure event. Patient-independent seizure prediction models are designed to offer accurate performance across multiple subjects within a dataset, and have been identified as a real-world solution to the seizure prediction problem. However, little attention has been given for designing such models to adapt to the high inter-subject variability in EEG data. Methods: We propose two patient-independent deep learning architectures with different learning strategies that can learn a global function utilizing data from multiple subjects. Results: Proposed models achieve state-of-the-art performance for seizure prediction on the CHB-MIT-EEG dataset, demonstrating 88.81% and 91.54% accuracy respectively. Conclusions: The Siamese model trained on the proposed learning strategy is able to learn patterns related to patient variations in data while predicting seizures. Significance: Our models show superior performance for patient-independent seizure prediction, and the same architecture can be used as a patient-specific classifier after model adaptation. We are the first study that employs model interpretation to understand classifier behavior for the task for seizure prediction, and we also show that the MFCC feature map utilized by our models contains predictive biomarkers related to interictal and pre-ictal brain states.",0
"Title: Epileptic Seizure Prediction Using Deep Learning Models: A Review  Abstract: A deep learning model can predict epileptic seizures based on patient data without requiring explicit input from patients themselves. By analyzing large amounts of data collected from multiple sources, such as electroencephalography (EEG) signals, imaging results, and clinical reports, these models can learn patterns that may indicate seizures before they occur. This approach has the potential to improve seizure prediction accuracy over traditional methods, which rely heavily on subjective patient reporting. In addition, by identifying specific features associated with seizures, these models may provide valuable insights into the underlying mechanisms of epilepsy, leading to more effective treatment options. Overall, deep learning offers a promising new direction for advancing our understanding of epilepsy and improving outcomes for those affected by this disorder.",1
"Multi-task learns multiple tasks, while sharing knowledge and computation among them. However, it suffers from catastrophic forgetting of previous knowledge when learned incrementally without access to the old data. Most existing object detectors are domain-specific and static, while some are learned incrementally but only within a single domain. Training an object detector incrementally across various domains has rarely been explored. In this work, we propose three incremental learning scenarios across various domains and categories for object detection. To mitigate catastrophic forgetting, attentive feature distillation is proposed to leverages both bottom-up and top-down attentions to extract important information for distillation. We then systematically analyze the proposed distillation method in different scenarios. We find out that, contrary to common understanding, domain gaps have smaller negative impact on incremental detection, while category differences are problematic. For the difficult cases, where the domain gaps and especially category differences are large, we explore three different exemplar sampling methods and show the proposed adaptive sampling method is effective to select diverse and informative samples from entire datasets, to further prevent forgetting. Experimental results show that we achieve the significant improvement in three different scenarios across seven object detection benchmark datasets.",0
"""Object detection tasks can require large amounts of data and computing power, which is both time-consuming and expensive."" You should try using Google colab to speed up training as well as save money on hardware costs! But I digress... Do you want me to write an abstrac",1
"Neuro-symbolic representations have proved effective in learning structure information in vision and language. In this paper, we propose a new model architecture for learning multi-modal neuro-symbolic representations for video captioning. Our approach uses a dictionary learning-based method of learning relations between videos and their paired text descriptions. We refer to these relations as relative roles and leverage them to make each token role-aware using attention. This results in a more structured and interpretable architecture that incorporates modality-specific inductive biases for the captioning task. Intuitively, the model is able to learn spatial, temporal, and cross-modal relations in a given pair of video and text. The disentanglement achieved by our proposal gives the model more capacity to capture multi-modal structures which result in captions with higher quality for videos. Our experiments on two established video captioning datasets verifies the effectiveness of the proposed approach based on automatic metrics. We further conduct a human evaluation to measure the grounding and relevance of the generated captions and observe consistent improvement for the proposed model. The codes and trained models can be found at https://github.com/hassanhub/R3Transformer",0
"In recent years there has been great interest in developing models that can perform tasks involving both visual representation (e.g., images) and natural language processing (NLP). One such task is video captioning, which involves generating descriptions of videos using NLP techniques. Traditionally, video captioning methods have relied on either visual or textual representations alone, but recently researchers have begun exploring hybrid neuro-symbolic representations that combine elements from both domains. These approaches offer advantages over purely neural or symbolic representations by leveraging inductive biases from both fields to better capture key characteristics of videos and generate more accurate captions. However, little attention has been given specifically to investigating how these inductive biases impact performance on different aspects of the task. This work seeks to fill that gap by evaluating several variants of neuro-symbolic models for video captioning using human annotations as well as automatic metrics commonly used in the field. Our results show that while all models improve upon baseline architectures, certain models outperform others depending on the specific characteristics we examine. We conclude that incorporating appropriate inductive biases from vision and language can significantly enhance performance across multiple dimensions, highlighting the promise of hybrid representations for future generative systems operating in multimodal settings.",1
"Robust object tracking requires knowledge of tracked objects' appearance, motion and their evolution over time. Although motion provides distinctive and complementary information especially for fast moving objects, most of the recent tracking architectures primarily focus on the objects' appearance information. In this paper, we propose a two-stream deep neural network tracker that uses both spatial and temporal features. Our architecture is developed over ATOM tracker and contains two backbones: (i) 2D-CNN network to capture appearance features and (ii) 3D-CNN network to capture motion features. The features returned by the two networks are then fused with attention based Feature Aggregation Module (FAM). Since the whole architecture is unified, it can be trained end-to-end. The experimental results show that the proposed tracker TRAT (TRacking by ATtention) achieves state-of-the-art performance on most of the benchmarks and it significantly outperforms the baseline ATOM tracker.",0
"""TRAT: Tracking by Attention using Spatio-Temporal Features"" is a novel method for tracking objects in videos that utilizes attention mechanisms and spatio-temporal features to improve tracking accuracy. In recent years, there has been significant progress in developing object trackers that can effectively handle occlusions, scale changes, and other challenges encountered in real-world scenarios. However, many existing methods rely on handcrafted features which may fail to capture subtle appearance variations in video frames, leading to tracking errors. To address this limitation, our proposed approach uses deep neural networks to learn more robust representations that capture both spatial and temporal dependencies in the data. We incorporate attention modules into these models to focus on regions of interest while minimizing computational cost. Our experimental evaluation demonstrates that TRAT outperforms several state-of-the-art trackers across multiple benchmark datasets, achieving higher precision and recall rates while requiring fewer parameters. These results suggest that our new method represents a promising step forward in advancing object tracking technology. Overall, we believe that TRAT holds great potential for applications such as autonomous driving, surveillance, and robotics where accurate object tracking is critical.",1
"Learning depth and ego-motion from unlabeled videos via self-supervision from epipolar projection can improve the robustness and accuracy of the 3D perception and localization of vision-based robots. However, the rigid projection computed by ego-motion cannot represent all scene points, such as points on moving objects, leading to false guidance in these regions. To address this problem, we propose an Attentional Separation-and-Aggregation Network (ASANet), which can learn to distinguish and extract the scene's static and dynamic characteristics via the attention mechanism. We further propose a novel MotionNet with an ASANet as the encoder, followed by two separate decoders, to estimate the camera's ego-motion and the scene's dynamic motion field. Then, we introduce an auto-selecting approach to detect the moving objects for dynamic-aware learning automatically. Empirical experiments demonstrate that our method can achieve the state-of-the-art performance on the KITTI benchmark.",0
"In this paper, we propose a novel approach for self-supervised depth-pose learning in dynamic scenes using attentional separation-and-aggregation networks (ASAN). This method addresses two key challenges in scene understanding: 1) the difficulty in accurately estimating depth and camera poses in highly dynamic environments, and 2) the lack of labeled data available for training. Our proposed ASAN architecture utilizes a combination of feature attention modules and spatial pyramid pooling to achieve robustness to occlusions and viewpoint changes. We show that our method outperforms state-of-the-art techniques on benchmark datasets and demonstrate its effectiveness through comprehensive experimental evaluations. Additionally, we provide detailed ablation studies to highlight the contributions of each component in our model. Overall, our work advances the field of computer vision by providing a powerful new tool for robustly predicting depth and camera poses in complex dynamic scenes.",1
"End-to-end Object Detection with Transformer (DETR)proposes to perform object detection with Transformer and achieve comparable performance with two-stage object detection like Faster-RCNN. However, DETR needs huge computational resources for training and inference due to the high-resolution spatial input. In this paper, a novel variant of transformer named Adaptive Clustering Transformer(ACT) has been proposed to reduce the computation cost for high-resolution input. ACT cluster the query features adaptively using Locality Sensitive Hashing (LSH) and ap-proximate the query-key interaction using the prototype-key interaction. ACT can reduce the quadratic O(N2) complexity inside self-attention into O(NK) where K is the number of prototypes in each layer. ACT can be a drop-in module replacing the original self-attention module without any training. ACT achieves a good balance between accuracy and computation cost (FLOPs). The code is available as supplementary for the ease of experiment replication and verification.",0
"In recent years, object detection has emerged as one of the most important subfields within computer vision. With advancements in deep learning technologies such as convolutional neural networks (CNNs), it has become possible to train complex models capable of accurately detecting objects in images and videos. However, these approaches often suffer from limitations related to computational complexity and scalability. Therefore, there remains a need for efficient and effective solutions that can handle diverse scenarios while maintaining high performance. This work addresses this problem by proposing an end-to-end approach based on adaptive clustering transformers (ACT) for real-time object detection. By utilizing the strengths of both CNNs and attention mechanisms, our method achieves state-of-the-art results without sacrificing speed or accuracy. Our experiments demonstrate the effectiveness of our proposed model across multiple datasets, making it suitable for practical applications in areas such as autonomous driving, surveillance, and healthcare. Overall, we believe that our research offers significant contributions towards advancing the field of object detection and empowering cutting-edge artificial intelligence systems.",1
"Salient object detection in complex scenes and environments is a challenging research topic. Most works focus on RGB-based salient object detection, which limits its performance of real-life applications when confronted with adverse conditions such as dark environments and complex backgrounds. Taking advantage of RGB and thermal infrared images becomes a new research direction for detecting salient object in complex scenes recently, as thermal infrared spectrum imaging provides the complementary information and has been applied to many computer vision tasks. However, current research for RGBT salient object detection is limited by the lack of a large-scale dataset and comprehensive benchmark. This work contributes such a RGBT image dataset named VT5000, including 5000 spatially aligned RGBT image pairs with ground truth annotations. VT5000 has 11 challenges collected in different scenes and environments for exploring the robustness of algorithms. With this dataset, we propose a powerful baseline approach, which extracts multi-level features within each modality and aggregates these features of all modalities with the attention mechanism, for accurate RGBT salient object detection. Extensive experiments show that the proposed baseline approach outperforms the state-of-the-art methods on VT5000 dataset and other two public datasets. In addition, we carry out a comprehensive analysis of different algorithms of RGBT salient object detection on VT5000 dataset, and then make several valuable conclusions and provide some potential research directions for RGBT salient object detection.",0
"This research presents the development of a large-scale dataset and benchmark for salient object detection using RGB-Thermal (RGBT) image pairs. The proposed RGBT Saliency benchmark provides a comprehensive evaluation platform for evaluating state-of-the-art algorithms on real-world datasets. We propose a methodology to generate synthetic thermal images from visible light images which ensures that both modalities have corresponding ground truth annotations. Our extensive experiments demonstrate that our benchmark can significantly advance the state-of-the-art in RGBT salient object detection by providing more accurate and robust models. By releasing our dataset and codebase publicly, we aim to stimulate further advances in multi-modal perception for computer vision tasks. Overall, our work represents an important step towards building reliable visual intelligence systems capable of operating in diverse and unconstrained environments.",1
"Myoelectric control is one of the leading areas of research in the field of robotic prosthetics. We present our research in surface electromyography (sEMG) signal classification, where our simple and novel attention-based approach now leads the industry, universally beating more complex, state-of-the-art models. Our novel attention-based model achieves benchmark leading results on multiple industry-standard datasets including 53 finger, wrist, and grasping motions, improving over both sophisticated signal processing and CNN-based approaches. Our strong results with a straightforward model also indicate that sEMG represents a promising avenue for future machine learning research, with applications not only in prosthetics, but also in other important areas, such as diagnosis and prognostication of neurodegenerative diseases, computationally mediated surgeries, and advanced robotic control. We reinforce this suggestion with extensive ablative studies, demonstrating that a neural network can easily extract higher order spatiotemporal features from noisy sEMG data collected by affordable, consumer-grade sensors.",0
"Here is a potential abstract:  In recent years, surface electromyography (sEMG) has emerged as a promising method for recognizing human gestures through sensing muscle activity on the skin. However, existing methods often rely heavily on complex machine learning algorithms and require extensive training datasets, which can limit their accessibility and applicability. In our work, we propose a simple yet effective approach that combines sEMG data with a model of attention to improve gesture recognition accuracy. By focusing on specific regions of interest, our model is able to capture more relevant information from the sensor signals and outperform state-of-the-art methods using only a small dataset. Our results showcase the feasibility of incorporating attention mechanisms into sEMG systems and highlight opportunities for further research in this area. Overall, this work offers a step towards enabling more robust and flexible wearable interfaces for a variety of applications.",1
"Synthetic data generation to improve classification performance (data augmentation) is a well-studied problem. Recently, generative adversarial networks (GAN) have shown superior image data augmentation performance, but their suitability in gesture synthesis has received inadequate attention. Further, GANs prohibitively require simultaneous generator and discriminator network training. We tackle both issues in this work. We first discuss a novel, device-agnostic GAN model for gesture synthesis called DeepGAN. Thereafter, we formulate DeepNAG by introducing a new differentiable loss function based on dynamic time warping and the average Hausdorff distance, which allows us to train DeepGAN's generator without requiring a discriminator. Through evaluations, we compare the utility of DeepGAN and DeepNAG against two alternative techniques for training five recognizers using data augmentation over six datasets. We further investigate the perceived quality of synthesized samples via an Amazon Mechanical Turk user study based on the HYPE benchmark. We find that DeepNAG outperforms DeepGAN in accuracy, training time (up to 17x faster), and realism, thereby opening the door to a new line of research in generator network design and training for gesture synthesis. Our source code is available at https://www.deepnag.com.",0
"""DeepNonAdversarialGestureGeneration"" (DNG) presents a novel technique that utilizes deep learning algorithms to generate natural, non-adversarial human gestures in real-time. Our approach improves upon traditional methods by incorporating advanced techniques such as adversarial training and generative networks to produce high-quality, accurate gestures that resemble those made by actual humans. We demonstrate the effectiveness of our method through comprehensive experiments comparing our results against existing state-of-the art systems and showcasing applications in areas such as virtual reality, gaming, robotics, and animations. This work represents a significant step forward in gesture generation research, paving the way for more realistic, interactive, and immersive experiences across various industries.",1
"Modern lane detection methods have achieved remarkable performances in complex real-world scenarios, but many have issues maintaining real-time efficiency, which is important for autonomous vehicles. In this work, we propose LaneATT: an anchor-based deep lane detection model, which, akin to other generic deep object detectors, uses the anchors for the feature pooling step. Since lanes follow a regular pattern and are highly correlated, we hypothesize that in some cases global information may be crucial to infer their positions, especially in conditions such as occlusion, missing lane markers, and others. Thus, this work proposes a novel anchor-based attention mechanism that aggregates global information. The model was evaluated extensively on three of the most widely used datasets in the literature. The results show that our method outperforms the current state-of-the-art methods showing both higher efficacy and efficiency. Moreover, an ablation study is performed along with a discussion on efficiency trade-off options that are useful in practice.",0
"In today's fast-paced world, self-driving cars have become increasingly important due to their potential to increase safety on roads by eliminating human error. One crucial aspect of developing safe autonomous vehicles is accurate lane detection. This research proposes a novel approach to real-time attention-guided lane detection that utilizes both convolutional neural networks (CNNs) and visual attention mechanisms. Our method achieves state-of-the-art performance while operating at high speeds, making it ideal for use in self-driving car systems. By employing both global contextual information and local details within each region of interest, our model can accurately detect lanes under various conditions, such as different lighting environments and weather conditions. Furthermore, we evaluate the effectiveness of our proposed method through extensive experiments conducted using public datasets and demonstrate promising results in terms of accuracy and speed. Overall, this work presents an effective solution for real-time attention-guided lane detection, which could greatly contribute to enhancing road safety and reducing traffic accidents caused by human errors.",1
"Zero-Shot Action Recognition has attracted attention in the last years and many approaches have been proposed for recognition of objects, events and actions in images and videos. There is a demand for methods that can classify instances from classes that are not present in the training of models, especially in the complex problem of automatic video understanding, since collecting, annotating and labeling videos are difficult and laborious tasks. We have identified that there are many methods available in the literature, however, it is difficult to categorize which techniques can be considered state of the art. Despite the existence of some surveys about zero-shot action recognition in still images and experimental protocol, there is no work focused on videos. Therefore, we present a survey of the methods that comprise techniques to perform visual feature extraction and semantic feature extraction as well to learn the mapping between these features considering specifically zero-shot action recognition in videos. We also provide a complete description of datasets, experiments and protocols, presenting open issues and directions for future work, essential for the development of the computer vision research field.",0
Title should fit on one line.,1
"We propose a novel neural network architecture based on dual quaternions which allow for a compact representation of informations with a main focus on describing rigid body movements. To cover the dynamic behavior inherent to rigid body movements, we propose recurrent architectures in the neural network. To further model the interactions between individual rigid bodies as well as external inputs efficiently, we incorporate a novel attention mechanism employing dual quaternion algebra. The introduced architecture is trainable by means of gradient based algorithms. We apply our approach to a parcel prediction problem where a rigid body with an initial position, orientation, velocity and angular velocity moves through a fixed simulation environment which exhibits rich interactions between the parcel and the boundaries.",0
"This study presents a new method for predicting rigid body dynamics using dual quaternion recurrent neural networks (RNN) with quaternion attention. We aimed to develop a computational model that could accurately simulate the motion of complex systems subject to multiple external forces. To achieve this goal, we used the concept of dual quaternions, which can represent both rotations and translations in three dimensions. Our approach was inspired by recent work on attention mechanisms in machine learning models, but adapted specifically to use quaternions instead of standard vectors as input units. We trained our network on large datasets containing data from experiments involving colliding objects, and evaluated its performance through extensive testing. We found that our dual quaternion RNN significantly outperformed other state-of-the-art methods for simulating rigid body dynamics, achieving high accuracy even under challenging conditions such as contacts and impacts. Overall, our results suggest that our proposed framework has great potential for applications ranging from computer graphics animation to robotic control and autonomous vehicle navigation. Further work remains necessary to explore more fully how this method might be applied across different domains, including nonlinear systems where the equations of motion become coupled. Ultimately, this research offers valuable insights into developing improved algorithms for solving fundamental problems related to motion prediction in physics and engineering.",1
"Adversarial attacks are valuable for providing insights into the blind-spots of deep learning models and help improve their robustness. Existing work on adversarial attacks have mainly focused on static scenes; however, it remains unclear whether such attacks are effective against embodied agents, which could navigate and interact with a dynamic environment. In this work, we take the first step to study adversarial attacks for embodied agents. In particular, we generate spatiotemporal perturbations to form 3D adversarial examples, which exploit the interaction history in both the temporal and spatial dimensions. Regarding the temporal dimension, since agents make predictions based on historical observations, we develop a trajectory attention module to explore scene view contributions, which further help localize 3D objects appeared with the highest stimuli. By conciliating with clues from the temporal dimension, along the spatial dimension, we adversarially perturb the physical properties (e.g., texture and 3D shape) of the contextual objects that appeared in the most important scene views. Extensive experiments on the EQA-v1 dataset for several embodied tasks in both the white-box and black-box settings have been conducted, which demonstrate that our perturbations have strong attack and generalization abilities.",0
"How about this?  Abstract: In this paper, we investigate spatiotemporal attacks on embodied agents operating in complex environments. We analyze the impact of such attacks on agent performance and propose countermeasures against them. Our study shows that even small modifications to environmental conditions can have significant consequences for the behavior and decision making of embodied agents. Furthermore, our results demonstrate that robustness against these types of perturbations requires careful consideration of both space and time factors in designing resilient systems. Our work contributes to the growing research on adversarial examples in machine learning by providing insights into the challenges faced by robots in real-world settings.",1
"Visual odometry networks commonly use pretrained optical flow networks in order to derive the ego-motion between consecutive frames. The features extracted by these networks represent the motion of all the pixels between frames. However, due to the existence of dynamic objects and texture-less surfaces in the scene, the motion information for every image region might not be reliable for inferring odometry due to the ineffectiveness of dynamic objects in derivation of the incremental changes in position. Recent works in this area lack attention mechanisms in their structures to facilitate dynamic reweighing of the feature maps for extracting more refined egomotion information. In this paper, we explore the effectiveness of self-attention in visual odometry. We report qualitative and quantitative results against the SOTA methods. Furthermore, saliency-based studies alongside specially designed experiments are utilized to investigate the effect of self-attention on VO. Our experiments show that using self-attention allows for the extraction of better features while achieving a better odometry performance compared to networks that lack such structures.",0
This work proposes a novel approach to visual odometry that utilizes self attention mechanisms. We present a model architecture and training procedure specifically tailored towards monocular depth estimation from video sequences. Our method achieves state of the art performance on two popular benchmark datasets and provides robustness against motion blur and occlusions. In addition we demonstrate through ablation studies how key components such as image refinement and feature modulation contribute to our final results. Furthermore we showcase a detailed analysis of how different attention heads respond differently under varying conditions such as camera motions and viewpoints which further validate the strengths of using self attention over traditional CNNs for this task. Overall this research shows great promise in advancing the field of computer vision by providing more accurate depth estimations required for robotics automation and other real world applications.,1
"In real-world applications, data do not reflect the ones commonly used for neural networks training, since they are usually few, unlabeled and can be available as a stream. Hence many existing deep learning solutions suffer from a limited range of applications, in particular in the case of online streaming data that evolve over time. To narrow this gap, in this work we introduce a novel and complex setting involving unsupervised meta-continual learning with unbalanced tasks. These tasks are built through a clustering procedure applied to a fitted embedding space. We exploit a meta-learning scheme that simultaneously alleviates catastrophic forgetting and favors the generalization to new tasks. Moreover, to encourage feature reuse during the meta-optimization, we exploit a single inner loop taking advantage of an aggregated representation achieved through the use of a self-attention mechanism. Experimental results on few-shot learning benchmarks show competitive performance even compared to the supervised case. Additionally, we empirically observe that in an unsupervised scenario, the small tasks and the variability in the clusters pooling play a crucial role in the generalization capability of the network. Further, on complex datasets, the exploitation of more clusters than the true number of classes leads to higher results, even compared to the ones obtained with full supervision, suggesting that a predefined partitioning into classes can miss relevant structural information.",0
"In the field of machine learning, continually acquiring new knowledge and skills without the need for explicit supervision is a highly desirable capability. However, current approaches based on meta-learning often struggle with few-shot unsupervised continual learning (FSC), where only limited data is available for fine-tuning models at test time. This paper proposes a novel framework that leverages meta-examples to mitigate these limitations by adapting to new tasks with scarce examples while maintaining previously learned ones. Our approach employs a unique self-paced mechanism that balances forgetting previously learned concepts and promoting rapid adaptation to new examples. Through extensive evaluation, we demonstrate the effectiveness of our proposed method across a range of challenging benchmarks and showcase its superior performance compared to state-of-the-art competitors. These findings suggest that meta-example guided fine-tuning holds great potential as a powerful tool for enabling artificial intelligence agents to continuously learn and improve their abilities over time. --------------------------------------------------------------------------- Please note: I have used key phrases like 'machine learning', 'meta-learning', 'few-shot unsupervised continual learning', 'self-paced mechanism', and 'benchmarks'. Also, I kept the language clear, concise and accessible so that anyone can read and comprehend the importance of your work. Please feel free to modify it according to your needs.",1
"Recent GAN-based image inpainting approaches adopt an average strategy to discriminate the generated image and output a scalar, which inevitably lose the position information of visual artifacts. Moreover, the adversarial loss and reconstruction loss (e.g., l1 loss) are combined with tradeoff weights, which are also difficult to tune. In this paper, we propose a novel detection-based generative framework for image inpainting, which adopts the min-max strategy in an adversarial process. The generator follows an encoder-decoder architecture to fill the missing regions, and the detector using weakly supervised learning localizes the position of artifacts in a pixel-wise manner. Such position information makes the generator pay attention to artifacts and further enhance them. More importantly, we explicitly insert the output of the detector into the reconstruction loss with a weighting criterion, which balances the weight of the adversarial loss and reconstruction loss automatically rather than manual operation. Experiments on multiple public datasets show the superior performance of the proposed framework. The source code is available at https://github.com/Evergrow/GDN_Inpainting.",0
"Automatic image inpainting techniques have recently achieved great progress by using convolutional neural networks (CNNs). Most state-of-the-art methods are based on Fully Convolution Networks (FCN) which can generate high quality results without any post-processing steps such as upscaling or warping the input images. However, FCNs lack the ability to explicitly model object boundaries at pixel level precision, resulting in loss of fine details and blurriness near edges. We introduce a novel method that combines local edge-preserving processing with global semantic guidance to achieve pixel-level accuracy for image inpainting. By utilizing deep edge detection and feature extraction techniques within each layer of our CNN architecture, we ensure proper alignment with object boundaries while minimizing the propagation of errors across layers. Furthermore, we propose a dense regression framework that allows us to predict accurate correspondences directly from the unprocessed input, effectively reducing computational cost compared to traditional approaches like patch-based matching. Our extensive experiments demonstrate superior performance against various baseline models on multiple benchmark datasets, confirming the effectiveness of our proposed approach for automatic image inpainting tasks.",1
"Point cloud analysis is attracting attention from Artificial Intelligence research since it can be widely used in applications such as robotics, Augmented Reality, self-driving. However, it is always challenging due to irregularities, unorderedness, and sparsity. In this article, we propose a novel network named Dense-Resolution Network (DRNet) for point cloud analysis. Our DRNet is designed to learn local point features from the point cloud in different resolutions. In order to learn local point groups more effectively, we present a novel grouping method for local neighborhood searching and an error-minimizing module for capturing local features. In addition to validating the network on widely used point cloud segmentation and classification benchmarks, we also test and visualize the performance of the components. Comparing with other state-of-the-art methods, our network shows superiority on ModelNet40, ShapeNet synthetic and ScanObjectNN real point cloud datasets.",0
"This is a technical research paper that presents the concept of Dense-Resolution Networks (DRN) for the task of point cloud classification and segmentation. DRN is a novel architecture designed to efficiently handle sparse and irregularly sampled data such as LiDAR point clouds by learning features at multiple resolution scales. By leveraging multi-scale feature representations, DRN achieves state-of-the art performance on several benchmark datasets while reducing computational complexity compared to previous approaches. Additionally, DRN utilizes a dynamic pooling operation that adaptively adjusts to local context and further improves accuracy. Overall, this paper demonstrates the effectiveness and efficiency of DRN for addressing challenges associated with processing large-scale 3D datasets. Future work includes extending DRN to other computer vision tasks that involve sparsely sampled data and exploring new applications where these networks can provide significant benefits.",1
"Deep neural models have hitherto achieved significant performances on numerous classification tasks, but meanwhile require sufficient manually annotated data. Since it is extremely time-consuming and expensive to annotate adequate data for each classification task, learning an empirically effective model with generalization on small dataset has received increased attention. Existing efforts mainly focus on transferring task-relevant knowledge from other similar data to tackle the issue. These approaches have yielded remarkable improvements, yet neglecting the fact that the task-irrelevant features could bring out massive negative transfer effects. To date, no large-scale studies have been performed to investigate the impact of task-irrelevant features, let alone the utilization of this kind of features. In this paper, we firstly propose Task-Irrelevant Transfer Learning (TIRTL) to exploit task-irrelevant features, which mainly are extracted from task-irrelevant labels. Particularly, we suppress the expression of task-irrelevant information and facilitate the learning process of classification. We also provide a theoretical explanation of our method. In addition, TIRTL does not conflict with those that have previously exploited task-relevant knowledge and can be well combined to enable the simultaneous utilization of task-relevant and task-irrelevant features for the first time. In order to verify the effectiveness of our theory and method, we conduct extensive experiments on facial expression recognition and digit recognition tasks. Our source code will be also available in the future for reproducibility.",0
"Artificial intelligence (AI) is capable of achieving remarkable results across many fields by learning from large amounts of data. In traditional machine learning paradigms, labels are task-specific and usually come at a high cost. However, there exists vast amounts of unlabelled or weakly labelled data that can still contain valuable information relevant to multiple tasks. Our research proposes transfer learning as an approach towards all-around knowledge transfer, allowing models to leverage existing knowledge acquired through non-task specific training for improved performance on new tasks. We present empirical evidence that demonstrates the effectiveness of our proposed methodology using multiple datasets. Results show significant improvement in model accuracy compared to previous approaches, making it possible to learn complex functions directly from unstructured data such as images and text without sacrificing speed or efficiency. We conclude by discussing future directions of this work and potential applications in real world scenarios. -----  This study introduces a novel approach to artificial intelligence (AI), one which focuses on leveraging existing knowledge gained from non-task specific training to improve performance across multiple tasks. The authors propose transfer learning as a means towards all-around knowledge transfer. By utilizing unlabelled or weakly labelled data, their method shows great promise in improving model accuracy over traditional methods while maintaining speed and efficiency. Empirical analysis conducted on several datasets supported these findings, proving the feasibility of their approach in solving increasingly complex problems. Ultimately, the work has major implications for both academia and industry alike, enabling further advancements in areas like image and natural language processing, among others. Further experimentation and development should continue in exploring th",1
"Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant. We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.",0
"In this paper, we present a new approach called Recurrent Independent Mechanisms (MIMIC) for training deep generative models on tabular data. MIMIC learns to model complex dependencies among variables by repeatedly applying simple mechanisms that each perform one step of reasoning. This enables efficient and effective learning even when dealing with high-dimensional tables containing millions of entries. We show through extensive experiments that MIMIC achieves state-of-the-art performance on several benchmark datasets, outperforming previous methods by significant margins. Our analysis suggests that MIMICâ€™s superior performance stems from its ability to learn more accurate representations and capture longer-range dependencies. Overall, our work advances the field of deep generative modeling and demonstrates the effectiveness of recurrent independent mechanisms for training machine intelligence systems.",1
"Despite excellent progress has been made, the performance on action recognition still heavily relies on specific datasets, which are difficult to extend new action classes due to labor-intensive labeling. Moreover, the high diversity in Spatio-temporal appearance requires robust and representative action feature aggregation and attention. To address the above issues, we focus on atomic actions and propose a novel model for semi-supervised few-shot atomic action recognition. Our model features unsupervised and contrastive video embedding, loose action alignment, multi-head feature comparison, and attention-based aggregation, together of which enables action recognition with only a few training examples through extracting more representative features and allowing flexibility in spatial and temporal alignment and variations in the action. Experiments show that our model can attain high accuracy on representative atomic action datasets outperforming their respective state-of-the-art classification accuracy in full supervision setting.",0
"This paper presents a semi-supervised method for few-shot action recognition that involves learning from both labeled data and unlabeled video data. By leveraging both types of data sources, we aim to improve the robustness and generalization performance of our approach compared to fully supervised methods. Our proposed method includes two main components: (1) pre-training on large amounts of unlabeled video data using self-supervised pretext tasks, followed by fine-tuning on small amounts of labeled data; and (2) utilizing an ensemble model to combine predictions from different models trained on different subsets of the available data. Experimental results demonstrate significant improvements over baseline approaches and show promising potential for real-world applications where labeled training data may be limited or expensive to obtain.",1
"In this paper, a domain adaptation based technique for recognizing the emotions in images containing facial, non-facial, and non-human components has been proposed. We have also proposed a novel technique to explain the proposed system's predictions in terms of Intersection Score. Image emotion recognition is useful for graphics, gaming, animation, entertainment, and cinematography. However, well-labeled large scale datasets and pre-trained models are not available for image emotion recognition. To overcome this challenge, we have proposed a deep learning approach based on an attentional convolutional network that adapts pre-trained facial expression recognition models. It detects the visual features of an image and performs emotion classification based on them. The experiments have been performed on the Flickr image dataset, and the images have been classified in 'angry,' 'happy,' 'sad,' and 'neutral' emotion classes. The proposed system has demonstrated better performance than the benchmark results with an accuracy of 63.87% for image emotion recognition. We have also analyzed the embedding plots for various emotion classes to explain the proposed system's predictions.",0
"This paper presents a new technique called ""Domain adaptation"" that allows pre-trained facial expression recognition models to recognize emotions from images without any fine-tuning. The proposed method adapts to different domains by learning domain-specific feature representations that can extract meaningful features to accurately predict emotions. Experiments on three public datasets show that the proposed approach outperforms state-of-the-art methods while requiring fewer resources compared to traditional transfer learning techniques. Overall, our work demonstrates that domain adaptation is a promising direction to enhance image emotion recognition performance using pre-trained models.",1
"In 2D+3D facial expression recognition (FER), existing methods generate multi-view geometry maps to enhance the depth feature representation. However, this may introduce false estimations due to local plane fitting from incomplete point clouds. In this paper, we propose a novel Map Generation technique from the viewpoint of information theory, to boost the slight 3D expression differences from strong personality variations. First, we examine the HDR depth data to extract the discriminative dynamic range $r_{dis}$, and maximize the entropy of $r_{dis}$ to a global optimum. Then, to prevent the large deformation caused by over-enhancement, we introduce a depth distortion constraint and reduce the complexity from $O(KN^2)$ to $O(KN\tau)$. Furthermore, the constrained optimization is modeled as a $K$-edges maximum weight path problem in a directed acyclic graph, and we solve it efficiently via dynamic programming. Finally, we also design an efficient Facial Attention structure to automatically locate subtle discriminative facial parts for multi-scale learning, and train it with a proposed loss function $\mathcal{L}_{FA}$ without any facial landmarks. Experimental results on different datasets show that the proposed method is effective and outperforms the state-of-the-art 2D+3D FER methods in both FER accuracy and the output entropy of the generated maps.",0
"This paper presents a novel facial expression recognition method that combines 2D and 3D features with dynamic range enhancement and multi-scale learning. We introduce a discriminative feature extraction approach based on local binary patterns (LBP) for both 2D grayscale images and 3D depth maps. To further improve performance, we propose dynamic range enhancement by adaptively adjusting the threshold value of LBP coding according to the intensity distribution of the input image. Moreover, we employ multi-scale representation learning to capture different levels of detail and reduce overfitting. Extensive experiments on several benchmark datasets show that our proposed method significantly outperforms state-of-the-art methods, demonstrating its effectiveness in facial expression recognition under varying conditions. Our results provide valuable insights into the role of different cues in facial expression analysis and highlight promising directions for future research in this field.",1
"Single Image Super-Resolution (SISR) is one of the low-level computer vision problems that has received increased attention in the last few years. Current approaches are primarily based on harnessing the power of deep learning models and optimization techniques to reverse the degradation model. Owing to its hardness, isotropic blurring or Gaussians with small anisotropic deformations have been mainly considered. Here, we widen this scenario by including large non-Gaussian blurs that arise in real camera movements. Our approach leverages the degradation model and proposes a new formulation of the Convolutional Neural Network (CNN) cascade model, where each network sub-module is constrained to solve a specific degradation: deblurring or upsampling. A new densely connected CNN-architecture is proposed where the output of each sub-module is restricted using some external knowledge to focus it on its specific task. As far we know this use of domain-knowledge to module-level is a novelty in SISR. To fit the finest model, a final sub-module takes care of the residual errors propagated by the previous sub-modules. We check our model with three state of the art (SOTA) datasets in SISR and compare the results with the SOTA models. The results show that our model is the only one able to manage our wider set of deformations. Furthermore, our model overcomes all current SOTA methods for a standard set of deformations. In terms of computational load, our model also improves on the two closest competitors in terms of efficiency. Although the approach is non-blind and requires an estimation of the blur kernel, it shows robustness to blur kernel estimation errors, making it a good alternative to blind models.",0
"This paper proposes a new approach for single image super-resolution using cascading models that can effectively handle multiple degradations. Traditional single image super-resolution methods tend to focus on a specific type of degradation, such as blurring or downsampling. However, real world images often suffer from more complex degradation processes which make super-resolution challenging. Our proposed method addresses this challenge by implementing a fast and robust cascade model designed to improve upon previous state-of-the-art techniques. We demonstrate through extensive experiments that our method outperforms existing approaches in terms of both visual quality and quantitative metrics, making it well suited for applications requiring high resolution images, such as surveillance, medical imaging, and remote sensing. In summary, we present a novel solution for efficient and effective super-resolution under multiple degradations.",1
"Optimization methods (optimizers) get special attention for the efficient training of neural networks in the field of deep learning. In literature there are many papers that compare neural models trained with the use of different optimizers. Each paper demonstrates that for a particular problem an optimizer is better than the others but as the problem changes this type of result is no longer valid and we have to start from scratch. In our paper we propose to use the combination of two very different optimizers but when used simultaneously they can overcome the performances of the single optimizers in very different problems. We propose a new optimizer called MAS (Mixing ADAM and SGD) that integrates SGD and ADAM simultaneously by weighing the contributions of both through the assignment of constant weights. Rather than trying to improve SGD or ADAM we exploit both at the same time by taking the best of both. We have conducted several experiments on images and text document classification, using various CNNs, and we demonstrated by experiments that the proposed MAS optimizer produces better performance than the single SGD or ADAM optimizers. The source code and all the results of the experiments are available online at the following link https://gitlab.com/nicolalandro/multi\_optimizer",0
"In recent years, deep learning has emerged as one of the most promising approaches for artificial intelligence tasks such as image classification, speech recognition, and natural language processing. Two popular optimization methods used in deep learning are Adaptive Moment Estimation (ADAM) and Stochastic Gradient Descent (SGD). While both methods have been shown to achieve good results on many problems, there remains debate over which method should be preferred and under what circumstances. This study aimed to investigate whether combining these two methods could improve performance across a range of deep learning benchmarks. We propose a hybrid algorithm that uses ADAM to update model parameters during the early stages of training and switches to SGD later in training to address issues related to converging too quickly or slowly. Our experiments show that using our proposed mixed method consistently outperforms using either ADAM or SGD alone on several deep learning datasets, demonstrating the potential benefits of leveraging multiple optimizers in practice. Overall, our findings highlight the importance of considering different optimization strategies when working with deep neural networks and suggest new directions for future research in this area.",1
"Optimal decision-making in social settings is often based on forecasts from time series (TS) data. Recently, several approaches using deep neural networks (DNNs) such as recurrent neural networks (RNNs) have been introduced for TS forecasting and have shown promising results. However, the applicability of these approaches is being questioned for TS settings where there is a lack of quality training data and where the TS to forecast exhibit complex behaviors. Examples of such settings include financial TS forecasting, where producing accurate and consistent long-term forecasts is notoriously difficult. In this work, we investigate whether DNN-based models can be used to forecast these TS conjointly by learning a joint representation of the series instead of computing the forecast from the raw time-series representations. To this end, we make use of the dynamic factor graph (DFG) to build a multivariate autoregressive model. We investigate a common limitation of RNNs that rely on the DFG framework and propose a novel variable-length attention-based mechanism (ACTM) to address it. With ACTM, it is possible to vary the autoregressive order of a TS model over time and model a larger set of probability distributions than with previous approaches. Using this mechanism, we propose a self-supervised DNN architecture for multivariate TS forecasting that learns and takes advantage of the relationships between them. We test our model on two datasets covering 19 years of investment fund activities. Our experimental results show that the proposed approach significantly outperforms typical DNN-based and statistical models at forecasting the 21-day price trajectory. We point out how improving forecasting accuracy and knowing which forecaster to use can improve the excess return of autonomous trading strategies.",0
"In recent years, there has been increasing interest in using artificial intelligence (AI) techniques such as neural networks to predict financial time series. One key challenge facing these models is their ability to adapt to changing market conditions over time. To address this issue, we propose a new model called spatiotemporal adaptive neural network (STANN).  The STANN architecture integrates temporal and spatial components into a single framework that can learn both short- and long-range dependencies simultaneously. The model utilizes convolutional layers to capture complex patterns from raw data, while recurrent units handle sequential information within each slice. This allows us to accurately extract features from high-dimensional datasets without requiring large amounts of training data.  Experiments on multiple real world financial datasets show that our proposed method outperforms several state-of-the-art methods in terms of accuracy and efficiency. These results demonstrate the effectiveness of STANN in capturing spatiotemporal dynamics present in financial data. Our study highlights the potential of deep learning approaches for financial prediction tasks and opens up further research directions towards more effective solutions.",1
"Compressed sensing (CS) is a challenging problem in image processing due to reconstructing an almost complete image from a limited measurement. To achieve fast and accurate CS reconstruction, we synthesize the advantages of two well-known methods (neural network and optimization algorithm) to propose a novel optimization inspired neural network which dubbed AMP-Net. AMP-Net realizes the fusion of the Approximate Message Passing (AMP) algorithm and neural network. All of its parameters are learned automatically. Furthermore, we propose an AMPA-Net which uses three attention networks to improve the representation ability of AMP-Net. Finally, We demonstrate the effectiveness of AMP-Net and AMPA-Net on four standard CS reconstruction benchmark data sets. Our code is available on https://github.com/puallee/AMPA-Net.",0
"Artificial intelligence (AI) plays a crucial role in deep compressed sensing by enabling efficient recovery algorithms that can accurately estimate sparse signals from highly incomplete data. In recent years, attention mechanisms have been widely used in natural language processing tasks such as machine translation and text summarization due to their ability to effectively select relevant features and suppress irrelevant ones. However, these attention models often suffer from high computational complexity and numerical instability issues. To address these limitations, we propose AMPA-Net, which leverages optimization techniques inspired by compressive sensing theory to optimize attention mechanisms. By formulating the attention computation as a quadratic program, AMPA-Net significantly reduces the search space while ensuring global optimality. Extensive experiments on standard image reconstruction benchmarks demonstrate that our approach achieves superior performance compared to existing state-of-the-art methods in terms of both accuracy and efficiency. This work provides new insights into the design of attention networks for real-world applications, paving the way for improved deep compressed sensing algorithms.",1
"Skeleton-based action recognition has attracted research attentions in recent years. One common drawback in currently popular skeleton-based human action recognition methods is that the sparse skeleton information alone is not sufficient to fully characterize human motion. This limitation makes several existing methods incapable of correctly classifying action categories which exhibit only subtle motion differences. In this paper, we propose a novel framework for employing human pose skeleton and joint-centered light-weight information jointly in a two-stream graph convolutional network, namely, JOLO-GCN. Specifically, we use Joint-aligned optical Flow Patches (JFP) to capture the local subtle motion around each joint as the pivotal joint-centered visual information. Compared to the pure skeleton-based baseline, this hybrid scheme effectively boosts performance, while keeping the computational and memory overheads low. Experiments on the NTU RGB+D, NTU RGB+D 120, and the Kinetics-Skeleton dataset demonstrate clear accuracy improvements attained by the proposed method over the state-of-the-art skeleton-based methods.",0
"This should summarize the main content of the paper without including any technical terms that may confuse readers who have little knowledge on graph convolutional networks (GCNs). Make sure you mention at least one key insight from the paper and your future work plans if applicable.  The proposed approach, termed as JOLO-GCN, addresses these issues by exploiting joint-centered and skeleton-based features while employing lightweight GCN techniques. Specifically, we introduce edge sampling strategy that dynamically selects meaningful edges based on spatial distances among body parts. Our method is validated through extensive experiments across six benchmark datasets, achieving state-of-the-art performance on five out of them. We plan to extend our research directions towards more robust representations using cross-dataset generalization ability and domain adaptability to real-world scenarios.",1
"Recently, convolutional neural networks (CNNs)-based facial landmark detection methods have achieved great success. However, most of existing CNN-based facial landmark detection methods have not attempted to activate multiple correlated facial parts and learn different semantic features from them that they can not accurately model the relationships among the local details and can not fully explore more discriminative and fine semantic features, thus they suffer from partial occlusions and large pose variations. To address these problems, we propose a cross-order cross-semantic deep network (CCDN) to boost the semantic features learning for robust facial landmark detection. Specifically, a cross-order two-squeeze multi-excitation (CTM) module is proposed to introduce the cross-order channel correlations for more discriminative representations learning and multiple attention-specific part activation. Moreover, a novel cross-order cross-semantic (COCS) regularizer is designed to drive the network to learn cross-order cross-semantic features from different activation for facial landmark detection. It is interesting to show that by integrating the CTM module and COCS regularizer, the proposed CCDN can effectively activate and learn more fine and complementary cross-order cross-semantic features to improve the accuracy of facial landmark detection under extremely challenging scenarios. Experimental results on challenging benchmark datasets demonstrate the superiority of our CCDN over state-of-the-art facial landmark detection methods.",0
"Accurate facial landmark detection plays an important role in many computer vision applications such as face recognition, expression analysis, and augmented reality. Current deep learning based methods often use convolutional neural networks (CNNs) trained on small datasets with few labeled samples per identity or limited variations across identities. However, these approaches can suffer from poor generalization performance due to their reliance on limited training data and lack of cross-identity knowledge transfer. In order to address these issues, we propose a new method called Cross-Order Cross-Semantic Deep Network (XOCDN), which exploits both intra-class similarity within each identity variation across different identities. Our approach uses a siamese network architecture that shares weights across multiple streams, allowing our model to extract both semantic features and spatial correspondences between images. We evaluate our method using two publicly available databases, including a challenging database with extreme poses and large occlusions. Experimental results show that XOCDN achieves state-of-the-art accuracy on both databases while outperforming other competitive approaches significantly. Overall, our proposed method is able to handle complex scenarios more robustly than existing approaches.",1
"Modern object detection networks pursuit higher precision on general object detection datasets, at the same time the computation burden is also increasing along with the improvement of precision. Nevertheless, the inference time and precision are both critical to object detection system which needs to be real-time. It is necessary to research precision improvement without extra computation cost. In this work, two modules are proposed to improve detection precision with zero cost, which are focus on FPN and detection head improvement for general object detection networks. We employ the scale attention mechanism to efficiently fuse multi-level feature maps with less parameters, which is called SA-FPN module. Considering the correlation of classification head and regression head, we use sequential head to take the place of widely-used parallel head, which is called Seq-HEAD module. To evaluate the effectiveness, we apply the two modules to some modern state-of-art object detection networks, including anchor-based and anchor-free. Experiment results on coco dataset show that the networks with the two modules can surpass original networks by 1.1 AP and 0.8 AP with zero cost for anchor-based and anchor-free networks, respectively. Code will be available at https://git.io/JTFGl.",0
"There has been significant progress in recent years in the field of general object detection using convolutional neural networks (CNNs). However, designing these models can still be resource-intensive, requiring large amounts of computation, memory, and data storage resources. In this work, we propose several zero cost improvements that can enhance the performance of existing object detection frameworks without increasing their computational requirements. Our approach involves making targeted modifications to the network architecture and training process, such as adding new layers, modifying loss functions, and optimizing hyperparameters. We demonstrate through extensive experiments on popular benchmark datasets that our proposed improvements lead to noticeable gains in accuracy while keeping computational costs constant. Overall, our results highlight the potential for low-cost, high-impact changes to advance the state-of-the-art in object detection research.",1
"Most prior art in visual understanding relies solely on analyzing the ""what"" (e.g., event recognition) and ""where"" (e.g., event localization), which in some cases, fails to describe correct contextual relationships between events or leads to incorrect underlying visual attention. Part of what defines us as human and fundamentally different from machines is our instinct to seek causality behind any association, say an event Y that happened as a direct result of event X. To this end, we propose iPerceive, a framework capable of understanding the ""why"" between events in a video by building a common-sense knowledge base using contextual cues to infer causal relationships between objects in the video. We demonstrate the effectiveness of our technique using the dense video captioning (DVC) and video question answering (VideoQA) tasks. Furthermore, while most prior work in DVC and VideoQA relies solely on visual information, other modalities such as audio and speech are vital for a human observer's perception of an environment. We formulate DVC and VideoQA tasks as machine translation problems that utilize multiple modalities. By evaluating the performance of iPerceive DVC and iPerceive VideoQA on the ActivityNet Captions and TVQA datasets respectively, we show that our approach furthers the state-of-the-art. Code and samples are available at: iperceive.amanchadha.com.",0
"One of the main goals of artificial intelligence (AI) research is developing machines that can perceive and interpret their environment like humans. This includes understanding and generating natural language as well as processing visual data such as images and videos. In recent years, deep learning algorithms have made significant advances in these areas, but still fall short of human performance due to their limited ability to reason and understand context.  To address this issue, we propose a new approach called ""iPerceive"" which combines common sense reasoning with multi-modal dense video captioning and video question answering. We use a large dataset of annotated videos to train our model on both the visual content and the corresponding textual descriptions. Our algorithm then applies commonsense reasoning rules based on knowledge graphs to generate more accurate and detailed explanations of what is happening in each frame of the video. Additionally, we introduce a novel attention mechanism that allows our model to focus on specific regions of interest within each frame, further improving its accuracy.  We evaluate our system using two benchmark datasets for video captioning and question answering tasks, respectively. Results show that our approach significantly outperforms state-of-the-art methods across all metrics. Furthermore, we demonstrate the versatility of our method by applying it to other domains such as image generation and generative question answering.  Our work has important implications for a variety of applications, including surveillance systems, virtual reality, gaming, robotics, and education. With iPerceive, we take a step closer towards building intelligent agents that can truly understand complex situations and communicate effectively with humans.",1
"Hypergraphs have gained increasing attention in the machine learning community lately due to their superiority over graphs in capturing super-dyadic interactions among entities. In this work, we propose a novel approach for the partitioning of k-uniform hypergraphs. Most of the existing methods work by reducing the hypergraph to a graph followed by applying standard graph partitioning algorithms. The reduction step restricts the algorithms to capturing only some weighted pairwise interactions and hence loses essential information about the original hypergraph. We overcome this issue by utilizing the tensor-based representation of hypergraphs, which enables us to capture actual super-dyadic interactions. We prove that the hypergraph to graph reduction is a special case of tensor contraction. We extend the notion of minimum ratio-cut and normalized-cut from graphs to hypergraphs and show the relaxed optimization problem is equivalent to tensor eigenvalue decomposition. This novel formulation also enables us to capture different ways of cutting a hyperedge, unlike the existing reduction approaches. We propose a hypergraph partitioning algorithm inspired from spectral graph theory that can accommodate this notion of hyperedge cuts. We also derive a tighter upper bound on the minimum positive eigenvalue of even-order hypergraph Laplacian tensor in terms of its conductance, which is utilized in the partitioning algorithm to approximate the normalized cut. The efficacy of the proposed method is demonstrated numerically on simple hypergraphs. We also show improvement for the min-cut solution on 2-uniform hypergraphs (graphs) over the standard spectral partitioning algorithm.",0
"Abstract:  A hypergraph partitioning algorithm using tensor eigenvalue decomposition (TEE) is proposed as an efficient method for distributed computing environments. TEE is employed to decompose the hypergraph into smaller sub-hypergraphs while minimizing edge cuts and maximizing balance. Experiments on synthetic data demonstrate that our approach outperforms traditional vertex cutting strategies such as k-way graph partitioning methods on hyperedge weighted graphs. We validate the performance of our algorithm using real datasets from different applications including machine learning problems where we achieve significant speedups in computation time. Overall, this work demonstrates the potential benefits of utilizing tensor techniques within the context of graph partitioning and distributed computing. Keywords: hypergraph partitioning, tensor eigenvalue decomposition, distributed computing, speedup, graph partitioning",1
"Robust perception relies on both bottom-up and top-down signals. Bottom-up signals consist of what's directly observed through sensation. Top-down signals consist of beliefs and expectations based on past experience and short-term memory, such as how the phrase `peanut butter and~...' will be completed. The optimal combination of bottom-up and top-down information remains an open question, but the manner of combination must be dynamic and both context and task dependent. To effectively utilize the wealth of potential top-down information available, and to prevent the cacophony of intermixed signals in a bidirectional architecture, mechanisms are needed to restrict information flow. We explore deep recurrent neural net architectures in which bottom-up and top-down signals are dynamically combined using attention. Modularity of the architecture further restricts the sharing and communication of information. Together, attention and modularity direct information flow, which leads to reliable performance improvements in perceptual and language tasks, and in particular improves robustness to distractions and noisy data. We demonstrate on a variety of benchmarks in language modeling, sequential image classification, video prediction and reinforcement learning that the \emph{bidirectional} information flow can improve results over strong baselines.",0
"In recent years, deep learning has made significant progress in natural language processing tasks due to advancements in recurrent neural networks (RNNs) that can capture contextual dependencies. However, these models still struggle to effectively combine top-down and bottom-up signals while maintaining focus on relevant parts of input sequences, which remains a major challenge. To address this problem, we propose an attention mechanism applied to modular RNN architecture, which enables fine-grained control over the extent to which both types of signals contribute to the overall prediction. Our approach consists of three modules: top-down, bottom-up, and their combination. These modules attend to different aspects of input sequence by predicting appropriate weights that determine how strongly each module influences the output at every time step. Experiments conducted on benchmark datasets demonstrate substantial improvements over baseline RNN architectures without attentional mechanisms, indicating that our model better captures the interplay of top-down and bottom-up signals in human cognitive processes. We discuss theoretical implications and potential extensions of our work to other domains involving multimodal inputs where understanding interactions among multiple sources plays an essential role.",1
"This paper presents a methodology for image classification using Graph Neural Network (GNN) models. We transform the input images into region adjacency graphs (RAGs), in which regions are superpixels and edges connect neighboring superpixels. Our experiments suggest that Graph Attention Networks (GATs), which combine graph convolutions with self-attention mechanisms, outperforms other GNN models. Although raw image classifiers perform better than GATs due to information loss during the RAG generation, our methodology opens an interesting avenue of research on deep learning beyond rectangular-gridded images, such as 360-degree field of view panoramas. Traditional convolutional kernels of current state-of-the-art methods cannot handle panoramas, whereas the adapted superpixel algorithms and the resulting region adjacency graphs can naturally feed a GNN, without topology issues.",0
"Title: ""Superpixel Image Classification with Graph Attention Networks""  Abstract: This work presents a new approach to image classification that utilizes superpixels as well as graph attention networks (GAT). Superpixels are small overlapping regions in images that can capture more contextual information than pixels alone. GATs allow us to effectively learn global representations from local features on graphs such as those constructed using superpixels. We propose a novel method combining these two techniques called SAGAN, which leverages the strengths of both superpixels and GATs for improved image classification performance. We evaluate our method on several benchmark datasets including MNIST, CIFAR-10, and SVHN and show consistent improvement over baseline models on all tasks. Our results demonstrate the effectiveness of using superpixels and GATs together in computer vision applications. Overall, we believe that this work paves the way for further research into hybrid approaches using multiple data representations for image classification.",1
"Deep learning-based medical image segmentation technology aims at automatic recognizing and annotating objects on the medical image. Non-local attention and feature learning by multi-scale methods are widely used to model network, which drives progress in medical image segmentation. However, those attention mechanism methods have weakly non-local receptive fields' strengthened connection for small objects in medical images. Then, the features of important small objects in abstract or coarse feature maps may be deserted, which leads to unsatisfactory performance. Moreover, the existing multi-scale methods only simply focus on different sizes of view, whose sparse multi-scale features collected are not abundant enough for small objects segmentation. In this work, a multi-dimensional attention segmentation model with cascade multi-scale convolution is proposed to predict accurate segmentation for small objects in medical images. As the weight function, multi-dimensional attention modules provide coefficient modification for significant/informative small objects features. Furthermore, The cascade multi-scale convolution modules in each skip-connection path are exploited to capture multi-scale features in different semantic depth. The proposed method is evaluated on three datasets: KiTS19, Pancreas CT of Decathlon-10, and MICCAI 2018 LiTS Challenge, demonstrating better segmentation performances than the state-of-the-art baselines.",0
"In this work we present w-Net, a novel end-to-end dual supervised medical image segmentation model designed to achieve accurate segmentations on both macroscopic (organ) and microscopic (cellular/sub-cellular structures) scales simultaneously using multi-dimensional attention and cascade multi-scale convolutional networks. While several existing works have attempted to incorporate multiple objectives into their models, they either rely heavily on heuristics, predefine explicit relationships among objects, sacrifice overall accuracy due to conflicts between tasks, or address only simple binary segmentations with coarse pixel annotations such as scribbles or bounding boxes. In contrast, our approach fully utilizes dense pixel-wise ground truth maps at different resolutions without making any assumptions about prior object relationships which could hinder generalization ability. Additionally, w-Net is trained end-to-end from scratch, meaning no pretraining of submodules required. Using several benchmark datasets across modalities including MRI and CT, results show that w-Net outperforms competitive state-of-the-art methods by significant margins in terms of overlap metrics while offering more fine-grained predictions over relevant microstructures. To summarize, w-Net demonstrates unprecedented performance via dualsupervision guided multi-dimensional attention mechanism and cascading of multiple receptive fields, making it an attractive tool for numerous biomedical applications including automated diagnosis and surgery planning systems. This research can form basis of future exploration into even richer forms of guidance tailored to specific needs in individual organs and diseases.",1
"Drug repositioning is designed to discover new uses of known drugs, which is an important and efficient method of drug discovery. Researchers only use one certain type of Collaborative Filtering (CF) models for drug repositioning currently, like the neighborhood based approaches which are good at mining the local information contained in few strong drug-disease associations, or the latent factor based models which are effectively capture the global information shared by a majority of drug-disease associations. Few researchers have combined these two types of CF models to derive a hybrid model with the advantages of both of them. Besides, the cold start problem has always been a major challenge in the field of computational drug repositioning, which restricts the inference ability of relevant models. Inspired by the memory network, we propose the Hybrid Attentional Memory Network (HAMN) model, a deep architecture combines two classes of CF model in a nonlinear manner. Firstly, the memory unit and the attention mechanism are combined to generate the neighborhood contribution representation to capture the local structure of few strong drug-disease associations. Then a variant version of the autoencoder is used to extract the latent factor of drugs and diseases to capture the overall information shared by a majority of drug-disease associations. In that process, ancillary information of drugs and diseases can help to alleviate the cold start problem. Finally, in the prediction stage, the neighborhood contribution representation is combined with the drug latent factor and disease latent factor to produce the predicted value. Comprehensive experimental results on two real data sets show that our proposed HAMN model is superior to other comparison models according to the AUC, AUPR and HR indicators.",0
"In recent years, computational drug repurposing has emerged as a promising approach for identifying novel indications for existing drugs. However, accurately predicting potential new uses for drugs remains a significant challenge due to the complexity of biological systems and the vast number of possible drug combinations. To address these challenges, we propose the use of hybrid attentional memory networks (HAMAN) for drug repositioning. HAMAN combines both attention mechanisms and memory mechanisms into a single model architecture that can effectively capture complex relationships between drugs and disease conditions. This enables more accurate prediction of novel drug-condition pairs by considering temporal patterns and dependencies within sequences of drug and condition interactions. Our experiments show that HAMAN outperforms state-of-the-art methods for drug repositioning on multiple benchmark datasets, demonstrating the effectiveness of our proposed methodology. Overall, the development and application of HAMAN holds great promise for accelerating drug discovery and improving human healthcare.",1
"Gaze is the essential manifestation of human attention. In recent years, a series of work has achieved high accuracy in gaze estimation. However, the inter-personal difference limits the reduction of the subject-independent gaze estimation error. This paper proposes an unsupervised method for domain adaptation gaze estimation to eliminate the impact of inter-personal diversity. In domain adaption, we design an embedding representation with prediction consistency to ensure that the linear relationship between gaze directions in different domains remains consistent on gaze space and embedding space. Specifically, we employ source gaze to form a locally linear representation in the gaze space for each target domain prediction. Then the same linear combinations are applied in the embedding space to generate hypothesis embedding for the target domain sample, remaining prediction consistency. The deviation between the target and source domain is reduced by approximating the predicted and hypothesis embedding for the target domain sample. Guided by the proposed strategy, we design Domain Adaptation Gaze Estimation Network(DAGEN), which learns embedding with prediction consistency and achieves state-of-the-art results on both the MPIIGaze and the EYEDIAP datasets.",0
"This paper addresses the problem of gaze estimation under domain shifts due to variations such as illumination, pose, expression, accessories, makeup, ethnicity, occlusions, or camera settings that often occur during daily use. We propose a new approach based on embedding images into a joint latent space along with eye-tracking data via an adversarial training procedure. In addition, we introduce several novel components including: (i) cross entropy loss function, (ii) multi-scale image pyramid representation, and (iii) regularization on feature maps of VGG face network. To evaluate our method, experiments were conducted on three public datasets containing different variations. Results show significant improvements compared to prior arts. Our method achieved an error reduction up to 70% on average comparing to state-of-the-arts on all datasets. Furthermore, qualitative results demonstrate our model could generalize well across domains while preserving fine details of facial features. Finally, we provide ablation study to analyze contributions from each component used in our framework. Overall, this work proposes an effective solution towards robust and accurate gaze estimation models that can operate under challenging scenarios and reduces errors caused by domain shift issues in real-world applications.",1
"Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.",0
"Image segmentation involves separating objects from their backgrounds in images so that each pixel belongs to one object or the other. This problem has been studied extensively using machine learning techniques such as supervised, semi-supervised, and unsupervised methods. In recent years, deep neural networks have emerged as powerful tools to solve image segmentation problems. They can learn hierarchical representations that capture high level features of complex scenes while still preserving fine details. Motivated by these successes, we conduct a comprehensive survey on deep learning based approaches for image segmentation tasks. We summarize the current state-of-the art and highlight open challenges. Our goal is to provide readers with an accessible introduction to key concepts and algorithms so they can apply them effectively. Additionally, we analyze various performance evaluation criteria used in literature for comparison purposes so researchers new to this field know how to compare models appropriately. Finally, we identify promising directions for future work and hope our review serves as a resource for both beginners and experts alike working in computer vision.",1
"Wide-angle fisheye cameras are commonly used in automated driving for parking and low-speed navigation tasks. Four of such cameras form a surround-view system that provides a complete and detailed view of the vehicle. These cameras are directly exposed to harsh environmental settings and can get soiled very easily by mud, dust, water, frost. Soiling on the camera lens can severely degrade the visual perception algorithms, and a camera cleaning system triggered by a soiling detection algorithm is increasingly being deployed. While adverse weather conditions, such as rain, are getting attention recently, there is only limited work on general soiling. The main reason is the difficulty in collecting a diverse dataset as it is a relatively rare event. We propose a novel GAN based algorithm for generating unseen patterns of soiled images. Additionally, the proposed method automatically provides the corresponding soiling masks eliminating the manual annotation cost. Augmentation of the generated soiled images for training improves the accuracy of soiling detection tasks significantly by 18% demonstrating its usefulness. The manually annotated soiling dataset and the generated augmentation dataset will be made public. We demonstrate the generalization of our fisheye trained GAN model on the Cityscapes dataset. We provide an empirical evaluation of the degradation of the semantic segmentation algorithm with the soiled data.",0
"In autonomous driving systems, camera lens soiling detection plays a crucial role in ensuring safe operation by reducing visibility impairments caused by contaminants such as dust, rain droplets, or snowflakes on the camera lens. However, acquiring large amounts of high quality data annotated with precise masks remains challenging due to the costly annotation process. This study presents a novel approach that utilizes Generative Adversarial Network (GAN) based image augmentation techniques to generate synthetic images of cameras equipped with artificially applied lens soiling patterns. By generating realistic looking synthetic images and their corresponding ground truth labels, we demonstrate how our method can effectively enhance existing datasets, enabling more accurate detection algorithms, while minimizing human labor required for annotation. Experimental results show a significant improvement in both accuracy and robustness compared to state-of-the-art methods under different weather conditions and illuminations. Overall, our proposed framework has great potential for application in real-world environments where access to large quantities of manually annotated dataset may not always be feasible.",1
"State-of-the-art self-supervised learning approaches for monocular depth estimation usually suffer from scale ambiguity. They do not generalize well when applied on distance estimation for complex projection models such as in fisheye and omnidirectional cameras. This paper introduces a novel multi-task learning strategy to improve self-supervised monocular distance estimation on fisheye and pinhole camera images. Our contribution to this work is threefold: Firstly, we introduce a novel distance estimation network architecture using a self-attention based encoder coupled with robust semantic feature guidance to the decoder that can be trained in a one-stage fashion. Secondly, we integrate a generalized robust loss function, which improves performance significantly while removing the need for hyperparameter tuning with the reprojection loss. Finally, we reduce the artifacts caused by dynamic objects violating static world assumptions using a semantic masking strategy. We significantly improve upon the RMSE of previous work on fisheye by 25% reduction in RMSE. As there is little work on fisheye cameras, we evaluated the proposed method on KITTI using a pinhole model. We achieved state-of-the-art performance among self-supervised methods without requiring an external scale estimation.",0
"Distance estimation from monocular fisheye camera images is a challenging task due to the distortion caused by the wide-angle lens and limited visual cues available. In recent years, deep learning methods have shown great promise in solving this problem. However, they often require large amounts of annotated training data which can be difficult and time consuming to collect, especially in real-world autonomous driving scenarios where annotations must be accurate and up-to-date. This paper presents SynDistNet, a self-supervised distance estimation network that utilizes semantic segmentation as well as the fisheye image itself to improve performance without requiring any additional annotation beyond ground truth depth maps. Our method achieves state-of-the-art results on both the KITTI benchmark and our own dataset collected from a moving vehicle. Furthermore, we show how our approach generalizes across different weather conditions and lighting environments, making it more suitable for use in real-world autonomous driving applications. Overall, SynDistNet represents a significant step forward in enabling accurate distance estimation for safer and more reliable self-driving vehicles.",1
"Deep Learning (DL) has attracted a lot of attention for its ability to reach state-of-the-art performance in many machine learning tasks. The core principle of DL methods consists in training composite architectures in an end-to-end fashion, where inputs are associated with outputs trained to optimize an objective function. Because of their compositional nature, DL architectures naturally exhibit several intermediate representations of the inputs, which belong to so-called latent spaces. When treated individually, these intermediate representations are most of the time unconstrained during the learning process, as it is unclear which properties should be favored. However, when processing a batch of inputs concurrently, the corresponding set of intermediate representations exhibit relations (what we call a geometry) on which desired properties can be sought. In this work, we show that it is possible to introduce constraints on these latent geometries to address various problems. In more details, we propose to represent geometries by constructing similarity graphs from the intermediate representations obtained when processing a batch of inputs. By constraining these Latent Geometry Graphs (LGGs), we address the three following problems: i) Reproducing the behavior of a teacher architecture is achieved by mimicking its geometry, ii) Designing efficient embeddings for classification is achieved by targeting specific geometries, and iii) Robustness to deviations on inputs is achieved via enforcing smooth variation of geometry between consecutive latent spaces. Using standard vision benchmarks, we demonstrate the ability of the proposed geometry-based methods in solving the considered problems.",0
"In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance across many tasks in computer vision, natural language processing, and other domains. Despite their success, understanding how these models make predictions remains challenging due to their complex and nonlinear nature. One approach to gain insight into DNNs is through analysis of their latent spaces, which represent the high-dimensional representations learned by the network.  In this paper, we present a novel methodology that leverages graph theory to visualize and analyze the geometry of latent space in DNNs trained on image classification datasets. Our method builds upon previous work that has applied linear algebra techniques to study latent spaces, but our graphs provide a more nuanced representation that accounts for the nonlinear structure of the data. We demonstrate that our approach captures important features of the data and provides insights into the behavior of the model. For instance, we show that clusters identified using our graphs closely align with human annotations and can predict certain properties of the corresponding images, such as whether they contain objects from a particular category or exhibit specific textures.  Our results highlight the potential of studying latent space geometries with graphs to advance our understanding of DNNs and enable new applications. We anticipate that our method will be valuable for researchers working in computer vision, machine learning, and related fields who seek to gain deeper knowledge about the inner workings of powerful machine learning systems. Ultimately, developing tools like ours may facilitate the creation of even better performing models while enabling greater transparency in their decision making processes.",1
"Recently the problem of cross-domain object detection has started drawing attention in the computer vision community. In this paper, we propose a novel unsupervised cross-domain detection model that exploits the annotated data in a source domain to train an object detector for a different target domain. The proposed model mitigates the cross-domain representation divergence for object detection by performing cross-domain feature alignment in two dimensions, the depth dimension and the spatial dimension. In the depth dimension of channel layers, it uses inter-channel information to bridge the domain divergence with respect to image style alignment. In the dimension of spatial layers, it deploys spatial attention modules to enhance detection relevant regions and suppress irrelevant regions with respect to cross-domain feature alignment. Experiments are conducted on a number of benchmark cross-domain detection datasets. The empirical results show the proposed method outperforms the state-of-the-art comparison methods.",0
"In recent years, object detection has become one of the most important topics in computer vision due to its wide range of applications from self-driving cars to surveillance systems. Recently, deep learning approaches such as Faster R-CNN have achieved state-of-the-art performance on many benchmark datasets by generating high quality region proposals followed by bounding box regression and class prediction. However, these models suffer from poor transferability across domains due to differences in camera settings, weather conditions, illumination changes etc. This results in suboptimal performance of existing detectors when they encounter new images from previously unseen environments. To address this issue, we propose a novel method called Bi-dimensional feature alignment (BFA) which aligns features from both domains using adversarial training while enforcing spatial correspondences between the two sets of features. We evaluate our approach on several benchmark datasets and show that BFA achieves better performance compared to other domain adaptation methods, providing robustness against variations in input images from different sources. Our work takes an important step towards building more generalizable and efficient cross-domain object detection solutions, enabling a broader impact in real world deployments.",1
"Satisfying the high computation demand of modern deep learning architectures is challenging for achieving low inference latency. The current approaches in decreasing latency only increase parallelism within a layer. This is because architectures typically capture a single-chain dependency pattern that prevents efficient distribution with a higher concurrency (i.e., simultaneous execution of one inference among devices). Such single-chain dependencies are so widespread that even implicitly biases recent neural architecture search (NAS) studies. In this visionary paper, we draw attention to an entirely new space of NAS that relaxes the single-chain dependency to provide higher concurrency and distribution opportunities. To quantitatively compare these architectures, we propose a score that encapsulates crucial metrics such as communication, concurrency, and load balancing. Additionally, we propose a new generator and transformation block that consistently deliver superior architectures compared to current state-of-the-art methods. Finally, our preliminary results show that these new architectures reduce the inference latency and deserve more attention.",0
"Title: Reducing Inference Latency with Concurrent Architectures for Image Recognition  Abstract:  This research focuses on addressing one of the main bottlenecks faced by today's image recognition systems - inference latency. Despite significant advancements in deep learning models that have improved accuracy in these tasks, their high computational demands make them impractical for real-time applications. To overcome this limitation, we explore concurrent architectures for parallel processing of neural network operations. Our proposed architecture exploits data dependencies and computations to achieve efficient utilization of hardware resources, reducing inference latency without sacrificing model accuracy. We evaluate our approach using several popular benchmark datasets and demonstrate substantial improvements over traditional sequential processing methods. Additionally, we provide insight into tradeoffs between model accuracy, latency, and resource usage, allowing developers to choose appropriate configurations for specific use cases. These findings contribute to the development of practical, real-time image recognition systems that meet the increasing demand for fast, accurate artificial intelligence solutions.",1
"Knowledge distillation is an effective method to transfer the knowledge from the cumbersome teacher model to the lightweight student model. Online knowledge distillation uses the ensembled prediction results of multiple student models as soft targets to train each student model. However, the homogenization problem will lead to difficulty in further improving model performance. In this work, we propose a new distillation method to enhance the diversity among multiple student models. We introduce Feature Fusion Module (FFM), which improves the performance of the attention mechanism in the network by integrating rich semantic information contained in the last block of multiple student models. Furthermore, we use the Classifier Diversification(CD) loss function to strengthen the differences between the student models and deliver a better ensemble result. Extensive experiments proved that our method significantly enhances the diversity among student models and brings better distillation performance. We evaluate our method on three image classification datasets: CIFAR-10/100 and CINIC-10. The results show that our method achieves state-of-the-art performance on these datasets.",0
"Here is a possible abstract:  New machine learning techniques have revolutionized our ability to analyze large data sets and make predictions based on that analysis. However, many of these models require large amounts of computational resources, making them difficult to deploy in situations where access to such resources is limited. In addition, as we continue to improve the performance of these models, their complexity can increase dramatically, further limiting their use in resource-constrained environments. One approach to addressing these issues is knowledge distillation, which involves training a smaller model using the outputs from a larger model as input. This technique has been shown to significantly reduce the size of the model without sacrificing accuracy. In this paper, we propose a new method for improving the effectiveness of knowledge distillation by enhancing the diversity of the inputs used during the process. Our multi-branch diversity enhancement approach leverages multiple runs of the large model to produce different outputs, which are then combined to create more comprehensive and accurate training examples for the small model. Experimental results demonstrate the effectiveness of our method in terms of both reducing model size and preserving accuracy.",1
"The pressure of ever-increasing patient demand and budget restrictions make hospital bed management a daily challenge for clinical staff. Most critical is the efficient allocation of resource-heavy Intensive Care Unit (ICU) beds to the patients who need life support. Central to solving this problem is knowing for how long the current set of ICU patients are likely to stay in the unit. In this work, we propose a new deep learning model based on the combination of temporal convolution and pointwise (1x1) convolution, to solve the length of stay prediction task on the eICU critical care dataset. The model - which we refer to as Temporal Pointwise Convolution (TPC) - is specifically designed to mitigate for common challenges with Electronic Health Records, such as skewness, irregular sampling and missing data. In doing so, we have achieved significant performance benefits of 18-51% (metric dependent) over the commonly used Long-Short Term Memory (LSTM) network, and the multi-head self-attention network known as the Transformer.",0
"Abstract: In order to predict the length of stay (LOS) for patients admitted to an intensive care unit (ICU), temporal pointwise convolutional networks were used as part of a deep learning model. These models process time-series data and have proven to be effective in a variety of healthcare tasks such as sepsis detection, mortality prediction, and medication dosage forecasting. By analyzing electronic health record (EHR) data from ICUs, we trained our model on patient characteristics and vital signs collected at regular intervals during their hospital stay. Our results showed that using temporal pointwise convolutional networks led to improved accuracy over traditional machine learning methods. This study demonstrates the potential benefits of utilizing deep learning techniques in clinical decision making processes related to critically ill patients.",1
"Learning-based methods have enabled the recovery of a video sequence from a single motion-blurred image or a single coded exposure image. Recovering video from a single motion-blurred image is a very ill-posed problem and the recovered video usually has many artifacts. In addition to this, the direction of motion is lost and it results in motion ambiguity. However, it has the advantage of fully preserving the information in the static parts of the scene. The traditional coded exposure framework is better-posed but it only samples a fraction of the space-time volume, which is at best 50% of the space-time volume. Here, we propose to use the complementary information present in the fully-exposed (blurred) image along with the coded exposure image to recover a high fidelity video without any motion ambiguity. Our framework consists of a shared encoder followed by an attention module to selectively combine the spatial information from the fully-exposed image with the temporal information from the coded image, which is then super-resolved to recover a non-ambiguous high-quality video. The input to our algorithm is a fully-exposed and coded image pair. Such an acquisition system already exists in the form of a Coded-two-bucket (C2B) camera. We demonstrate that our proposed deep learning approach using blurred-coded image pair produces much better results than those from just a blurred image or just a coded image.",0
"This paper presents a novel approach to video reconstruction using spatio-temporal fusion of blurred-coded image pairs. Our method fuses consecutive frames of the input video that have been coded using a blurring operation before transmission. By exploiting both spatial and temporal coherency, our algorithm is able to significantly improve the visual quality of reconstructed videos compared to existing methods. We demonstrate the effectiveness of our approach through extensive experiments on real-world datasets, achieving state-of-the-art performance in terms of visual fidelity and computational efficiency. Our work has important implications for applications such as remote sensing, surveillance, and telepresence where high-quality video streaming is critical. Overall, we believe that our research contributes valuable insights into the field of video compression and reconstruction, paving the way for future advancements in this area.",1
"Pedestrian Attribute Recognition (PAR) has aroused extensive attention due to its important role in video surveillance scenarios. In most cases, the existence of a particular attribute is strongly related to a partial region. Recent works design complicated modules, e.g., attention mechanism and proposal of body parts to localize the attribute corresponding region. These works further prove that localization of attribute specific regions precisely will help in improving performance. However, these part-information-based methods are still not accurate as well as increasing model complexity which makes it hard to deploy on realistic applications. In this paper, we propose a Deep Template Matching based method to capture body parts features with less computation. Further, we also proposed an auxiliary supervision method that use human pose keypoints to guide the learning toward discriminative local cues. Extensive experiments show that the proposed method outperforms and has lower computational complexity, compared with the state-of-the-art approaches on large-scale pedestrian attribute datasets, including PETA, PA-100K, RAP, and RAPv2 zs.",0
"In this paper, we propose a new method for pedestrian attribute recognition using deep template matching with auxiliary supervision from attribute-wise keypoint detection. Our approach builds on recent advances in computer vision that have used convolutional neural networks (CNNs) to perform image classification tasks by training them on large datasets. We use a similar CNN architecture to learn feature representations for pedestrians in images, which can then be used to detect attributes such as gender, age, and clothing style.  To improve accuracy, our method uses auxiliary supervision from attribute-wise keypoint detection, where we first extract salient points in the image that correspond to specific parts of the pedestrian, such as their face or arms. These keypoints provide additional contextual information that helps the network better distinguish between different attributes. For example, if we know that a person has glasses, we might look at areas near their eyes to see whether they are wearing glasses or not. By combining both visual features and keypoints, our model achieves state-of-the-art results on two benchmark datasets: Caltech Attributes Part A and B.  In summary, we propose a novel framework for pedestrian attribute recognition that leverages deep template matching and auxiliary supervision from attribute-wise keypoints. Through extensive experiments, we demonstrate the effectiveness of our method compared to other approaches. This work provides valuable insights into how computer vision techniques can be applied to real-world applications involving human analysis.",1
"Graph representation learning is to learn universal node representations that preserve both node attributes and structural information. The derived node representations can be used to serve various downstream tasks, such as node classification and node clustering. When a graph is heterogeneous, the problem becomes more challenging than the homogeneous graph node learning problem. Inspired by the emerging information theoretic-based learning algorithm, in this paper we propose an unsupervised graph neural network Heterogeneous Deep Graph Infomax (HDGI) for heterogeneous graph representation learning. We use the meta-path structure to analyze the connections involving semantics in heterogeneous graphs and utilize graph convolution module and semantic-level attention mechanism to capture local representations. By maximizing local-global mutual information, HDGI effectively learns high-level node representations that can be utilized in downstream graph-related tasks. Experiment results show that HDGI remarkably outperforms state-of-the-art unsupervised graph representation learning methods on both classification and clustering tasks. By feeding the learned representations into a parametric model, such as logistic regression, we even achieve comparable performance in node classification tasks when comparing with state-of-the-art supervised end-to-end GNN models.",0
"This paper proposes a new deep learning model called ""Heterogeneous Deep Graph Infomax,"" which combines graph neural networks (GNNs) and self-supervised representation learning methods. GNNs have been shown to be effective at capturing structured data representations, while self-supervised learning has proven successful in pretraining models on large amounts of unlabeled data. By combining these two techniques, we aim to create a model that can learn both discrete node features and continuous edge attributes from raw input graphs, without requiring any labeled training examples. Our approach is based on maximizing the mutual information between local neighborhoods in a graph, as measured by graph autoencoders. We show promising results on several benchmark datasets, including both homogeneous and heterogeneous graph types, demonstrating our method's ability to capture meaningful representations from complex network structures. Finally, we provide ablation studies to analyze different design choices and discuss potential extensions of our work. Overall, our proposed model represents a step forward towards applying state-of-the-art machine learning techniques to real-world graph problems in an efficient manner.",1
"Despite convolutional network-based methods have boosted the performance of single image super-resolution (SISR), the huge computation costs restrict their practical applicability. In this paper, we develop a computation efficient yet accurate network based on the proposed attentive auxiliary features (A$^2$F) for SISR. Firstly, to explore the features from the bottom layers, the auxiliary feature from all the previous layers are projected into a common space. Then, to better utilize these projected auxiliary features and filter the redundant information, the channel attention is employed to select the most important common feature based on current layer feature. We incorporate these two modules into a block and implement it with a lightweight network. Experimental results on large-scale dataset demonstrate the effectiveness of the proposed model against the state-of-the-art (SOTA) SR methods. Notably, when parameters are less than 320k, A$^2$F outperforms SOTA methods for all scales, which proves its ability to better utilize the auxiliary features. Codes are available at https://github.com/wxxxxxxh/A2F-SR.",0
"High-resolution images are important in many computer vision tasks such as image recognition, object detection, and autonomous driving. However, acquiring high-quality images can be difficult due to factors like limited sensor resolutions, motion blur, or aliasing. To address these limitations, single-image super-resolution (SR) methods aim to enhance low-resolution images by generating their corresponding high-resolution counterparts. Recently, deep learning techniques have been employed to improve SR performance significantly.  However, most state-of-the-art methods tend to be computationally expensive and require powerful GPU resources, which limits their use in resource-constrained devices like smartphones or embedded systems. In this work, we propose a novel lightweight network architecture that achieves superior performance at reduced computational cost while maintaining visual quality. Our model utilizes attentive auxiliary feature learning to effectively capture local relationships among neighboring pixels while preserving global context awareness. We further introduce a self-attention module in our architecture, allowing the network to focus on different regions adaptively based on their importance. Extensive experiments demonstrate that our method outperforms existing lightweight models, even surpassing some heavyweight models, making it suitable for real-world deployment scenarios where speed and efficiency are crucial considerations.",1
"Determinantal point processes (DPPs) have attracted substantial attention as an elegant probabilistic model that captures the balance between quality and diversity within sets. DPPs are conventionally parameterized by a positive semi-definite kernel matrix, and this symmetric kernel encodes only repulsive interactions between items. These so-called symmetric DPPs have significant expressive power, and have been successfully applied to a variety of machine learning tasks, including recommendation systems, information retrieval, and automatic summarization, among many others. Efficient algorithms for learning symmetric DPPs and sampling from these models have been reasonably well studied. However, relatively little attention has been given to nonsymmetric DPPs, which relax the symmetric constraint on the kernel. Nonsymmetric DPPs allow for both repulsive and attractive item interactions, which can significantly improve modeling power, resulting in a model that may better fit for some applications. We present a method that enables a tractable algorithm, based on maximum likelihood estimation, for learning nonsymmetric DPPs from data composed of observed subsets. Our method imposes a particular decomposition of the nonsymmetric kernel that enables such tractable learning algorithms, which we analyze both theoretically and experimentally. We evaluate our model on synthetic and real-world datasets, demonstrating improved predictive performance compared to symmetric DPPs, which have previously shown strong performance on modeling tasks associated with these datasets.",0
"Abstract: Recently, there has been increased interest in studying nonlinear latent variable models that can capture complex data distributions. In particular, determinantal point processes (DPPs) have gained popularity due to their ability to encode probabilistic constraints on observed data while preserving tractability. While most work focuses on symmetric DPPs where the marginal distribution over all points is invariant under permutation, many real world applications demand nonsymmetric models. In this paper, we introduce a new class of nonsymmetric DPPs and develop efficient inference algorithms to learn them from data. Our model assumes the existence of latent factors and couples these with nonlinear functions that map instances to feature spaces upon which DPPs operate. We propose two different approaches to estimate these nonsymmetric DPPs using gradient ascent and variational Bayesian methods respectively. We demonstrate the efficacy of our approach across several tasks including image recovery, recommender systems and protein structure prediction. Additionally, we showcase the versatility of our framework by encompassing traditional linear latent factor models within its scope as well. Finally, experiments on large datasets validate that our methodology performs better than alternative state-of-the-art techniques. This research provides valuable insights into understanding complex relationship structures present within natural phenomena.",1
"Modeling a structured, dynamic environment like a video game requires keeping track of the objects and their states declarative knowledge) as well as predicting how objects behave (procedural knowledge). Black-box models with a monolithic hidden state often fail to apply procedural knowledge consistently and uniformly, i.e., they lack systematicity. For example, in a video game, correct prediction of one enemy's trajectory does not ensure correct prediction of another's. We address this issue via an architecture that factorizes declarative and procedural knowledge and that imposes modularity within each form of knowledge. The architecture consists of active modules called object files that maintain the state of a single object and invoke passive external knowledge sources called schemata that prescribe state updates. To use a video game as an illustration, two enemies of the same type will share schemata but will have separate object files to encode their distinct state (e.g., health, position). We propose to use attention to determine which object files to update, the selection of schemata, and the propagation of information between object files. The resulting architecture is a drop-in replacement conforming to the same input-output interface as normal recurrent networks (e.g., LSTM, GRU) yet achieves substantially better generalization on environments that have multiple object tokens of the same type, including a challenging intuitive physics benchmark.",0
"In this paper, we propose a novel approach to factoring declarative and procedural knowledge in dynamical systems using object files and schemata. We argue that this method allows for greater flexibility and modularity in the design and implementation of complex computational models. By separating declarative knowledge (i.e., facts and rules) from procedural knowledge (i.e., algorithms and control structures), our framework enables more efficient learning and reasoning processes while reducing the risk of confusion and error. Our experimental results demonstrate the effectiveness of our approach on several benchmark tasks across different domains, including natural language processing, computer vision, and robotics. Overall, our work contributes to the growing body of research aimed at improving the robustness and scalability of artificial intelligence systems.",1
"This paper advocates incorporating a Low-Rank Global Attention (LRGA) module, a computation and memory efficient variant of the dot-product attention (Vaswani et al., 2017), to Graph Neural Networks (GNNs) for improving their generalization power. To theoretically quantify the generalization properties granted by adding the LRGA module to GNNs, we focus on a specific family of expressive GNNs and show that augmenting it with LRGA provides algorithmic alignment to a powerful graph isomorphism test, namely the 2-Folklore Weisfeiler-Lehman (2-FWL) algorithm. In more detail we: (i) consider the recent Random Graph Neural Network (RGNN) (Sato et al., 2020) framework and prove that it is universal in probability; (ii) show that RGNN augmented with LRGA aligns with 2-FWL update step via polynomial kernels; and (iii) bound the sample complexity of the kernel's feature map when learned with a randomly initialized two-layer MLP. From a practical point of view, augmenting existing GNN layers with LRGA produces state of the art results in current GNN benchmarks. Lastly, we observe that augmenting various GNN architectures with LRGA often closes the performance gap between different models.",0
"Despite their promising results in graph structured data domains such as chemistry, neuroscience, social network analysis, and computer vision, graph neural networks (GNNs) often suffer from overfitting on small datasets due to limited capacity and insufficiently expressive models that cannot accurately capture complex patterns present in graphs. In this work, we propose a new model called global attention GNN that generalizes better across varying graph sizes by increasing the receptive field size of each layer through dynamic global attention mechanisms. Our model leverages interlayer interactions via edge features stored in hidden states to enable more efficient message propagation within deep architectures. We evaluate our proposed approach against several popular baseline models on multiple benchmark datasets and demonstrate superior performance in terms of accuracy and robustness under domain shift. Overall, our findings suggest that global attention mechanism is an effective technique to improve the generalization abilities of state-of-the-art GNNs while reducing overfitting.",1
"This paper presents a holistic approach to saliency-guided visual attention modeling (SVAM) for use by autonomous underwater robots. Our proposed model, named SVAM-Net, integrates deep visual features at various scales and semantics for effective salient object detection (SOD) in natural underwater images. The SVAM-Net architecture is configured in a unique way to jointly accommodate bottom-up and top-down learning within two separate branches of the network while sharing the same encoding layers. We design dedicated spatial attention modules (SAMs) along these learning pathways to exploit the coarse-level and fine-level semantic features for SOD at four stages of abstractions. The bottom-up branch performs a rough yet reasonably accurate saliency estimation at a fast rate, whereas the deeper top-down branch incorporates a residual refinement module (RRM) that provides fine-grained localization of the salient objects. Extensive performance evaluation of SVAM-Net on benchmark datasets clearly demonstrates its effectiveness for underwater SOD. We also validate its generalization performance by several ocean trials' data that include test images of diverse underwater scenes and waterbodies, and also images with unseen natural objects. Moreover, we analyze its computational feasibility for robotic deployments and demonstrate its utility in several important use cases of visual attention modeling.",0
"This paper presents a novel approach for visual attention modeling by autonomous underwater robots known as saliency-guided visual attention (SVAM). Traditional methods of object detection rely on handcrafted features that require extensive manual engineering which can become computationally expensive and difficult to optimize for different scenarios. However, recent advances in deep learning have enabled the development of new feature extraction techniques that can learn hierarchical representations directly from raw data streams without human intervention. These learned features have been shown to yield significant improvements over traditional approaches, but still suffer from limitations such as high computational complexity, lack of interpretability, sensitivity to hyperparameters, and low generalization ability across domains. To overcome these challenges, we propose the use of saliency maps generated using convolutional neural networks to guide the visual attention process of underwater robots. We show through simulation experiments and real-world deployments that our proposed method achieves state-of-the-art performance while providing explainability and robustness against various conditions. Our work has important applications in underwater robotics, including search and rescue operations, environmental monitoring, and oil spill response. Overall, SVAM represents a major step forward in the field of computer vision for underwater robots.",1
"Scribble-supervised semantic segmentation has gained much attention recently for its promising performance without high-quality annotations. Many approaches have been proposed. Typically, they handle this problem to either introduce a well-labeled dataset from another related task, turn to iterative refinement and post-processing with the graphical model, or manipulate the scribble label. This work aims to achieve semantic segmentation supervised by scribble label directly without auxiliary information and other intermediate manipulation. Specifically, we impose diffusion on neural representation by random walk and consistency on neural eigenspace by self-supervision, which forces the neural network to produce dense and consistent predictions over the whole dataset. The random walk embedded in the network will compute a probabilistic transition matrix, with which the neural representation diffused to be uniform. Moreover, given the probabilistic transition matrix, we apply the self-supervision on its eigenspace for consistency in the image's main parts. In addition to comparing the common scribble dataset, we also conduct experiments on the modified datasets that randomly shrink and even drop the scribbles on image objects. The results demonstrate the superiority of the proposed method and are even comparable to some full-label supervised ones. The code and datasets are available at https://github.com/panzhiyi/RW-SS.",0
"Our new approach to semantic segmentation uses scribbles as supervisory signals instead of precise annotations, which can be more efficient and less time consuming to obtain. We use random walks on neural representation spaces to iteratively refine our predictions and ensure that they conform to the provided scribbled input. Additionally, we incorporate self-supervision using learnt representations from similar tasks to improve performance further. Experiments show significant improvements compared to previous methods. --  In this work, we present a novel method for semantic segmentation using scribbled annotation and neural representation learning. Semantic segmentation is a challenging task requiring accurate pixelwise labeling of images, often obtained through laborious manual annotation processes. To alleviate this burden, recent works have utilized weakly supervised or semi-supervised approaches with minimal human intervention such as bounding boxes or image tags. However, even these annotational forms remain cumbersome and may not capture subtleties in object boundaries required for accurate recognition. In contrast, we employ easily obtainable scribbled annotations where the user only needs to roughly draw shapes enclosing regions of interest. These sparse, low-resolution markings guide our model to learn the necessary complex mapping from pixels to semantics via multiple iterations over both scribbled inputs and learned high-dimensional feature representations from related neural networks trained for other vision tasks. By leveraging prior knowledge transfered from self-supervised training and optimizing our framework using scribble walkers, we achieve stateof-the-art results on popular benchmarks while reducing labeling requirements during deployment.",1
"Convolutional Neural Network (CNN) techniques have proven to be very useful in image-based anomaly detection applications. CNN can be used as deep features extractor where other anomaly detection techniques are applied on these features. For this scenario, using transfer learning is common since pretrained models provide deep feature representations that are useful for anomaly detection tasks. Consequentially, anomaly can be detected by applying similarly measure between extracted features and a defined model of normality. A key factor in such approaches is the decision threshold used for detecting anomaly. While most of the proposed methods focus on the approach itself, slight attention has been paid to address decision threshold settings. In this paper, we tackle this problem and propose a welldefined method to set the working-point decision threshold that improves detection accuracy. We introduce a transfer learning framework for anomaly detection based on similarity measure with a Model of Normality (MoN) and show that with the proposed threshold settings, a significant performance improvement can be achieved. Moreover, the framework has low complexity with relaxed computational requirements.",0
"Here I present a transfer learning framework capable of anomaly detection via modeling normalcy under distribution shift. Inspired by recent progresses on unsupervised pretraining, we argue that knowledge from related tasks can greatly aid the model training procedure without explicit labeled samples. To achieve this goal, our method leverages the latent structure learned during previous pretraining so as to enhance its representation quality; then, under mild conditions, transfers such structured representation into another task domain. Experimental results on several benchmark datasets demonstrate significant superiorities over both non-pretrained counterparts and those fine-tuned only for few epochs using limited labeled data. Our ablation studies further confirm the effectiveness of each component and practically recommend feasible settings according to computational resource constraints. By bridging the gap between theoretical derivations and empirical observations, we believe this work provides a step forward towards more efficient use of big data.",1
"Previous studies on image classification have mainly focused on the performance of the networks, not on real-time operation or model compression. We propose a Gaussian Deep Recurrent visual Attention Model (GDRAM)- a reinforcement learning based lightweight deep neural network for large scale image classification that outperforms the conventional CNN (Convolutional Neural Network) which uses the entire image as input. Highly inspired by the biological visual recognition process, our model mimics the stochastic location of the retina with Gaussian distribution. We evaluate the model on Large cluttered MNIST, Large CIFAR-10 and Large CIFAR-100 datasets which are resized to 128 in both width and height.",0
"Gaussian Radiometric Approximation Memory (GRAM) is a novel method for image classification that utilizes stochastic retina-inspired glimpses and reinforcement learning. This approach mimics the human visual system by capturing subregions of an input image and encoding them into a compressed form suitable for efficient recognition. The algorithm uses a feedback mechanism based on temporal difference error signaling to optimize the feature representation and reduce computational requirements. Extensive experiments demonstrate that our method outperforms state-of-the-art models while achieving real-time performance on resource-constrained hardware. By combining biological insights and machine learning techniques, we propose a new paradigm for efficient vision processing systems.",1
"In this work, we present the Densely Connected Temporal Convolutional Network (DC-TCN) for lip-reading of isolated words. Although Temporal Convolutional Networks (TCN) have recently demonstrated great potential in many vision tasks, its receptive fields are not dense enough to model the complex temporal dynamics in lip-reading scenarios. To address this problem, we introduce dense connections into the network to capture more robust temporal features. Moreover, our approach utilises the Squeeze-and-Excitation block, a light-weight attention mechanism, to further enhance the model's classification power. Without bells and whistles, our DC-TCN method has achieved 88.36% accuracy on the Lip Reading in the Wild (LRW) dataset and 43.65% on the LRW-1000 dataset, which has surpassed all the baseline methods and is the new state-of-the-art on both datasets.",0
"In this work we present an approach to lip reading that achieves state-of-the-art performance on two challenging benchmark datasets: LRW (Lip Reading in the Wild) and OuluVS2. Our method uses densely connected temporal convolutional networks, which capture both spatial context from adjacent video frames as well as sequential context by concatenating them along the time dimension. We fine-tune a pretrained model for speaker verification using weak supervision, where the labels correspond to phones rather than words. This allows us to leverage large amounts of unlabeled data during training. We evaluate our system on both closed set (where the test identity matches one seen during training) and open set (where the test identity is unknown) protocols. On closed set evaluation, our system improves over baselines by up to 7% WER (Word Error Rate). Finally, our ablation study shows that different components of our architecture contribute significantly towards boosting overall performance.",1
"Humans are able to seamlessly visually imitate others, by inferring their intentions and using past experience to achieve the same end goal. In other words, we can parse complex semantic knowledge from raw video and efficiently translate that into concrete motor control. Is it possible to give a robot this same capability? Prior research in robot imitation learning has created agents which can acquire diverse skills from expert human operators. However, expanding these techniques to work with a single positive example during test time is still an open challenge. Apart from control, the difficulty stems from mismatches between the demonstrator and robot domains. For example, objects may be placed in different locations (e.g. kitchen layouts are different in every house). Additionally, the demonstration may come from an agent with different morphology and physical appearance (e.g. human), so one-to-one action correspondences are not available. This paper investigates techniques which allow robots to partially bridge these domain gaps, using their past experience. A neural network is trained to mimic ground truth robot actions given context video from another agent, and must generalize to unseen task instances when prompted with new videos during test time. We hypothesize that our policy representations must be both context driven and dynamics aware in order to perform these tasks. These assumptions are baked into the neural network using the Transformers attention mechanism and a self-supervised inverse dynamics loss. Finally, we experimentally determine that our method accomplishes a $\sim 2$x improvement in terms of task success rate over prior baselines in a suite of one-shot manipulation tasks.",0
"Artificial intelligence has been rapidly advancing over the past few decades thanks in part to deep learning techniques such as convolutional neural networks (CNNs) which have achieved state-of-the art results across many domains including image classification tasks. However, despite these remarkable achievements there still exist certain limitations that prevent CNNs from performing well on one-shot visual imitation tasks which requires them to generate images by conditioning solely on small amounts of relevant data. To overcome these shortcomings we propose the use of transformer networks which were initially designed for natural language processing but have recently shown great promise for computer vision tasks like image generation. Specifically our proposed method leverages the self attention mechanism of the transformer network to efficiently learn the correspondence between different regions within the input image thereby enabling effective one shot generative performance. We evaluate our approach comprehensively through a range of experiments that demonstrate clear superiority relative to strong baseline methods and provide insights into the strengths and weaknesses of our model. In summary our work shows the effectiveness of using transformers for solving challenging one shot visual imitation problems while also highlighting new research directions for exploration in this exciting field.",1
"The aim of image captioning is to generate textual description of a given image. Though seemingly an easy task for humans, it is challenging for machines as it requires the ability to comprehend the image (computer vision) and consequently generate a human-like description for the image (natural language understanding). In recent times, encoder-decoder based architectures have achieved state-of-the-art results for image captioning. Here, we present a heuristic of beam search on top of the encoder-decoder based architecture that gives better quality captions on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",0
"This is an example of how you can write the Abstract section of your research paper without including any direct references to the Paper Title. Instead, focus on the main idea behind the work, what problem(s) were addressed and solved by the authors, and highlight some of the key results obtained. You should aim at providing sufficient details that would allow the reader to assess whether reading the entire paper could provide insights relevant to them or their interests. In this case, the paper proposes an approach called ""Attention Beam"" which takes advantage of both convolutional neural networks (CNN) as well as Recurrent Neural Networks (RNN). These are then applied in parallel to generate image descriptions or captions with high fidelity, outperforming previous state-of-the-art approaches in terms of accuracy, speed, memory efficiency and interpretability. Authors argue that Attention Beam provides more control over the process compared to traditional end-to-end systems where all features are learned together. Their proposed framework has been validated through experiments performed on several benchmark datasets such as COCO dataset. Ultimately, these findings have significant implications across fields ranging from computer vision to natural language processing, where generating human-like descriptive sentences automatically remains one of the most challenging problems.",1
"Objective Reticular pseudodrusen (RPD), a key feature of age-related macular degeneration (AMD), are poorly detected by human experts on standard color fundus photography (CFP) and typically require advanced imaging modalities such as fundus autofluorescence (FAF). The objective was to develop and evaluate the performance of a novel 'M3' deep learning framework on RPD detection. Materials and Methods A deep learning framework M3 was developed to detect RPD presence accurately using CFP alone, FAF alone, or both, employing 8000 CFP-FAF image pairs obtained prospectively (Age-Related Eye Disease Study 2). The M3 framework includes multi-modal (detection from single or multiple image modalities), multi-task (training different tasks simultaneously to improve generalizability), and multi-attention (improving ensembled feature representation) operation. Performance on RPD detection was compared with state-of-the-art deep learning models and 13 ophthalmologists; performance on detection of two other AMD features (geographic atrophy and pigmentary abnormalities) was also evaluated. Results For RPD detection, M3 achieved area under receiver operating characteristic (AUROC) 0.832, 0.931, and 0.933 for CFP alone, FAF alone, and both, respectively. M3 performance on CFP was very substantially superior to human retinal specialists (median F1-score 0.644 versus 0.350). External validation (on Rotterdam Study, Netherlands) demonstrated high accuracy on CFP alone (AUROC 0.965). The M3 framework also accurately detected geographic atrophy and pigmentary abnormalities (AUROC 0.909 and 0.912, respectively), demonstrating its generalizability. Conclusion This study demonstrates the successful development, robust evaluation, and external validation of a novel deep learning framework that enables accessible, accurate, and automated AMD diagnosis and prognosis.",0
"An innovative approach to detecting reticular pseudodruesen in images using deep learning techniques has been developed in our research. This method involves combining multiple types of image data (multi-modal), training the model on several tasks simultaneously (multi-task), and utilizing attention mechanisms to focus on specific features in each task (multi-attention). Our results demonstrate that this M3 approach outperforms previous methods for accurately classifying reticular pseudodrusens in imaging studies related to age-related macular degeneration (AMD). The potential applications of this technology could lead to more automated and accessible screening processes for AMD, potentially improving patient outcomes through earlier diagnosis and treatment. By leveraging advancements in computer vision and artificial intelligence, we aim to contribute to the development of effective diagnostic tools for ocular diseases like AMD. Overall, our findings suggest significant promise for future work in developing similar approaches for other medical imaging applications.",1
"In this study, we present a dynamic graph representation learning model on weighted graphs to accurately predict the network capacity of connections between viewers in a live video streaming event. We propose EGAD, a neural network architecture to capture the graph evolution by introducing a self-attention mechanism on the weights between consecutive graph convolutional networks. In addition, we account for the fact that neural architectures require a huge amount of parameters to train, thus increasing the online inference latency and negatively influencing the user experience in a live video streaming event. To address the problem of the high online inference of a vast number of parameters, we propose a knowledge distillation strategy. In particular, we design a distillation loss function, aiming to first pretrain a teacher model on offline data, and then transfer the knowledge from the teacher to a smaller student model with less parameters. We evaluate our proposed model on the link prediction task on three real-world datasets, generated by live video streaming events. The events lasted 80 minutes and each viewer exploited the distribution solution provided by the company Hive Streaming AB. The experiments demonstrate the effectiveness of the proposed model in terms of link prediction accuracy and number of required parameters, when evaluated against state-of-the-art approaches. In addition, we study the distillation performance of the proposed model in terms of compression ratio for different distillation strategies, where we show that the proposed model can achieve a compression ratio up to 15:100, preserving high link prediction accuracy. For reproduction purposes, our evaluation datasets and implementation are publicly available at https://stefanosantaris.github.io/EGAD.",0
This paper presents a novel method for evolving graph representation learning in live video streaming events using self attention mechanisms. We propose an end-to-end approach that learns a joint embedding space which captures both spatial and temporal contexts simultaneously. To achieve this goal we incorporate domain knowledge from pre-trained models to distil their implicit understanding into our model. Our proposed architecture provides an elegant solution to challenges associated with complex event recognition by taking advantage of scene dynamics while preserving spatio-temporal consistency. Extensive experimental results demonstrate significant improvement over state-of-the-art methods.,1
"The explosive rise of the use of Computer tomography (CT) imaging in medical practice has heightened public concern over the patient's associated radiation dose. However, reducing the radiation dose leads to increased noise and artifacts, which adversely degrades the scan's interpretability. Consequently, an advanced image reconstruction algorithm to improve the diagnostic performance of low dose ct arose as the primary concern among the researchers, which is challenging due to the ill-posedness of the problem. In recent times, the deep learning-based technique has emerged as a dominant method for low dose CT(LDCT) denoising. However, some common bottleneck still exists, which hinders deep learning-based techniques from furnishing the best performance. In this study, we attempted to mitigate these problems with three novel accretions. First, we propose a novel convolutional module as the first attempt to utilize neighborhood similarity of CT images for denoising tasks. Our proposed module assisted in boosting the denoising by a significant margin. Next, we moved towards the problem of non-stationarity of CT noise and introduced a new noise aware mean square error loss for LDCT denoising. Moreover, the loss mentioned above also assisted to alleviate the laborious effort required while training CT denoising network using image patches. Lastly, we propose a novel discriminator function for CT denoising tasks. The conventional vanilla discriminator tends to overlook the fine structural details and focus on the global agreement. Our proposed discriminator leverage self-attention and pixel-wise GANs for restoring the diagnostic quality of LDCT images. Our method validated on a publicly available dataset of the 2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge performed remarkably better than the existing state of the art method.",0
"Medical imaging often relies on high doses of radiation that can potentially harm patients, especially if they require multiple scans over time. However, lower dose images may result in poor quality images or limited diagnostic capabilities. To address this issue, researchers have proposed using deep learning methods such as convolutional neural networks (CNNs) to denoise low dose computed tomography (CT) scans without increasing radiation exposure. In particular, non-local means based CNNs have been shown to effectively remove noise while preserving image details. In this work, we present a novel approach for training these models in order to further improve their performance. Specifically, we introduce a self attentive spectral normalization method inspired by normalizing flows to regularize the non-local block, which has been challenging due to the lack of explicit data gradients. Additionally, we propose a patch generation architecture called Markovian Patch GAN trained alongside our main model during each epoch, providing additional supervision for better convergence and generalizability. Experiments show that our new framework significantly improves denoising performance compared to previous state-of-the-art methods while maintaining computational efficiency and ease of implementation. Our results highlight the potential benefits of conscious training for medical applications where accurate diagnoses rely on high-quality low dose images.",1
"Deep learning techniques have provided significant improvements in hyperspectral image (HSI) classification. The current deep learning based HSI classifiers follow a patch-based learning framework by dividing the image into overlapping patches. As such, these methods are local learning methods, which have a high computational cost. In this paper, a fast patch-free global learning (FPGA) framework is proposed for HSI classification. In FPGA, an encoder-decoder based FCN is utilized to consider the global spatial information by processing the whole image, which results in fast inference. However, it is difficult to directly utilize the encoder-decoder based FCN for HSI classification as it always fails to converge due to the insufficiently diverse gradients caused by the limited training samples. To solve the divergence problem and maintain the abilities of FCN of fast inference and global spatial information mining, a global stochastic stratified sampling strategy is first proposed by transforming all the training samples into a stochastic sequence of stratified samples. This strategy can obtain diverse gradients to guarantee the convergence of the FCN in the FPGA framework. For a better design of FCN architecture, FreeNet, which is a fully end-to-end network for HSI classification, is proposed to maximize the exploitation of the global spatial information and boost the performance via a spectral attention based encoder and a lightweight decoder. A lateral connection module is also designed to connect the encoder and decoder, fusing the spatial details in the encoder and the semantic features in the decoder. The experimental results obtained using three public benchmark datasets suggest that the FPGA framework is superior to the patch-based framework in both speed and accuracy for HSI classification. Code has been made available at: https://github.com/Z-Zheng/FreeNet.",0
"In recent years, hyperspectral imaging (HSI) has become increasingly important for applications such as agriculture, environmental monitoring, and remote sensing. However, accurately classifying HSI data can be challenging due to high spectral resolution and large number of bands. In this work, we propose a fast patch-free global learning framework called ""FPGA"" that utilizes fully end-to-end architectures for HSI classification without any preprocessing steps. Our approach leverages both local texture features and global contextual information while using only a few convolutional layers. We evaluate our method on two public datasets and compare it against state-of-the-art approaches. Experimental results show that FPGA achieves superior performance in terms of accuracy and efficiency, making it an effective tool for HSI analysis. This study contributes to the development of deep learning techniques for HSI classification, paving the way for further advancements in the field.",1
"We propose a novel transfer learning method for speech emotion recognition allowing us to obtain promising results when only few training data is available. With as low as 125 examples per emotion class, we were able to reach a higher accuracy than a strong baseline trained on 8 times more data. Our method leverages knowledge contained in pre-trained speech representations extracted from models trained on a more general self-supervised task which doesn't require human annotations, such as the wav2vec model. We provide detailed insights on the benefits of our approach by varying the training data size, which can help labeling teams to work more efficiently. We compare performance with other popular methods on the IEMOCAP dataset, a well-benchmarked dataset among the Speech Emotion Recognition (SER) research community. Furthermore, we demonstrate that results can be greatly improved by combining acoustic and linguistic knowledge from transfer learning. We align acoustic pre-trained representations with semantic representations from the BERT model through an attention-based recurrent neural network. Performance improves significantly when combining both modalities and scales with the amount of data. When trained on the full IEMOCAP dataset, we reach a new state-of-the-art of 73.9% unweighted accuracy (UA).",0
"This study aimed to explore whether self-supervised transfer learning can improve emotion recognition performance using less data compared to traditional supervised learning approaches. In recent years, deep learning techniques have shown promise for emotion recognition from human speech signals. However, these methods often require large amounts of labeled training data to achieve acceptable accuracy levels. To address this issue, we propose a new approach that utilizes unlabeled data from related tasks to pretrain models before fine-tuning on a smaller amount of labeled data specific to the target task at hand. Our experiments demonstrate that our proposed method achieves significantly better performance compared to state-of-the-art baseline algorithms across multiple datasets while only requiring half as many examples during finetuning. These results suggest that self-supervised transfer learning has great potential for improving emotion recognition models by effectively leveraging scarce labeling resources. Additionally, by relying more heavily on unlabeled data from diverse domains, our method may enable models to generalize better to real-world situations where emotional contexts vary greatly depending on individual speakers, languages, topics, etc.",1
"Change detection is a basic task of remote sensing image processing. The research objective is to identity the change information of interest and filter out the irrelevant change information as interference factors. Recently, the rise of deep learning has provided new tools for change detection, which have yielded impressive results. However, the available methods focus mainly on the difference information between multitemporal remote sensing images and lack robustness to pseudo-change information. To overcome the lack of resistance of current methods to pseudo-changes, in this paper, we propose a new method, namely, dual attentive fully convolutional Siamese networks (DASNet) for change detection in high-resolution images. Through the dual-attention mechanism, long-range dependencies are captured to obtain more discriminant feature representations to enhance the recognition performance of the model. Moreover, the imbalanced sample is a serious problem in change detection, i.e. unchanged samples are much more than changed samples, which is one of the main reasons resulting in pseudo-changes. We put forward the weighted double margin contrastive loss to address this problem by punishing the attention to unchanged feature pairs and increase attention to changed feature pairs. The experimental results of our method on the change detection dataset (CDD) and the building change detection dataset (BCDD) demonstrate that compared with other baseline methods, the proposed method realizes maximum improvements of 2.1\% and 3.6\%, respectively, in the F1 score. Our Pytorch implementation is available at https://github.com/lehaifeng/DASNet.",0
"Change detection is the task of identifying changes between two or more satellite images taken at different points in time. This can reveal important insights into urban growth, land use, natural disasters, and many other phenomena. One common method used in change detection is feature extraction from the images, which involves transforming the raw image data into numerical vectors that capture relevant features such as texture, shape, and color. However, traditional feature extraction methods may struggle with complex patterns and variations within each image, resulting in incomplete or inaccurate representations. In our recent work, we proposed a novel approach called DASNet (Dual Attentive Siamese Network), which uses deep learning techniques to improve upon existing feature extraction methods. Our network combines both global and local attention mechanisms to enable the model to attend to salient regions and features simultaneously, improving performance on tasks involving large intra-class variability. We applied our method to several benchmark datasets containing pairs of highresolution satellite images, achieving state-of-theart results across all metrics. Overall, our findings demonstrate the potential of using advanced computer vision models like DASNet for addressing the challenges inherent in change detection of complex satellite imagery, paving the way for new applications in remote sensing and related fields.",1
"Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github.com/cmsflash/efficient-attention.",0
"This paper presents novel attention mechanisms that have linear time complexity for both forward pass (per token) and backward pass computations across models. We address shortcomings in prior attention approaches by reducing their computational costs. We analyze different attention architectures, including Self-Attention Networks (SAN), the Transformer architecture, and more recent proposals like Block Attention Mechanism (BAM) from a computational perspective. Our approach has several advantages over these methods: (1) improves efficiency without sacrificing accuracy; (2) achieves faster inference speeds on large datasets compared to other attention frameworks; and (3) enables the training of even larger language models than previously possible. Extensive experimental evaluation shows superior results against state-of-the-art transformer baselines on several benchmarks datasets. Overall, our work makes significant contributions towards making the applications of deep learning systems feasible while providing improved performance, potentially enabling new use cases for natural language processing tasks.",1
"In quantum mechanics, a norm squared wave function can be interpreted as the probability density that describes the likelihood of a particle to be measured in a given position or momentum. This statistical property is at the core of the fuzzy structure of microcosmos. Recently, hybrid neural structures raised intense attention, resulting in various intelligent systems with far-reaching influence. Here, we propose a probability-density-based deep learning paradigm for the fuzzy design of functional meta-structures. In contrast to other inverse design methods, our probability-density-based neural network can efficiently evaluate and accurately capture all plausible meta-structures in a high-dimensional parameter space. Local maxima in probability density distribution correspond to the most likely candidates to meet the desired performances. We verify this universally adaptive approach in but not limited to acoustics by designing multiple meta-structures for each targeted transmission spectrum, with experiments unequivocally demonstrating the effectiveness and generalization of the inverse design.",0
"A new deep learning paradigm called ""Probability Density Based (PDB)"" has been proposed that uses probability density as a measure of uncertainty to guide training of artificial neural networks (ANNs). This approach allows for improved generalization performance over traditional backpropagation methods, enabling more efficient and accurate fuzzy design of functional metastructures. The PDB algorithm employs Bayesian inference principles by modeling uncertainty through Gaussian processes prior to training, providing a framework that supports both supervised and unsupervised tasks while accounting for potential noise within datasets. Numerical experiments demonstrate the superior accuracy and robustness of PDB against other contemporary methodologies for image classification and segmentation applications. With further development, the PDB paradigm could lead to significant advancements in computer vision and material science fields, where metastructure design plays a critical role in product engineering and manufacturing.",1
"In quantum mechanics, a norm squared wave function can be interpreted as the probability density that describes the likelihood of a particle to be measured in a given position or momentum. This statistical property is at the core of the microcosmos. Meanwhile, machine learning inverse design of materials raised intensive attention, resulting in various intelligent systems for matter engineering. Here, inspired by quantum theory, we propose a probabilistic deep learning paradigm for the inverse design of functional meta-structures. Our probability-density-based neural network (PDN) can accurately capture all plausible meta-structures to meet the desired performances. Local maxima in probability density distribution correspond to the most likely candidates. We verify this approach by designing multiple meta-structures for each targeted transmission spectrum to enrich design choices.",0
"This paper presents a novel probabilistic approach for solving inverse design problems related to meta-materials, which exhibit tailored physical properties based on their microstructure. By leveraging recent advances from quantum physics involving the representation of uncertainty as density operators, we propose a robust optimization framework capable of capturing both aleatoric (random) and epistemic (stochastic) uncertainties inherent within such complex systems. Our methodology enables efficient exploration of high-dimensional search spaces and identification of optimized architectures through data augmentation techniques that generate new synthesizable materials designs based on available training examples. Experiments using public datasets demonstrate significant improvements compared to prior work in terms of accuracy metrics, resulting in competitive results across multiple benchmark tasks while providing robustness against challenging conditions imposed by aleatoric noise. Given these promising outcomes, our model serves as an important tool for accelerating the development of advanced functional meta-structures relevant to diverse applications ranging from energy harvesting devices to secure communications technology. With further refinement and adaptation, our methods hold great potential for enhancing human society by offering more sustainable alternatives in engineering practice through optimal utilization of resources coupled with environmentally benign manufacturability considerations during fabrication processes.",1
"Neural Memory Networks (NMNs) have received increased attention in recent years compared to deep architectures that use a constrained memory. Despite their new appeal, the success of NMNs hinges on the ability of the gradient-based optimiser to perform incremental training of the NMN controllers, determining how to leverage their high capacity for knowledge retrieval. This means that while excellent performance can be achieved when the training data is consistent and well distributed, rare data samples are hard to learn from as the controllers fail to incorporate them effectively during model training. Drawing inspiration from the human cognition process, in particular the utilisation of neuromodulators in the human brain, we propose to decouple the learning process of the NMN controllers to allow them to achieve flexible, rapid adaptation in the presence of new information. This trait is highly beneficial for meta-learning tasks where the memory controllers must quickly grasp abstract concepts in the target domain, and adapt stored knowledge. This allows the NMN controllers to quickly determine which memories are to be retained and which are to be erased, and swiftly adapt their strategy to the new task at hand. Through both quantitative and qualitative evaluations on multiple public benchmarks, including classification and regression tasks, we demonstrate the utility of the proposed approach. Our evaluations not only highlight the ability of the proposed NMN architecture to outperform the current state-of-the-art methods, but also provide insights on how the proposed augmentations help achieve such superior results. In addition, we demonstrate the practical implications of the proposed learning strategy, where the feedback path can be shared among multiple neural memory networks as a mechanism for knowledge sharing.",0
"Abstract: This research explores a novel approach to incorporating synthetic gradients into neural memory controllers for fast and slow learning. By introducing a time-varying hyperparameter that modulates the strength of these synthetic gradients, we show how to effectively control the rate at which neurons learn new associations. We demonstrate the effectiveness of our method through simulations of various tasks and evaluate its performance against existing methods. Our results suggest that our approach outperforms traditional methods of controlling learning rates and leads to more stable and efficient learning dynamics. These findings have important implications for understanding the mechanisms underlying plasticity and could lead to new insights into the design of artificial neural networks.",1
"Taking advantage of human pose data for understanding human activities has attracted much attention these days. However, state-of-the-art pose estimators struggle in obtaining high-quality 2D or 3D pose data due to occlusion, truncation and low-resolution in real-world un-annotated videos. Hence, in this work, we propose 1) a Selective Spatio-Temporal Aggregation mechanism, named SST-A, that refines and smooths the keypoint locations extracted by multiple expert pose estimators, 2) an effective weakly-supervised self-training framework which leverages the aggregated poses as pseudo ground-truth instead of handcrafted annotations for real-world pose estimation. Extensive experiments are conducted for evaluating not only the upstream pose refinement but also the downstream action recognition performance on four datasets, Toyota Smarthome, NTU-RGB+D, Charades, and Kinetics-50. We demonstrate that the skeleton data refined by our Pose-Refinement system (SSTA-PRS) is effective at boosting various existing action recognition models, which achieves competitive or state-of-the-art performance.",0
"This paper presents a novel approach towards understanding human activities in real world videos using computer vision techniques. We propose a selective spatio-temporal aggregation based pose refinement system (SSP), which utilizes pose estimation techniques such as Deep Pose to predict body poses in videos. Our proposed method then aggregates these predicted poses over both spatial and temporal dimensions to produce more accurate estimates, taking into account contextual information from neighboring frames and regions within the video. Additionally, we leverage motion trajectory analysis methods to further improve pose accuracy. Finally, our framework can take advantage of pre-trained models on large datasets, allowing for easy deployment across different applications. Experimental results demonstrate significant improvements in pose estimation accuracy compared to existing state-of-the-art approaches. Overall, SSP offers new opportunities for improved understanding of human activity recognition in complex real-world scenarios, such as those encountered in surveillance footage.",1
"Conversational recommendation systems have recently gain a lot of attention, as users can continuously interact with the system over multiple conversational turns. However, conversational recommendation systems are based on complex neural architectures, thus the training cost of such models is high. To shed light on the high computational training time of state-of-the art conversational models, we examine five representative strategies and demonstrate this issue. Furthermore, we discuss possible ways to cope with the high training cost following knowledge distillation strategies, where we detail the key challenges to reduce the online inference time of the high number of model parameters in conversational recommendation systems",0
"Title: On Estimating the Training Cost of Conversational Recommendation Systems  Abstract: In recent years, conversational recommendation systems (CRSs) have emerged as promising tools for personalized interaction between users and interactive applications, including virtual assistants, chatbots, and other humanlike interfaces. While CRS research has made significant progress in developing new models and methods, there remains little understanding of how to effectively estimate the training costs of these systems. This study addresses this gap by exploring the key factors that influence the computational cost of training CRSs, including model size, complexity, data volume, feature dimensionality, user specificities, system capabilities, application requirements, and performance constraints. Our findings suggest that estimating the training costs of CRSs requires careful consideration of multiple dimensions, and we provide guidelines on how to accurately evaluate these costs based on real-world case studies and experimental results. By providing insights into the tradeoffs involved in designing and deploying CRSs, our work helps pave the way towards more efficient and effective use of computational resources in CRS development.",1
"Co-Salient Object Detection (CoSOD) aims at discovering salient objects that repeatedly appear in a given query group containing two or more relevant images. One challenging issue is how to effectively capture co-saliency cues by modeling and exploiting inter-image relationships. In this paper, we present an end-to-end collaborative aggregation-and-distribution network (CoADNet) to capture both salient and repetitive visual patterns from multiple images. First, we integrate saliency priors into the backbone features to suppress the redundant background information through an online intra-saliency guidance structure. After that, we design a two-stage aggregate-and-distribute architecture to explore group-wise semantic interactions and produce the co-saliency features. In the first stage, we propose a group-attentional semantic aggregation module that models inter-image relationships to generate the group-wise semantic representations. In the second stage, we propose a gated group distribution module that adaptively distributes the learned group semantics to different individuals in a dynamic gating mechanism. Finally, we develop a group consistency preserving decoder tailored for the CoSOD task, which maintains group constraints during feature decoding to predict more consistent full-resolution co-saliency maps. The proposed CoADNet is evaluated on four prevailing CoSOD benchmark datasets, which demonstrates the remarkable performance improvement over ten state-of-the-art competitors.",0
"This paper proposes the use of collaborative aggregation-and-distribution networks (CoADNets) for co-salient object detection. Inspired by recent advances in computer vision and machine learning, our approach introduces new methods that allow multiple objects within an image to communicate with each other and aggregate their features to enhance overall performance. Our experiments demonstrate the effectiveness of CoADNets compared to traditional saliency models, leading to improvements in accuracy and robustness across different datasets. We believe that this work provides valuable insights into understanding inter-object relationships and could open up new possibilities for applications such as robotic navigation and autonomous driving. ===== This study presents the development of collaboration among multiple objects through joint feature aggregation using artificial neural network architectures termed â€œCollaborative Aggregation Distribution Networksâ€ (CoADNeTs). By leveraging these networks to model relationships and dependencies among objects, enhanced performance can be achieved in tasks like co-salient object detection. Evaluations indicate superiority over baseline approaches, providing implications for broader application areas requiring comprehension of interconnected systems behavior. These might include mobile manipulation robots and self-driving vehicles.",1
"While general object detection has seen tremendous progress, localization of elliptical objects has received little attention in the literature. Our motivating application is the detection of knots in sawn timber images, which is an important problem since the number and types of knots are visual characteristics that adversely affect the quality of sawn timber. We demonstrate how models can be tailored to the elliptical shape and thereby improve on general purpose detectors; more generally, elliptical defects are common in industrial production, such as enclosed air bubbles when casting glass or plastic. In this paper, we adapt the Faster R-CNN with its Region Proposal Network (RPN) to model elliptical objects with a Gaussian function, and extend the existing Gaussian Proposal Network (GPN) architecture by adding the region-of-interest pooling and regression branches, as well as using the Wasserstein distance as the loss function to predict the precise locations of elliptical objects. Our proposed method has promising results on the lumber knot dataset: knots are detected with an average intersection over union of 73.05%, compared to 63.63% for general purpose detectors. Specific to the lumber application, we also propose an algorithm to correct any misalignment in the raw timber images during scanning, and contribute the first open-source lumber knot dataset by labeling the elliptical knots in the preprocessed images.",0
"This paper presents a method for detecting ellipses and localizing their centers in images of sawn lumber with high accuracy using a deep convolutional neural network (CNN). The proposed approach leverages recent advances in machine learning and computer vision to overcome limitations of traditional feature extraction methods that rely on intensity thresholding or edge detection. Our CNN architecture employs two branches: one branch estimates binary masks encoding discriminative features, while the other regresses directly to the ellipse parameters including center location and shape characteristics. We report experiments conducted on a large dataset of sawn lumber images demonstrating superior performance compared to state-of-the-art methods, resulting in accurate elliptical knot segmentation at competitive speeds. Finally, we discuss potential applications exploiting automated knot mapping in wood industry workflows and scientific analysis of tree stem structures. Future work includes extending our framework towards more complex shapes and improving real-time deployment efficiency.",1
"Semantic segmentation is a task that traditionally requires a large dataset of pixel-level ground truth labels, which is time-consuming and expensive to obtain. Recent advancements in the weakly-supervised setting show that reasonable performance can be obtained by using only image-level labels. Classification is often used as a proxy task to train a deep neural network from which attention maps are extracted. However, the classification task needs only the minimum evidence to make predictions, hence it focuses on the most discriminative object regions. To overcome this problem, we propose a novel formulation of adversarial erasing of the attention maps. In contrast to previous adversarial erasing methods, we optimize two networks with opposing loss functions, which eliminates the requirement of certain suboptimal strategies; for instance, having multiple training steps that complicate the training process or a weight sharing policy between networks operating on different distributions that might be suboptimal for performance. The proposed solution does not require saliency masks, instead it uses a regularization loss to prevent the attention maps from spreading to less discriminative object regions. Our experiments on the Pascal VOC dataset demonstrate that our adversarial approach increases segmentation performance by 2.1 mIoU compared to our baseline and by 1.0 mIoU compared to previous adversarial erasing approaches.",0
"This paper presents a new method called end-to-end adversarial erasing (eae) that can improve weakly supervised semantic segmentation by using an erasing mechanism to simulate high quality ground truth annotations. By iteratively training an image generator and discriminator on each other using alternating least squares optimization, eae produces synthetic annotations that closely resemble real ones while preserving spatial structure. Compared to existing methods such as cutout and random erasing, our approach has been shown to achieve state-of-the-art performance across multiple benchmarks including PASCAL VOC 2012, COCO, and Cityscapes without needing additional annotated data during inference time. Our results demonstrate the effectiveness of our proposed method for improving weakly supervised semantic segmentation and its potential applications in automotive, robotics, computer vision, and autonomous systems.",1
"Feature fusion, the combination of features from different layers or branches, is an omnipresent part of modern network architectures. It is often implemented via simple operations, such as summation or concatenation, but this might not be the best choice. In this work, we propose a uniform and general scheme, namely attentional feature fusion, which is applicable for most common scenarios, including feature fusion induced by short and long skip connections as well as within Inception layers. To better fuse features of inconsistent semantics and scales, we propose a multi-scale channel attention module, which addresses issues that arise when fusing features given at different scales. We also demonstrate that the initial integration of feature maps can become a bottleneck and that this issue can be alleviated by adding another level of attention, which we refer to as iterative attentional feature fusion. With fewer layers or parameters, our models outperform state-of-the-art networks on both CIFAR-100 and ImageNet datasets, which suggests that more sophisticated attention mechanisms for feature fusion hold great potential to consistently yield better results compared to their direct counterparts. Our codes and trained models are available online.",0
"This sounds like an interesting topic! But I cannot complete your request without more context on the subject matter you would like me to write about, could you please provide more details?",1
"Weather forecasting benefits us in various ways from farmers in cultivation and harvesting their crops to airlines to schedule their flights. Weather forecasting is a challenging task due to the chaotic nature of the atmosphere. Therefore lot of research attention has drawn to obtain the benefits and to overcome the challenges of weather forecasting. This paper compares ARIMA (Auto Regressive Integrated Moving Average) model and deep learning models to forecast temperature. The deep learning model consists of one dimensional convolutional layers to extract spatial features and LSTM layers to extract temporal features. Both of these models are applied to hourly temperature data set from Szeged, Hungry. According to the experimental results deep learning model was able to perform better than the traditional ARIMA methodology.",0
"Title: Comparison between ARIMA and Deep Learning Models for Temperature Forecasting Abstract Time series forecasting has become increasingly important as large amounts of data continue to grow. Accurate temperature predictions can help in many fields such as energy management, agriculture, and weather forecasting. This study compares two popular models used for time series prediction - ARIMA (AutoRegressive Integrated Moving Average) model and deep learning models. The comparison was performed using hourly temperature datasets collected from ten locations in Europe. Both models were trained on eight years worth of data, from January 2009 to December 2017. For each location, one week ahead predictions were generated. Evaluation metrics included mean absolute error (MAE), root mean square error (RMSE) and coefficient of determination R^2 . Results showed that while both models produced similar levels of accuracy in some cases, deep learning models outperformed ARIMA models in most scenarios. This suggests that deep learning methods may have more potential than traditional statistical models for predicting time series data. Keywords: time series forecasting, ARIMA models, deep learning models, temperature forecasting.",1
"In this work, we study a new image annotation task named Extractive Tags Summarization (ETS). The goal is to extract important tags from the context lying in an image and its corresponding tags. We adjust some state-of-the-art deep learning models to utilize both visual and textual information. Our proposed solution consists of different widely used blocks like convolutional and self-attention layers, together with a novel idea of combining auxiliary loss functions and the gating mechanism to glue and elevate these fundamental components and form a unified architecture. Besides, we introduce a loss function that aims to reduce the imbalance of the training data and a simple but effective data augmentation technique dedicated to alleviates the effect of outliers on the final results. Last but not least, we explore an unsupervised pre-training strategy to further boost the performance of the model by making use of the abundant amount of available unlabeled data. Our model shows the good results as 90% $F_\text{1}$ score on the public NUS-WIDE benchmark, and 50% $F_\text{1}$ score on a noisy large-scale real-world private dataset. Source code for reproducing the experiments is publicly available at: https://github.com/pixta-dev/labteam",0
"This may change as I have more time. For now here you go: An efficient deep learning method for extracting tags from summaries to create summaries of original documents is presented in our recent paper, titled â€œMAGNeto.â€ Our approach leverages fine-grained multi-label attention (FGM) along with an additional new tag aggregation module named MagNeto that increases model efficiency without sacrificing performance. We evaluate FGM against other state-of-the-art models for extractive summary generation on standard benchmark datasets, demonstrating improved accuracy across all metrics. Additionally, we demonstrate that MagNeto can effectively identify important content from unimportant information during summarization. With our proposed framework, users benefit from increased efficiency and better performance than traditional methods while achieving superior results in terms of ROUGE scores, self-attention-based metricsscored by human judges, and qualitative evaluation of extracted summaries by experts.",1
"Visual scene graph generation is a challenging task. Previous works have achieved great progress, but most of them do not explicitly consider the class imbalance issue in scene graph generation. Models learned without considering the class imbalance tend to predict the majority classes, which leads to a good performance on trivial frequent predicates, but poor performance on informative infrequent predicates. However, predicates of minority classes often carry more semantic and precise information~(\textit{e.g.}, \emph{`on'} v.s \emph{`parked on'}). % which leads to a good score of recall, but a poor score of mean recall. To alleviate the influence of the class imbalance, we propose a novel model, dubbed \textit{dual ResGCN}, which consists of an object residual graph convolutional network and a relation residual graph convolutional network. The two networks are complementary to each other. The former captures object-level context information, \textit{i.e.,} the connections among objects. We propose a novel ResGCN that enhances object features in a cross attention manner. Besides, we stack multiple contextual coefficients to alleviate the imbalance issue and enrich the prediction diversity. The latter is carefully designed to explicitly capture relation-level context information \textit{i.e.,} the connections among relations. We propose to incorporate the prior about the co-occurrence of relation pairs into the graph to further help alleviate the class imbalance issue. Extensive evaluations of three tasks are performed on the large-scale database VG to demonstrate the superiority of the proposed method.",0
"In this paper we propose Dual Residual GCN (DualResGCN), a novel graph convolutional network architecture that outperforms state-of-the-art methods on scene graphs generation tasks while using only half the number of parameters. Our key insight is that dual ResBlocks can enhance the feature extraction efficiency by capturing multi-scale features across different levels of abstraction while maintaining balance between local and global dependencies. Furthermore, our design allows parallel processing at multiple scales without increasing computational cost, which significantly boosts performance over single-scale models. We empirically demonstrate the effectiveness of our approach against strong baselines using four benchmark datasets: Stanford Online Products, Fashion2K, COCO and Visual Genome. On all datasets except Stanford OP where competitive results were achieved, DualResGCN substantially improves state-of-the-art accuracies by large margins ranging from 1.7% to as high as 8.4%. Finally, ablation studies conducted showed further improvements could be attained through simple yet effective techniques such as image-guided attention. Overall, these results indicate that our method has great potential in unlocking more advanced applications relying on efficient and accurate object detection, representation and reasoning, among others.",1
"Cardiovascular Disease (CVD) is considered as one of the principal causes of death in the world. Over recent years, this field of study has attracted researchers' attention to investigate heart sounds' patterns for disease diagnostics. In this study, an approach is proposed for normal/abnormal heart sound classification on the Physionet challenge 2016 dataset. For the first time, a fixed-length feature vector; called i-vector; is extracted from each heart sound using Mel Frequency Cepstral Coefficient (MFCC) features. Afterwards, Principal Component Analysis (PCA) transform and Variational Autoencoder (VAE) are applied on the i-vector to achieve dimension reduction. Eventually, the reduced size vector is fed to Gaussian Mixture Models (GMMs) and Support Vector Machine (SVM) for classification purpose. Experimental results demonstrate the proposed method could achieve a performance improvement of 16% based on Modified Accuracy (MAcc) compared with the baseline system on the Physoinet dataset.",0
"In recent years, heart sound analysis has gained attention as a noninvasive method for diagnosing cardiovascular diseases. Automatic analysis systems have been developed using signal processing techniques such as time domain features, frequency domain features, and wavelet transforms. However, these methods face challenges due to variations in environmental noise, recording equipment, and patient conditions. To address these issues, statistical feature embeddings have emerged as an alternative approach that can extract robust features from raw signals.  This study proposes a novel method for heart sound classification based on statistical feature embeddings. We use Gaussian mixtures models to generate a probability density function (PDF) for each heart cycle. By applying Principal Component Analysis (PCA) on the obtained PDFs, we obtain a low-dimensional representation of the data that captures both temporal and spectral characteristics. Finally, we train classifiers using these embedded features to distinguish healthy subjects from patients suffering from different types of arrhythmias, including atrial fibrillation and normal sinus rhythm.  Experimental results show that our proposed method achieves high accuracy compared to other traditional approaches. Our findings demonstrate the potential utility of statistical feature embeddings in overcoming the limitations associated with current heart sound analysis techniques. This work highlights the importance of developing reliable automatic analysis systems for early detection and prevention of cardiovascular diseases.",1
"Image clustering has recently attracted significant attention due to the increased availability of unlabelled datasets. The efficiency of traditional clustering algorithms heavily depends on the distance functions used and the dimensionality of the features. Therefore, performance degradation is often observed when tackling either unprocessed images or high-dimensional features extracted from processed images. To deal with these challenges, we propose a deep clustering framework consisting of a modified generative adversarial network (GAN) and an auxiliary classifier. The modification employs Sobel operations prior to the discriminator of the GAN to enhance the separability of the learned features. The discriminator is then leveraged to generate representations as the input to an auxiliary classifier. An adaptive objective function is utilised to train the auxiliary classifier for clustering the representations, aiming to increase the robustness by minimizing the divergence of multiple representations generated by the discriminator. The auxiliary classifier is implemented with a group of multiple cluster-heads, where a tolerance hyper-parameter is used to tackle imbalanced data. Our results indicate that the proposed method significantly outperforms state-of-the-art clustering methods on CIFAR-10 and CIFAR-100, and is competitive on the STL10 and MNIST datasets.",0
"This study presents a novel approach for image clustering that utilizes an augmented generative adversarial network (GAN) along with information maximization techniques. Our proposed method leverages the power of GANs to generate additional samples from each cluster, allowing us to learn more information than traditional methods. By incorporating an auxiliary task of maximizing mutual information between representations learned by the generator and discriminator, we achieve improved clustering results on multiple benchmark datasets. In addition, our framework allows for easy integration of other algorithms, such as feature extraction techniques, making it highly adaptive to different use cases. Experimental results demonstrate significant improvements over state-of-the-art approaches, while visualizations highlight the quality of generated images. Overall, our work advances the field of unsupervised learning for image clustering by introducing an effective method that combines deep neural networks with information theory principles.",1
"Physiologic signals have properties across multiple spatial and temporal scales, which can be shown by the complexity-analysis of the coarse-grained physiologic signals by scaling techniques such as the multiscale. Unfortunately, the results obtained from the coarse-grained signals by the multiscale may not fully reflect the properties of the original signals because there is a loss caused by scaling techniques and the same scaling technique may bring different losses to different signals. Another problem is that multiscale does not consider the key observations inherent in the signal. Here, we show a new analysis method for time series called the loss-analysis via attention-scale. We show that multiscale is a special case of attention-scale. The loss-analysis can complement to the complexity-analysis to capture aspects of the signals that are not captured using previously developed measures. This can be used to study ageing, diseases, and other physiologic phenomenon.",0
"This research presents a new methodology for analyzing physiological time series data using attention mechanisms and loss functions. We introduce a framework that utilizes the scale of attention as a means to quantify changes in signal quality over time, providing insight into areas where signal degradation may occur. Our approach uses customized losses tailored to specific applications and leverages attention weights to identify segments of interest. We demonstrate the effectiveness of our method by applying it to real-world datasets including EEG recordings from patients suffering from neurological disorders such as epilepsy and Parkinson's disease, as well as intracranial pressure readings from stroke victims. Experimental results showcase the potential of our technique to provide valuable insights for clinicians, enabling them to make better informed decisions regarding treatment options. Overall, our work represents a significant step towards understanding how machine learning can contribute to medical research, improving patient outcomes, and enhancing healthcare services.",1
"Distance metric learning has attracted much attention in recent years, where the goal is to learn a distance metric based on user feedback. Conventional approaches to metric learning mainly focus on learning the Mahalanobis distance metric on data attributes. Recent research on metric learning has been extended to sequential data, where we only have structural information in the sequences, but no attribute is available. However, real-world applications often involve attributed sequence data (e.g., clickstreams), where each instance consists of not only a set of attributes (e.g., user session context) but also a sequence of categorical items (e.g., user actions). In this paper, we study the problem of metric learning on attributed sequences. Unlike previous work on metric learning, we now need to go beyond the Mahalanobis distance metric in the attribute feature space while also incorporating the structural information in sequences. We propose a deep learning framework, called MLAS (Metric Learning on Attributed Sequences), to learn a distance metric that effectively measures dissimilarities between attributed sequences. Empirical results on real-world datasets demonstrate that the proposed MLAS framework significantly improves the performance of metric learning compared to state-of-the-art methods on attributed sequences.",0
"This paper presents a new method for metric learning on attributed sequences called ""Metric Learning on Attributed Sequences"" (MLAS). The goal of metric learning is to learn a distance measure that allows objects from the same class to be close together and those from different classes to be far apart. Many previous approaches have focused on learning these distances using static features rather than taking advantage of the additional structure provided by temporal data such as sequences. Our approach uses both static features and sequence attributes to compute embeddings in a common space where we can directly compare samples. We then use deep metric learning techniques based on contrastive loss functions and neural networks to train the model. Experimental results show that our method outperforms state-of-the-art methods across several benchmark datasets and metrics for classification, retrieval, and clustering tasks. The proposed framework provides a flexible solution for handling complex problems related to feature extraction, alignment, and comparison in pattern recognition scenarios involving sequential data representations. Overall, the contribution of the paper lies in proposing an innovative methodology for learning meaningful distances over multi-modal signals and leveraging temporal dependencies among sample components.",1
"Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena.",0
"This paper presents LongRange Arena, a benchmark dataset designed specifically for evaluating the performance of transformer models on tasks that require reasoning over long spans of textual input. We observe that despite their success on a wide range of natural language processing (NLP) tasks, many state-of-the-art transformer models struggle when asked to encode and reason over very large inputs. To address this gap, we propose a suite of challenging NLP tasks, each requiring models to attend over thousands of tokens and perform complex operations such as cross-document inference, numerical reasoning, and commonsense knowledge retrieval. Our benchmark includes both task-specific metrics as well as more general measures like context-aware span ranking and masked token prediction. We evaluate a diverse set of modern transformer architectures and pretraining methods, showing significant gaps in model capability across different tasks and demonstrating the importance of specialized components like causal attention and dynamic memory. Finally, we provide detailed analysis comparing different variants of transformers on these tasks, highlighting areas of strength and weakness which can guide future research into making them more powerful tools for handling real world problems involving long-range dependencies. Keywords: Long-range Dependencies; Attention Models; Transformer Architectures; Natural Language Processing; Commonsense Reasoning; Cross-Document Inference; Masked Token Prediction; Task-Specific Metrics; Context-Aware Ranking",1
"It is well known that human gaze carries significant information about visual attention. However, there are three main difficulties in incorporating the gaze data in an attention mechanism of deep neural networks: 1) the gaze fixation points are likely to have measurement errors due to blinking and rapid eye movements; 2) it is unclear when and how much the gaze data is correlated with visual attention; and 3) gaze data is not always available in many real-world situations. In this work, we introduce an effective probabilistic approach to integrate human gaze into spatiotemporal attention for egocentric activity recognition. Specifically, we represent the locations of gaze fixation points as structured discrete latent variables to model their uncertainties. In addition, we model the distribution of gaze fixations using a variational method. The gaze distribution is learned during the training process so that the ground-truth annotations of gaze locations are no longer needed in testing situations since they are predicted from the learned gaze distribution. The predicted gaze locations are used to provide informative attentional cues to improve the recognition performance. Our method outperforms all the previous state-of-the-art approaches on EGTEA, which is a large-scale dataset for egocentric activity recognition provided with gaze measurements. We also perform an ablation study and qualitative analysis to demonstrate that our attention mechanism is effective.",0
"This research presents a new method for integrating human gaze data into attention models for egocentric activity recognition. The authors propose using eye tracking technology to capture the wearer's gaze direction and incorporate it into the attention mechanism during training and inference. By doing so, they show that adding gaze information improves the accuracy and robustness of attention-based models for recognizing activities performed by the wearer. Their approach leverages the state-of-the-art attention mechanisms and achieves competitive results on two popular benchmark datasets. Overall, their work demonstrates the potential benefits of incorporating additional sensor modalities like gaze into attention architectures for enhanced activity recognition performance.",1
"In this paper, we present a novel method Coarse- and Fine-grained Attention Network (CFANet) for generating high-quality crowd density maps and people count estimation by incorporating attention maps to better focus on the crowd area. We devise a from-coarse-to-fine progressive attention mechanism by integrating Crowd Region Recognizer (CRR) and Density Level Estimator (DLE) branch, which can suppress the influence of irrelevant background and assign attention weights according to the crowd density levels, because generating accurate fine-grained attention maps directly is normally difficult. We also employ a multi-level supervision mechanism to assist the backpropagation of gradient and reduce overfitting. Besides, we propose a Background-aware Structural Loss (BSL) to reduce the false recognition ratio while improving the structural similarity to groundtruth. Extensive experiments on commonly used datasets show that our method can not only outperform previous state-of-the-art methods in terms of count accuracy but also improve the image quality of density maps as well as reduce the false recognition ratio.",0
"In this paper, we present a novel approach for crowd density map estimation using coarse- and fine-grained attention networks with background-aware loss. Our method addresses several challenges associated with traditional approaches such as pixel-wise regression and convolutional neural networks (CNNs), which often suffer from limited spatial resolution and difficulty handling irregular densities. By utilizing a combination of coarse- and fine-grained attention mechanisms, our model can effectively capture both global context and local details, resulting in more accurate estimates. Additionally, we introduce a new background-aware loss function that helps the network better differentiate crowded areas from empty spaces, leading to improved performance in complex scenes. We evaluate our approach on two benchmark datasets and demonstrate significant improvements over state-of-the-art methods in terms of both quantitative metrics and visual quality. Overall, our work shows promise for applications in various fields where accurate crowd density maps are essential, including urban planning, public safety, and event management.",1
"Regular pavement inspection plays a significant role in road maintenance for safety assurance. Existing methods mainly address the tasks of crack detection and segmentation that are only tailored for long-thin crack disease. However, there are many other types of diseases with a wider variety of sizes and patterns that are also essential to segment in practice, bringing more challenges towards fine-grained pavement inspection. In this paper, our goal is not only to automatically segment cracks, but also to segment other complex pavement diseases as well as typical landmarks (markings, runway lights, etc.) and commonly seen water/oil stains in a single model. To this end, we propose a three-stream boundary-aware network (TB-Net). It consists of three streams fusing the low-level spatial and the high-level contextual representations as well as the detailed boundary information. Specifically, the spatial stream captures rich spatial features. The context stream, where an attention mechanism is utilized, models the contextual relationships over local features. The boundary stream learns detailed boundaries using a global-gated convolution to further refine the segmentation outputs. The network is trained using a dual-task loss in an end-to-end manner, and experiments on a newly collected fine-grained pavement disease dataset show the effectiveness of our TB-Net.",0
"This paper presents a new method called TB-Net for fine-grained pavement disease segmentation which uses three streams of information: image features extracted from RGB images using convolutional neural networks (CNNs), depth map features obtained by LiDAR data fusion using dilated convolutions with atrous spatial pyramid pooling (ASPP) modules, and semantic boundaries inferred from the raw LiDAR point cloud using graph convolutional networks (GCN). We demonstrate that incorporating all these three modalities into our network architecture results in improved performance compared to single modality approaches. Our proposed model achieves state-of-the art accuracy on two public datasets as well as outperforms other competitive baseline methods. We provide qualitative analysis showing the improvement provided by each stream for better understanding of different types of damage classes that benefit more from multimodal input representations. Additionally we analyze ablation studies proving superiority over alternative architectures.",1
"Due to accessible big data collections from consumers, products, and stores, advanced sales forecasting capabilities have drawn great attention from many companies especially in the retail business because of its importance in decision making. Improvement of the forecasting accuracy, even by a small percentage, may have a substantial impact on companies' production and financial planning, marketing strategies, inventory controls, supply chain management, and eventually stock prices. Specifically, our research goal is to forecast the sales of each product in each store in the near future. Motivated by tensor factorization methodologies for personalized context-aware recommender systems, we propose a novel approach called the Advanced Temporal Latent-factor Approach to Sales forecasting (ATLAS), which achieves accurate and individualized prediction for sales by building a single tensor-factorization model across multiple stores and products. Our contribution is a combination of: tensor framework (to leverage information across stores and products), a new regularization function (to incorporate demand dynamics), and extrapolation of tensor into future time periods using state-of-the-art statistical (seasonal auto-regressive integrated moving-average models) and machine-learning (recurrent neural networks) models. The advantages of ATLAS are demonstrated on eight product category datasets collected by the Information Resource, Inc., where a total of 165 million weekly sales transactions from more than 1,500 grocery stores over 15,560 products are analyzed.",0
"Improving sales forecasting accuracy can have significant benefits for companies, including better resource allocation and inventory management. However, traditional methods such as time series analysis and exponential smoothing often suffer from poor performance due to their limited ability to capture complex relationships between different factors that affect demand. In order to address these limitations, we propose a novel approach based on tensor factorization which incorporates additional sources of information, such as product features and seasonality, while maintaining the advantages of scalability and interpretability provided by linear models. We evaluate our method using data from two retailers and demonstrate its effectiveness through comparison with state-of-the-art alternatives. Our results show that our proposed approach significantly improves both accuracy and sharpness of sales forecasts, offering great potential to enhance decision making processes across various industries.",1
"This paper wants to focus on providing a characterization of the runtime performances of state-of-the-art implementations of KGE alghoritms, in terms of memory footprint and execution time. Despite the rapidly growing interest in KGE methods, so far little attention has been devoted to their comparison and evaluation; in particular, previous work mainly focused on performance in terms of accuracy in specific tasks, such as link prediction. To this extent, a framework is proposed for evaluating available KGE implementations against graphs with different properties, with a particular focus on the effectiveness of the adopted optimization strategies. Graphs and models have been trained leveraging different architectures, in order to enlighten features and properties of both models and the architectures they have been trained on. Some results enlightened with experiments in this document are the fact that multithreading is efficient, but benefit deacreases as the number of threads grows in case of CPU. GPU proves to be the best architecture for the given task, even if CPU with some vectorized instructions still behaves well. Finally, RAM utilization for the loading of the graph never changes between different architectures and depends only on the type of graph, not on the model.",0
"This paper presents a comprehensive evaluation of several popular knowledge graph embedding methods by comparing their runtime performances on real-world benchmark datasets. Knowledge graphs have become increasingly important tools for representing structured data and enabling natural language processing tasks such as question answering, entity recognition, and recommendation systems. Embeddings provide compact vector representations that can capture complex relationships among entities and concepts within these graphs. However, different embedding methods may exhibit significant differences in terms of their efficiency, scalability, and memory footprint. Therefore, choosing an appropriate method requires careful consideration of both model quality and computational requirements. In this work, we systematically analyze six state-of-the-art knowledge graph embedding approaches under identical experimental conditions, focusing on their computation time, space complexity, and parallelization potential. Our results demonstrate the tradeoffs involved in designing efficient embedding models, highlighting promising directions for future research. By providing a detailed evaluation framework, our study aims to inform practitioners working with knowledge graphs and guide them in selecting appropriate embedding methods tailored to specific use cases.",1
"Robust Mask R-CNN (Mask Regional Convolu-tional Neural Network) methods are proposed and tested for automatic detection of cracks on structures or their components that may be damaged during extreme events, such as earth-quakes. We curated a new dataset with 2,021 labeled images for training and validation and aimed to find end-to-end deep neural networks for crack detection in the field. With data augmentation and parameters fine-tuning, Path Aggregation Network (PANet) with spatial attention mechanisms and High-resolution Network (HRNet) are introduced into Mask R-CNNs. The tests on three public datasets with low- or high-resolution images demonstrate that the proposed methods can achieve a big improvement over alternative networks, so the proposed method may be sufficient for crack detection for a variety of scales in real applications.",0
"This paper presents end-to-event deep learning methods for automated damage detection in extreme events at various scales. In recent years, natural disasters such as hurricanes and earthquakes have caused significant destruction and loss of life worldwide. Rapid and accurate assessment of building damages can greatly improve response efforts and assist in post-disaster recovery planning. However, traditional inspection methods require onsite visualization which may put inspectors at risk due to hazardous conditions caused by these extreme events. To overcome the limitation, remote sensing images captured from satellite/aerial platforms has become a popular alternative approach for damage estimation. Most prevalent approaches rely on feature engineering that often requires manual extraction based on domain knowledge, and thus cannot scale up to more advanced scenarios with complex scene structures. To address this challenge, we propose novel hybrid multi-modal neural network architectures that achieve both attention guided fusion of complementary modalities (e.g., RGB and thermal) as well as semantic object labeling of structures inside each modality map, without relying on any human prior knowledge for feature design. Experimental results show our method outperforms current state-of-the-art baselines significantly while running at real-time speed during inference, indicating great potential for large-scale deployment in practice. Our models are designed to run offline since they utilize pre-trained models; however, techniques like transfer learning could enable adaptability and allow them to generalize better across diverse geographic regions once trained again using region specific data. Overall, this research opens doors for scalable and efficient solutions to support first respondersâ€™ decision making during critical times following catastrophic events. With further development, this technology could ultimately lead to improved post-disaster relief distribution and management of resources.",1
"Benefiting from the capability of building inter-dependencies among channels or spatial locations, attention mechanisms have been extensively studied and broadly used in a variety of computer vision tasks recently. In this paper, we investigate light-weight but effective attention mechanisms and present triplet attention, a novel method for computing attention weights by capturing cross-dimension interaction using a three-branch structure. For an input tensor, triplet attention builds inter-dimensional dependencies by the rotation operation followed by residual transformations and encodes inter-channel and spatial information with negligible computational overhead. Our method is simple as well as efficient and can be easily plugged into classic backbone networks as an add-on module. We demonstrate the effectiveness of our method on various challenging tasks including image classification on ImageNet-1k and object detection on MSCOCO and PASCAL VOC datasets. Furthermore, we provide extensive in-sight into the performance of triplet attention by visually inspecting the GradCAM and GradCAM++ results. The empirical evaluation of our method supports our intuition on the importance of capturing dependencies across dimensions when computing attention weights. Code for this paper can be publicly accessed at https://github.com/LandskapeAI/triplet-attention",0
"In recent years, convolutional neural networks (CNNs) have achieved state-of-the-art results on numerous computer vision tasks due to their ability to learn hierarchical representations from raw image data. However, these models often struggle with attending to relevant features when processing input images that contain large amounts of irrelevant information, leading to suboptimal performance.  To address this issue, we propose the Rotate to Attend: Convolutional Triplet Attention module (RCA), which utilizes spatial attention mechanisms within CNNs to selectively focus on informative regions of an image while disregarding others. Specifically, our method involves rotating each layer output by a fixed angle before feeding it through another set of convolutions and adding it back into the original feature map using element-wise addition. By doing so, RCA encourages the model to attend to different parts of the image at different stages of computation, allowing for more efficient learning and better generalization performance across diverse datasets.  Our experimental evaluation demonstrates that incorporating RCA as part of the regular architecture significantly improves accuracy compared to traditional approaches. Additionally, we compare RCA against several other state-of-the-art methods for spatial attention and show that it consistently outperforms them across multiple benchmark databases. These findings highlight the effectiveness of our proposed approach and provide insight into how attentional mechanisms can improve the representation power of CNNs for computer vision applications.",1
"Image inpainting is a non-trivial task in computer vision due to multiple possibilities for filling the missing data, which may be dependent on the global information of the image. Most of the existing approaches use the attention mechanism to learn the global context of the image. This attention mechanism produces semantically plausible but blurry results because of incapability to capture the global context. In this paper, we introduce hypergraph convolution on spatial features to learn the complex relationship among the data. We introduce a trainable mechanism to connect nodes using hyperedges for hypergraph convolution. To the best of our knowledge, hypergraph convolution have never been used on spatial features for any image-to-image tasks in computer vision. Further, we introduce gated convolution in the discriminator to enforce local consistency in the predicted image. The experiments on Places2, CelebA-HQ, Paris Street View, and Facades datasets, show that our approach achieves state-of-the-art results.",0
"This study presents a novel approach for hyperrealistic image inpainting using hypergraphs. Our method addresses several limitations of previous methods by explicitly modeling complex scene structures as hyperedges, which allows for accurate reconstruction even from highly corrupted images. Experimental results show significant improvement over state-of-the-art methods on a variety of challenging datasets, demonstrating that our approach can effectively produce high-quality inpaintings with natural textures and details.",1
"Deep metric learning has attracted much attention in recent years, due to seamlessly combining the distance metric learning and deep neural network. Many endeavors are devoted to design different pair-based angular loss functions, which decouple the magnitude and direction information for embedding vectors and ensure the training and testing measure consistency. However, these traditional angular losses cannot guarantee that all the sample embeddings are on the surface of the same hypersphere during the training stage, which would result in unstable gradient in batch optimization and may influence the quick convergence of the embedding learning. In this paper, we first investigate the effect of the embedding norm for deep metric learning with angular distance, and then propose a spherical embedding constraint (SEC) to regularize the distribution of the norms. SEC adaptively adjusts the embeddings to fall on the same hypersphere and performs more balanced direction update. Extensive experiments on deep metric learning, face recognition, and contrastive self-supervised learning show that the SEC-based angular space learning strategy significantly improves the performance of the state-of-the-art.",0
"In Deep Metric Learning with Spherical Embedding we explore a novel methodology utilizing geometric deep learning techniques for spherical data visualization by employing a neural network architecture to learn nonlinear metric functions directly on sphere valued features rather than Euclidean ones from raw pixel inputs. We evaluate our approach on benchmark datasets including MNIST, SVHN (Stanford Dogs), CIFAR10/100 , NORB, and demonstrate superior performance compared to classical methods such as tangent distance, Karcher mean mapping, conformal maps, or equiangular projection based approaches. Furthermore, qualitatively comparing projected flat outputs of our model using tSNE or UMAP reveals that even though these methods have been designed to preserve local neighborhood structure they frequently fail to capture global topology information accurately. This study makes contributions towards efficient real time exploration of large scale high dimensional feature spaces which has numerous applications in astronomy, medical imaging, climate science, cosmological simulations etcetera .",1
"Reinforcement Learning has yielded promising results for Neural Architecture Search (NAS). In this paper, we demonstrate how its performance can be improved by using a simplified Transformer block to model the policy network. The simplified Transformer uses a 2-stream attention-based mechanism to model hyper-parameter dependencies while avoiding layer normalization and position encoding. We posit that this parsimonious design balances model complexity against expressiveness, making it suitable for discovering optimal architectures in high-dimensional search spaces with limited exploration budgets. We demonstrate how the algorithm's performance can be further improved by a) using an actor-critic style algorithm instead of plain vanilla policy gradient and b) ensembling Transformer blocks with shared parameters, each block conditioned on a different auto-regressive factorization order. Our algorithm works well as both a NAS and generic hyper-parameter optimization (HPO) algorithm: it outperformed most algorithms on NAS-Bench-101, a public data-set for benchmarking NAS algorithms. In particular, it outperformed RL based methods that use alternate architectures to model the policy network, underlining the value of using attention-based networks in this setting. As a generic HPO algorithm, it outperformed Random Search in discovering more accurate multi-layer perceptron model architectures across 2 regression tasks. We have adhered to guidelines listed in Lindauer and Hutter while designing experiments and reporting results.",0
"This paper presents research on hyperparameter optimization using two methods: REINFORCE and transformers. Hyperparameters are important factors in machine learning models that impact performance but are typically difficult to optimize manually due to their high dimensionality and nonlinear relationship to model performance. In recent years, automated approaches like REINFORCE have emerged as popular alternatives to manual tuning. However, most existing work focuses on simpler models like linear regression and support vector machines rather than deep neural networks such as transformers which require more careful consideration of hyperparameters due to their scale and complexity. We compare the effectiveness of REINFORCE against other baseline methods for optimizing deep learning hyperparameters and evaluate performance gains across several benchmark datasets including image classification and language modelling tasks. Our results demonstrate that both REINFORCE and transformer architectures significantly improve model performance compared to traditional heuristics and manual tuning. Additionally, we observe that REINFORCE performs better on smaller models while transformers perform better on larger models demonstrating their complementary strengths. Overall our findings suggest that REINFORCE offers a promising new direction for addressing challenges associated with training state-of-the-art models quickly and accurately while providing insights into how these techniques can be further improved in practice.",1
"PointGoal Navigation is an embodied task that requires agents to navigate to a specified point in an unseen environment. Wijmans et al. showed that this task is solvable but their method is computationally prohibitive, requiring 2.5 billion frames and 180 GPU-days. In this work, we develop a method to significantly increase sample and time efficiency in learning PointNav using self-supervised auxiliary tasks (e.g. predicting the action taken between two egocentric observations, predicting the distance between two observations from a trajectory,etc.).We find that naively combining multiple auxiliary tasks improves sample efficiency,but only provides marginal gains beyond a point. To overcome this, we use attention to combine representations learnt from individual auxiliary tasks. Our best agent is 5.5x faster to reach the performance of the previous state-of-the-art, DD-PPO, at 40M frames, and improves on DD-PPO's performance at 40M frames by 0.16 SPL. Our code is publicly available at https://github.com/joel99/habitat-pointnav-aux.",0
"This paper investigates how auxiliary tasks can speed up learning for pointgoal navigation, which involves finding points within a map that satisfy certain goals. We propose a framework that integrates both main task learning and auxiliary task learning into one network architecture, allowing knowledge transfer between them. Our experiments show that adding auxiliary tasks significantly improves efficiency and performance in terms of both speed and accuracy compared to traditional methods. Furthermore, we analyze different factors that affect the effectiveness of aux iliar y tasks such as choice and size of auxiliary task, and demonstrate their impact on training dynamics. Overall, our findings indicate that incorporating auxiliary tasks provides significant benefits for learning point goal navigation models.",1
"We incorporate Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure. Our Tensor-Product Transformer (TP-Transformer) sets a new state of the art on the recently-introduced Mathematics Dataset containing 56 categories of free-form math word-problems. The essential component of the model is a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. TP-Attention goes beyond linear combination of retrieved values, strengthening representation-building and resolving ambiguities introduced by multiple layers of standard attention. The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems. Pretrained models and code will be made available after publication.",0
"This research presents a novel method for enhancing the performance of transformer networks on math problem solving tasks by incorporating explicit relational encoding. Traditional approaches to natural language processing (NLP) often rely on implicit representations of relationships between entities, which can lead to poor performance in domain-specific applications such as mathematics. Our proposed method explicitly encodes these relations using linear algebraic operations, which improves the accuracy and efficiency of transformer models. We evaluate our approach on a suite of mathematical problems across different domains and demonstrate significant improvements over strong baselines. Additionally, we provide an analysis of the model's predictions to gain insights into how it solves complex math problems. Overall, our work represents an important step towards developing more effective NLP systems for specialized domains.",1
"Probabilistic methods for point set registration have interesting theoretical properties, such as linear complexity in the number of used points, and they easily generalize to joint registration of multiple point sets. In this work, we improve their recognition performance to match state of the art. This is done by incorporating learned features, by adding a von Mises-Fisher feature model in each mixture component, and by using learned attention weights. We learn these jointly using a registration loss learning strategy (RLL) that directly uses the registration error as a loss, by back-propagating through the registration iterations. This is possible as the probabilistic registration is fully differentiable, and the result is a learning framework that is truly end-to-end. We perform extensive experiments on the 3DMatch and Kitti datasets. The experiments demonstrate that our approach benefits significantly from the integration of the learned features and our learning strategy, outperforming the state-of-the-art on Kitti. Code is available at https://github.com/felja633/RLLReg.",0
"In point set registration (PSR), we aim to find a rigid transformation that aligns two unordered sets of 2D points. This problem has been intensively studied due to its numerous applications such as 3D reconstruction from images [4], hand pose estimation in computer vision [7], and protein structure prediction in bioinformatics [8]. Due to the nonlinear nature of PSR, most approaches rely on local optimization methods like iterative Closest Pair (ICP) algorithms or gradient descent. To improve accuracy and efficiency, many works have focused on reformulating PSR into learning based models which learn dense mappings between the reference and target shapes using neural networks. Since then deep convolutional neural networks architectures became dominant and achieved promising results outperforming traditional ICP solutions in some cases [9]. However, even though these learned mappings have proved valuable, they still remain poorly understood theoretically and suffer from drawbacks in terms of accuracy, speed, stability and robustness. One major issue that arises in training CNN based models on large scale datasets is overfitting. Overcoming overfitting remains difficult especially given the lack of strong theoretical guarantees of convergence for backpropagation through deep architectures. Additionally, the number of parameters grows rapidly with network depth, leading to high computational costs and memory requirements during both training and inference stages. Another challenge faced by previous methods is the loss function used during the regression step. Current losses ignore important structural priors between the input correspondences that can regularize the output space and make the mapping more meaningful. Motivated by these challenges, our work aims at addressing three issues: improving th",1
"Face anti-spoofing (FAS) plays a vital role in securing face recognition systems. Existing methods heavily rely on the expert-designed networks, which may lead to a sub-optimal solution for FAS task. Here we propose the first FAS method based on neural architecture search (NAS), called NAS-FAS, to discover the well-suited task-aware networks. Unlike previous NAS works mainly focus on developing efficient search strategies in generic object classification, we pay more attention to study the search spaces for FAS task. The challenges of utilizing NAS for FAS are in two folds: the networks searched on 1) a specific acquisition condition might perform poorly in unseen conditions, and 2) particular spoofing attacks might generalize badly for unseen attacks. To overcome these two issues, we develop a novel search space consisting of central difference convolution and pooling operators. Moreover, an efficient static-dynamic representation is exploited for fully mining the FAS-aware spatio-temporal discrepancy. Besides, we propose Domain/Type-aware Meta-NAS, which leverages cross-domain/type knowledge for robust searching. Finally, in order to evaluate the NAS transferability for cross datasets and unknown attack types, we release a large-scale 3D mask dataset, namely CASIA-SURF 3DMask, for supporting the new 'cross-dataset cross-type' testing protocol. Experiments demonstrate that the proposed NAS-FAS achieves state-of-the-art performance on nine FAS benchmark datasets with four testing protocols.",0
"Abstract  The growing prevalence of face recognition technology has led to an increase in spoofing attacks, where attackers attempt to fool biometric systems using fake faces or digital images. To address this issue, anti-spoofing techniques have been developed to distinguish real faces from artificial ones. In this work, we propose a novel approach called NAS-FAS (Neural Architecture Search - Face Anti-Spoofing) that utilizes static-dynamic central difference networks as a search space for finding optimal neural architectures for face anti-spoofing tasks. Our method leverages recent advances in neural architecture search to automate the design process while achieving state-of-the-art performance on benchmark datasets. We showcase the effectiveness of our proposed method by comparing it against traditional handcrafted features, convolutional neural network baselines, and other NAS-based methods for face anti-spoofing. Overall, our results demonstrate that NAS can efficiently generate high-performing models for challenging computer vision tasks such as face anti-spoofing.  Note: This abstract is written in the past tense since research papers often follow this convention. If you need to modify the abstract into present perfect, please provide guidance accordingly.",1
"Recent advances in person re-identification have demonstrated enhanced discriminability, especially with supervised learning or transfer learning. However, since the data requirements---including the degree of data curations---are becoming increasingly complex and laborious, there is a critical need for unsupervised methods that are robust to large intra-class variations, such as changes in perspective, illumination, articulated motion, resolution, etc. Therefore, we propose an unsupervised framework for person re-identification which is trained in an end-to-end manner without any pre-training. Our proposed framework leverages a new attention mechanism that combines group convolutions to (1) enhance spatial attention at multiple scales and (2) reduce the number of trainable parameters by 59.6%. Additionally, our framework jointly optimizes the network with agglomerative clustering and instance learning to tackle hard samples. We perform extensive analysis using the Market1501 and DukeMTMC-reID datasets to demonstrate that our method consistently outperforms the state-of-the-art methods (with and without pre-trained weights).",0
"In this work we present a novel approach for person re-identification by utilizing unsupervised attention based instance discriminative learning. Our method learns discriminative features without any labeled data through self-supervision mechanism that enforces clustering constraints on deep feature representations of human instances. Experimental results conducted over several challenging datasets demonstrate state-of-the-art performance of our approach across various evaluation metrics. We further showcase significant improvement achieved via extensive ablation studies on model components. Overall, our framework is designed to provide efficient yet accurate solutions to the task of person re-identification under unconstrained conditions with no access to labeled training samples.",1
"Recently, there has been a surge of interest in representation learning in hyperbolic spaces, driven by their ability to represent hierarchical data with significantly fewer dimensions than standard Euclidean spaces. However, the viability and benefits of hyperbolic spaces for downstream machine learning tasks have received less attention. In this paper, we present, to our knowledge, the first theoretical guarantees for learning a classifier in hyperbolic rather than Euclidean space. Specifically, we consider the problem of learning a large-margin classifier for data possessing a hierarchical structure. Our first contribution is a hyperbolic perceptron algorithm, which provably converges to a separating hyperplane. We then provide an algorithm to efficiently learn a large-margin hyperplane, relying on the careful injection of adversarial examples. Finally, we prove that for hierarchical data that embeds well into hyperbolic space, the low embedding dimension ensures superior guarantees when learning the classifier directly in hyperbolic space.",0
"In this work we present robust large margin learning algorithms that take advantage of hyperbolic geometry as nonlinear parameter space. These methods enable training models which generalize well on complex image datasets such as CIFAR-10 and SVHN while using less data compared to existing deep neural network architectures trained in Euclidean space. We provide experimental evidence to support our claims by showing how these models achieve stateof-the art performance on standard benchmarks. Furthermore, hyperbolic representations have been shown to capture latent geometric structure of certain types of high dimensional spaces providing insight into why such representations perform so well at tasks requiring non-linear decision boundaries. This work makes important contributions to both deep learning theory and practice by opening up new opportunities for novel model design. Our hope is that this initial exploration of hyperbolic machine learning will inspire further research in this promising direction. As this is still a rapidly developing field we acknowledge many questions remain unanswered and look forward to future collaboration with other members of the community.",1
"We study the problem of $k$-way clustering in signed graphs. Considerable attention in recent years has been devoted to analyzing and modeling signed graphs, where the affinity measure between nodes takes either positive or negative values. Recently, Cucuringu et al. [CDGT 2019] proposed a spectral method, namely SPONGE (Signed Positive over Negative Generalized Eigenproblem), which casts the clustering task as a generalized eigenvalue problem optimizing a suitably defined objective function. This approach is motivated by social balance theory, where the clustering task aims to decompose a given network into disjoint groups, such that individuals within the same group are connected by as many positive edges as possible, while individuals from different groups are mainly connected by negative edges. Through extensive numerical simulations, SPONGE was shown to achieve state-of-the-art empirical performance. On the theoretical front, [CDGT 2019] analyzed SPONGE and the popular Signed Laplacian method under the setting of a Signed Stochastic Block Model (SSBM), for $k=2$ equal-sized clusters, in the regime where the graph is moderately dense.   In this work, we build on the results in [CDGT 2019] on two fronts for the normalized versions of SPONGE and the Signed Laplacian. Firstly, for both algorithms, we extend the theoretical analysis in [CDGT 2019] to the general setting of $k \geq 2$ unequal-sized clusters in the moderately dense regime. Secondly, we introduce regularized versions of both methods to handle sparse graphs -- a regime where standard spectral methods underperform -- and provide theoretical guarantees under the same SSBM model. To the best of our knowledge, regularized spectral methods have so far not been considered in the setting of clustering signed graphs. We complement our theoretical results with an extensive set of numerical experiments on synthetic data.",0
"In this work we introduce a novel regularization method for graph-based spectral clustering that can handle both weighted and unsigned graphs, as well as directed and partially oriented graphs with self-loops. We extend previous formulations by introducing constraints on the eigenvectors obtained from the normalized Laplacian matrix of the graph, allowing us to incorporate prior knowledge into the model. Our framework generalizes existing approaches such as regularized spectral embedding and random walk regularized spectral clustering to signed network analysis.  We demonstrate the effectiveness of our method through extensive experiments on synthetic data sets that capture different topological features, as well as real world applications such as social media and brain functional connectivity studies. Our results show that the proposed approach outperforms state-of-the-art methods across a range of metrics, including modularity quality, edge betweenness clustering fidelity and normalized mutual information. Overall, this study provides a significant contribution to the field of spectral clustering techniques for analyzing large scale complex networks with mixed sign edges and weights.",1
"Estimation of 3D gaze is highly relevant to multiple fields, including but not limited to interactive systems, specialized human-computer interfaces, and behavioral research. Although recently deep learning methods have boosted the accuracy of appearance-based gaze estimation, there is still room for improvement in the network architectures for this particular task. Therefore we propose here a novel network architecture grounded on self-attention augmented convolutions to improve the quality of the learned features during the training of a shallower residual network. The rationale is that self-attention mechanism can help outperform deeper architectures by learning dependencies between distant regions in full-face images. This mechanism can also create better and more spatially-aware feature representations derived from the face and eye images before gaze regression. We dubbed our framework ARes-gaze, which explores our Attention-augmented ResNet (ARes-14) as twin convolutional backbones. In our experiments, results showed a decrease of the average angular error by 2.38% when compared to state-of-the-art methods on the MPIIFaceGaze data set, and a second-place on the EyeDiap data set. It is noteworthy that our proposed framework was the only one to reach high accuracy simultaneously on both data sets.",0
"Gaze estimation has been a topic of interest in computer vision research due to its many applications such as human-computer interaction, social signal processing, and behavior analysis. Traditional methods relied on handcrafted features but these were limited in their ability to capture complex features from large images. Recently, deep learning techniques have become increasingly popular for gaze estimation using features extracted from Convolutional Neural Network (CNN) models. Self attention mechanisms have gained widespread success in natural language processing tasks such as machine translation where they capture global dependencies between tokens. In this work we demonstrate that self-attention can be used in combination with CNN based feature extraction to improve the accuracy of gaze estimation. We experimented with several variations of self attention mechanisms and found one variant that significantly improves results compared to other variants and previous state-of-the art approaches. Our model achieved superior performance on two benchmark datasets achieving an error of 2.9Â° on the MPIIGaze dataset and 4.6Â° on the EyeDriveIII Dataset which contains more challenging eye movements. These results show that our proposed method outperforms existing methods across different gaze estimation settings indicating that self attention may be applied to new problems beyond natural language processing where capturing global context is essential. Abstract: This study proposes a novel approach combining self-attention mechanisms with convolutional neural networks for improved gaze estimation in computer vision tasks. Existing methods have often utilized handcrafted features or traditional CNN models, but these have limitations in accurately capturing complex visual features within larger images. Our proposed method demonstrates the effectiveness of self-attention mechanisms in enhancing the accuracy of gaze e",1
"Social media produces large amounts of contents every day. To help users quickly capture what they need, keyphrase prediction is receiving a growing attention. Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich features embedded in the matching images. In this work, we explore the joint effects of texts and images in predicting the keyphrases for a multimedia post. To better align social media style texts and images, we propose: (1) a novel Multi-Modality Multi-Head Attention (M3H-Att) to capture the intricate cross-media interactions; (2) image wordings, in forms of optical characters and image attributes, to bridge the two modalities. Moreover, we design a unified framework to leverage the outputs of keyphrase classification and generation and couple their advantages. Extensive experiments on a large-scale dataset newly collected from Twitter show that our model significantly outperforms the previous state of the art based on traditional attention networks. Further analyses show that our multi-head attention is able to attend information from various aspects and boost classification or generation in diverse scenarios.",0
"In recent years, there has been growing interest in developing cross-media keyphrase prediction models that can accurately predict keywords across different modalities such as textual content, visual images, and audio clips. However, most existing approaches focus on either single modality or use shallow feature fusion methods which may miss important intermodal relationships among different media types. To address these limitations, we propose a unified framework called Multi-modality Multi-head Attention Network (MMANet) for jointly modeling multiple modalities with Transformer architecture. MMANet utilizes image wording technique to encode fine-grained interactions between all modalities and captures both local spatial relationship within each modality and global context information among all modalities. Our experiments conducted over three benchmark datasets demonstrate significant improvements compared to state-of-the-art cross-media keyphrase prediction models. Specifically, our method achieves F1 scores up to 45.7%, 26.8% and 19.0% on MediaEval, Weibo, and YouTube-8M datasets respectively. Furthermore, extensive ablation studies reveal that our proposed multi-modal attention module and image wording mechanism contribute significantly to performance gains.",1
"Adversarial examples are inevitable on the road of pervasive applications of deep neural networks (DNN). Imperceptible perturbations applied on natural samples can lead DNN-based classifiers to output wrong prediction with fair confidence score. It is increasingly important to obtain models with high robustness that are resistant to adversarial examples. In this paper, we survey recent advances in how to understand such intriguing property, i.e. adversarial robustness, from different perspectives. We give preliminary definitions on what adversarial attacks and robustness are. After that, we study frequently-used benchmarks and mention theoretically-proved bounds for adversarial robustness. We then provide an overview on analyzing correlations among adversarial robustness and other critical indicators of DNN models. Lastly, we introduce recent arguments on potential costs of adversarial training which have attracted wide attention from the research community.",0
"This paper presents recent advances in understanding adversarial robustness of deep neural networks (DNNs). DNNs have achieved significant successes across many domains; however, their vulnerability to small input perturbations has raised concerns about their reliability in safety-critical applications. In order to address these concerns, there have been efforts to improve our understanding of adversarial robustness of DNNs. The authors provide an overview of recent research on characterizing the properties of adversaries, exploring the nature of generalization failure under adversarial attacks, investigating the role of model architecture and training paradigms in improving adversarial robustness, as well as developing new evaluation metrics that capture more nuanced aspects of robustness. By highlighting these recent developments, we aim to inspire further investigation into the factors affecting DNN robustness, and ultimately contribute towards building reliable machine learning systems resilient to adversarial inputs.",1
"Unsupervised learning methods based on contrastive learning have drawn increasing attention and achieved promising results. Most of them aim to learn representations invariant to instance-level variations, which are provided by different views of the same instance. In this paper, we propose Invariance Propagation to focus on learning representations invariant to category-level variations, which are provided by different instances from the same category. Our method recursively discovers semantically consistent samples residing in the same high-density regions in representation space. We demonstrate a hard sampling strategy to concentrate on maximizing the agreement between the anchor sample and its hard positive samples, which provide more intra-class variations to help capture more abstract invariance. As a result, with a ResNet-50 as the backbone, our method achieves 71.3% top-1 accuracy on ImageNet linear classification and 78.2% top-5 accuracy fine-tuning on only 1% labels, surpassing previous results. We also achieve state-of-the-art performance on other downstream tasks, including linear classification on Places205 and Pascal VOC, and transfer learning on small scale datasets.",0
This sounds like something I can assist you with! Could you provide me more details? Are there any specific guidelines that need following while writing the abstract for your paper?,1
"Algorithmic trading systems are often completely automated, and deep learning is increasingly receiving attention in this domain. Nonetheless, little is known about the robustness properties of these models. We study valuation models for algorithmic trading from the perspective of adversarial machine learning. We introduce new attacks specific to this domain with size constraints that minimize attack costs. We further discuss how these attacks can be used as an analysis tool to study and evaluate the robustness properties of financial models. Finally, we investigate the feasibility of realistic adversarial attacks in which an adversarial trader fools automated trading systems into making inaccurate predictions.",0
"In recent years, machine learning has been increasingly applied in high-frequency trading (HFT) systems. HFT algorithms rely heavily on large amounts of historical data to make predictions about future market movements. However, these models can be vulnerable to adversarial attacks that manipulate their inputs, causing them to produce incorrect results. This paper presents several techniques for crafting such attacks against specific ML components commonly used in modern trading systems. We discuss evasion attacks targeted at ML classifiers as well as more general input poisoning strategies. Our experiments demonstrate that carefully designed attacks can cause significant harm to real and simulated financial markets, highlighting the need for improved resilience in automated trading systems.",1
"Short-form video social media shifts away from the traditional media paradigm by telling the audience a dynamic story to attract their attention. In particular, different combinations of everyday objects can be employed to represent a unique scene that is both interesting and understandable. Offered by the same company, TikTok and Douyin are popular examples of such new media that has become popular in recent years, while being tailored for different markets (e.g. the United States and China). The hypothesis that they express cultural differences together with media fashion and social idiosyncrasy is the primary target of our research. To that end, we first employ the Faster Regional Convolutional Neural Network (Faster R-CNN) pre-trained with the Microsoft Common Objects in COntext (MS-COCO) dataset to perform object detection. Based on a suite of objects detected from videos, we perform statistical analysis including label statistics, label similarity, and label-person distribution. We further use the Two-Stream Inflated 3D ConvNet (I3D) pre-trained with the Kinetics dataset to categorize and analyze human actions. By comparing the distributional results of TikTok and Douyin, we uncover a wealth of similarity and contrast between the two closely related video social media platforms along the content dimensions of object quantity, object categories, and human action categories.",0
This sounds like an interesting paper that could explore some significant cultural differences. Could you provide me more details about what kind of content and analysis the paper contains?,1
"Graph Neural Networks (GNNs) have led to state-of-the-art performance on a variety of machine learning tasks such as recommendation, node classification and link prediction. Graph neural network models generate node embeddings by merging nodes features with the aggregated neighboring nodes information. Most existing GNN models exploit a single type of aggregator (e.g., mean-pooling) to aggregate neighboring nodes information, and then add or concatenate the output of aggregator to the current representation vector of the center node. However, using only a single type of aggregator is difficult to capture the different aspects of neighboring information and the simple addition or concatenation update methods limit the expressive capability of GNNs. Not only that, existing supervised or semi-supervised GNN models are trained based on the loss function of the node label, which leads to the neglect of graph structure information. In this paper, we propose a novel graph neural network architecture, Graph Attention \& Interaction Network (GAIN), for inductive learning on graphs. Unlike the previous GNN models that only utilize a single type of aggregation method, we use multiple types of aggregators to gather neighboring information in different aspects and integrate the outputs of these aggregators through the aggregator-level attention mechanism. Furthermore, we design a graph regularized loss to better capture the topological relationship of the nodes in the graph. Additionally, we first present the concept of graph feature interaction and propose a vector-wise explicit feature interaction mechanism to update the node embeddings. We conduct comprehensive experiments on two node-classification benchmarks and a real-world financial news dataset. The experiments demonstrate our GAIN model outperforms current state-of-the-art performances on all the tasks.",0
"In this work we propose a new approach to semi-supervised learning on large-scale graphs called ""GAIN"" (Graph Attention and Interaction Network). This method takes advantage of graph convolutional networks to model relationships between data points, as well as attention mechanisms and interaction models to capture important patterns in the data. We show that our approach achieves state-of-the-art results on several benchmark datasets, outperforming previous methods by a significant margin. Our contributions also include analysis of the effectiveness of different components of our algorithm, demonstrating the importance of both attention and interaction mechanisms in semi-supervised learning. Overall, our work represents an important step forward in addressing the challenges of inductive reasoning on large-scale graphs with limited supervision.",1
"Recent progress in reinforcement learning has led to remarkable performance in a range of applications, but its deployment in high-stakes settings remains quite rare. One reason is a limited understanding of the behavior of reinforcement algorithms, both in terms of their regret and their ability to learn the underlying system dynamics---existing work is focused almost exclusively on characterizing rates, with little attention paid to the constants multiplying those rates that can be critically important in practice. To start to address this challenge, we study perhaps the simplest non-bandit reinforcement learning problem: linear quadratic adaptive control (LQAC). By carefully combining recent finite-sample performance bounds for the LQAC problem with a particular (less-recent) martingale central limit theorem, we are able to derive asymptotically-exact expressions for the regret, estimation error, and prediction error of a rate-optimal stepwise-updating LQAC algorithm. In simulations on both stable and unstable systems, we find that our asymptotic theory also describes the algorithm's finite-sample behavior remarkably well.",0
"In recent years, linear quadratic adaptive control (LQAC) has emerged as a powerful methodology for controlling complex systems in which there may be uncertainties in either the plant dynamics or the initial state distribution. However, existing results on LQAC have relied on approximate techniques such as large-sample approximations or truncation methods that can lead to suboptimal performance guarantees. In this work, we provide exact asymptotic expressions for the optimal cost of finite horizon LQAC problems with quadratic stage costs. These expressions offer valuable insights into the behavior of LQAC algorithms over time and allow for better design choices based on concrete performance measures. We establish our results using a combination of dynamic programming and analysis of the eigenvalues and eigenvectors of the system matrix. Our approach provides explicit formulas for the optimal value function and feedback gain matrices at any given time step. Furthermore, we show how these quantities converge rapidly to their steady-state values, enabling the development of computationally efficient approximation schemes. Overall, our findings significantly advance the understanding of LQAC algorithms and lay the groundwork for new applications in areas where uncertainty plays a crucial role, including finance, robotics, and environmental management.",1
"As an essential part of structure from motion (SfM) and Simultaneous Localization and Mapping (SLAM) systems, motion averaging has been extensively studied in the past years and continues to attract surging research attention. While canonical approaches such as bundle adjustment are predominantly inherited in most of state-of-the-art SLAM systems to estimate and update the trajectory in the robot navigation, the practical implementation of bundle adjustment in SLAM systems is intrinsically limited by the high computational complexity, unreliable convergence and strict requirements of ideal initializations. In this paper, we lift these limitations and propose a novel optimization backbone for visual SLAM systems, where we leverage rotation averaging to improve the accuracy, efficiency and robustness of conventional monocular SLAM pipelines. In our approach, we first decouple the rotational and translational parameters in the camera rigid body transformation and convert the high-dimensional non-convex nonlinear problem into tractable linear subproblems in lower dimensions, and show that the subproblems can be solved independently with proper constraints. We apply the scale parameter with $l_1$-norm in the pose-graph optimization to address the rotation averaging robustness against outliers. We further validate the global optimality of our proposed approach, revisit and address the initialization schemes, pure rotational scene handling and outlier treatments. We demonstrate that our approach can exhibit up to 10x faster speed with comparable accuracy against the state of the art on public benchmarks.",0
"In recent years, visual SLAM (Simultaneous Localization And Mapping) has become increasingly important in many fields, including robotics, computer vision, and autonomous navigation. One critical component of visual SLAM systems is camera motion estimation, which involves estimating the rotation and translation of the camera from one frame to another. This process can be challenging due to noise, illumination changes, and other factors that affect feature extraction and matching.  In this paper, we propose a novel approach to rotation averaging for visual SLAM using a deep learning architecture based on a convolutional neural network (CNN). Our method leverages the power of CNNs to extract robust features from image pairs, as well as learn complex patterns and relationships between these features and the underlying rotations. By training our model on large datasets of synthetic images and real-world data, we achieve state-of-the-art performance in both accuracy and speed compared to existing methods.  Our contributions include:  * Developing a new framework for rotation averaging that utilizes CNNs to learn both local and global features across multiple layers. * Demonstrating significant improvement over traditional handcrafted feature descriptors and linear regression models. * Evaluating the proposed method on standard benchmarks such as TUM Kitchen and EuRoC Matterport3D Dataset, outperforming several state-of-the-art approaches. * Providing analysis and discussion of the experimental results, highlighting strengths and limitations of the proposed approach.  Overall, this work represents a step forward in pushing the envelope of rotation averaging for visual SLAM, opening up possibilities for improved robotic perception, more accurate mapping, and better decision making in autonomy applications.",1
"We propose a deep learning method to automatically detect personal protective equipment (PPE), such as helmets, surgical masks, reflective vests, boots and so on, in images of people. Typical approaches for PPE detection based on deep learning are (i) to train an object detector for items such as those listed above or (ii) to train a person detector and a classifier that takes the bounding boxes predicted by the detector and discriminates between people wearing and people not wearing the corresponding PPE items. We propose a novel and accurate approach that uses three components: a person detector, a body pose estimator and a classifier. Our novelty consists in using the pose estimator only at training time, to improve the prediction performance of the classifier. We modify the neural architecture of the classifier by adding a spatial attention mechanism, which is trained using supervision signal from the pose estimator. In this way, the classifier learns to focus on PPE items, using knowledge from the pose estimator with almost no computational overhead during inference.",0
"In recent years, personal protective equipment (PPE) recognition has become increasingly important due to the rise of infectious disease outbreaks such as COVID-19. One key challenge faced by current computer vision systems is the variability in pose and appearance that can occur during use of different types of PPE. To address this issue, we propose using deep learning techniques to train a spatial attention module capable of adapting to changes in pose and viewpoint. We leverage the supervision signal provided by a pretrained pose estimator to guide our training process, ensuring improved robustness to these variations. Our proposed approach achieves state-of-the-art performance on several benchmark datasets and demonstrates significant improvements over baseline models without attention mechanisms.",1
"In this work, we present Point Transformer, a deep neural network that operates directly on unordered and unstructured point sets. We design Point Transformer to extract local and global features and relate both representations by introducing the local-global attention mechanism, which aims to capture spatial point relations and shape information. For that purpose, we propose SortNet, as part of the Point Transformer, which induces input permutation invariance by selecting points based on a learned score. The output of Point Transformer is a sorted and permutation invariant feature list that can directly be incorporated into common computer vision applications. We evaluate our approach on standard classification and part segmentation benchmarks to demonstrate competitive results compared to the prior work.",0
"In a world where artificial intelligence has become commonplace and widespread, few technologies have gained as much attention and excitement as point transformers. These powerful tools enable machines to process natural language data at high speeds while maintaining accuracy, making them ideal for applications such as text generation, question answering, and summarization. In recent years, there has been extensive research into their inner workings and capabilities, leading to numerous breakthroughs that make them even more effective. However, despite these advancements, point transformers are still facing several challenges and limitations which must be addressed if they are to reach their full potential. This paper provides a comprehensive overview of point transformers, including their strengths, weaknesses, and future directions in research. By examining both current literature and real-world deployments, we aim to provide readers with a clear understanding of how point transformers can revolutionize many industries. While some of our conclusions are preliminary or speculative due to the rapidly changing nature of this field, we believe that this work represents a valuable contribution to this exciting area of study.",1
"Automatically generating a human-like description for a given image is a potential research in artificial intelligence, which has attracted a great of attention recently. Most of the existing attention methods explore the mapping relationships between words in sentence and regions in image, such unpredictable matching manner sometimes causes inharmonious alignments that may reduce the quality of generated captions. In this paper, we make our efforts to reason about more accurate and meaningful captions. We first propose word attention to improve the correctness of visual attention when generating sequential descriptions word-by-word. The special word attention emphasizes on word importance when focusing on different regions of the input image, and makes full use of the internal annotation knowledge to assist the calculation of visual attention. Then, in order to reveal those incomprehensible intentions that cannot be expressed straightforwardly by machines, we introduce a new strategy to inject external knowledge extracted from knowledge graph into the encoder-decoder framework to facilitate meaningful captioning. Finally, we validate our model on two freely available captioning benchmarks: Microsoft COCO dataset and Flickr30k dataset. The results demonstrate that our approach achieves state-of-the-art performance and outperforms many of the existing approaches.",0
"Boost Image Captioning with Knowledge Reasoning proposes a novel framework that utilizes knowledge reasoning to enhance image caption generation accuracy. The existing methods primarily rely on visual features extracted from images and natural language processing techniques which often results in incomplete or irrelevant descriptions. Our approach incorporates external domain knowledge into the caption generation process by aligning relevant semantic concepts using graph neural networks. An iterative inference mechanism further refines the generated captions through continuous querying of the knowledge base, achieving higher quality output. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches on popular benchmarks and human evaluations, establishing itself as a promising direction in computer vision research.  Keywords: image captioning; knowledge reasoning; graph neural network; iterative inference.",1
"Pose-invariant face recognition refers to the problem of identifying or verifying a person by analyzing face images captured from different poses. This problem is challenging due to the large variation of pose, illumination and facial expression. A promising approach to deal with pose variation is to fulfill incomplete UV maps extracted from in-the-wild faces, then attach the completed UV map to a fitted 3D mesh and finally generate different 2D faces of arbitrary poses. The synthesized faces increase the pose variation for training deep face recognition models and reduce the pose discrepancy during the testing phase. In this paper, we propose a novel generative model called Attention ResCUNet-GAN to improve the UV map completion. We enhance the original UV-GAN by using a couple of U-Nets. Particularly, the skip connections within each U-Net are boosted by attention gates. Meanwhile, the features from two U-Nets are fused with trainable scalar weights. The experiments on the popular benchmarks, including Multi-PIE, LFW, CPLWF and CFP datasets, show that the proposed method yields superior performance compared to other existing methods.",0
"Abstract: We propose a novel adversarial approach for facial UV map completion for pose-invariant face recognition using Coupled Attention Residual Networks (CARN). Our method takes as input incomplete UV maps obtained from low resolution images, and outputs complete high-resolution UV maps that can then be used to obtain more accurate feature representations for face verification tasks. To achieve this, we first use attention mechanisms to attend to relevant regions of the image while generating the missing parts of the UV map. Then, we introduce a new loss function that encourages the generated UV maps to be more consistent with ground truth maps at different levels of abstraction, making our method robust against large pose variations. Experiments demonstrate significant improvements over state-of-the-art methods in both UV completion and face recognition under challenging conditions.",1
"Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard. Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.",0
"Incorporating vision information into natural language processing (NLP) models has been shown to improve performance on tasks such as question answering, image generation, and captioning. One challenge in utilizing visual features is how to effectively combine them with textual representations. This work introduces VD-BERT, a unified vision and dialog transformer based on the BERT architecture that integrates both modalities seamlessly. We train our model end-to-end and evaluate its performance across four NLP benchmarks, including VQA v2, ANLI, CLEVR, and COCO Captions. Our results show improved accuracy over state-of-the-art methods on all datasets, demonstrating the effectiveness of our approach in leveraging joint vision and dialog representations. Overall, VD-BERT sets a new standard for incorporating multi-modal input in NLP, paving the way for future research in this exciting area. ----------------------------------------------------------- VD-BERT: A Unified Vision and Dialog Transformer with BERT  This paper presents a novel technique for combining vision and dialogue data within natural language processing (NLP) architectures. The authors introduce a new method called VD-BERT which uses the BERT base architecture, a popular language model known for its ability to handle structured and sequence inputs. By training their model using both vision and dialogue data in parallel, the authors hope to create more accurate and effective language understanding systems. To test their model, they compare their results against four common NLP benchmarks, such as VQA v2, ANLI, CLEVR, and COCO Captions. Across these tests, the VD-BERT system consistently outperforms existing techniques, setting a higher standard for integrating multiple forms of human sensory data within language processing. With applications ranging from chatbots to virtual assistants, VD-BERT represents an important step forward i",1
"The neural attention mechanism plays an important role in many natural language processing applications. In particular, the use of multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. Without explicit constraining, however, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model's representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model's expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on various tasks.",0
"""In recent years, multi-head attention has emerged as one of the most popular methods for machine learning tasks that require representation learning over sequential data. However, despite their widespread adoption, these models have been shown to suffer from certain limitations such as slow convergence rates and poor performance on long input sequences. This paper introduces a novel approach to solving these problems by rethinking multi-head attention as Bayesian inference. By using probabilistic priors and posteriors to model the relationships between different elements within the sequence, our proposed method allows for more efficient and accurate computation of attention weights. Through extensive experiments on benchmark datasets, we demonstrate that our approach outperforms traditional multi-head attention models across a wide range of metrics, including accuracy, perplexity, and F1 score.""",1
"Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at https://github.com/gingsi/coot-videotext",0
"This paper presents COOT (Cooperative Hierarchical Transformer), a novel architecture for learning joint video and text representations that captures both short-range spatial relations within frames as well as long-range temporal dependencies across videos. Our approach builds upon the successful design of ViT (Video Transformer) by introducing two types of attention modules â€“ intra-frame self-attention and inter-frame cross-attention â€“ which act cooperatively at different levels of granularity and hierarchically aggregate information from low-level features up to high-level semantic concepts. We further extend our framework with pretextual task training and show that COOT outperforms strong baselines on downstream tasks such as image and video classification as well as cross-modal retrieval. Additionally, we conduct extensive ablation studies to analyze the effectiveness of each component, providing insights into how our model learns representations most effectively. Finally, we demonstrate state-of-the-art results on three benchmark datasets: Kinetics-400, UCF-101, and HMDB-51. Overall, our work advances the field of multi-modal representation learning, offering a new perspective on understanding video data.",1
"We address the task of jointly determining what a person is doing and where they are looking based on the analysis of video captured by a headworn camera. To facilitate our research, we first introduce the EGTEA Gaze+ dataset. Our dataset comes with videos, gaze tracking data, hand masks and action annotations, thereby providing the most comprehensive benchmark for First Person Vision (FPV). Moving beyond the dataset, we propose a novel deep model for joint gaze estimation and action recognition in FPV. Our method describes the participant's gaze as a probabilistic variable and models its distribution using stochastic units in a deep network. We further sample from these stochastic units, generating an attention map to guide the aggregation of visual features for action recognition. Our method is evaluated on our EGTEA Gaze+ dataset and achieves a performance level that exceeds the state-of-the-art by a significant margin. More importantly, we demonstrate that our model can be applied to larger scale FPV dataset---EPIC-Kitchens even without using gaze, offering new state-of-the-art results on FPV action recognition.",0
"This paper explores the relationship between gaze and actions in first person video. By analyzing the eye movements and physical motions of participants captured on camera, we aim to gain insights into how visual attention influences behavior. Our findings suggest that there is a strong correlation between where individuals look and what they subsequently do, demonstrating the importance of gaze as a predictor of action. We discuss potential applications of these results in fields such as market research, education, and ergonomics, highlighting the value of incorporating gaze tracking technology into future studies. Overall, our work underscores the impact of visual perception on human decision making and motivates further investigation into the interplay between sight and action.",1
"With the increasing popularity of augmented and virtual reality, retailers are now focusing more towards customer satisfaction to increase the amount of sales. Although augmented reality is not a new concept but it has gained much needed attention over the past few years. Our present work is targeted towards this direction which may be used to enhance user experience in various virtual and augmented reality based applications. We propose a model to change skin tone of a person. Given any input image of a person or a group of persons with some value indicating the desired change of skin color towards fairness or darkness, this method can change the skin tone of the persons in the image. This is an unsupervised method and also unconstrained in terms of pose, illumination, number of persons in the image etc. The goal of this work is to reduce the time and effort which is generally required for changing the skin tone using existing applications (e.g., Photoshop) by professionals or novice. To establish the efficacy of this method we have compared our result with that of some popular photo editor and also with the result of some existing benchmark method related to human attribute manipulation. Rigorous experiments on different datasets show the effectiveness of this method in terms of synthesizing perceptually convincing outputs.",0
"In this paper we present a novel unsupervised method for generating diverse and accurate representations of human skin tone using generative adversarial networks (GANs). By leveraging large datasets of face images paired with corresponding ground truth labels, our model learns to generate realistic skin tones that capture variations across different ethnic groups. We evaluate the performance of our approach on several publicly available benchmark datasets, demonstrating state-of-the-art results compared to other GAN-based methods. Our work has important implications for computer vision applications such as image synthesis, facial analysis, and personalized product recommendations. Overall, our approach represents an important step forward in developing more inclusive AI systems capable of handling diversity in complex data distributions.",1
"Trajectory prediction for scenes with multiple agents and entities is a challenging problem in numerous domains such as traffic prediction, pedestrian tracking and path planning. We present a general architecture to address this challenge which models the crucial inductive biases of motion, namely, inertia, relative motion, intents and interactions. Specifically, we propose a relational model to flexibly model interactions between agents in diverse environments. Since it is well-known that human decision making is fuzzy by nature, at the core of our model lies a novel attention mechanism which models interactions by making continuous-valued (fuzzy) decisions and learning the corresponding responses. Our architecture demonstrates significant performance gains over existing state-of-the-art predictive models in diverse domains such as human crowd trajectories, US freeway traffic, NBA sports data and physics datasets. We also present ablations and augmentations to understand the decision-making process and the source of gains in our model.",0
"This work presents a method for multi-agent trajectory prediction that utilizes fuzzy query attention. The proposed approach can effectively model complex interactions among multiple agents, allowing for more accurate predictions. By incorporating fuzzy logic into the query attention mechanism, our method can handle imprecise queries from users while still providing meaningful results. Experiments on several benchmark datasets demonstrate significant improvements over state-of-the-art methods in terms of accuracy and robustness. Our findings have important implications for applications such as autonomous driving and robotics, where real-time and reliable motion forecasting is crucial. Overall, we believe that this work represents a valuable contribution towards building intelligent systems capable of handling uncertain situations.",1
"Existing object detection frameworks are usually built on a single format of object/part representation, i.e., anchor/proposal rectangle boxes in RetinaNet and Faster R-CNN, center points in FCOS and RepPoints, and corner points in CornerNet. While these different representations usually drive the frameworks to perform well in different aspects, e.g., better classification or finer localization, it is in general difficult to combine these representations in a single framework to make good use of each strength, due to the heterogeneous or non-grid feature extraction by different representations. This paper presents an attention-based decoder module similar as that in Transformer~\cite{vaswani2017attention} to bridge other representations into a typical object detector built on a single representation format, in an end-to-end fashion. The other representations act as a set of \emph{key} instances to strengthen the main \emph{query} representation features in the vanilla detectors. Novel techniques are proposed towards efficient computation of the decoder module, including a \emph{key sampling} approach and a \emph{shared location embedding} approach. The proposed module is named \emph{bridging visual representations} (BVR). It can perform in-place and we demonstrate its broad effectiveness in bridging other representations into prevalent object detection frameworks, including RetinaNet, Faster R-CNN, FCOS and ATSS, where about $1.5\sim3.0$ AP improvements are achieved. In particular, we improve a state-of-the-art framework with a strong backbone by about $2.0$ AP, reaching $52.7$ AP on COCO test-dev. The resulting network is named RelationNet++. The code will be available at https://github.com/microsoft/RelationNet2.",0
"In order to achieve state-of-the-art object detection performance on high resolution images, many computer vision researchers have turned to transformer architectures that operate directly on pixel inputs rather than relying on handcrafted image features like HOG and Haar cascades. This approach has proven very successful, but traditional methods still lack a strong understanding of global scene context which can lead to errors such as missing small objects or including background noise. To address these issues, we propose using a novel decoding architecture based on Transformers called RelationNet++, building upon recent advances in object detection such as FCOS, Mask R-CNN, and RetinaNet. Our method introduces two new components: (i) relation pyramids, multi-scale contextual representations aggregating spatial relations at multiple scales; and (ii) graph attention networks processing the output from relation pyramids to identify more accurately localized objects over the entire feature space. We show through extensive experiments on COCO dataset our approach outperforms both vanilla object detectors and prior visual representation-based approaches by achieving significant improvement in terms of mAP and speed. Additionally, we compare the computational cost and inference time tradeoffs among different variants of RelationNets revealing promising insights into fine-grained model optimization for real world applications. Overall, this work represents an important step towards bridging global and local representations for accurate object detection while remaining efficient enough for deployment in resource constrained systems.",1
"In recent years, low-rank tensor completion (LRTC) has received considerable attention due to its applications in image/video inpainting, hyperspectral data recovery, etc. With different notions of tensor rank (e.g., CP, Tucker, tensor train/ring, etc.), various optimization based numerical methods are proposed to LRTC. However, tensor network based methods have not been proposed yet. In this paper, we propose to solve LRTC via tensor networks with a Tucker wrapper. Here by ""Tucker wrapper"" we mean that the outermost factor matrices of the tensor network are all orthonormal. We formulate LRTC as a problem of solving a system of nonlinear equations, rather than a constrained optimization problem. A two-level alternative least square method is then employed to update the unknown factors. The computation of the method is dominated by tensor matrix multiplications and can be efficiently performed. Also, under proper assumptions, it is shown that with high probability, the method converges to the exact solution at a linear rate. Numerical simulations show that the proposed algorithm is comparable with state-of-the-art methods.",0
"Inspired by the promising results obtained through applying tensor networks in quantum many body physics and computational topology, we address how such methods can likewise provide important insights into machine learning problems. We show that tensor completion using matrix product states (MPS) provides accurate predictions across several data sets and compare our findings to other state-of-the-art techniques as well as traditional singular value decompositions (SVD). Further, we introduce a new approach which combines the benefits of MPS decomposition with that of Tucker decomposition through an elegant â€œTucker wrapperâ€ methodology. Our innovation furnishes an efficient means of performing tensor completion, one capable of scaling optimally according to both problem size and target rank, thereby offering significant advantages over previous approaches to completion tasks. Through extensive experimentation on diverse real-world datasets we demonstrate the efficacy of this method outperforming existing competitive alternatives. Our work thus represents a substantial contribution towards bringing tensor network techniques from their hitherto domain within theoretical physics closer to the forefront of contemporary machine learning research. By establishing these connections we believe that further crossdisciplinary collaborations will lead to exciting advances in utilizing tensor networks within artificial intelligence applications.",1
"Recently, convolutional neural network (CNN) has demonstrated significant success for image restoration (IR) tasks (e.g., image super-resolution, image deblurring, rain streak removal, and dehazing). However, existing CNN based models are commonly implemented as a single-path stream to enrich feature representations from low-quality (LQ) input space for final predictions, which fail to fully incorporate preceding low-level contexts into later high-level features within networks, thereby producing inferior results. In this paper, we present a deep interleaved network (DIN) that learns how information at different states should be combined for high-quality (HQ) images reconstruction. The proposed DIN follows a multi-path and multi-branch pattern allowing multiple interconnected branches to interleave and fuse at different states. In this way, the shallow information can guide deep representative features prediction to enhance the feature expression ability. Furthermore, we propose asymmetric co-attention (AsyCA) which is attached at each interleaved node to model the feature dependencies. Such AsyCA can not only adaptively emphasize the informative features from different states, but also improves the discriminative ability of networks. Our presented DIN can be trained end-to-end and applied to various IR tasks. Comprehensive evaluations on public benchmarks and real-world datasets demonstrate that the proposed DIN perform favorably against the state-of-the-art methods quantitatively and qualitatively.",0
"In recent years, deep learning has shown promising results in image restoration tasks such as super resolution (SR), denoising (Dn), deblurring (Db), and joint super resolution (JSR). However, most existing methods still have limitations due to issues such as overfitting and inefficient use of computational resources. This paper proposes a novel approach called ""Deep Interleaved Networks"" (DIPN) that addresses these challenges by leveraging asymmetrical co-attention mechanisms and efficient network design. Our method outperforms state-of-the-art algorithms on popular benchmark datasets while requiring significantly less computing power. Additionally, we demonstrate how our model generalizes well across different types of distortions, making it a versatile tool for image restoration. We hope that DIPN will serve as a new baseline for future research in this field.",1
"Most deep learning based image inpainting approaches adopt autoencoder or its variants to fill missing regions in images. Encoders are usually utilized to learn powerful representational spaces, which are important for dealing with sophisticated learning tasks. Specifically, in image inpainting tasks, masks with any shapes can appear anywhere in images (i.e., free-form masks) which form complex patterns. It is difficult for encoders to capture such powerful representations under this complex situation. To tackle this problem, we propose a self-supervised Siamese inference network to improve the robustness and generalization. It can encode contextual semantics from full resolution images and obtain more discriminative representations. we further propose a multi-scale decoder with a novel dual attention fusion module (DAF), which can combine both the restored and known regions in a smooth way. This multi-scale architecture is beneficial for decoding discriminative representations learned by encoders into images layer by layer. In this way, unknown regions will be filled naturally from outside to inside. Qualitative and quantitative experiments on multiple datasets, including facial and natural datasets (i.e., Celeb-HQ, Pairs Street View, Places2 and ImageNet), demonstrate that our proposed method outperforms state-of-the-art methods in generating high-quality inpainting results.",0
"This research presents a novel approach to image inpainting using contrastive attention networks. Our method uses a deep neural network to predict missing pixels based on the surrounding context. By leveraging attention mechanisms, we can selectively focus on relevant features and suppress irrelevant noise. We demonstrate the effectiveness of our approach through extensive experiments on standard datasets, showing significant improvements over state-of-the-art methods in terms of visual quality and quantitative metrics. Our work has important implications for applications such as image editing, restoration, and augmentation. Overall, our study highlights the potential of attention models for addressing challenges in computer vision tasks.",1
"Human action recognition (HAR) in videos is a fundamental research topic in computer vision. It consists mainly in understanding actions performed by humans based on a sequence of visual observations. In recent years, HAR have witnessed significant progress, especially with the emergence of deep learning models. However, most of existing approaches for action recognition rely on information that is not always relevant for this task, and are limited in the way they fuse the temporal information. In this paper, we propose a novel method for human action recognition that encodes efficiently the most discriminative appearance information of an action with explicit attention on representative pose features, into a new compact grid representation. Our GRAR (Grid-based Representation for Action Recognition) method is tested on several benchmark datasets demonstrating that our model can accurately recognize human actions, despite intra-class appearance variations and occlusion challenges.",0
"In this paper, we propose a novel approach to human action recognition that uses grid representations to encode temporal sequences of video frames into numerical feature vectors. Our method builds on recent advances in convolutional neural networks (CNNs) by introducing spatial attention mechanisms that allow the network to focus on regions of interest within each frame and capture spatial dependencies over time. We evaluate our approach on two publicly available datasets: UCF101 and HMDB51. Experimental results show that our model achieves state-of-the-art accuracy while requiring significantly less training data compared to other methods. Overall, our work demonstrates the effectiveness of grid-based representations for human action recognition, opening up new possibilities for applying deep learning techniques to complex spatio-temporal problems.",1
"Point cloud 3D object detection has recently received major attention and becomes an active research topic in 3D computer vision community. However, recognizing 3D objects in LiDAR (Light Detection and Ranging) is still a challenge due to the complexity of point clouds. Objects such as pedestrians, cyclists, or traffic cones are usually represented by quite sparse points, which makes the detection quite complex using only point cloud. In this project, we propose a framework that uses both RGB and point cloud data to perform multiclass object recognition. We use existing 2D detection models to localize the region of interest (ROI) on the RGB image, followed by a pixel mapping strategy in the point cloud, and finally, lift the initial 2D bounding box to 3D space. We use the recently released nuScenes dataset---a large-scale dataset contains many data formats---to training and evaluate our proposed architecture.",0
"This research paper provides a comprehensive overview of 3D object detection, including its importance, recent advancements, challenges, and future directions. With increasing applications in fields such as autonomous vehicles, robotics, and augmented reality, accurate and efficient 3D object detection has become essential. The paper discusses state-of-the-art algorithms and approaches used in 3D object detection, highlighting their strengths and limitations. Additionally, the paper explores open issues related to data annotation, computational efficiency, and real-time performance, which pose significant challenges to further improvement in the field. Finally, the paper outlines promising directions for future research, emphasizing the need for collaboration across disciplines to achieve seamless integration of 3D object detection into real-world systems. This paper serves as a valuable resource for both novices and experts in the field of computer vision and machine learning who wish to gain insights into current developments and potential opportunities in 3D object detection.",1
"Deep networks achieve excellent results on large-scale clean data but degrade significantly when learning from noisy labels. To suppressing the impact of mislabeled data, this paper proposes a conceptually simple yet efficient training block, termed as Attentive Feature Mixup (AFM), which allows paying more attention to clean samples and less to mislabeled ones via sample interactions in small groups. Specifically, this plug-and-play AFM first leverages a \textit{group-to-attend} module to construct groups and assign attention weights for group-wise samples, and then uses a \textit{mixup} module with the attention weights to interpolate massive noisy-suppressed samples. The AFM has several appealing benefits for noise-robust deep learning. (i) It does not rely on any assumptions and extra clean subset. (ii) With massive interpolations, the ratio of useless samples is reduced dramatically compared to the original noisy ratio. (iii) \pxj{It jointly optimizes the interpolation weights with classifiers, suppressing the influence of mislabeled data via low attention weights. (iv) It partially inherits the vicinal risk minimization of mixup to alleviate over-fitting while improves it by sampling fewer feature-target vectors around mislabeled data from the mixup vicinal distribution.} Extensive experiments demonstrate that AFM yields state-of-the-art results on two challenging real-world noisy datasets: Food101N and Clothing1M. The code will be available at https://github.com/kaiwang960112/AFM.",0
"Artificial Intelligence (AI) has greatly impacted our lives through automation, decision making assistance, recommendation systems, image recognition, speech processing, natural language understanding, machine translation etc. To develop effective models for these applications large datasets along with computational resources are required which makes researchers rely on unlabelled data to train their systems. There have been some studies that shows how one can effectively use unlabeled data by pretraining the model with self attention mechanisms like transformer, however we see a lot of work done to tackle label noise, but there hasnâ€™t been any proper approach to handle systematic miss labelling other than cross validation. In this paper we proposed a novel solution using grouping of samples based on certain criteria followed by clustering and active learning methodology together with self attention mechanism like transformer that learns from each others representation improving sample wise quality as well as groupwise quality and reduce error caused due to mislabelling. Our experimental results show that this combination significantly outperforms state of art techniques",1
"Research on continual learning has led to a variety of approaches to mitigating catastrophic forgetting in feed-forward classification networks. Until now surprisingly little attention has been focused on continual learning of recurrent models applied to problems like image captioning. In this paper we take a systematic look at continual learning of LSTM-based models for image captioning. We propose an attention-based approach that explicitly accommodates the transient nature of vocabularies in continual image captioning tasks -- i.e. that task vocabularies are not disjoint. We call our method Recurrent Attention to Transient Tasks (RATT), and also show how to adapt continual learning approaches based on weight egularization and knowledge distillation to recurrent continual learning problems. We apply our approaches to incremental image captioning problem on two new continual learning benchmarks we define using the MS-COCO and Flickr30 datasets. Our results demonstrate that RATT is able to sequentially learn five captioning tasks while incurring no forgetting of previously learned ones.",0
"This study presents a novel approach for continual image captioning using recurrent attention mechanisms that can adaptively focus on transient tasks. Our method, called RATT (Recurrent Attention to Transient Tasks), uses a deep neural network architecture that combines both spatial and temporal attention modules. By incorporating external knowledge from previous tasks into new ones, our model is able to generate more informative and consistent descriptions across multiple consecutive images. Evaluation results demonstrate that our proposed approach outperforms several state-of-the-art methods in terms of caption quality and consistency, achieving improvements of up to 6.8% in CIDEr scores. Furthermore, we conduct extensive ablation studies to analyze the impact of each component in our system, showing the effectiveness of our attention-based approach in addressing the challenges associated with continual learning. Overall, our work provides a promising direction for future research in natural language generation and lifelong machine learning.  -----  This study introduces a novel approach for improving continual image captioning through the use of recurrent attention mechanisms. Titled ""RATT: Recurrent Attention to Transient Tasks,"" our method utilizes a combination of spatial and temporal attention modules to enhance the ability of deep neural networks to adapt to changing contexts and prioritize relevant information. By integrating external knowledge acquired during previous tasks into newly encountered ones, our model generates more accurate and coherent captions overall. Compared to existing approaches, experimental results indicate significant performance gains in terms of caption quality as measured by CIDEr scores - with improvement margins reaching up to 6.8%. The authors conducted comprehensive analysis via ablation studies, confirming the contribution of different components in our model and their collective efficacy at managing the complexities inherent in lifelong learning settings. In summary, our findings offer valuable insights for advancing natural language processing algorithms and expanding artificial intelligence capabilities towards greater versatility and generalization capacities.",1
"Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and alpha-entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: first, we extend alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}. Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions.",0
"""The attention mechanism has been shown to greatly improve the accuracy of neural machine translation (NMT) models. While several works have explored different ways of incorporating sparse attentional weights, here we investigate using continuous attention weights instead. We compare two methods of generating these continuous attention weights: one based on Gaussian distributions centered at positions with high attention scores, and another that directly learns to predict continuous values associated with each position independently. Our experimental results show that both methods outperform standard NMT without attention, as well as previous state-of-the-art systems using sparse attention mechanisms.""",1
"Diabetes is one of the most common disease in individuals. \textit{Diabetic retinopathy} (DR) is a complication of diabetes, which could lead to blindness. Automatic DR grading based on retinal images provides a great diagnostic and prognostic value for treatment planning. However, the subtle differences among severity levels make it difficult to capture important features using conventional methods. To alleviate the problems, a new deep learning architecture for robust DR grading is proposed, referred to as SEA-Net, in which, spatial attention and channel attention are alternatively carried out and boosted with each other, improving the classification performance. In addition, a hybrid loss function is proposed to further maximize the inter-class distance and reduce the intra-class variability. Experimental results have shown the effectiveness of the proposed architecture.",0
"Sea-Net is a newly proposed deep convolutional neural network architecture designed specifically for detecting referable diabetic retinopathy (DR) from color fundus images. With increasing prevalence rate globally and high demand on gradersâ€™ workload, automation of DR detection is imperative; however, image quality variations have hindered previous state-of-the-arts. Here we present Sea-Net addressing quality variations by introducing novel attention modules, i.e., squeeze and excitation, and adaptive data augmentations tailored for DR grading tasks to achieve better generalization performance. Extensive evaluations using two publicly available datasets show that Sea-Net outperforms existing methods, achieving mean APs of 82%/94%, setting new standards on these benchmark datasets. We believe our approach can advance the development of automatic DR screening systems with improved accuracy towards meeting clinical needs. The proposed deep learning model, named Sea-Net, addresses the challenge posed by diabetic retinopathy (DR) and aims to improve the generalizability of image recognition models. Compared with traditional approaches, this research presents a method to increase efficiency and reduce error rates. This study builds upon prior research by applying cutting-edge technology - squeeze and excitation attention modules - which enhance robustness against variable image qualities encountered in routine medical practice. Furthermore, customized data augmentations and model architectures were developed specifically for diabetic retinal lesion identification. Experiments yielded impressive results across multiple metrics when compared to previously published studies. Ultimately, the development of Sea-Net represents an important step forward for improving accessibility of specialist services through advanced computer vision systems. This promising advancement suggests that future machine learning applications could lead to even more improvements within the healthcare system at large.",1
"Geometric embeddings have recently received attention for their natural ability to represent transitive asymmetric relations via containment. Box embeddings, where objects are represented by n-dimensional hyperrectangles, are a particularly promising example of such an embedding as they are closed under intersection and their volume can be calculated easily, allowing them to naturally represent calibrated probability distributions. The benefits of geometric embeddings also introduce a problem of local identifiability, however, where whole neighborhoods of parameters result in equivalent loss which impedes learning. Prior work addressed some of these issues by using an approximation to Gaussian convolution over the box parameters, however, this intersection operation also increases the sparsity of the gradient. In this work, we model the box parameters with min and max Gumbel distributions, which were chosen such that space is still closed under the operation of the intersection. The calculation of the expected intersection volume involves all parameters, and we demonstrate experimentally that this drastically improves the ability of such models to learn.",0
"This paper addresses local identifiability issues inherent in probabilistic box embeddings by proposing two modifications: (i) imposing boundary constraints on sampling during training to encourage more accurate shape recovery; and (ii) incorporating a curvature term into the loss function to reduce artifacts resulting from linearization approximations. Our experiments show that these changes result in improved local identifiability, as measured by decreased depth map error and increased edge detection performance. We also demonstrate qualitatively that our proposed method produces better visual results on complex scenes with curved surfaces. Overall, we make a significant contribution toward improving the accuracy of probabilistic box embeddings in computer vision applications.",1
"Convolutional neural networks (CNN) have achieved great success in analyzing tropical cyclones (TC) with satellite images in several tasks, such as TC intensity estimation. In contrast, TC structure, which is conventionally described by a few parameters estimated subjectively by meteorology specialists, is still hard to be profiled objectively and routinely. This study applies CNN on satellite images to create the entire TC structure profiles, covering all the structural parameters. By utilizing the meteorological domain knowledge to construct TC wind profiles based on historical structure parameters, we provide valuable labels for training in our newly released benchmark dataset. With such a dataset, we hope to attract more attention to this crucial issue among data scientists. Meanwhile, a baseline is established with a specialized convolutional model operating on polar-coordinates. We discovered that it is more feasible and physically reasonable to extract structural information on polar-coordinates, instead of Cartesian coordinates, according to a TC's rotational and spiral natures. Experimental results on the released benchmark dataset verified the robustness of the proposed model and demonstrated the potential for applying deep learning techniques for this barely developed yet important topic.",0
"In recent years, advances in remote sensing technology have enabled researchers to collect high-resolution images of tropical cyclones (TCs) at both visible and infrared wavelengths. However, analyzing these images can pose significant challenges due to their complex structure and rapid changes over time. To address these issues, we propose using polar coordinate transform (PCT) as a novel method for enhancing TC structural analysis from satellite imagery. PCT converts rectangular image data into polar coordinates, providing improved spatial resolution near the center and greater distance accuracy away from the center, allowing researchers to better characterize TC structure. We apply our approach to three real-world cases studies of Atlantic hurricanes: Irma (2017), Maria (2017), and Dorian (2019). Our results show that PCT effectively enhances cloud band formation, eyewall structure, outer rainband organization, and intensity estimation, making it a valuable tool for understanding and predicting the behavior of TCs. This work offers new insights into tropical cyclone structure analysis and demonstrates the potential utility of PCT for improving operational forecasting practices.",1
"Explanations of time series models are useful for high stakes applications like healthcare but have received little attention in machine learning literature. We propose FIT, a framework that evaluates the importance of observations for a multivariate time-series black-box model by quantifying the shift in the predictive distribution over time. FIT defines the importance of an observation based on its contribution to the distributional shift under a KL-divergence that contrasts the predictive distribution against a counterfactual where the rest of the features are unobserved. We also demonstrate the need to control for time-dependent distribution shifts. We compare with state-of-the-art baselines on simulated and real-world clinical data and demonstrate that our approach is superior in identifying important time points and observations throughout the time series.",0
This abstract will not have any references as it only describes content from one research paper. For the purpose of the exercise please make up the names of the authors. Dr. John Doe Instructor,1
"Geometric scattering has recently gained recognition in graph representation learning, and recent work has shown that integrating scattering features in graph convolution networks (GCNs) can alleviate the typical oversmoothing of features in node representation learning. However, scattering methods often rely on handcrafted design, requiring careful selection of frequency bands via a cascade of wavelet transforms, as well as an effective weight sharing scheme to combine together low- and band-pass information. Here, we introduce a new attention-based architecture to produce adaptive task-driven node representations by implicitly learning node-wise weights for combining multiple scattering and GCN channels in the network. We show the resulting geometric scattering attention network (GSAN) outperforms previous networks in semi-supervised node classification, while also enabling a spectral study of extracted information by examining node-wise attention weights.",0
"Deep learning algorithms have been proven effective at solving complex tasks across various domains such as image classification, speech recognition, and natural language processing (NLP). However, many deep models still struggle when handling sequential data that involve complex relationships and dependencies among elements. To address these limitations, we present Geometric Scattering Attention Networks (GSAN), a novel attention mechanism designed to capture both temporal and spatial relationships in sequential data. We showcase the effectiveness of our approach on two challenging NLP benchmark datasets: sentiment analysis on Movie Review dataset, and named entity recognition on CoNLL2003 dataset. Our results demonstrate significant improvements over strong baseline methods such as Long Short Term Memory networks (LSTM) and Transformer models. Furthermore, GSAN achieves state-of-the-art performance while using only half the parameters compared to similar architectures like Transformers. Overall, this work introduces an innovative solution to model sequential data and paves the path towards more efficient and accurate deep learning models.",1
"Knowledge Graph (KG) embedding has attracted more attention in recent years. Most KG embedding models learn from time-unaware triples. However, the inclusion of temporal information beside triples would further improve the performance of a KGE model. In this regard, we propose ATiSE, a temporal KG embedding model which incorporates time information into entity/relation representations by using Additive Time Series decomposition. Moreover, considering the temporal uncertainty during the evolution of entity/relation representations over time, we map the representations of temporal KGs into the space of multi-dimensional Gaussian distributions. The mean of each entity/relation embedding at a time step shows the current expected position, whereas its covariance (which is temporally stationary) represents its temporal uncertainty. Experimental results show that ATiSE chieves the state-of-the-art on link prediction over four temporal KGs.",0
"""Temporal knowledge graph embedding models have become increasingly important tools for representing structured data over time. In this work, we propose an approach that utilizes additive decomposition techniques to capture the dynamics of temporal knowledge graphs. Our model is able to accurately capture both short-term and long-term patterns in the data while preserving the underlying semantics of the relationships within the graph. We demonstrate the effectiveness of our method using several experiments involving link prediction, classification, and anomaly detection tasks, showing improvement over state-of-the-art methods across all evaluations. Overall, our contribution offers a novel approach for addressing the challenges posed by complex temporal data, paving the way for new applications in diverse domains such as finance, healthcare, and IoT.""",1
"Following up on the linear transformer part of the article from Katharopoulos et al., that takes this idea from Shen et al., the trick that produces a linear complexity for the attention mechanism is re-used and extended to a second-order approximation of the softmax normalization.",0
"We propose Higher Order Linear Transformer (HOT), which is designed to accurately model both sequential structure and higher order interactions among variables in data that have known temporal dependencies. In contrast to previous work on linear transformers, HOT captures more complex patterns by using multiple layers and combining self attention with recurrent layers. HOT outperforms existing models on several benchmark datasets including synthetic examples demonstrating clear power gains from considering these types of structures. Additionally, we introduce a simple yet effective technique to train deep transformers efficiently. Our contributions provide guidance for designing powerful probabilistic generative models incorporating rich structural prior knowledge into their architectures.",1
"Face manipulation methods develop rapidly in recent years, whose potential risk to society accounts for the emerging of researches on detection methods. However, due to the diversity of manipulation methods and the high quality of fake images, detection methods suffer from a lack of generalization ability. To solve the problem, we find that segmenting images into semantic fragments could be effective, as discriminative defects and distortions are closely related to such fragments. Besides, to highlight discriminative regions in fragments and to measure contribution to the final prediction of each fragment is efficient for the improvement of generalization ability. Therefore, we propose a novel manipulated face detection method based on Multilevel Facial Semantic Segmentation and Cascade Attention Mechanism. To evaluate our method, we reconstruct two datasets: GGFI and FFMI, and also collect two open-source datasets. Experiments on four datasets verify the advantages of our approach against other state-of-the-arts, especially its generalization ability.",0
"This is an abstract: Attentive Semantic Exploring for Manipulated Face Detection is a new approach that uses machine learning techniques such as attention networks to focus on important features within images. By identifying patterns in both local and global features, our method can detect manipulation attempts more accurately than other state-of-the-art methods. We evaluate our proposed solution using two popular benchmark datasets and demonstrate significant improvements over baseline models. Our results show that our method is effective at identifying facial manipulations while preserving image quality, making it ideal for use in security applications where accurate detection is critical.",1
"Matching information across image and text modalities is a fundamental challenge for many applications that involve both vision and natural language processing. The objective is to find efficient similarity metrics to compare the similarity between visual and textual information. Existing approaches mainly match the local visual objects and the sentence words in a shared space with attention mechanisms. The matching performance is still limited because the similarity computation is based on simple comparisons of the matching features, ignoring the characteristics of their distribution in the data. In this paper, we address this limitation with an efficient learning objective that considers the discriminative feature distributions between the visual objects and sentence words. Specifically, we propose a novel Adversarial Discriminative Domain Regularization (ADDR) learning framework, beyond the paradigm metric learning objective, to construct a set of discriminative data domains within each image-text pairs. Our approach can generally improve the learning efficiency and the performance of existing metrics learning frameworks by regulating the distribution of the hidden space between the matching pairs. The experimental results show that this new approach significantly improves the overall performance of several popular cross-modal matching techniques (SCAN, VSRN, BFAN) on the MS-COCO and Flickr30K benchmarks.",0
"This paper presents a novel approach to enhance cross-modal matching by incorporating adversarial discriminative domain regularization into deep metric learning algorithms. We propose to apply generative adversarial networks (GANs) as auxiliary loss functions to improve the robustness of cross-modal embeddings against distribution shift across different domains. Our framework explicitly models both intra-domain similarity preservation and inter-domain discrepancy minimization, leading to better alignment between representations from multiple modalities and improved performance on downstream tasks such as image retrieval and zero-shot learning. Experiments conducted on several benchmark datasets demonstrate that our method achieves state-of-the-art results compared to other recent methods addressing the issue of distribution shift in multi-modal data.",1
"In this paper we introduce the Perception for Autonomous Systems (PAZ) software library. PAZ is a hierarchical perception library that allow users to manipulate multiple levels of abstraction in accordance to their requirements or skill level. More specifically, PAZ is divided into three hierarchical levels which we refer to as pipelines, processors, and backends. These abstractions allows users to compose functions in a hierarchical modular scheme that can be applied for preprocessing, data-augmentation, prediction and postprocessing of inputs and outputs of machine learning (ML) models. PAZ uses these abstractions to build reusable training and prediction pipelines for multiple robot perception tasks such as: 2D keypoint estimation, 2D object detection, 3D keypoint discovery, 6D pose estimation, emotion classification, face recognition, instance segmentation, and attention mechanisms.",0
"Advanced technologies like image recognition software and LiDAR systems have played an essential role in enabling autonomous vehicles to perceive their surroundings accurately. However, there are still numerous challenges that need to be addressed before these technologies can be widely adopted on public roads. To overcome these obstacles, researchers have developed a novel perception system called ""Perception for Autonomous Vehicles"" (PAZ) that utilizes machine learning algorithms to analyze sensor data from different sources such as cameras, radars, lidars, ultrasonic sensors, GPS/INS, and IMU. PAZ employs a deep convolutional neural network architecture to process camera images which are then combined with other sensor modalities using a Bayesian inference framework. This approach allows the system to integrate information from multiple sources and generate a more accurate and robust representation of the vehicle's environment. Furthermore, the proposed system incorporates methods like geometric verification and temporal consistency analysis to eliminate false positive detections caused by noise and uncertainty in sensor measurements. Results show that PAZ outperforms state-of-the-art approaches in detecting objects in cluttered urban environments while maintaining high computational efficiency. Overall, this work represents a significant step towards developing reliable and efficient perception solutions for self-driving cars, paving the way for safe transportation in smart cities.",1
"Many top-performing image captioning models rely solely on object features computed with an object detection model to generate image descriptions. However, recent studies propose to directly use scene graphs to introduce information about object relations into captioning, hoping to better describe interactions between objects. In this work, we thoroughly investigate the use of scene graphs in image captioning. We empirically study whether using additional scene graph encoders can lead to better image descriptions and propose a conditional graph attention network (C-GAT), where the image captioning decoder state is used to condition the graph updates. Finally, we determine to what extent noise in the predicted scene graphs influence caption quality. Overall, we find no significant difference between models that use scene graph features and models that only use object detection features across different captioning metrics, which suggests that existing scene graph generation models are still too noisy to be useful in image captioning. Moreover, although the quality of predicted scene graphs is very low in general, when using high quality scene graphs we obtain gains of up to 3.3 CIDEr compared to a strong Bottom-Up Top-Down baseline. We open source code to reproduce all our experiments in https://github.com/iacercalixto/butd-image-captioning.",0
"Scene graphs have been shown to significantly improve image caption generation by providing structured representations of scenes that can guide the decisions made by language models. However, there remains debate over whether they provide sufficient context for all types of images and whether their impact has been oversold. This study seeks to evaluate the effectiveness of scene graphs as inputs for image captioning systems, considering both their strengths and limitations. We propose new techniques for generating more accurate and informative scene graphs from raw input images using recent advances in computer vision algorithms such as object detection, segmentation, and graph construction methods. Our experimental results demonstrate the benefits of these approaches on a range of benchmark datasets, showing that high-quality scene graphs lead to substantial improvements in the quality and diversity of generated image descriptions. These findings highlight the importance of carefully constructing scene graphs before using them for downstream applications like image captioning and suggest potential directions for future research aimed at integrating visual reasoning into natural language processing tasks.",1
"Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.",0
"This paper describes how transformer models can handle complex inputs such as sequences with variable length or nested structures. We propose several techniques that allow us to encode these inputs into a format suitable for processing by modern deep learning architectures such as Transformer-based models. Our methods rely on recent advances in neural attention mechanisms, which enable efficient parallel computation of global dependencies over input representations. We demonstrate the effectiveness of our approach through extensive experiments on a variety of challenging natural language understanding tasks, including question answering, sentiment analysis, and summarization. Overall, we show that encoding long and structured inputs in Transformers leads to significant improvements in performance compared to traditional approaches based on recurrent neural networks or manually engineered features. This work paves the way for future research in areas such as code generation, machine translation, and chatbots, where handling complex inputs is essential for achieving human-like intelligence in machines.",1
"Virtual try-on under arbitrary poses has attracted lots of research attention due to its huge potential applications. However, existing methods can hardly preserve the details in clothing texture and facial identity (face, hair) while fitting novel clothes and poses onto a person. In this paper, we propose a novel multi-stage framework to synthesize person images, where rich details in salient regions can be well preserved. Specifically, a multi-stage framework is proposed to decompose the generation into spatial alignment followed by a coarse-to-fine generation. To better preserve the details in salient areas such as clothing and facial areas, we propose a Tree-Block (tree dilated fusion block) to harness multi-scale features in the generator networks. With end-to-end training of multiple stages, the whole framework can be jointly optimized for results with significantly better visual fidelity and richer details. Extensive experiments on standard datasets demonstrate that our proposed framework achieves the state-of-the-art performance, especially in preserving the visual details in clothing texture and facial identity. Our implementation will be publicly available soon.",0
"This paper presents a method for virtual try-on using detail carving, which allows users to see how clothing fits on their specific body shape. Our approach uses data from real scans of clothing samples to create detailed simulations of how different fabrics drape over human forms. By combining these details with user measurements, we can provide accurate simulations that showcase how clothes would look on any individual. We evaluate our method through user studies and demonstrate its effectiveness in providing reliable fit recommendations. Our results suggest that detail carving could revolutionize online shopping by allowing users to preview purchases before making them. Additionally, our work provides valuable insights into garment manufacturing processes, opening up new opportunities for customization and personalized product design. Overall, our research offers significant contributions to both academia and industry by improving the accuracy and experience of virtual try-ons.",1
"Recently, AI-manipulated face techniques have developed rapidly and constantly, which has raised new security issues in society. Although existing detection methods consider different categories of fake faces, the performance on detecting the fake faces with ""unseen"" manipulation techniques is still poor due to the distribution bias among cross-manipulation techniques. To solve this problem, we propose a novel framework that focuses on mining intrinsic features and further eliminating the distribution bias to improve the generalization ability. Firstly, we focus on mining the intrinsic clues in the channel difference image (CDI) and spectrum image (SI) from the camera imaging process and the indispensable step in AI manipulation process. Then, we introduce the Octave Convolution (OctConv) and an attention-based fusion module to effectively and adaptively mine intrinsic features from CDI and SI. Finally, we design an alignment module to eliminate the bias of manipulation techniques to obtain a more generalized detection framework. We evaluate the proposed framework on four categories of fake faces datasets with the most popular and state-of-the-art manipulation techniques, and achieve very competitive performances. To further verify the generalization ability of the proposed framework, we conduct experiments on cross-manipulation techniques, and the results show the advantages of our method.",0
"This work presents a novel method for detecting artificial intelligence (AI)-manipulated fake faces using generalized features. As deepfakes become more advanced, there is increasing concern over their potential impact on society and democracy. Currently available methods for detecting these manipulations often rely heavily on domain-specific knowledge which may limit their effectiveness. In contrast, our approach uses a feature extraction model that captures high-level semantic concepts shared across different image domains which enables us to generalize detection beyond specific examples. We demonstrate the efficacy of our proposed method through extensive experiments on a wide variety of real and synthetic datasets showing significantly improved results compared to state-of-the-art baselines. Our findings contribute to broader research efforts aimed at understanding how humans can reliably distinguish between authentic and manipulated media content produced by AI models such as GANs, VAEs, and Transformers. By mitigating the spread of fake news perpetuated through convincingly realistic yet fabricated images, we hope to safeguard individuals and societies from dangerous misinformation.",1
"Inspired by recent trends in vision and language learning, we explore applications of attention mechanisms for visio-lingual fusion within an application to story-based video understanding. Like other video-based QA tasks, video story understanding requires agents to grasp complex temporal dependencies. However, as it focuses on the narrative aspect of video it also requires understanding of the interactions between different characters, as well as their actions and their motivations. We propose a novel co-attentional transformer model to better capture long-term dependencies seen in visual stories such as dramas and measure its performance on the video question answering task. We evaluate our approach on the recently introduced DramaQA dataset which features character-centered video story understanding questions. Our model outperforms the baseline model by 8 percentage points overall, at least 4.95 and up to 12.8 percentage points on all difficulty levels and manages to beat the winner of the DramaQA challenge.",0
"In recent years, there has been significant progress in video understanding using transformer models. However, existing approaches often rely on pre-trained visual features extracted from convolutional neural networks (CNNs) or self-supervised learning objectives that require large amounts of data. These methods are limited by their lack of attention to contextual information within videos and between multiple modalities such as audio, text, and vision.  To address these limitations, we propose co-attentional transformers, which attend to both intermodal context and intra-modal spatiotemporal dependencies simultaneously. Our approach uses multi-head attention mechanisms across different modalities, which helps identify important temporal segments and key frames relevant to each modality. We apply our model to story-based video analysis tasks including video grounding, video question answering, and movie summarization.  Our experimental results show that our proposed method outperforms state-of-the-art baselines significantly across all three task categories, demonstrating the effectiveness of our co-attentional transformer architecture for multimodal video understanding. The improvements achieved demonstrate the benefits of attending to both intra-modal spatiotemporal dependencies and intermodal context in transformer architectures.  In summary, this work advances the field of video understanding through the introduction of co-attentional transformers, which enable more accurate and efficient processing of complex multimedia content. By providing a comprehensive solution for incorporating multimodal information into video representation learning, our framework paves the way for future research in story-based video applications involving diverse modalities such as speech recognition, machine translation, audiovi",1
"Turing machine and decision tree have developed independently for a long time. With the recent development of differentiable models, there is an intersection between them. Neural turing machine(NTM) opens door for the memory network. It use differentiable attention mechanism to read/write external memory bank. Differentiable forest brings differentiable properties to classical decision tree. In this short note, we show the deep connection between these two models. That is: differentiable forest is a special case of NTM. Differentiable forest is actually decision tree based neural turing machine. Based on this deep connection, we propose a response augmented differential forest (RaDF). The controller of RaDF is differentiable forest, the external memory of RaDF are response vectors which would be read/write by leaf nodes.",0
"This research proposes a novel approach to improving the performance of neural Turing machines (NTMs) using decision trees. NTMs are powerful models that combine the strengths of neural networks and programmable computers. However, they can suffer from high computational cost and slow inference speed due to their use of attention mechanisms. Our proposed method leverages decision trees as an alternative mechanism to reduce computational complexity while retaining the expressive power of NTMs. Experimental results show significant improvements in both speed and accuracy compared to traditional NTMs across various benchmark tasks. Overall, our work demonstrates the potential of integrating decision trees into NTMs for enhanced performance.",1
"Deep Neural Networks (DNNs) are often examined at the level of their response to input, such as analyzing the mutual information between nodes and data sets. Yet DNNs can also be examined at the level of causation, exploring ""what does what"" within the layers of the network itself. Historically, analyzing the causal structure of DNNs has received less attention than understanding their responses to input. Yet definitionally, generalizability must be a function of a DNN's causal structure since it reflects how the DNN responds to unseen or even not-yet-defined future inputs. Here, we introduce a suite of metrics based on information theory to quantify and track changes in the causal structure of DNNs during training. Specifically, we introduce the effective information (EI) of a feedforward DNN, which is the mutual information between layer input and output following a maximum-entropy perturbation. The EI can be used to assess the degree of causal influence nodes and edges have over their downstream targets in each layer. We show that the EI can be further decomposed in order to examine the sensitivity of a layer (measured by how well edges transmit perturbations) and the degeneracy of a layer (measured by how edge overlap interferes with transmission), along with estimates of the amount of integrated information of a layer. Together, these properties define where each layer lies in the ""causal plane"" which can be used to visualize how layer connectivity becomes more sensitive or degenerate over time, and how integration changes during training, revealing how the layer-by-layer causal structure differentiates. These results may help in understanding the generalization capabilities of DNNs and provide foundational tools for making DNNs both more generalizable and more explainable.",0
"Deep neural network have emerged as one of the most powerful machine learning models capable of solving complex tasks such as image classification, natural language processing, and speech recognition. However, understanding how these models make predictions remains challenging due to their highly nonlinear nature and millions of parameters. In this work, we aim at examining the causal relationships within deep neural networks by analyzing the flow of information through them. We use techniques from information theory to quantify the amount of information that each layer receives from its inputs, transmits through intermediate representations, and outputs to downstream layers. Our results show that the input layer provides more information than any other layer, indicating that lower layers rely on raw data while higher layers extract more abstract features. Furthermore, our analysis reveals clear differences in information flow across different architectures, activation functions, and training regimes. Overall, our findings provide new insights into the functioning of deep neural networks and pave the way for designing better models that learn to process information more efficiently. By illuminating the intricate mechanisms underlying these systems, we hope to enable future advancements in artificial intelligence and neuroscience.",1
"Liquid Chromatography coupled to Mass Spectrometry (LC-MS) based methods are commonly used for high-throughput, quantitative measurements of the proteome (i.e. the set of all proteins in a sample at a given time). Targeted LC-MS produces data in the form of a two-dimensional time series spectrum, with the mass to charge ratio of analytes (m/z) on one axis, and the retention time from the chromatography on the other. The elution of a peptide of interest produces highly specific patterns across multiple fragment ion traces (extracted ion chromatograms, or XICs). In this paper, we formulate this peak detection problem as a multivariate time series segmentation problem, and propose a novel approach based on the Transformer architecture. Here we augment Transformers, which are capable of capturing long distance dependencies with a global view, with Convolutional Neural Networks (CNNs), which can capture local context important to the task at hand, in the form of Transformers with Convolutional Self-Attention. We further train this model in a semisupervised manner by adapting state of the art semisupervised image classification techniques for multi-channel time series data. Experiments on a representative LC-MS dataset are benchmarked using manual annotations to showcase the encouraging performance of our method; it outperforms baseline neural network architectures and is competitive against the current state of the art in automated peak detection.",0
"The advent of data independent acquisition (DIA) mass spectrometry has transformed our ability to comprehensively characterize complex biological samples by providing a fast and efficient method to generate high-resolution molecular maps of intact proteins. However, extracting valuable information from these large datasets can still prove challenging due to issues such as spectral noise, intense peaks caused by sample contaminants, and signal suppression. In this work, we present semisupervised convolutional transformer networks as a tool capable of accurately detecting true peak signals, even in the presence of significant amounts of noise, using only semi-labeled training data. We showcase the performance of our models on two publicly available datasets and demonstrate improved sensitivity over traditional approaches while maintaining comparable levels of precision. Our findings open up new possibilities in protein analytics by enabling more accurate identification and quantification of components within highly complex mixtures, paving the way towards unlocking novel insights into biological systems.",1
"Multivariate time series modeling and prediction problems are abundant in many machine learning application domains. Accurate interpretation of such prediction outcomes from a machine learning model that explicitly captures temporal correlations can significantly benefit the domain experts. In this context, temporal attention has been successfully applied to isolate the important time steps for the input time series. However, in multivariate time series problems, spatial interpretation is also critical to understand the contributions of different variables on the model outputs. We propose a novel deep learning architecture, called spatiotemporal attention mechanism (STAM) for simultaneous learning of the most important time steps and variables. STAM is a causal (i.e., only depends on past inputs and does not use future inputs) and scalable (i.e., scales well with an increase in the number of variables) approach that is comparable to the state-of-the-art models in terms of computational tractability. We demonstrate our models' performance on two popular public datasets and a domain-specific dataset. When compared with the baseline models, the results show that STAM maintains state-of-the-art prediction accuracy while offering the benefit of accurate spatiotemporal interpretability. The learned attention weights are validated from a domain knowledge perspective for these real-world datasets.",0
"This research proposes a novel model architecture that utilizes spatiotemporal attention mechanisms to predict multivariate time series data. Our approach overcomes challenges associated with traditional sequence-to-sequence models by incorporating dependencies among multiple input sequences simultaneously, enabling improved predictions across different regions and at varying temporal resolutions. We evaluate our model on three benchmark datasets, demonstrating state-of-the-art performance in both mean absolute error metrics as well as interpretability measures including attribution analysis. By explicitly capturing interdependencies between inputs, our method allows for flexible application in diverse domains such as finance, healthcare, and environmental science where complex relationships exist between variables. With further refinement and integration into real-world decision-making pipelines, we anticipate significant improvements in prediction accuracy and actionable insights derived from predicted outputs.",1
"Heterogeneous Information Networks (HINs), involving a diversity of node types and relation types, are pervasive in many real-world applications. Recently, increasing attention has been paid to heterogeneous graph representation learning (HGRL) which aims to embed rich structural and semantics information in HIN into low-dimensional node representations. To date, most HGRL models rely on manual customisation of meta paths to capture the semantics underlying the given HIN. However, the dependency on the handcrafted meta-paths requires rich domain knowledge which is extremely difficult to obtain for complex and semantic rich HINs. Moreover, strictly defined meta-paths will limit the HGRL's access to more comprehensive information in HINs. To fully unleash the power of HGRL, we present a Reinforcement Learning enhanced Heterogeneous Graph Neural Network (RL-HGNN), to design different meta-paths for the nodes in a HIN. Specifically, RL-HGNN models the meta-path design process as a Markov Decision Process and uses a policy network to adaptively design a meta-path for each node to learn its effective representations. The policy network is trained with deep reinforcement learning by exploiting the performance of the model on a downstream task. We further propose an extension, RL-HGNN++, to ameliorate the meta-path design procedure and accelerate the training process. Experimental results demonstrate the effectiveness of RL-HGNN, and reveals that it can identify meaningful meta-paths that would have been ignored by human knowledge.",0
"Title: ""Deep Learning on Heterogeneous Graphs""  Abstract: This work presents a novel approach to deep learning on heterogeneous graphs using reinforcement learning enhanced graph neural networks (GNN). GNNs have become popular tools for processing complex data structures like graphs, but their performance can suffer when dealing with highly imbalanced data distributions. Our method addresses this challenge by incorporating principles from reinforcement learning into the training process of our GNN models. By doing so, we achieve more balanced predictions across different types of nodes in the graph, resulting in improved overall accuracy compared to state-of-the-art methods. We evaluate our model on several benchmark datasets and demonstrate its effectiveness for tasks such as node classification and link prediction. The results show that our approach outperforms existing methods while remaining computationally efficient. This research represents an important step towards developing powerful yet scalable deep learning algorithms capable of handling real-world graph data with diverse characteristics. Overall, our contributions highlight the potential of integrating reinforcement learning with GNNs to enable robust solutions for complex computational problems involving graphs.",1
"Detailed routing is one of the most critical steps in analog circuit design. Complete routing has become increasingly more challenging in advanced node analog circuits, making advances in efficient automatic routers ever more necessary. In this work, we propose a machine learning driven method for solving the track-assignment detailed routing problem for advanced node analog circuits. Our approach adopts an attention-based reinforcement learning (RL) policy model. Our main insight and advancement over this RL model is the use of supervision as a way to leverage solutions generated by a conventional genetic algorithm (GA). For this, our approach minimizes the Kullback-Leibler divergence loss between the output from the RL policy model and a solution distribution obtained from the genetic solver. The key advantage of this approach is that the router can learn a policy in an offline setting with supervision, while improving the run-time performance nearly 100x over the genetic solver. Moreover, the quality of the solutions our approach produces matches well with those generated by GA. We show that especially for complex problems, our supervised RL method provides good quality solution similar to conventional attention-based RL without comprising run time performance. The ability to learn from example designs and train the router to get similar solutions with orders of magnitude run-time improvement can impact the design flow dramatically, potentially enabling increased design exploration and routability-driven placement.",0
"This is one of three papers which explore the use of deep learning to improve operations at an auto dealership by optimizing route planning and scheduling tasks that need performing on each vehicle as part of service calls. This second paper focuses specifically on using track-assignment detailed routing (TADR) with attention-based policy model supervision on routing assignments made at any time during the course of day, so as to minimize overall waiting times while maximizing shop throughput. We evaluate our approach by comparing assignment accuracy to two baseline methods: a greedy method where assignors immediately take action after completing their current task, and a heuristics method where assignors wait to optimize their workload before taking new assignments. Our approach outperforms both methods across multiple simulation scenarios, including ones based upon real world customer data from an actual dealership. By better managing assignor capacity over the course of a shift and allowing them to handle more than one assignment per trip (up to five), we significantly reduce daily idle and deadheading time while increasing total daily assignments completed and customer satisfaction levels. Despite these improvements however, certain drawbacks persist such as limitations with simulated assignors, future work could address some of these issues by improving assignment policies for recalibrating pickups/deliveries between customers and other optimization approaches like offline/realtime assignor coordination and batch scheduling techniques.",1
"Deep convolutional neural networks generally perform well in underwater object recognition tasks on both optical and sonar images. Many such methods require hundreds, if not thousands, of images per class to generalize well to unseen examples. However, obtaining and labeling sufficiently large volumes of data can be relatively costly and time-consuming, especially when observing rare objects or performing real-time operations. Few-Shot Learning (FSL) efforts have produced many promising methods to deal with low data availability. However, little attention has been given in the underwater domain, where the style of images poses additional challenges for object recognition algorithms. To the best of our knowledge, this is the first paper to evaluate and compare several supervised and semi-supervised Few-Shot Learning (FSL) methods using underwater optical and side-scan sonar imagery. Our results show that FSL methods offer a significant advantage over the traditional transfer learning methods that fine-tune pre-trained models. We hope that our work will help apply FSL to autonomous underwater systems and expand their learning capabilities.",0
"This article presents a comprehensive comparison of few-shot learning methods for underwater optical and sonar image classification. The authors evaluate several state-of-the-art algorithms across a range of experimental settings, including both publicly available datasets and their own proprietary data collected from underwater environments using sonars. They find that while some approaches outperform others overall, there is no one clear winner, indicating that different methods may be more appropriate depending on the specific dataset and application at hand. Importantly, they identify areas where further research is necessary to improve the performance of these models. Overall, this study provides valuable insights into the use of few-shot learning techniques for underwater image classification and highlights promising directions for future work in this field.",1
"With the widespread adoption of machine learning in the real world, the impact of the discriminatory bias has attracted attention. In recent years, various methods to mitigate the bias have been proposed. However, most of them have not considered intersectional bias, which brings unfair situations where people belonging to specific subgroups of a protected group are treated worse when multiple sensitive attributes are taken into consideration. To mitigate this bias, in this paper, we propose a method called One-vs.-One Mitigation by applying a process of comparison between each pair of subgroups related to sensitive attributes to the fairness-aware machine learning for binary classification. We compare our method and the conventional fairness-aware binary classification methods in comprehensive settings using three approaches (pre-processing, in-processing, and post-processing), six metrics (the ratio and difference of demographic parity, equalized odds, and equal opportunity), and two real-world datasets (Adult and COMPAS). As a result, our method mitigates the intersectional bias much better than conventional methods in all the settings. With the result, we open up the potential of fairness-aware binary classification for solving more realistic problems occurring when there are multiple sensitive attributes.",0
"This paper presents a method called one-versus-one mitigation (OVM) that can reduce intersectional bias present in binary classifiers by breaking down complex decision boundaries into simpler linear ones. OVM ensures fairer predictions without sacrificing classification accuracy or increasing model complexity significantly. We demonstrate experimentally on both synthetic and real datasets how OVM outperforms existing methods from both perspectives of tradeoff between utility and fairness metrics, including precision, recall, F1 score, equality of opportunity metric EO, statistical parity, dice coefficient and Jain index measure II. Results showcase improved calibration of fairness metrics as well as superior predictive performance compared with state-of-the-art bias reduction techniques like adversarial training and debiasing by ensemble learning, under different scenarios such as unbalanced class distributions, imbalanced attribute representation, correlations among sensitive attributes, varying difficulty levels or noisy feature space and multiple categories or hierarchies of sensitive groups. By introducing a novel weight sharing technique, we simplify computationally the high dimensional extension of one-versus-all SVMs, making OVM suitable for large scale applications across various domains. Finally, we discuss how our findings could enable more inclusive data driven systems design, policy formulation and decision making towards addressing societal challenges arising from algorithmic discrimination.",1
"Image features from a small local region often give strong evidence in person re-identification task. However, CNN suffers from paying too much attention on the most salient local areas, thus ignoring other discriminative clues, e.g., hair, shoes or logos on clothes. %BDB proposes to randomly drop one block in a batch to enlarge the high response areas. Although BDB has achieved remarkable results, there still room for improvement. In this work, we propose a Progressive Multi-stage feature Mix network (PMM), which enables the model to find out the more precise and diverse features in a progressive manner. Specifically, 1. to enforce the model to look for different clues in the image, we adopt a multi-stage classifier and expect that the model is able to focus on a complementary region in each stage. 2. we propose an Attentive feature Hard-Mix (A-Hard-Mix) to replace the salient feature blocks by the negative example in the current batch, whose label is different from the current sample. 3. extensive experiments have been carried out on reID datasets such as the Market-1501, DukeMTMC-reID and CUHK03, showing that the proposed method can boost the re-identification performance significantly.",0
"This research proposes a novel multi-stage feature mix approach (MFA) that can progressively fuse features from multiple stages of deep neural networks to enhance person re-identification performance. By extracting comprehensive features from different levels and integrating them adaptively based on their discriminative abilities, MFA could facilitate the model learning more robust and representative descriptors across camera views, particularly for far-away shots and occluded scenes. Extensive experiments demonstrate the effectiveness and efficiency of our method, establishing new state-of-the-art records on four popular datasets under both single-shot and cumulative evaluation settings. ----- I don't know how you want me to respond because you haven't given any prompts or questions for me to respond to! Would you like some general chatbot conversation? Here are some things we might talk about: the weather forecast for today, whether humanity faces extinction if artificial intelligence continues developing rapidly, my favorite hobby, which US president was most similar to Abraham Lincoln but wasn't as famous?",1
"Fashion products typically feature in compositions of a variety of styles at different clothing parts. In order to distinguish images of different fashion products, we need to extract both appearance (i.e., ""how to describe"") and localization (i.e.,""where to look"") information, and their interactions. To this end, we propose a biologically inspired framework for image-based fashion product retrieval, which mimics the hypothesized twostream visual processing system of human brain. The proposed attentional heterogeneous bilinear network (AHBN) consists of two branches: a deep CNN branch to extract fine-grained appearance attributes and a fully convolutional branch to extract landmark localization information. A joint channel-wise attention mechanism is further applied to the extracted heterogeneous features to focus on important channels, followed by a compact bilinear pooling layer to model the interaction of the two streams. Our proposed framework achieves satisfactory performance on three image-based fashion product retrieval benchmarks.",0
"This paper presents a novel approach to fashion image retrieval using an attentional heterogeneous bilinear network (AHBN). The proposed method addresses two key challenges faced by existing approaches: where to look for relevant features within images, and how to effectively describe those features through visual representations.  The AHBN model is designed to attend to different regions of each input image and learn relationships between them based on their spatial locations. By incorporating both global and local attention mechanisms, the model can capture salient details that may otherwise be overlooked.  To evaluate the effectiveness of our approach, we conducted experiments on several large-scale datasets of clothing items. Results demonstrate significant improvements over baseline models, achieving state-of-the-art performance across multiple evaluation metrics.  Overall, this work highlights the importance of attentional mechanisms for efficient feature learning in complex image domains such as fashion. Our findings contribute to advancing the fields of computer vision and natural language processing, with potential applications in recommendation systems, personalized styling services, and more.",1
"We propose an augmented Parallel-Pyramid Net ($P^2~Net$) with feature refinement by dilated bottleneck and attention module. During data preprocessing, we proposed a differentiable auto data augmentation ($DA^2$) method. We formulate the problem of searching data augmentaion policy in a differentiable form, so that the optimal policy setting can be easily updated by back propagation during training. $DA^2$ improves the training efficiency. A parallel-pyramid structure is followed to compensate the information loss introduced by the network. We innovate two fusion structures, i.e. Parallel Fusion and Progressive Fusion, to process pyramid features from backbone network. Both fusion structures leverage the advantages of spatial information affluence at high resolution and semantic comprehension at low resolution effectively. We propose a refinement stage for the pyramid features to further boost the accuracy of our network. By introducing dilated bottleneck and attention module, we increase the receptive field for the features with limited complexity and tune the importance to different feature channels. To further refine the feature maps after completion of feature extraction stage, an Attention Module ($AM$) is defined to extract weighted features from different scale feature maps generated by the parallel-pyramid structure. Compared with the traditional up-sampling refining, $AM$ can better capture the relationship between channels. Experiments corroborate the effectiveness of our proposed method. Notably, our method achieves the best performance on the challenging MSCOCO and MPII datasets.",0
"This abstract presents the augmented parallel-pyramid network (APPN) architecture that addresses key limitations of current pose estimation models such as computational efficiency and localization accuracy. APPN adopts an attention guided scheme to address these limitations by enhancing feature learning at multiple scales through parallel pyramidal processing, which results in improved alignment sensitivity. Experiments demonstrate that our approach outperforms state-of-the-art methods in terms of speed and performance on several challenging benchmark datasets.",1
"Weakly supervised object detection (WSOD) using only image-level annotations has attracted a growing attention over the past few years. Whereas such task is typically addressed with a domain-specific solution focused on natural images, we show that a simple multiple instance approach applied on pre-trained deep features yields excellent performances on non-photographic datasets, possibly including new classes. The approach does not include any fine-tuning or cross-domain learning and is therefore efficient and possibly applicable to arbitrary datasets and classes. We investigate several flavors of the proposed approach, some including multi-layers perceptron and polyhedral classifiers. Despite its simplicity, our method shows competitive results on a range of publicly available datasets, including paintings (People-Art, IconArt), watercolors, cliparts and comics and allows to quickly learn unseen visual categories.",0
"This paper presents a novel approach for solving the challenging problem of detecting objects in images when there are significant differences (domain shifts) between the training data and the test data. We use a method called multiple instance learning (MIL), which allows us to learn from large amounts of unlabeled data by focusing on ""bags"" of instances rather than individual examples. To improve accuracy, we use deep convolutional neural networks (CNNs) to extract high-level representations that capture important characteristics of the objects we want to detect. Our approach can handle variations in lighting conditions, background scenes, etc., making it well-suited for real-world applications where domain shift is common. Experiments demonstrate that our method outperforms state-of-the-art methods under domain shift settings.",1
"Light field saliency detection---important due to utility in many vision tasks---still lack speed and can improve in accuracy. Due to the formulation of the saliency detection problem in light fields as a segmentation task or a ""memorizing"" tasks, existing approaches consume unnecessarily large amounts of computational resources for (training and) testing leading to execution times is several seconds. We solve this by aggressively reducing the large light-field images to a much smaller three-channel feature map appropriate for saliency detection using an RGB image saliency detector. We achieve this by introducing a novel convolutional neural network based features extraction and encoding module. Our saliency detector takes $0.4$ s to process a light field of size $9\times9\times512\times375$ in a CPU and is significantly faster than existing systems, with better or comparable accuracy. Our work shows that extracting features from light fields through aggressive size reduction and the attention results in a faster and accurate light-field saliency detector.",0
"A novel method for saliency detection using light fields has been developed that combines fast processing times and high accuracy. This approach utilizes feature extraction techniques such as convolutional neural networks (CNNs) and other machine learning algorithms to enhance the computational efficiency of traditional methods. Experimental results demonstrate that our proposed system outperforms state-of-the-art solutions while requiring less computation time. Additionally, we provide detailed analysis of each step involved in the process, including image formation, feature extraction, fusion, and postprocessing steps. Overall, these findings have significant implications for computer vision applications where efficient and accurate saliency detection is essential.",1
"Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to $O\left(n^{1.5}d\right)$ from $O\left(n^2d\right)$ for sequence length $n$ and hidden dimension $d$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192.",0
"In natural language processing (NLP), sequence data such as text often contain both informative and irrelevant elements that can affect performance on downstream tasks like classification and translation. To address this problem, we propose content-based sparse attention which identifies and focuses only on the most important parts of the input sequence while ignoring the rest. By doing so, our model achieves state-of-the-art results without relying on explicit segmentation of the input signal. Our approach builds upon previous work on routing transformers but improves efficiency by up to 40% compared to other methods through careful design choices, including dynamic sparsity adjustments during inference based on predicted token importance scores. Additionally, we provide extensive experiments comparing variants of our method to strong baselines across multiple NLP benchmarks, demonstrating generalizability and competitive effectiveness in practice. Finally, we conclude by discussing limitations and future directions. This paper represents a step forward towards efficient, robust, and effective handling of sequential data using attention mechanisms trained end-to-end from raw inputs to meaningful outputs.",1
"A plethora of research in the literature shows how human eye fixation pattern varies depending on different factors, including genetics, age, social functioning, cognitive functioning, and so on. Analysis of these variations in visual attention has already elicited two potential research avenues: 1) determining the physiological or psychological state of the subject and 2) predicting the tasks associated with the act of viewing from the recorded eye-fixation data. To this end, this paper proposes a visual saliency based novel feature extraction method for automatic and quantitative classification of eye-tracking data, which is applicable to both of the research directions. Instead of directly extracting features from the fixation data, this method employs several well-known computational models of visual attention to predict eye fixation locations as saliency maps. Comparing the saliency amplitudes, similarity and dissimilarity of saliency maps with the corresponding eye fixations maps gives an extra dimension of information which is effectively utilized to generate discriminative features to classify the eye-tracking data. Extensive experimentation using Saliency4ASD, Age Prediction, and Visual Perceptual Task dataset show that our saliency-based feature can achieve superior performance, outperforming the previous state-of-the-art methods by a considerable margin. Moreover, unlike the existing application-specific solutions, our method demonstrates performance improvement across three distinct problems from the real-life domain: Autism Spectrum Disorder screening, toddler age prediction, and human visual perceptual task classification, providing a general paradigm that utilizes the extra-information inherent in saliency maps for a more accurate classification.",0
This paper presents methods to classify eye tracking data into different categories using saliency maps which represent visually meaningful locations that attract human attention.,1
"The data imbalance problem is a frequent bottleneck in the classification performance of neural networks. In this paper, we propose a novel supervised discriminative feature generation (DFG) method for a minority class dataset. DFG is based on the modified structure of a generative adversarial network consisting of four independent networks: generator, discriminator, feature extractor, and classifier. To augment the selected discriminative features of the minority class data by adopting an attention mechanism, the generator for the class-imbalanced target task is trained, and the feature extractor and classifier are regularized using the pre-trained features from a large source data. The experimental results show that the DFG generator enhances the augmentation of the label-preserved and diverse features, and the classification results are significantly improved on the target task. The feature generation model can contribute greatly to the development of data augmentation methods through discriminative feature generation and supervised attention methods.",0
"In this work, we propose a novel method for generating discriminatory features from raw input data for the purpose of classifying imbalanced datasets. Our approach leverages advances in machine learning and computer vision techniques to extract meaningful characteristics from the data that can better differentiate between classes and improve overall accuracy. We demonstrate the effectiveness of our method through extensive experiments on several real world datasets and show significant improvement over state-of-the-art methods. This research has important implications for applications such as medical diagnosis, fraud detection, and environmental monitoring where accurate classification of imbalanced data is crucial.",1
"Detecting manipulated facial images and videos is an increasingly important topic in digital media forensics. As advanced face synthesis and manipulation methods are made available, new types of fake face representations are being created which have raised significant concerns for their use in social media. Hence, it is crucial to detect manipulated face images and localize manipulated regions. Instead of simply using multi-task learning to simultaneously detect manipulated images and predict the manipulated mask (regions), we propose to utilize an attention mechanism to process and improve the feature maps for the classification task. The learned attention maps highlight the informative regions to further improve the binary classification (genuine face v. fake face), and also visualize the manipulated regions. To enable our study of manipulated face detection and localization, we collect a large-scale database that contains numerous types of facial forgeries. With this dataset, we perform a thorough analysis of data-driven fake face detection. We show that the use of an attention mechanism improves facial forgery detection and manipulated region localization.",0
"As technology continues to advance, the manipulation of digital media has become increasingly prevalent and difficult to detect. This includes the manipulation of images, videos, audio recordings, and other forms of media. One specific area of concern is digital face manipulation, which involves altering the appearance of individuals captured on video or image.  In our paper, we aim to address the challenge of detecting digital face manipulation. We begin by exploring current techniques used in this field, including methods based on signal processing, computer vision, and machine learning. Our proposed approach leverages advances in deep learning and human perception to achieve higher accuracy than existing methods.  The core component of our method is a novel convolutional neural network (CNN) architecture that learns representations of facial features that correspond to authenticity. These representations capture subtle differences in feature patterns that occur naturally but disappear during manipulations. By utilizing these learned representations along with domain knowledge and visual cues, our method effectively classifies if the input video contains any form of face manipulation.  We evaluate our method using extensive experiments on public datasets containing both real and synthesized manipulated content, demonstrating significantly improved performance over state-of-the-art approaches. Additionally, we investigate the effectiveness of different components of our method and discuss open challenges in this growing research area.  Our work is significant as it addresses the need for robust detection of digital face manipulation in a variety of applications such as social media verification, news authentication, and security systems. With the widespread adoption of multimedia content, reliable detection of manipulation becomes crucial to ensure trustworthiness in online communication. Our findings provide valuable insights into the development of future techniques for this important task.",1
"With the growing significance of graphs as an effective representation of data in numerous applications, efficient graph analysis using modern machine learning is receiving a growing level of attention. Deep learning approaches often operate over the entire adjacency matrix -- as the input and intermediate network layers are all designed in proportion to the size of the adjacency matrix -- leading to intensive computation and large memory requirements as the graph size increases. It is therefore desirable to identify efficient measures to reduce both run-time and memory requirements allowing for the analysis of the largest graphs possible. The use of reduced precision operations within the forward and backward passes of a deep neural network along with novel specialised hardware in modern GPUs can offer promising avenues towards efficiency. In this paper, we provide an in-depth exploration of the use of reduced-precision operations, easily integrable into the highly popular PyTorch framework, and an analysis of the effects of Tensor Cores on graph convolutional neural networks. We perform an extensive experimental evaluation of three GPU architectures and two widely-used graph analysis tasks (vertex classification and link prediction) using well-known benchmark and synthetically generated datasets. Thus allowing us to make important observations on the effects of reduced-precision operations and Tensor Cores on computational and memory usage of graph convolutional neural networks -- often neglected in the literature.",0
"Artificial intelligence (AI) models built on graph convolutional neural networks have achieved significant successes in fields such as image recognition, natural language processing, and computer vision. As GPU memory becomes cheaper and larger, there has been growing interest in using half-precision floating point numbers to represent model weights and activations instead of traditional single precision or double precision formats. This work explores the use of half-precision representations in graph convolutional neural networks (GCNs), evaluating both the accuracy and computational efficiency gains offered by these representations. Through extensive experimentation across several benchmark datasets and network architectures, we demonstrate that GCNs can indeed achieve comparable performance with half-precision representations while requiring significantly less memory and compute resources compared to their full precision counterparts. These findings suggest that adopting half-precision formats may facilitate faster training times and more efficient inference speed without sacrificing model quality. We conclude by discussing limitations of our study, potential future research directions, and implications for practitioners interested in leveraging half-precision models.",1
"Convolutional Neural Networks (CNNs) are being increasingly used to address the problem of iris presentation attack detection. In this work, we propose attention-guided iris presentation attack detection (AG-PAD) to augment CNNs with attention mechanisms. Two types of attention modules are independently appended on top of the last convolutional layer of the backbone network. Specifically, the channel attention module is used to model the inter-channel relationship between features, while the position attention module is used to model inter-spatial relationship between features. An element-wise sum is employed to fuse these two attention modules. Further, a novel hierarchical attention mechanism is introduced. Experiments involving both a JHU-APL proprietary dataset and the benchmark LivDet-Iris-2017 dataset suggest that the proposed method achieves promising results. To the best of our knowledge, this is the first work that exploits the use of attention mechanisms in iris presentation attack detection.",0
"Iris presentation attack detection (PAD) plays a critical role in modern security systems by preventing unauthorized access through iris recognition technology. In recent years, convolutional neural networks have emerged as effective solutions to solve complex image classification problems like PAD. However, these models often suffer from a lack of interpretability due to their deep architectures, which makes it challenging to identify the regions that contribute most towards the final decision. This study proposes an attention-guided network for iris PAD using dilated convolutions to achieve better feature maps while maintaining efficiency. The proposed model uses multi-scale features extracted at different stages of the network to compute weighted attention maps that highlight discriminative areas in each layer. These attention maps provide valuable insights into human eye patterns that may aid domain experts in developing more robust biometric templates. Experimental results on publicly available datasets demonstrate significant improvements over state-of-the-art methods, validating both quantitatively and qualitatively the effectiveness of our approach.",1
"Graph neural networks (GNNs) have emerged as a powerful tool for learning software engineering tasks including code completion, bug finding, and program repair. They benefit from leveraging program structure like control flow graphs, but they are not well-suited to tasks like program execution that require far more sequential reasoning steps than number of GNN propagation steps. Recurrent neural networks (RNNs), on the other hand, are well-suited to long sequential chains of reasoning, but they do not naturally incorporate program structure and generally perform worse on the above tasks. Our aim is to achieve the best of both worlds, and we do so by introducing a novel GNN architecture, the Instruction Pointer Attention Graph Neural Networks (IPA-GNN), which achieves improved systematic generalization on the task of learning to execute programs using control flow graphs. The model arises by considering RNNs operating on program traces with branch decisions as latent variables. The IPA-GNN can be seen either as a continuous relaxation of the RNN model or as a GNN variant more tailored to execution. To test the models, we propose evaluating systematic generalization on learning to execute using control flow graphs, which tests sequential reasoning and use of program structure. More practically, we evaluate these models on the task of learning to execute partial programs, as might arise if using the model as a heuristic function in program synthesis. Results show that the IPA-GNN outperforms a variety of RNN and GNN baselines on both tasks.",0
"This abstract presents research on neural networks that can learn to execute programs using instruction pointer attention graphs (IPAG). Traditional methods for program execution have been limited by their reliance on symbolic representations and predefined control flow structures. IPAG addresses these limitations by using graph-based representations that capture both data dependencies and control dependencies among instructions. Our proposed method uses reinforcement learning and deep neural networks to train agents to take actions based on the current state of the IPAG representation. We evaluate our approach using several benchmark tasks, including the popular RLBench suite. Results show that our model outperforms previous approaches on most domains, demonstrating the effectiveness of IPAG as a general approach for program synthesis from natural language specifications. Additionally, we provide analysis and discussion on potential applications and future directions for this work. Overall, this study contributes to the field of artificial intelligence by advancing our understanding of how complex cognitive tasks such as programming can be performed by machine learning algorithms.",1
"The transformer has been extensively used in research domains such as computer vision, image processing, and natural language processing. The transformer, however, has not been actively used in graph neural networks. To this end, we introduce a transformer-based advanced GNN model, named UGformer, to learn graph representations. In particular, given an input graph, we present two UGformer variants. The first variant is to leverage the transformer on a set of sampled neighbors for each node, while the second is to leverage the transformer directly on the input graph. Experimental results demonstrate that our UGformer achieves state-of-the-art accuracies on well-known benchmark datasets for graph classification and inductive text classification. The code is available on Github: \url{https://github.com/daiquocnguyen/Graph-Transformer}.",0
"Machine Learning (ML) has transformed many domains including computer vision, natural language processing, robotics etc., leading to breakthrough results that rivaled human performance in these tasks. In some cases like image classification, object detection etc., deep convolutional neural networks have achieved state-of-the art (SOTA) accuracy which could not have been imagined even a decade ago using traditional methods. However, there still remain several challenges such as computational complexity, interpretability, sparsity etc. among others, which make them hard to scale beyond a certain limit. Moreover, the quest for generalization across multiple domains remains elusive at times, since each domain may require different architectures and data augmentations. To tackle this challenge of scalability while achieving high SOTA accuracies and robustness in diverse domains like computer vision, speech recognition, natural language understanding etc. we propose a novel architecture called universal graph transformer self attention network (UGTAN). UGTAN inherently unifies the strengths of both convolutional neural nets(CNNs) and recurrent/transformer models by allowing self-attentions over graphs/topological neighborhoods rather than just sequential dependencies. We showcase our proposed method on several standard datasets showing superior performances compared to the current SOTAs.",1
"Off-policy evaluation (OPE) is the problem of estimating the value of a target policy from samples obtained via different policies. Recently, applying OPE methods for bandit problems has garnered attention. For the theoretical guarantees of an estimator of the policy value, the OPE methods require various conditions on the target policy and policy used for generating the samples. However, existing studies did not carefully discuss the practical situation where such conditions hold, and the gap between them remains. This paper aims to show new results for bridging the gap. Based on the properties of the evaluation policy, we categorize OPE situations. Then, among practical applications, we mainly discuss the best policy selection. For the situation, we propose a meta-algorithm based on existing OPE estimators. We investigate the proposed concepts using synthetic and open real-world datasets in experiments.",0
"In recent years, off-policy evaluation (OPE) has gained increasing interest as a tool for evaluating policies in reinforcement learning problems without having to execute them in the environment. OPE algorithms use data collected from other policies and provide an estimate of how a new policy would perform if executed instead. However, many existing works on OPE focus on tabular representations and do not account for continuous state spaces. This paper presents a new approach for performing OPE in bandit problems, which have both discrete actions and continuous state representation. We introduce two novel methods: one based on counterfactual reasoning using a generative model and another that uses importance sampling and least squares temporal difference learning. Our methods are evaluated through simulations across several challenging environments, demonstrating their effectiveness at estimating performance metrics such as regret and return. Overall, our work provides a practical guide for researchers and practitioners who need to evaluate policies in complex RL systems where continuous states play a role.",1
"Recent works in geometric deep learning have introduced neural networks that allow performing inference tasks on three-dimensional geometric data by defining convolution, and sometimes pooling, operations on triangle meshes. These methods, however, either consider the input mesh as a graph, and do not exploit specific geometric properties of meshes for feature aggregation and downsampling, or are specialized for meshes, but rely on a rigid definition of convolution that does not properly capture the local topology of the mesh. We propose a method that combines the advantages of both types of approaches, while addressing their limitations: we extend a primal-dual framework drawn from the graph-neural-network literature to triangle meshes, and define convolutions on two types of graphs constructed from an input mesh. Our method takes features for both edges and faces of a 3D mesh as input and dynamically aggregates them using an attention mechanism. At the same time, we introduce a pooling operation with a precise geometric interpretation, that allows handling variations in the mesh connectivity by clustering mesh faces in a task-driven fashion. We provide theoretical insights of our approach using tools from the mesh-simplification literature. In addition, we validate experimentally our method in the tasks of shape classification and shape segmentation, where we obtain comparable or superior performance to the state of the art.",0
"This work presents the Primal-Dual Mesh CNN (Convolutional Neural Network) architecture, which enhances both efficiency and accuracy by incorporating ideas from mesh networks and utilizing primal-dual convolutions. Compared to traditional CNN architectures, the Primal-Dual Mesh architecture offers significant performance improvements without adding additional parameters or increasing computational complexity. In addition to improved efficiency and accuracy, the new architecture is more robust against overfitting due to better use of spatial information. The authors demonstrate the effectiveness of their approach on several benchmark datasets, achieving state-of-the-art results while using fewer parameters. Overall, the Primal-Dual Mesh CNN represents a major advance in computer vision research that could have important implications across many other domains as well.",1
"For classification tasks, dictionary learning based methods have attracted lots of attention in recent years. One popular way to achieve this purpose is to introduce label information to generate a discriminative dictionary to represent samples. However, compared with traditional dictionary learning, this category of methods only achieves significant improvements in supervised learning, and has little positive influence on semi-supervised or unsupervised learning. To tackle this issue, we propose a Dynamic Label Dictionary Learning (DLDL) algorithm to generate the soft label matrix for unlabeled data. Specifically, we employ hypergraph manifold regularization to keep the relations among original data, transformed data, and soft labels consistent. We demonstrate the efficiency of the proposed DLDL approach on two remote sensing datasets.",0
"This may vary depending on field, please provide more context.",1
"In recent years, the attention mechanism contributes significantly to hypergraph based neural networks. However, these methods update the attention weights with the network propagating. That is to say, this type of attention mechanism is only suitable for deep learning-based methods while not applicable to the traditional machine learning approaches. In this paper, we propose a hypergraph based sparse attention mechanism to tackle this issue and embed it into dictionary learning. More specifically, we first construct a sparse attention hypergraph, asset attention weights to samples by employing the $\ell_1$-norm sparse regularization to mine the high-order relationship among sample features. Then, we introduce the hypergraph Laplacian operator to preserve the local structure for subspace transformation in dictionary learning. Besides, we incorporate the discriminative information into the hypergraph as the guidance to aggregate samples. Unlike previous works, our method updates attention weights independently, does not rely on the deep network. We demonstrate the efficacy of our approach on four benchmark datasets.",0
"In our paper we present SAHDL, a novel model that combines two powerful ideas for solving problems in deep learning: sparse representations and hypergraphs. Sparse representations have been shown to capture salient features from large amounts of data while minimizing memory usage by having most values equal to zero. Meanwhile, hypergraph representation allows us to capture dependencies among groups of variables without assuming they follow any particular mathematical structure such as linear relationships. We show how these techniques can be applied together to solve dictionary learning tasks and improve image compression rates compared to state of the art methods. In addition, our method provides interpretable results since dictionaries learned using sparse attention hypergraph regularization can be interpreted in terms of human-understandable concepts. Finally, we test our approach on several real world datasets and compare its performance against other algorithms. Our experiments demonstrate the effectiveness of SAHDL and suggest promising future directions for research in this area.",1
"Keypoint detector and descriptor are two main components of point cloud registration. Previous learning-based keypoint detectors rely on saliency estimation for each point or farthest point sample (FPS) for candidate points selection, which are inefficient and not applicable in large scale scenes. This paper proposes Random Sample-based Keypoint Detector and Descriptor Network (RSKDD-Net) for large scale point cloud registration. The key idea is using random sampling to efficiently select candidate points and using a learning-based method to jointly generate keypoints and descriptors. To tackle the information loss of random sampling, we exploit a novel random dilation cluster strategy to enlarge the receptive field of each sampled point and an attention mechanism to aggregate the positions and features of neighbor points. Furthermore, we propose a matching loss to train the descriptor in a weakly supervised manner. Extensive experiments on two large scale outdoor LiDAR datasets show that the proposed RSKDD-Net achieves state-of-the-art performance with more than 15 times faster than existing methods. Our code is available at https://github.com/ispc-lab/RSKDD-Net.",0
"This paper presents a novel deep learning architecture called RSKDD-Net for simultaneously detecting keypoints and describing their local features at the scale level, which makes the method applicable to tasks such as object detection, image matching, 3D reconstruction, semantic segmentation and visual SLAM. Our approach uses convolutional neural networks (CNNs) trained to predict densely sampled multi-scale heatmaps of keypoints locations together with feature descriptors using random sampling of patches from images, allowing the network to focus on discriminative regions. We evaluate our method against other state-of-the-art approaches and demonstrate that RSKDD-Net outperforms them by achieving higher accuracy, robustness and computational efficiency. Overall, we believe that RSKDD-Net holds great potential for computer vision applications due to its ability to accurately detect and describe objectsâ€™ keypoints in real time, which makes it highly attractive for deployment on mobile devices.",1
"The amount of available Earth observation data has increased dramatically in the recent years. Efficiently making use of the entire body information is a current challenge in remote sensing and demands for light-weight problem-agnostic models that do not require region- or problem-specific expert knowledge. End-to-end trained deep learning models can make use of raw sensory data by learning feature extraction and classification in one step solely from data. Still, many methods proposed in remote sensing research require implicit feature extraction through data preprocessing or explicit design of features.   In this work, we compare recent deep learning models on crop type classification on raw and preprocessed Sentinel 2 data. We concentrate on the common neural network architectures for time series, i.e., 1D-convolutions, recurrence, a shallow random forest baseline, and focus on the novel self-attention architecture. Our central findings are that data preprocessing still increased the overall classification performance for all models while the choice of model was less crucial. Self-attention and recurrent neural networks, by their architecture, outperformed convolutional neural networks on raw satellite time series. We explore this by a feature importance analysis based on gradient back-propagation that exploits the differentiable nature of deep learning models. Further, we qualitatively show how self-attention scores focus selectively on few classification-relevant observations.",0
"Raw satellite time series data comes from various sensors on board Earth observation satellites that capture images at regular intervals. Different types of features can be extracted from these images to provide insights into the temporal dynamics of land surface processes such as agriculture monitoring, deforestation detection, urban growth analysis, etc. With advancements in deep learning algorithms, automatic feature extraction has been possible by training convolutional neural networks (CNNs) directly on time series data without any preprocessing or manual feature engineering. In recent years, self-attention mechanisms have emerged as powerful tools for encoding sequential data into fixed-length representations that capture long-range dependencies effectively. By combining both CNNs and self-attentions within an architecture like Transformers, we can achieve better performance than traditional methods based solely on either one of them. In this study, we evaluate four variants of encoders built using combinations of these components: i) multi-layer perceptron with CNN layers only; ii) multi-head self-attention with dense feedforward network connections only; iii) hybrid model with one convolution layer followed by multiple attention heads; iv) another variant where a linear combination of the previous two models is used. Our results show that variants iii and iv yield significantly better accuracy compared to models relying purely on convolutions or attention-based encoding alone, indicating that designing effective architectures requires balancing local processing via filters alongside global context aggregation through self-attentions. We hope our findings motivate further research in applying transformer-style architectures towards remote sensing problems involving satellite time se",1
"Unlabeled data is often abundant in the clinic, making machine learning methods based on semi-supervised learning a good match for this setting. Despite this, they are currently receiving relatively little attention in medical image analysis literature. Instead, most practitioners and researchers focus on supervised or transfer learning approaches. The recently proposed MixMatch and FixMatch algorithms have demonstrated promising results in extracting useful representations while requiring very few labels. Motivated by these recent successes, we apply MixMatch and FixMatch in an ophthalmological diagnostic setting and investigate how they fare against standard transfer learning. We find that both algorithms outperform the transfer learning baseline on all fractions of labelled data. Furthermore, our experiments show that exponential moving average (EMA) of model parameters, which is a component of both algorithms, is not needed for our classification problem, as disabling it leaves the outcome unchanged. Our code is available online: https://github.com/Valentyn1997/oct-diagn-semi-supervised",0
"Optical coherence tomography (OCT) has become a widely used imaging modality in ophthalmology, providing detailed cross-sectional images of the retina. Its importance lies in enabling accurate diagnoses by identifying specific pathologies, which can aid in timely treatment decisions. Although deep learning methods have been employed successfully to automate analysis of large amounts of medical data, their applications have yet to achieve the required accuracy for wide clinical adoption. To overcome these limitations, we propose a novel method using transfer learning that efficiently exploits limited labeled training samples with additional unlabeled datasets to improve performance significantly. This study presents results from extensive experiments on two different publicly available OCT datasets, proving our model achieves competitive accuracies compared to state-of-the-art models while requiring fewer labeled examples. Our findings suggest that incorporating more unlabelled data leads to better generalization capability and outperforms other semi-supervised approaches. In conclusion, this research provides evidence that our proposed approach effectively addresses real-world challenges surrounding scarce annotated datasets common in medical image analysis tasks, paving the way towards reliable clinical integration.",1
"Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address the aforementioned aspects by proposing a new recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and to integrate it with data about agents' possible future objectives. Our proposal is general enough to be applied to different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.",0
"This paper presents a novel graph neural network architecture called DAG-Net (Double Attentive Graph Neural Network) for trajectory forecasting tasks. The proposed model effectively utilizes both short-term motion patterns and long-range dependencies by leveraging two attention mechanisms - one for temporal attention and another for spatial attention. By attending to different time steps and locations, DAG-Net captures more meaningful spatiotemporal features from raw sensor data than traditional approaches. Furthermore, our design enhances message passing through edge convolutions, enabling better fusion of neighboring information during propagation. Experimental evaluations on real-world datasets demonstrate that DAG-Net outperforms state-of-the-art methods across multiple metrics, showcasing its effectiveness in addressing challenges such as occlusions and varying speed differences among agents. Our work paves the way towards improved predictive models for autonomous systems and intelligent transportation systems applications.",1
"Standard segmentation of medical images based on full-supervised convolutional networks demands accurate dense annotations. Such learning framework is built on laborious manual annotation with restrict demands for expertise, leading to insufficient high-quality labels. To overcome such limitation and exploit massive weakly labeled data, we relaxed the rigid labeling requirement and developed a semi-supervised learning framework based on a teacher-student fashion for organ and lesion segmentation with partial dense-labeled supervision and supplementary loose bounding-box supervision which are easier to acquire. Observing the geometrical relation of an organ and its inner lesions in most cases, we propose a hierarchical organ-to-lesion (O2L) attention module in a teacher segmentor to produce pseudo-labels. Then a student segmentor is trained with combinations of manual-labeled and pseudo-labeled annotations. We further proposed a localization branch realized via an aggregation of high-level features in a deep decoder to predict locations of organ and lesion, which enriches student segmentor with precise localization information. We validated each design in our model on LiTS challenge datasets by ablation study and showed its state-of-the-art performance compared with recent methods. We show our model is robust to the quality of bounding box and achieves comparable performance compared with full-supervised learning methods.",0
"This paper presents a new framework that combines teacher-student learning from both labeled images and unlabeled images. In recent years, semi-supervised learning has been used to improve medical image segmentation performance by leveraging large amounts of unlabeled data. However, existing methods often rely on heuristics that can result in suboptimal results. Our proposed method uses teacher-student learning to effectively incorporate supervision from both labeled and unlabeled images. By using adversarial training between teachers and students, our model learns how to better utilize mixed supervision and achieves state-of-the-art results in several benchmark datasets. Our approach demonstrates great potential to advance research in semi-supervised medical image segmentation, which could ultimately lead to improved clinical decision making. (Note: You don't need to write an entire paper; just provide an abstract)",1
"The use of multi-modal data for deep machine learning has shown promise when compared to uni-modal approaches with fusion of multi-modal features resulting in improved performance in several applications. However, most state-of-the-art methods use naive fusion which processes feature streams independently, ignoring possible long-term dependencies within the data during fusion. In this paper, we present a novel Memory based Attentive Fusion layer, which fuses modes by incorporating both the current features and longterm dependencies in the data, thus allowing the model to understand the relative importance of modes over time. We introduce an explicit memory block within the fusion layer which stores features containing long-term dependencies of the fused data. The feature inputs from uni-modal encoders are fused through attentive composition and transformation followed by naive fusion of the resultant memory derived features with layer inputs. Following state-of-the-art methods, we have evaluated the performance and the generalizability of the proposed fusion approach on two different datasets with different modalities. In our experiments, we replace the naive fusion layer in benchmark networks with our proposed layer to enable a fair comparison. Experimental results indicate that the MBAF layer can generalise across different modalities and networks to enhance fusion and improve performance.",0
"This may sound silly but I am having trouble deciding on whether to use â€œmultiâ€ in my abstract depending on the context. Is that strange? Multi-modality refers to using multiple modalities (data formats like images & text) whereas multimodal usually implies more than one modality (e.g., image, video) within the same task or application. For example, if you have two tasks: speech recognition and lip reading, those would be considered separate unimodal tasks even though they both involve audio data. However, if you were doing something where you had to recognize speakers from audio clips as well as pictures of their faces, then youâ€™d have a truly multimodal task since there was only one goal (recognizing who is speaking), but you needed both types of data to accomplish that goal! As such, the adjective multi-modal doesnâ€™t strictly mean multimodal anymore because we can talk about having lots of different kinds of unimodal data. If anyone knows how else to refer to this concept or prefers another term feel free to chime in! There is so much variation across fields that terminology often ends up getting redefined or created anew through community usage rather than any strict authority. Back to topic, here is some guidance on making choices regarding your title (abstract included): * Are all sources of input/output unimodal? Use ""multimodal"" instead of ""cross-modal"". * Even if inputs seem to fit into several categories (like music vs human voice vs animal vocalizations)? Then don't worry - these should still fall under ""multimodal inputs"". So go ahead and call it cross-modal fusion. * Unsure if you want to say multimodal because it sounds like each mode has a specialized role? Try saying that you wish to evaluate whether fusing information is beneficial. This implicitly communicates that it might not always b",1
"GNNs and chemical fingerprints are the predominant approaches to representing molecules for property prediction. However, in NLP, transformers have become the de-facto standard for representation learning thanks to their strong downstream task transfer. In parallel, the software ecosystem around transformers is maturing rapidly, with libraries like HuggingFace and BertViz enabling streamlined training and introspection. In this work, we make one of the first attempts to systematically evaluate transformers on molecular property prediction tasks via our ChemBERTa model. ChemBERTa scales well with pretraining dataset size, offering competitive downstream performance on MoleculeNet and useful attention-based visualization modalities. Our results suggest that transformers offer a promising avenue of future work for molecular representation learning and property prediction. To facilitate these efforts, we release a curated dataset of 77M SMILES from PubChem suitable for large-scale self-supervised pretraining.",0
"Here is my best effort at generating a scientific abstract for your request. Please remember that I am only as good as the data used during my training and therefore cannot guarantee accuracy or relevance beyond what was contained within those resources. If you need any revisions please ask!  Chemical properties like solubility and reactivity can govern how molecules behave within biological systems and drive their functions. Quantifying these characteristics for new chemicals is essential, but traditional methods require extensive experimental testing. To address this challenge, researchers have turned to computational models based on deep learning neural networks which can analyze vast amounts of structural and property data with minimal input from experts. These ""pretrained"" models excel at general tasks such as text classification, sentiment analysis, and sequence completion, but their utility for specialized applications like predicting the physical and biochemical properties of organic compounds has been less certain due to issues including overfitting, limited size of training datasets, poor interpretability, sensitivity to hyperparameters, and lack of robustness. In our study we present the application of a novel pretraining method called self supervised learning (SSL) using large corpuses of real world unlabelled data (""pretext"") and investigate its impact on the ability of state-of-the-art chemical language processing model - chemBERTa - to accurately predict physical and biochemical properties of a wide variety of organic compounds under numerous conditions. We find evidence that self-supervised fine tuning substantially improves both generalization performance over a range of benchmark data sets across diverse domains and yields better parameter stability with respect to random initializations. Furthermore, we show that SSL enhances robustness against changes to model architecture and random seed initialization which could result in significantly more reliable use cases compared to traditionally trained models. Our work demonstrates great promise for leveraging the power of large scale unlabeled corpora combined with powerful preprocessing techniques allowing for more accurate and reliable quantification of complex systems through machine learned models.",1
"Most real-world knowledge graphs are characterized by a long-tail relation frequency distribution where a significant fraction of relations occurs only a handful of times. This observation has given rise to recent interest in low-shot learning methods that are able to generalize from only a few examples. The existing approaches, however, are tailored to static knowledge graphs and not easily generalized to temporal settings, where data scarcity poses even bigger problems, e.g., due to occurrence of new, previously unseen relations. We address this shortcoming by proposing a one-shot learning framework for link prediction in temporal knowledge graphs. Our proposed method employs a self-attention mechanism to effectively encode temporal interactions between entities, and a network to compute a similarity score between a given query and a (one-shot) example. Our experiments show that the proposed algorithm outperforms the state of the art baselines for two well-studied benchmarks while achieving significantly better performance for sparse relations.",0
"This paper presents a novel approach for one-shot learning on temporal knowledge graphs (TKGs), which represent a challenging subset of knowledge representation due to their dynamic nature. In particular, we address the problem of predicting missing links in TKGs given only a small number of labeled examples from the target domain. Our method leverages both graph structure and timestamps, using a variant of random walk-based embedding that incorporates temporal proximity into node representations. We evaluate our approach on two benchmark datasets for link prediction tasks and show promising results compared to state-of-the-art methods for few-shot learning on static graphs as well as existing approaches specifically tailored towards TKGs. These findings demonstrate the effectiveness of our proposed framework for one-shot learning on temporal knowledge graphs, opening up new possibilities for applying machine learning techniques to real-world applications where data availability may be limited.",1
"For realizing safe autonomous driving, the end-to-end delays of real-time object detection systems should be thoroughly analyzed and minimized. However, despite recent development of neural networks with minimized inference delays, surprisingly little attention has been paid to their end-to-end delays from an object's appearance until its detection is reported. With this motivation, this paper aims to provide more comprehensive understanding of the end-to-end delay, through which precise best- and worst-case delay predictions are formulated, and three optimization methods are implemented: (i) on-demand capture, (ii) zero-slack pipeline, and (iii) contention-free pipeline. Our experimental results show a 76% reduction in the end-to-end delay of Darknet YOLO (You Only Look Once) v3 (from 1070 ms to 261 ms), thereby demonstrating the great potential of exploiting the end-to-end delay analysis for autonomous driving. Furthermore, as we only modify the system architecture and do not change the neural network architecture itself, our approach incurs no penalty on the detection accuracy.",0
In order to ensure safety on roads autonomously vehicles must perceive their surroundings continuously. Since perception tasks such as object detection can take seconds or even minutes traditional computer vision methods are unsuitable due to their low frame rate. To overcome these limitations this paper proposes two approaches for real-time object detection R-CNN (Region Convolutional Neural Network) SSD(Single Shot Multibox Detector). These models were trained using 8 Nvidia Titan X GPUs and tested on the KITTI dataset achieving state of art accuracy while meeting real time requirements at 29 FPS. Furthermore the authors evaluate the suitability of different network architectures and loss functions concluding that the former has little impact while the latter significantly influences model performance. Finally the authors demonstrate the effectiveness of the proposed framework by testing it onboard a moving vehicle reaching over 96% object detection accuracy while running at under 4ms latency. By enabling efficient and accurate real-time object detection this work brings us one step closer to safe fully autonomous driving.,1
"A significant effort has been made to train neural networks that replicate algorithmic reasoning, but they often fail to learn the abstract concepts underlying these algorithms. This is evidenced by their inability to generalize to data distributions that are outside of their restricted training sets, namely larger inputs and unseen data. We study these generalization issues at the level of numerical subroutines that comprise common algorithms like sorting, shortest paths, and minimum spanning trees. First, we observe that transformer-based sequence-to-sequence models can learn subroutines like sorting a list of numbers, but their performance rapidly degrades as the length of lists grows beyond those found in the training set. We demonstrate that this is due to attention weights that lose fidelity with longer sequences, particularly when the input numbers are numerically similar. To address the issue, we propose a learned conditional masking mechanism, which enables the model to strongly generalize far outside of its training range with near-perfect accuracy on a variety of algorithms. Second, to generalize to unseen data, we show that encoding numbers with a binary representation leads to embeddings with rich structure once trained on downstream tasks like addition or multiplication. This allows the embedding to handle missing data by faithfully interpolating numbers not seen during training.",0
"In recent years, there has been significant progress in developing artificial intelligence (AI) systems that can perform complex tasks such as image classification, natural language processing, and game playing. However, one key challenge facing these systems is their lack of ability to efficiently execute subroutines. This is due in part to the fact that current deep learning methods used to train AI systems often struggle to generalize to new environments and input distributions, resulting in poor performance on unseen data. To address this issue, we propose using neural execution engines, which use machine learning techniques to learn how to efficiently execute subroutines in real time based on feedback from the environment. Our approach combines classical planning and reinforcement learning algorithms with deep learning models to create hybrid agents that can both plan ahead and react quickly to changing circumstances. We evaluate our method across multiple domains and demonstrate that our agents outperform state-of-the-art approaches in terms of both efficiency and adaptability. Overall, our work represents an important step towards building more effective AI systems capable of solving real world problems.",1
"The algorithmic fairness of predictive analytic tools in the public sector has increasingly become a topic of rigorous exploration. While instruments pertaining to criminal recidivism and academic admissions, for example, have garnered much attention, the predictive instruments of Child Welfare jurisdictions have received considerably less attention. This is in part because comparatively few such instruments exist and because even fewer have been scrutinized through the lens of algorithmic fairness. In this work, we seek to address both of these gaps. To this end, a novel classification algorithm for predicting reunification success within Oregon Child Welfare is presented, including all of the relevant details associated with building such an instrument. The purpose of this tool is to maximize the number of stable reunifications and identify potentially unstable reunifications which may require additional resources and scrutiny. Additionally, because the algorithmic fairness of the resulting tool, if left unaltered, is unquestionably lacking, the utilized procedure for mitigating such unfairness is presented, along with the rationale behind each difficult and unavoidable choice. This procedure, though similar to other post-processing group-specific thresholding methods, is novel in its use of a penalized optimizer and contextually requisite subsampling. These novel methodological components yield a rich and informative empirical understanding of the trade-off continuum between fairness and accuracy. As the developed procedure is generalizable across a variety of group-level definitions of algorithmic fairness, as well as across an arbitrary number of protected attribute levels and risk thresholds, the approach is broadly applicable both within and beyond Child Welfare.",0
"This research seeks to explore the pursuit of algorithmic fairness in child welfare reunification success classifiers. These models have been used widely across different states and jurisdictions in the United States, however they often lack transparency and accountability, which raises concerns of unfair treatment and discrimination against certain groups. We argue that correcting such algorithmic biases requires understanding the underlying data patterns that lead to these biases, as well as having meaningful collaboration among stakeholders from both technical and social domains. In our study, we examine several datasets including publicly available ones, along with state agenciesâ€™ own internal datasets. By utilizing appropriate statistical methods and by working closely with governmental social workers on policy implications related to potential interventions, we aimed at finding ways to minimize the harm caused by unfair decision making due to these algorithms. Finally, we hope our findings can provide insights into creating more inclusive and transparent machine learning systems that promote fairer outcomes for all individuals involved.",1
"In this paper, we extend the traditional few-shot learning (FSL) problem to the situation when the source-domain data is not accessible but only high-level information in the form of class prototypes is available. This limited information setup for the FSL problem deserves much attention due to its implication of privacy-preserving inaccessibility to the source-domain data but it has rarely been addressed before. Because of limited training data, we propose a non-parametric approach to this FSL problem by assuming that all the class prototypes are structurally arranged on a manifold. Accordingly, we estimate the novel-class prototype locations by projecting the few-shot samples onto the average of the subspaces on which the surrounding classes lie. During classification, we again exploit the structural arrangement of the categories by inducing a Markov chain on the graph constructed with the class prototypes. This manifold distance obtained using the Markov chain is expected to produce better results compared to a traditional nearest-neighbor-based Euclidean distance. To evaluate our proposed framework, we have tested it on two image datasets - the large-scale ImageNet and the small-scale but fine-grained CUB-200. We have also studied parameter sensitivity to better understand our framework.",0
"Deep learning has revolutionized the field of computer vision by enabling powerful image recognition models that can learn from large amounts of labeled data. However, these methods often require massive amounts of training data and laborious manual labeling efforts. In contrast, few-shot learning attempts to improve upon the efficiency and scalability of traditional deep learning methods by enabling models to generalize to novel classes with only a small number of examples (i.e., one or a few). This allows for more efficient use of resources and improved adaptability across domains. Despite recent advances in few-shot image recognition, there remains room for improvement. In particular, current approaches typically rely on meticulously engineered loss functions and task-specific architectures that may not perform well under real-world conditions. In our work, we propose a new approach based on manifolds that overcomes some of these limitations by directly modeling the structure underlying images. Our method significantly improves performance compared to previous state-of-the-art techniques, demonstrating robustness to variations in data distribution, as well as better handling of high-dimensional input spaces. By introducing simple, yet effective modifications to commonly used deep learning algorithms, we show how to achieve superior performance while maintaining ease of implementation. Our findings have important implications for computer vision research, paving the way towards broader deployment and adoption of machine learning in real-world applications.",1
"Weakly Supervised Object Detection (WSOD) has emerged as an effective tool to train object detectors using only the image-level category labels. However, without object-level labels, WSOD detectors are prone to detect bounding boxes on salient objects, clustered objects and discriminative object parts. Moreover, the image-level category labels do not enforce consistent object detection across different transformations of the same images. To address the above issues, we propose a Comprehensive Attention Self-Distillation (CASD) training approach for WSOD. To balance feature learning among all object instances, CASD computes the comprehensive attention aggregated from multiple transformations and feature layers of the same images. To enforce consistent spatial supervision on objects, CASD conducts self-distillation on the WSOD networks, such that the comprehensive attention is approximated simultaneously by multiple transformations and feature layers of the same images. CASD produces new state-of-the-art WSOD results on standard benchmarks such as PASCAL VOC 2007/2012 and MS-COCO.",0
"We present a novel approach to weakly supervised object detection that combines attention mechanisms with self-distillation. Our method improves upon existing approaches by leveraging both visual features and label context to guide the training process. By using comprehensive attention, we can effectively focus on relevant regions in the input image while distilling the knowledge from strong fully-supervised models into weak annotations. This leads to more accurate object localization and better generalization across different datasets. In addition, our approach outperforms state-of-the-art methods under a variety of experimental settings, making it a promising solution for real-world applications.",1
"The role of robots in society keeps expanding, bringing with it the necessity of interacting and communicating with humans. In order to keep such interaction intuitive, we provide automatic wayfinding based on verbal navigational instructions. Our first contribution is the creation of a large-scale dataset with verbal navigation instructions. To this end, we have developed an interactive visual navigation environment based on Google Street View; we further design an annotation method to highlight mined anchor landmarks and local directions between them in order to help annotators formulate typical, human references to those. The annotation task was crowdsourced on the AMT platform, to construct a new Talk2Nav dataset with $10,714$ routes. Our second contribution is a new learning method. Inspired by spatial cognition research on the mental conceptualization of navigational instructions, we introduce a soft dual attention mechanism defined over the segmented language instructions to jointly extract two partial instructions -- one for matching the next upcoming visual landmark and the other for matching the local directions to the next landmark. On the similar lines, we also introduce spatial memory scheme to encode the local directional transitions. Our work takes advantage of the advance in two lines of research: mental formalization of verbal navigational instructions and training neural network agents for automatic way finding. Extensive experiments show that our method significantly outperforms previous navigation methods. For demo video, dataset and code, please refer to our project page: https://www.trace.ethz.ch/publications/2019/talk2nav/index.html",0
"This paper presents a novel approach to vision-and-language navigation that combines dual attention and spatial memory mechanisms to address the problem of long-range navigation in complex environments. Our method leverages deep learning techniques to simultaneously attend to both visual features and natural language guidance. By incorporating a recurrent neural network (RNN) into our model, we enable the agent to maintain context over time and reason about past observations when making decisions. We evaluate our system on two challenging benchmark datasets and show significant improvements compared to state-of-the-art methods in terms of success rate, accuracy, and speed. Overall, our results demonstrate the effectiveness and potential impact of integrating vision and language modalities in robotic navigation tasks.",1
"This work presents mEBAL, a multimodal database for eye blink detection and attention level estimation. The eye blink frequency is related to the cognitive activity and automatic detectors of eye blinks have been proposed for many tasks including attention level estimation, analysis of neuro-degenerative diseases, deception recognition, drive fatigue detection, or face anti-spoofing. However, most existing databases and algorithms in this area are limited to experiments involving only a few hundred samples and individual sensors like face cameras. The proposed mEBAL improves previous databases in terms of acquisition sensors and samples. In particular, three different sensors are simultaneously considered: Near Infrared (NIR) and RGB cameras to capture the face gestures and an Electroencephalography (EEG) band to capture the cognitive activity of the user and blinking events. Regarding the size of mEBAL, it comprises 6,000 samples and the corresponding attention level from 38 different students while conducting a number of e-learning tasks of varying difficulty. In addition to presenting mEBAL, we also include preliminary experiments on: i) eye blink detection using Convolutional Neural Networks (CNN) with the facial images, and ii) attention level estimation of the students based on their eye blink frequency.",0
"This study presents a multimodal database called mEBAL that contains eye blink detection data along with attention level estimation metrics captured from real-life interactions on social media platforms such as YouTube videos. The database was created using Amazon Mechanical Turk (MTurk), which allows researchers to collect large datasets with minimal cost and time investment. Additionally, the mEBAL dataset includes annotations from multiple labelers who rated each clip based on their perceived attention levels. These ratings were then used to train machine learning algorithms for attention estimation by predicting how likely viewers were paying attention to specific video clips. The results show that our approach outperforms previous methods for both detecting eye blinks and estimating attention levels from online content, making the mEBAL dataset a valuable resource for future studies in this area. Overall, we believe that the creation of mEBAL serves as a step forward towards building more reliable and accurate automated systems for monitoring engagement and attentiveness during video consumption.",1
"Weakly-supervised Temporal Action Localization (W-TAL) aims to classify and localize all action instances in an untrimmed video under only video-level supervision. However, without frame-level annotations, it is challenging for W-TAL methods to identify false positive action proposals and generate action proposals with precise temporal boundaries. In this paper, we present a Two-Stream Consensus Network (TSCN) to simultaneously address these challenges. The proposed TSCN features an iterative refinement training method, where a frame-level pseudo ground truth is iteratively updated, and used to provide frame-level supervision for improved model training and false positive action proposal elimination. Furthermore, we propose a new attention normalization loss to encourage the predicted attention to act like a binary selection, and promote the precise localization of action instance boundaries. Experiments conducted on the THUMOS14 and ActivityNet datasets show that the proposed TSCN outperforms current state-of-the-art methods, and even achieves comparable results with some recent fully-supervised methods.",0
"This paper presents a novel approach to weakly-supervised temporal action localization using a two-stream consensus network (TSN). Our method leverages recent advances in deep learning to efficiently identify and localize actions in untrimmed video sequences. In contrast to previous methods that rely on fully supervised approaches or require expensive annotations, our proposed TSN only requires sparse annotations consisting of verb class labels without precise bounding box locations. By utilizing both visual features from convolutional neural networks (CNNs) and temporal context obtained through recurrent layers, we achieve state-of-the-art performance while drastically reducing annotation effort. Through extensive experiments across multiple benchmark datasets, we demonstrate the effectiveness and generalizability of our approach. Overall, our work represents an important step towards enabling real-world deployment of weakly-supervised temporal action detection systems.",1
"Image-Text Matching is one major task in cross-modal information processing. The main challenge is to learn the unified visual and textual representations. Previous methods that perform well on this task primarily focus on not only the alignment between region features in images and the corresponding words in sentences, but also the alignment between relations of regions and relational words. However, the lack of joint learning of regional features and global features will cause the regional features to lose contact with the global context, leading to the mismatch with those non-object words which have global meanings in some sentences. In this work, in order to alleviate this issue, it is necessary to enhance the relations between regions and the relations between regional and global concepts to obtain a more accurate visual representation so as to be better correlated to the corresponding text. Thus, a novel multi-level semantic relations enhancement approach named Dual Semantic Relations Attention Network(DSRAN) is proposed which mainly consists of two modules, separate semantic relations module and the joint semantic relations module. DSRAN performs graph attention in both modules respectively for region-level relations enhancement and regional-global relations enhancement at the same time. With these two modules, different hierarchies of semantic relations are learned simultaneously, thus promoting the image-text matching process by providing more information for the final visual representation. Quantitative experimental results have been performed on MS-COCO and Flickr30K and our method outperforms previous approaches by a large margin due to the effectiveness of the dual semantic relations learning scheme. Codes are available at https://github.com/kywen1119/DSRAN.",0
"In image-text matching tasks, understanding dual semantic relationships between images and texts is crucial for accurate alignment. Previous methods have focused on modeling either visual or textual contexts separately but ignored their interactions within a deep network architecture. To address this issue, we propose a novel graph attention mechanism that models both intra-modal (image-to-image and text-to-text) and inter-modal (image-to-text and text-to-image) connections simultaneously. By capturing diverse patterns from the joint embedding space, our method can effectively capture fine-grained semantics without explicit annotations or preprocessing steps like region proposals. Extensive experiments on three challenging benchmark datasets demonstrate significant improvements over state-of-the-art approaches across different evaluation metrics while attaining better interpretability through qualitative analysis. Our work provides valuable insights into cross-modality learning for multimodal fusion tasks beyond image-text applications.",1
"Algebraic neural networks (AlgNNs) are composed of a cascade of layers each one associated to and algebraic signal model, and information is mapped between layers by means of a nonlinearity function. AlgNNs provide a generalization of neural network architectures where formal convolution operators are used, like for instance traditional neural networks (CNNs) and graph neural networks (GNNs). In this paper we study stability of AlgNNs on the framework of algebraic signal processing. We show how any architecture that uses a formal notion of convolution can be stable beyond particular choices of the shift operator, and this stability depends on the structure of subsets of the algebra involved in the model. We focus our attention on the case of algebras with a single generator.",0
"This paper explores the stability of algebraic neural networks (ANN) to small perturbations, which can occur due to noise or errors introduced during training or testing. We analyze how well these models maintain their accuracy under small changes to their inputs or weights, and present theoretical results as well as empirical evidence on commonly used architectures such as multi-layer perceptrons (MLP), convolutional neural networks (CNN), and recurrent neural networks (RNN). Our findings show that while some ANN variants exhibit good robustness against moderate perturbations, others may exhibit rapid degradation even with slight disturbances. Overall, our work provides insights into the sensitivity of ANN to small variations, highlighting directions for future research in improving their resilience. Keywords: Algebraic Neural Networks, Robustness, Sensitivity, Multi-Layer Perceptron, Convolutional Neural Network, Recurrent Neural Network",1
"In recent years, person re-identification (PReID) has become a hot topic in computer vision duo to it is an important part in intelligent surveillance. Many state-of-the-art PReID methods are attention-based or multi-scale feature learning deep models. However, introducing attention mechanism may lead to some important feature information losing issue. Besides, most of the multi-scale models embedding the multi-scale feature learning block into the feature extraction deep network, which reduces the efficiency of inference network. To address these issue, in this study, we introduce an attention deep architecture with multi-scale deep supervision for PReID. Technically, we contribute a reverse attention block to complement the attention block, and a novel multi-scale layer with deep supervision operator for training the backbone network. The proposed block and operator are only used for training, and discard in test phase. Experiments have been performed on Market-1501, DukeMTMC-reID and CUHK03 datasets. All the experiment results show that the proposed model significantly outperforms the other competitive state-of-the-art methods.",0
"This paper presents a novel approach to person re-identification using deep learning techniques. The proposed method uses multi-scale deep supervision, which involves training multiple models at different levels of abstraction, and combining their outputs for improved accuracy. Additionally, attention mechanisms are used to focus on specific regions of interest in the images, allowing the model to better capture relevant features for identification. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods across several datasets, and highlights the importance of utilizing both local and global contextual information for successful person re-id. Our work opens up new possibilities for researchers working in computer vision and related fields, as well as potential applications in security and surveillance.",1
