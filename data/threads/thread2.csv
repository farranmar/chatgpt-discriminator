"Graph classification aims to extract accurate information from graph-structured data for classification and is becoming more and more important in graph learning community. Although Graph Neural Networks (GNNs) have been successfully applied to graph classification tasks, most of them overlook the scarcity of labeled graph data in many applications. For example, in bioinformatics, obtaining protein graph labels usually needs laborious experiments. Recently, few-shot learning has been explored to alleviate this problem with only given a few labeled graph samples of test classes. The shared sub-structures between training classes and test classes are essential in few-shot graph classification. Exiting methods assume that the test classes belong to the same set of super-classes clustered from training classes. However, according to our observations, the label spaces of training classes and test classes usually do not overlap in real-world scenario. As a result, the existing methods don't well capture the local structures of unseen test classes. To overcome the limitation, in this paper, we propose a direct method to capture the sub-structures with well initialized meta-learner within a few adaptation steps. More specifically, (1) we propose a novel framework consisting of a graph meta-learner, which uses GNNs based modules for fast adaptation on graph data, and a step controller for the robustness and generalization of meta-learner; (2) we provide quantitative analysis for the framework and give a graph-dependent upper bound of the generalization error based on our framework; (3) the extensive experiments on real-world datasets demonstrate that our framework gets state-of-the-art results on several few-shot graph classification tasks compared to baselines.",0
"One possible abstract:  A critical challenge in graph classification tasks is to learn from few labeled examples while adapting to new domains quickly without sacrificing performance on previously seen ones. Recent developments have suggested learning algorithms that can successfully perform meta-learning (i.e., ""learning to learn"") by training on multiple related tasks to improve generalization across tasks; however, these methods still encounter difficulties when faced with limited data scenarios. To address this gap, we propose a novel method termed the Adaptive-Step Graph Meta-Learner (ASGML), which effectively adapts to new domains by leveraging both global task relations and local sample adaptation through a hierarchical two-step updating mechanism, using only a handful of annotated samples per class. ASGML progressively integrates knowledge learned during previous steps into current model updates via iterative optimization. Our results show consistent improvements over competitive baselines in terms of accuracy and efficiency for datasets spanning diverse applications including social networks, bioinformatics, and web graphs. The efficacy of our framework demonstrates its potential as a powerful tool for tackling real world graph mining problems requiring minimal supervision.",1
"The success of deep learning has revolutionized many fields of research including areas of computer vision, text and speech processing. Enormous research efforts have led to numerous methods that are capable of efficiently analyzing data, especially in the Euclidean space. However, many problems are posed in non-Euclidean domains modeled as general graphs with complex connection patterns. Increased problem complexity and computational power constraints have limited early approaches to static and small-sized graphs. In recent years, a rising interest in machine learning on graph-structured data has been accompanied by improved methods that overcome the limitations of their predecessors. These methods paved the way for dealing with large-scale and time-dynamic graphs. This work aims to provide an overview of early and modern graph neural network based machine learning methods for node-level prediction tasks. Under the umbrella of taxonomies already established in the literature, we explain the core concepts and provide detailed explanations for convolutional methods that have had strong impact. In addition, we introduce common benchmarks and present selected applications from various areas. Finally, we discuss open problems for further research.",0
"Graph neural networks have emerged as powerful models for graph data due to their ability to capture complex relationships between nodes and edges. In recent years, there has been significant interest in using graph neural networks for predictive tasks such as node classification and regression. However, current state-of-the-art methods primarily focus on edge-level predictions and lack effective ways to model node features in a local context. This paper proposes a novel graph neural network architecture that makes explicit use of node features and incorporates both spatial and spectral information through attention mechanisms. Our method can accurately learn from graph structure while capturing relevant node feature representations by focusing on neighborhood patterns rather than relying exclusively on global information propagation. Experimental results demonstrate improved performance over baseline approaches on multiple benchmark datasets across various domains. We believe our contributions will pave the way towards more efficient and accurate solutions for node-level prediction problems in graphs.",1
"We consider the problem of whether a given decision model, working with structured data, has individual fairness. Following the work of Dwork, a model is individually biased (or unfair) if there is a pair of valid inputs which are close to each other (according to an appropriate metric) but are treated differently by the model (different class label, or large difference in output), and it is unbiased (or fair) if no such pair exists. Our objective is to construct verifiers for proving individual fairness of a given model, and we do so by considering appropriate relaxations of the problem. We construct verifiers which are sound but not complete for linear classifiers, and kernelized polynomial/radial basis function classifiers. We also report the experimental results of evaluating our proposed algorithms on publicly available datasets.",0
"In recent years, machine learning models have been increasingly used to make important decisions that affect individuals, such as loan approvals, hiring decisions, and sentencing recommendations. However, concerns about individual fairness in these decision-making processes have arisen due to potential biases present in training data and algorithms. This study aims to address these concerns by proposing novel techniques for verifying individual fairness in machine learning models.  The proposed methods utilize statistical testing procedures to examine whether certain subgroups of the population are treated unfairly by the model. These tests can detect differences in treatment across different demographic groups, and provide insight into how individual fairness might be improved. We demonstrate the effectiveness of our approach on several real-world datasets commonly used in fairness evaluations, including those involving credit scoring, job applications, and criminal justice outcomes.  Results show that our methods are able to successfully identify cases where individual fairness may be lacking. Furthermore, we find evidence suggesting that common practices in the field of machine learning may actually worsen individual fairness rather than improving it. Our work contributes to the growing literature on fairness in artificial intelligence, emphasizing the importance of formal methods for ensuring equitable decision making. Ultimately, we hope that our research can inform more ethical use of machine learning models in high stakes decision contexts.",1
"Graph deep learning has recently emerged as a powerful ML concept allowing to generalize successful deep neural architectures to non-Euclidean structured data. Such methods have shown promising results on a broad spectrum of applications ranging from social science, biomedicine, and particle physics to computer vision, graphics, and chemistry. One of the limitations of the majority of the current graph neural network architectures is that they are often restricted to the transductive setting and rely on the assumption that the underlying graph is known and fixed. In many settings, such as those arising in medical and healthcare applications, this assumption is not necessarily true since the graph may be noisy, partially- or even completely unknown, and one is thus interested in inferring it from the data. This is especially important in inductive settings when dealing with nodes not present in the graph at training time. Furthermore, sometimes such a graph itself may convey insights that are even more important than the downstream task. In this paper, we introduce Differentiable Graph Module (DGM), a learnable function predicting the edge probability in the graph relevant for the task, that can be combined with convolutional graph neural network layers and trained in an end-to-end fashion. We provide an extensive evaluation of applications from the domains of healthcare (disease prediction), brain imaging (gender and age prediction), computer graphics (3D point cloud segmentation), and computer vision (zero-shot learning). We show that our model provides a significant improvement over baselines both in transductive and inductive settings and achieves state-of-the-art results.",0
"In the world of deep learning, graph convolutional networks have proven themselves to be powerful tools for analyzing complex graphs such as social network data or protein structures. However, many current models rely on fixed architectures that cannot easily adapt to different types of problems or datasets. This can lead to suboptimal results or even failure in some cases.  To address these issues, researchers proposed the differentiable graph module (DGM), which allows for flexible and modular construction of graph convolutional networks. Instead of relying on predefined connections between nodes, the DGM uses a learnable attention mechanism to selectively combine node features from any arbitrary graph structure.  The authors evaluated their approach on several benchmark datasets including citation, co-purchase, and bioinformatics domains. They found that the DGM outperformed state-of-the-art graph convolutional networks across all tasks while reducing model size and computational complexity. Moreover, they showed that the DGM generalizes well to larger graphs, making it suitable for real-world applications where scale and accuracy are critical.  Overall, the differentiable graph module represents a significant advancement in graph convolutional networks by enabling efficient adaptation to diverse graph topologies without sacrificing performance. Its ability to learn optimal representations directly from raw graph data makes it highly attractive for future research in areas ranging from computer vision to natural language processing.",1
"Applying machine learning algorithms to private data, such as financial or medical data, while preserving their confidentiality, is a difficult task. Homomorphic Encryption (HE) is acknowledged for its ability to allow computation on encrypted data, where both the input and output are encrypted, which therefore enables secure inference on private data. Nonetheless, because of the constraints of HE, such as its inability to evaluate non-polynomial functions or to perform arbitrary matrix multiplication efficiently, only inference of linear models seem usable in practice in the HE paradigm so far.   In this paper, we propose Cryptotree, a framework that enables the use of Random Forests (RF), a very powerful learning procedure compared to linear regression, in the context of HE. To this aim, we first convert a regular RF to a Neural RF, then adapt this to fit the HE scheme CKKS, which allows HE operations on real values. Through SIMD operations, we are able to have quick inference and prediction results better than the original RF on encrypted data.",0
"This is a technical research paper that presents a new method for efficiently predicting structured data from encrypted databases while maintaining high accuracy. The authors introduce ""Cryptotree,"" a novel algorithm that uses recursive encryption and depth-first search techniques to make lightning-fast predictions. They evaluate their approach using both real-world datasets and theoretical simulations, demonstrating significant improvements over existing methods in terms of speed and accuracy. In addition, the authors provide insight into potential future applications of Cryptotree, such as personalized medicine and financial fraud detection. Overall, this paper offers a promising solution for enabling secure machine learning on sensitive data, paving the way for a wide range of valuable use cases.",1
"Graph Attention Network (GAT) and GraphSAGE are neural network architectures that operate on graph-structured data and have been widely studied for link prediction and node classification. One challenge raised by GraphSAGE is how to smartly combine neighbour features based on graph structure. GAT handles this problem through attention, however the challenge with GAT is its scalability over large and dense graphs. In this work, we proposed a new architecture to address these issues that is more efficient and is capable of incorporating different edge type information. It generates node representations by attending to neighbours sampled from weighted multi-step transition probabilities. We conduct experiments on both transductive and inductive settings. Experiments achieved comparable or better results on several graph benchmarks, including the Cora, Citeseer, Pubmed, PPI, Twitter, and YouTube datasets.",0
"This paper presents a new approach to graph representation learning using adaptive sampling techniques. Traditional methods rely on fixed graph structures which can limit their ability to capture important features and patterns present in complex datasets. Our proposed method uses a combination of random walks and sampling algorithms to explore different subgraphs within the original graph. By doing so, we can identify diverse and representative samples that better reflect the overall structure and characteristics of the dataset.  We use these sampled graphs as input to our neural network architecture, allowing us to learn meaningful representations that generalize well across multiple tasks. We evaluate the performance of our model on several benchmark datasets and show that it outperforms state-of-the-art methods in terms of accuracy and robustness.  The contributions of this work include: (i) a novel framework for exploring large graphs using adaptive sampling; (ii) a detailed analysis of how different types of sampling affect graph representation learning; and (iii) empirical evidence demonstrating the effectiveness of our approach on several challenging problems. Our findings have implications for a range of applications including recommender systems, fraud detection, and social network analysis. Overall, our research highlights the importance of considering graph topology and structure in order to achieve high quality representation learning.",1
"Nowadays, graph-structured data are increasingly used to model complex systems. Meanwhile, detecting anomalies from graph has become a vital research problem of pressing societal concerns. Anomaly detection is an unsupervised learning task of identifying rare data that differ from the majority. As one of the dominant anomaly detection algorithms, One Class Support Vector Machine has been widely used to detect outliers. However, those traditional anomaly detection methods lost their effectiveness in graph data. Since traditional anomaly detection methods are stable, robust and easy to use, it is vitally important to generalize them to graph data. In this work, we propose One Class Graph Neural Network (OCGNN), a one-class classification framework for graph anomaly detection. OCGNN is designed to combine the powerful representation ability of Graph Neural Networks along with the classical one-class objective. Compared with other baselines, OCGNN achieves significant improvements in extensive experiments.",0
"Abstract: One Class GNN (OCGNN) has been applied successfully in several real-world use cases for anomaly detection on attributed networks such as molecules in chemistry, proteins sequences, and social network data. In each application, OC2GNN first learns a representation from normal examples only and then uses that representation for detecting anomalies. With limited labeled data available for these tasks, training deep learning models can often fail due to overfitting. To mitigate this problem, one approach involves pretraining a model using larger amounts of unlabeled data before fine-tuning with small amounts of labeled data. This study investigates how well OC2GNN performs compared to other methods like Autoencoders and Generative Adversarial Networks. Our results show that while pretrained autoencoders work surprisingly well at both preserving structure and reducing reconstruction error they are outperformed by our proposed model which works directly on graphs with no need for graph reductions or other preprocessing steps required for most other approaches on graphs. We further demonstrate that the performance benefits extend to multiple different applications and datasets. Overall, we find strong evidence that one class GNN is an effective method for anomaly detection on attributed networks, achieving better accuracy than previously reported state-of-the-art results across many datasets.",1
"Buying a home is one of the most important buying decisions people have to make in their life. The latest research on real-estate appraisal focuses on incorporating image data in addition to structured data into the modeling process. This research measures the prediction performance of satellite images and structured data by using convolutional neural networks. The resulting CNN model trained performs 7% better in MAE than the advanced baseline of a neural network trained on structured data. Moreover, sliding-window heatmap provides visual interpretability of satellite images, revealing that neighborhood structures are essential in the price estimation.",0
This would be easier if you gave me some context.,1
"Graph Convolutional Networks (GCNs) have been successfully applied to analyze non-grid data, where the classical convolutional neural networks (CNNs) cannot be directly used. One similarity shared by GCNs and CNNs is the requirement of massive amount of labeled data for network training. In addition, GCNs need the adjacency matrix as input to define the relationship between those non-grid data, which leads to all of data including training, validation and test data typically forms only one graph structures data for training. Furthermore, the adjacency matrix is usually pre-defined and stationary, which makes the data augmentation strategies cannot be employed on the constructed graph structures data to augment the amount of training data. To further improve the learning capacity and model performance under the limited training data, in this paper, we propose two types of self-supervised learning strategies to exploit available information from the input graph structure data itself. Our proposed self-supervised learning strategies are examined on two representative GCN models with three public citation network datasets - Citeseer, Cora and Pubmed. The experimental results demonstrate the generalization ability as well as the portability of our proposed strategies, which can significantly improve the performance of GCNs with the power of self-supervised learning in improving feature learning.",0
"Title: Unsupervised Learning of Graph Representations via Message Passing Neural Networks  Graph neural networks (GNNs) have emerged as powerful tools for modeling complex relationships between nodes in graph data structures. While supervised training has proven effective in improving GNN performance on specific tasks, unsupervised methods can offer more robust representations by capturing general patterns across graphs without task-specific labels. We present a new approach that leverages self-supervision from a simple message passing algorithm based on graph signal processing theory. Our method adapts pre-trained GCNs to better align their node embeddings with those produced by our self-supervised method, resulting in superior performance on downstream node classification benchmarks. In addition, we evaluate the quality of the learned representations using link prediction and clustering metrics. Experiments show consistent improvements over existing state-of-the-art unsupervised techniques on a wide range of datasets. This work represents a step towards developing practical and efficient unsupervised learning algorithms capable of generating high-quality graph representations that transfer well across diverse domains and tasks.",1
"We present Graph Random Neural Features (GRNF), a novel embedding method from graph-structured data to real vectors based on a family of graph neural networks. The embedding naturally deals with graph isomorphism and preserves the metric structure of the graph domain, in probability. In addition to being an explicit embedding method, it also allows us to efficiently and effectively approximate graph metric distances (as well as complete kernel functions); a criterion to select the embedding dimension trading off the approximation accuracy with the computational cost is also provided. GRNF can be used within traditional processing methods or as a training-free input layer of a graph neural network. The theoretical guarantees that accompany GRNF ensure that the considered graph distance is metric, hence allowing to distinguish any pair of non-isomorphic graphs.",0
"This paper proposes a new method called graph random neural features (GRAF) for distance-preserving graph representations that achieve state-of-the-art accuracy on several tasks including link prediction, node classification, and clustering. GRAF builds upon recent advances in graph representation learning by using deep neural networks to learn global, nonlinear features from graphs. These features can then be used as input for machine learning models to solve downstream problems such as graph regression, ranking, or binary classification. Unlike previous methods, GRAF randomly selects pairs of nodes from the original graph at each training iteration and learns their corresponding pairwise distances directly from data, resulting in more robust and accurate feature embeddings. Extensive experiments demonstrate the effectiveness of our approach across different benchmark datasets and model architectures. Our code will be made publicly available to facilitate future research in the field.",1
"Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of VAEs. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without forcing it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data, we introduce an ex-post density estimation step that can be readily applied also to existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. \footnote{An implementation is available at: \url{https://github.com/ParthaEth/Regularized_autoencoders-RAE-}}",0
"Abstract:  In recent years, autoencoder have gained significant attention as powerful tools in unsupervised machine learning due to their ability to generate data representations that capture underlying patterns within datasets. However, current methods for training these models often suffer from issues related to optimization difficulties, poor interpretability, and inability to handle arbitrary likelihood functions. Here we present a new methodology for generating compressed yet informative data encodings using deterministic autoencoders (dAEs). Our approach utilizes a non-probabilistic framework that learns to map raw inputs onto latent codes directly, allowing us to leverage more efficient optimizers while providing transparency into model behavior. We demonstrate through extensive experiments on both synthetic and real-world datasets the effectiveness of our proposed dAE model compared against state-of-the-art variational autoencoders. Our results show improved performance across multiple evaluation metrics, including generation fidelity, reconstruction accuracy, and encoding compactness. Overall, these findings indicate that dAEs provide a viable alternative for those seeking enhanced capabilities beyond traditional variational approaches. Keywords: Autoencoding; Deep Learning; Non-Probabilistic Frameworks; Data Compression",1
"Graph classification is an important task on graph-structured data with many real-world applications. The goal of graph classification task is to train a classifier using a set of training graphs. Recently, Graph Neural Networks (GNNs) have greatly advanced the task of graph classification. When building a GNN model for graph classification, the graphs in the training set are usually assumed to be identically distributed. However, in many real-world applications, graphs in the same dataset could have dramatically different structures, which indicates that these graphs are likely non-identically distributed. Therefore, in this paper, we aim to develop graph neural networks for graphs that are not non-identically distributed. Specifically, we propose a general non-IID graph neural network framework, i.e., Non-IID-GNN. Given a graph, Non-IID-GNN can adapt any existing graph neural network model to generate a sample-specific model for this graph. Comprehensive experiments on various graph classification benchmarks demonstrate the effectiveness of the proposed framework. We will release the code of the proposed framework upon the acceptance of the paper.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful tools for processing data that can be represented by graphs. Unlike traditional deep learning models, GNNs can learn representations on nonlinear entities such as nodes and edges without requiring heavy preprocessing steps. However, these methods assume independent and identically distributed (IID) node features, which may limit their performance when applied to real-world datasets where IID assumptions do not hold. This paper introduces a novel framework called non-IID graph neural networks (NIGNNs), aimed at overcoming these limitations and improving the accuracy and interpretability of GNN predictions on non-IID data. The proposed approach combines multiple layers of representation learning with robust feature generation techniques to capture complex patterns and dependencies across highly variable datasets. Results demonstrate that NIGNNs achieve significant improvements in predictive accuracy compared to existing IID GNN architectures. These findings provide new insights into the impact of distributional heterogeneity in graph data and highlight the importance of developing more flexible GNN architectures that can adapt to diverse and unpredictable conditions.",1
"Graph Neural Networks (GNNs) are efficient approaches to process graph-structured data. Modelling long-distance node relations is essential for GNN training and applications. However, conventional GNNs suffer from bad performance in modelling long-distance node relations due to limited-layer information propagation. Existing studies focus on building deep GNN architectures, which face the over-smoothing issue and cannot model node relations in particularly long distance. To address this issue, we propose to model long-distance node relations by simply relying on shallow GNN architectures with two solutions: (1) Implicitly modelling by learning to predict node pair relations (2) Explicitly modelling by adding edges between nodes that potentially have the same label. To combine our two solutions, we propose a model-agnostic training framework named HighwayGraph, which overcomes the challenge of insufficient labeled nodes by sampling node pairs from the training set and adopting the self-training method. Extensive experimental results show that our HighwayGraph achieves consistent and significant improvements over four representative GNNs on three benchmark datasets.",0
"Abstracts should ideally follow this structure: objective - methodology - results - conclusion.  Objective: Provide a detailed account of the proposed model and explain how HighwayGraph improves general graph neural network by modelling long-distance node relations. Methodology: Describe the key components of HighwayGraph including edge convolutions, multi-scale aggregation, skip connections, attention mechanism, and message passing. Results: Demonstrate experiment results that showcase superior performance compared to previous state-of-the-art methods on benchmark datasets, including enhanced representation learning, better scalability, and improved accuracy in prediction tasks. Conclusion: Emphasize the impact of HighwayGraph as an essential tool for advancing graph neural networks and future research possibilities.",1
"Learning on graph structured data has drawn increasing interest in recent years. Frameworks like Graph Convolutional Networks (GCNs) have demonstrated their ability to capture structural information and obtain good performance in various tasks. In these frameworks, node aggregation schemes are typically used to capture structural information: a node's feature vector is recursively computed by aggregating features of its neighboring nodes. However, most of aggregation schemes treat all connections in a graph equally, ignoring node feature similarities. In this paper, we re-interpret node aggregation from the perspective of kernel weighting, and present a framework to consider feature similarity in an aggregation scheme. Specifically, we show that normalized adjacency matrix is equivalent to a neighbor-based kernel matrix in a Krein Space. We then propose feature aggregation as the composition of the original neighbor-based kernel and a learnable kernel to encode feature similarities in a feature space. We further show how the proposed method can be extended to Graph Attention Network (GAT). Experimental results demonstrate better performance of our proposed framework in several real-world applications.",0
"Abstract: Modern computer vision problems require powerful models which can handle complex tasks such as object recognition in images and videos. One popular approach to achieve these requirements has been using deep neural networks and specifically convolutional neural networks (CNN). Recently, graph neural networks (GNN) have shown promising results on several computer vision datasets. However, most GNN architectures rely heavily on standard kernel functions which may limit their expressive power compared to CNNs. To address this limitation, we propose the use of composite kernels within the context of GNNs. In particular, we combine elementwise operations (such as multiplication, division, square root) with regular kernel functions (such as ReLU, sigmoid, softmax). We show that our proposed architecture achieves competitive performance while reducing model size and number of parameters, making it more efficient than traditional approaches.",1
"Graph Neural Networks (GNNs) are powerful to learn the representation of graph-structured data. Most of the GNNs use the message-passing scheme, where the embedding of a node is iteratively updated by aggregating the information of its neighbors. To achieve a better expressive capability of node influences, attention mechanism has grown to be popular to assign trainable weights to the nodes in aggregation. Though the attention-based GNNs have achieved remarkable results in various tasks, a clear understanding of their discriminative capacities is missing. In this work, we present a theoretical analysis of the representational properties of the GNN that adopts the attention mechanism as an aggregator. Our analysis determines all cases when those attention-based GNNs can always fail to distinguish certain distinct structures. Those cases appear due to the ignorance of cardinality information in attention-based aggregation. To improve the performance of attention-based GNNs, we propose cardinality preserved attention (CPA) models that can be applied to any kind of attention mechanisms. Our experiments on node and graph classification confirm our theoretical analysis and show the competitive performance of our CPA models.",0
"Title: Improving attention mechanism in graph neural networks through cardinality preservation Abstract: Graph neural networks (GNNs) have emerged as powerful tools for modeling complex data structures represented by graphs. One key component of GNN architectures is their attention mechanism, which allows the network to selectively focus on certain aspects of the input graph during training. However, current attention mechanisms in GNNs suffer from several limitations that can lead to suboptimal performance. In particular, they may struggle to capture the unique characteristics of different types of graphs, such as those with varying levels of connectivity or differing node degrees. This can result in poor accuracy and missed opportunities for learning important patterns from the data. To address these issues, we propose a novel approach called card",1
"Graph Convolutional Networks (GCNs) have been widely used due to their outstanding performance in processing graph-structured data. However, the undirected graphs limit their application scope. In this paper, we extend spectral-based graph convolution to directed graphs by using first- and second-order proximity, which can not only retain the connection properties of the directed graph, but also expand the receptive field of the convolution operation. A new GCN model, called DGCN, is then designed to learn representations on the directed graph, leveraging both the first- and second-order proximity information. We empirically show the fact that GCNs working only with DGCNs can encode more useful information from graph and help achieve better performance when generalized to other models. Moreover, extensive experiments on citation networks and co-purchase datasets demonstrate the superiority of our model against the state-of-the-art methods.",0
"Artificial intelligence has made significant advancements over recent years due to deep learning techniques such as convolutional neural networks (CNNs). In particular, graph CNNs have emerged as a powerful tool for modeling irregular data structures like graphs by exploiting their underlying spatial patterns. These models can learn representations that capture important features from raw data, making them valuable for tasks like node classification, link prediction, community detection, and edge classification. While these methods have been successful, they often suffer from limited interpretability since they rely on the learned weights between nodes which may change during inference time and cannot be directly visualized/inspected. This lack of transparency makes it difficult to apply graph CNNs across different domains. Therefore, we propose Directed Graph Convolutional Networks (DGCNs) aimed at addressing these limitations. By introducing direct connections between neighboring layers within our DGCN architecture, we enforce sparsity constraints to make our learned weights interpretable while still maintaining high accuracy for several datasets commonly used as benchmarks. Our results showcase improved performance compared to state-of-the-art GCNs while providing a more transparent interpretation process. With these advantages, DGCNs could potentially revolutionize the field of AI and further push the boundaries of applications using graph data.",1
"Representation learning over graph structured data has been mostly studied in static graph settings while efforts for modeling dynamic graphs are still scant. In this paper, we develop a novel hierarchical variational model that introduces additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN) to capture both topology and node attribute changes in dynamic graphs. We argue that the use of high-level latent random variables in this variational GRNN (VGRNN) can better capture potential variability observed in dynamic graphs as well as the uncertainty of node latent representation. With semi-implicit variational inference developed for this new VGRNN architecture (SI-VGRNN), we show that flexible non-Gaussian latent representations can further help dynamic graph analytic tasks. Our experiments with multiple real-world dynamic graph datasets demonstrate that SI-VGRNN and VGRNN consistently outperform the existing baseline and state-of-the-art methods by a significant margin in dynamic link prediction.",0
"Deep learning has revolutionized many fields by demonstrating state-of-the-art performance on numerous tasks such as image classification, speech recognition, natural language processing, etc. Among all these deep models, recurrent neural networks (RNNs) have been shown to be particularly powerful due to their ability to model sequential data. However, training RNNs remains challenging, especially as they suffer from issues like vanishing gradients and exploding activations. To address these problems, different architectures have emerged, one of which is graph-based methods that operate over the weights of the network rather than the gradients. In this work, we propose variational graph regularization (VGR), a novel technique based on graph theory, which can be applied to any recurrent neural network architecture. Our method introduces new constraints on the weight matrices of each layer that enforce similarity between neighboring layers and improve stability during backpropagation. We demonstrate the effectiveness of VGR through extensive experiments on several benchmark datasets such as handwriting generation, sentiment analysis, and machine translation. Results show that our approach outperforms current state-of-the-art models while requiring less computational resources. Overall, VGR opens up new possibilities for training fast, stable, and accurate recurrent neural networks.",1
"Automated anatomical labeling plays a vital role in coronary artery disease diagnosing procedure. The main challenge in this problem is the large individual variability inherited in human anatomy. Existing methods usually rely on the position information and the prior knowledge of the topology of the coronary artery tree, which may lead to unsatisfactory performance when the main branches are confusing. Motivated by the wide application of the graph neural network in structured data, in this paper, we propose a conditional partial-residual graph convolutional network (CPR-GCN), which takes both position and CT image into consideration, since CT image contains abundant information such as branch size and spanning direction. Two majority parts, a Partial-Residual GCN and a conditions extractor, are included in CPR-GCN. The conditions extractor is a hybrid model containing the 3D CNN and the LSTM, which can extract 3D spatial image features along the branches. On the technical side, the Partial-Residual GCN takes the position features of the branches, with the 3D spatial image features as conditions, to predict the label for each branches. While on the mathematical side, our approach twists the partial differential equation (PDE) into the graph modeling. A dataset with 511 subjects is collected from the clinic and annotated by two experts with a two-phase annotation process. According to the five-fold cross-validation, our CPR-GCN yields 95.8% meanRecall, 95.4% meanPrecision and 0.955 meanF1, which outperforms state-of-the-art approaches.",0
"Here is the Abstract: A recent development in the field of medical imaging has been the increasing use of artificial intelligence (AI) to automate tasks such as diagnosis, segmentation, and labeling of various structures within images. In particular, graph convolutional networks (GCNs) have shown promising results in these areas due to their ability to handle non-grid data and perform feature learning on irregular graphs. However, one challenge facing GCNs is that they may suffer from high computation cost and overfitting when dealing with large datasets. In order to address these issues, we propose a novel method called conditional partial-residual GCN (CPR-GCN), which combines both GCN layers and residual connections based on the idea of partial connections. We demonstrate our method on the task of automated anatomical labeling of coronary arteries, where previous approaches have relied heavily on pre-processing steps before applying deep learning models. Our experimental results show that our proposed model outperforms state-of-the-art methods while reducing computational costs and preventing overfitting. Overall, our work represents a significant step forward in the application of AI to medical image analysis, and opens up new possibilities for improving patient care through more efficient and accurate diagnostics.",1
"In this paper, we propose a dimensionality reduction method applied to tensor-structured data as a hidden layer (we call it TensorProjection Layer) in a convolutional neural network. Our proposed method transforms input tensors into ones with a smaller dimension by projection. The directions of projection are viewed as training parameters associated with our proposed layer and trained via a supervised learning criterion such as minimization of the cross-entropy loss function. We discuss the gradients of the loss function with respect to the parameters associated with our proposed layer. We also implement simple numerical experiments to evaluate the performance of the TensorProjection Layer.",0
"In traditional image classification tasks, preprocessing steps such as normalization and feature extraction can greatly impact the performance of the model. One approach that has recently gained popularity is using convolutional neural networks (CNNs) due to their strong ability to learn robust features. However, these models often struggle with high-dimensional data which increases the risk of overfitting. To address this problem, a novel method called the tensor projection layer was introduced. This method uses a nonlinear projection matrix to map the input tensors into a lower dimensional space without losing any information required for efficient computation. The resulting projected output is then passed through the network to improve accuracy and reduce computational costs. Experimental results show a significant improvement in accuracy compared to standard methods of feature reduction such as PCA and tSNE on multiple benchmark datasets including MNIST, CIFAR-10, and ImageNet. Furthermore, the proposed method exhibits better generalization properties than competing techniques. Overall, the tensor projection layer provides a powerful tool for dimensionality reduction in CNN architectures leading to more effective training and higher test accuracies.",1
"We present a prior for manifold structured data, such as surfaces of 3D shapes, where deep neural networks are adopted to reconstruct a target shape using gradient descent starting from a random initialization. We show that surfaces generated this way are smooth, with limiting behavior characterized by Gaussian processes, and we mathematically derive such properties for fully-connected as well as convolutional networks. We demonstrate our method in a variety of manifold reconstruction applications, such as point cloud denoising and interpolation, achieving considerably better results against competitive baselines while requiring no training data. We also show that when training data is available, our method allows developing alternate parametrizations of surfaces under the framework of AtlasNet, leading to a compact network architecture and better reconstruction results on standard image to shape reconstruction benchmarks.",0
"This paper presents a new method for training deep neural networks that uses multiple losses in parallel, each optimized with a different prior. We call this approach ""Deep Manifold Prior."" By using multiple priors, we can guide the network towards solutions that are more generalizable across tasks, robust against input perturbations, and efficient in terms of model size. We demonstrate the effectiveness of our method on several benchmark datasets and compare its performance to state-of-the-art alternatives. Our results show that Deep Manifold Prior significantly improves both accuracy and robustness, while requiring fewer parameters and computational resources than other methods. Overall, our work offers a new perspective on how to design deep learning algorithms that achieve better generalization through explicit control over the inductive biases present in the learned models.",1
"Graph-structured data arise in many scenarios. A fundamental problem is to quantify the similarities of graphs for tasks such as classification. Graph kernels are positive-semidefinite functions that decompose graphs into substructures and compare them. One problem in the effective implementation of this idea is that the substructures are not independent, which leads to high-dimensional feature space. In addition, graph kernels cannot capture the high-order complex interactions between vertices. To mitigate these two problems, we propose a framework called DeepMap to learn deep representations for graph feature maps. The learnt deep representation for a graph is a dense and low-dimensional vector that captures complex high-order interactions in a vertex neighborhood. DeepMap extends Convolutional Neural Networks (CNNs) to arbitrary graphs by aligning vertices across graphs and building the receptive field for each vertex. We empirically validate DeepMap on various graph classification benchmarks and demonstrate that it achieves state-of-the-art performance.",0
"This paper presents DeepMap: a method that uses deep neural networks to learn representations on graphs that capture important features. This approach allows us to handle complex dependencies and interactions across nodes. We demonstrate how our system outperforms competing state-of-the-art graph classification methods by accurately predicting node labels on several benchmark datasets. Our contributions include both qualitative results (visualizations) as well as quantitative improvements. This paper introduces the use of deep neural networks to learn representations of graphs. We introduce the DeepMap algorithm which leverages these deep learning models to capture complex relationships across nodes in graphs. Specifically, we apply these learned representations to the task of graph classification where our model shows significant improvement over competing approaches. In addition, we provide visualizations demonstrating the effectiveness of our approach on multiple benchmark datasets. Overall, our work advances the state-of-art in graph representation learning while providing meaningful insights into real world applications.",1
"Object Cluster Hierarchies is a new variant of Hierarchical Cluster Analysis that gains interest in the field of Machine Learning. Being still at an early stage of development, the lack of tools for systematic analysis of Object Cluster Hierarchies inhibits its further improvement. In this paper we address this issue by proposing a generator of synthetic hierarchical data that can be used for benchmarking Object Cluster Hierarchy methods. The article presents a thorough empirical and theoretical analysis of the generator and provides guidance on how to control its parameters. Conducted experiments show the usefulness of the data generator that is capable of producing a wide range of differently structured data. Further, benchmarking datasets that mirror the most common types of hierarchies are generated and made available to the public, together with the developed generator (http://kio.pwr.edu.pl/?page\_id=396).",0
"The aim of this research is to develop a hierarchical data generator that can create datasets with varying levels of complexity to benchmark clustering methods. To achieve this goal, we propose using a tree-structured stick breaking process (TSSB) as a foundation for generating synthetic data. The TSSB approach allows us to control the level of variability within each cluster while maintaining a consistent structure across all clusters in the hierarchy. We demonstrate how our method can generate datasets with different characteristics such as number of clusters, cluster size, mixture ratio, variance, etc., making it suitable for evaluating clustering algorithms under diverse conditions. We evaluate the performance of our data generator through extensive experiments and compare it against existing state-of-the-art techniques. Our results show that the proposed method generates high-quality datasets that can effectively challenge clustering algorithms while providing meaningful insights into their strengths and limitations. This work has significant implications for advancing the field of unsupervised learning by enabling more robust evaluation of clustering algorithms and driving further innovation in this area.",1
"Vector representations of graphs and relational structures, whether hand-crafted feature vectors or learned representations, enable us to apply standard data analysis and machine learning techniques to the structures. A wide range of methods for generating such embeddings have been studied in the machine learning and knowledge representation literature. However, vector embeddings have received relatively little attention from a theoretical point of view.   Starting with a survey of embedding techniques that have been used in practice, in this paper we propose two theoretical approaches that we see as central for understanding the foundations of vector embeddings. We draw connections between the various approaches and suggest directions for future research.",0
"Title: Word2Vec, Node2Vec, Graph2Vec, X2Vec: From Scalar Fields to Complex Networks  In recent years, vector embeddings have emerged as a powerful tool in natural language processing, computer vision, and other data-driven fields. These methods transform high-dimensional data into lower-dimensional vectors while preserving essential properties such as distances and relations among elements. In particular, the popular word2vec algorithm has proven effective in capturing latent semantic structure from text corpora. Extensions of these techniques have been applied to various forms of structured data beyond plain texts, including graphs representing social networks, knowledge bases, and chemical compounds. Despite their widespread use, there remains a lack of a unified framework that encompasses all these developments under one umbrella. This paper addresses this gap by presenting a comprehensive survey of state-of-the-art approaches to vector embeddings of structured data. We focus on four key frameworks  word2vec, node2vec, graph2vec, and X2vec  which represent increasing levels of complexity in terms of the types of data they can handle. By reviewing each approach in turn and highlighting their unique features and capabilities, we aim to provide researchers and practitioners with a clear understanding of how these methods work and how to choose the most appropriate technique for a given problem. Finally, we discuss open challenges and future directions for further advancing the theory and applications of vector embeddings in different domains. Our ultimate goal is to establish a solid foundation for the development of more advanced models that can effectively capture complex relationships within and across diverse datasets.",1
"Over the past few years, we have seen fundamental breakthroughs in core problems in machine learning, largely driven by advances in deep neural networks. At the same time, the amount of data collected in a wide array of scientific domains is dramatically increasing in both size and complexity. Taken together, this suggests many exciting opportunities for deep learning applications in scientific settings. But a significant challenge to this is simply knowing where to start. The sheer breadth and diversity of different deep learning techniques makes it difficult to determine what scientific problems might be most amenable to these methods, or which specific combination of methods might offer the most promising first approach. In this survey, we focus on addressing this central issue, providing an overview of many widely used deep learning models, spanning visual, sequential and graph structured data, associated tasks and different training methods, along with techniques to use deep learning with less data and better interpret these complex models --- two central considerations for many scientific use cases. We also include overviews of the full design process, implementation tips, and links to a plethora of tutorials, research summaries and open-sourced deep learning pipelines and pretrained models, developed by the community. We hope that this survey will help accelerate the use of deep learning across different scientific domains.",0
"As more scientific research depends on large datasets and computational analysis, deep learning has emerged as a powerful tool for data exploration and discovery. This survey presents current applications of deep learning techniques in diverse fields such as physics, astronomy, biology, chemistry, environmental science, geoscience, and engineering. We highlight key aspects of these methods and discuss their advantages, limitations, and opportunities. Our study reveals exciting advancements that offer new pathways to improve our understanding of complex systems. By providing examples from recent studies, we aim to inspire scientists across disciplines and encourage interdisciplinary collaborations to accelerate breakthroughs in domains ranging from earth sciences to life sciences and beyond. To ensure accessibility to both researchers and practitioners, we provide comprehensive background material before delving into technical details of specific approaches. Furthermore, we offer suggestions for future work in deep learning for scientific inquiry. Overall, this work underscores the vast potential of machine intelligence in transforming the ways we discover knowledge, make predictions, and uncover patterns hidden within massive amounts of data.",1
"Technology and the fruition of cultural heritage are becoming increasingly more entwined, especially with the advent of smart audio guides, virtual and augmented reality, and interactive installations. Machine learning and computer vision are important components of this ongoing integration, enabling new interaction modalities between user and museum. Nonetheless, the most frequent way of interacting with paintings and statues still remains taking pictures. Yet images alone can only convey the aesthetics of the artwork, lacking is information which is often required to fully understand and appreciate it. Usually this additional knowledge comes both from the artwork itself (and therefore the image depicting it) and from an external source of knowledge, such as an information sheet. While the former can be inferred by computer vision algorithms, the latter needs more structured data to pair visual content with relevant information. Regardless of its source, this information still must be be effectively transmitted to the user. A popular emerging trend in computer vision is Visual Question Answering (VQA), in which users can interact with a neural network by posing questions in natural language and receiving answers about the visual content. We believe that this will be the evolution of smart audio guides for museum visits and simple image browsing on personal smartphones. This will turn the classic audio guide into a smart personal instructor with which the visitor can interact by asking for explanations focused on specific interests. The advantages are twofold: on the one hand the cognitive burden of the visitor will decrease, limiting the flow of information to what the user actually wants to hear; and on the other hand it proposes the most natural way of interacting with a guide, favoring engagement.",0
"Visual question answering (VQA) has been increasingly attracted attention from researchers due to its ability to provide precise answers through natural language and image data fusion. In the domain of cultural heritage, VQA can offer valuable assistance to both professionals and visitors by supplying accurate responses while providing contextual knowledge that traditional Q&A systems cannot capture. By integrating multiple sources, including text documents, images, videos and even geospatial data, this technology offers users access to diverse and detailed information. This paper presents our recent work on developing novel approaches for fine-grained visual question answering applied to different tasks within the realm of cultural heritage. We demonstrate how these methods effectively address challenges related to ambiguity, heterogeneity and scale inherent in these applications. Our results showcase the strengths of these strategies towards enhancing user experience in exploring rich multimedia archives and collections. Furthermore, we discuss potential future directions for leveraging large-scale datasets, deep learning models and collaborative filtering techniques as well as adapting existing ones for specific needs across diverse domains.",1
"We present a novel framework that can combine multi-domain learning (MDL), data imputation (DI) and multi-task learning (MTL) to improve performance for classification and regression tasks in different domains. The core of our method is an adversarial autoencoder that can: (1) learn to produce domain-invariant embeddings to reduce the difference between domains; (2) learn the data distribution for each domain and correctly perform data imputation on missing data. For MDL, we use the Maximum Mean Discrepancy (MMD) measure to align the domain distributions. For DI, we use an adversarial approach where a generator fill in information for missing data and a discriminator tries to distinguish between real and imputed values. Finally, using the universal feature representation in the embeddings, we train a classifier using MTL that given input from any domain, can predict labels for all domains. We demonstrate the superior performance of our approach compared to other state-of-art methods in three distinct settings, DG-DI in image recognition with unstructured data, MTL-DI in grade estimation with structured data and MDMTL-DI in a selection process using mixed data.",0
"Machine learning has revolutionized many fields by enabling computers to learn from data without explicit programming. However, real world applications often involve multiple datasets that are diverse across domains, which hinders progress towards generalizing machine learning models. In this work we propose novel multi-domain adversarial autoencoding approach capable of both unifying different domains into one shared space while preserving their unique characteristics. Our algorithm uses two discriminators; global and local which operate at domain level and sample level respectively. These discriminators assist generator in generating coherent samples, as well as filling missing values present within each dataset. We evaluate our framework on benchmark datasets spanning various domains including image, text and tabular structured data. Experimental results show significant improvement over traditional imputation techniques as well as comparative baselines. Further ablation studies exhibit robustness and scalability of proposed model. This research can potentially enhance utility of existing applications as well as enable new possibilities such as few shot cross-domain transfer learning. Overall, our contribution addresses current limitations in developing models suitable for handling multiple diverse datasets encountered during practice in various domains.",1
"We introduce AutoGluon-Tabular, an open-source AutoML framework that requires only a single line of Python to train highly accurate machine learning models on an unprocessed tabular dataset such as a CSV file. Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers. Experiments reveal that our multi-layer combination of many models offers better use of allocated training time than seeking out the best. A second contribution is an extensive evaluation of public and commercial AutoML platforms including TPOT, H2O, AutoWEKA, auto-sklearn, AutoGluon, and Google AutoML Tables. Tests on a suite of 50 classification and regression tasks from Kaggle and the OpenML AutoML Benchmark reveal that AutoGluon is faster, more robust, and much more accurate. We find that AutoGluon often even outperforms the best-in-hindsight combination of all of its competitors. In two popular Kaggle competitions, AutoGluon beat 99% of the participating data scientists after merely 4h of training on the raw data.",0
"In recent years, there has been a surge in interest in AutoML tools that can automatically select machine learning (ML) pipelines from large models spaces, while outperforming handcrafted models on novel distributions. However, despite their popularity, most existing AutoML systems have failed to address the critical problem of model interpretability. This lack of interpretability hinders understanding of how these black box models work and makes them unacceptable for use cases involving sensitive data such as healthcare records, personal finances, etc., where transparency is crucial. To address this issue, we propose AutoGluon-Tabular, which seamlessly combines the strengths of two state-of-the-art technologies - AutoGluon and Tabular-CVAE. Unlike traditional AutoML systems that rely solely on numerical features, our approach leverages structured data to achieve greater accuracy by incorporating domain knowledge into feature engineering processes. Our experiments show that AutoGluon-Tabular achieves better performance than other AutoML baselines across multiple datasets and ML tasks including regression, classification, and time series forecasting. Additionally, through ablation studies, we demonstrate that each component in AutoGluon-Tabular contributes significantly to improving robustness and accuracy. In summary, AutoGluon-Tabular provides both strong performance guarantees and interpretability, making it an ideal choice for businesses seeking reliable yet transparent solutions for automating ML pipeline development.",1
"Computing the similarity between two data points plays a vital role in many machine learning algorithms. Metric learning has the aim of learning a good metric automatically from data. Most existing studies on metric learning for tree-structured data have adopted the approach of learning the tree edit distance. However, the edit distance is not amenable for big data analysis because it incurs high computation cost. In this paper, we propose a new metric learning approach for tree-structured data with pq-grams. The pq-gram distance is a distance for ordered labeled trees, and has much lower computation cost than the tree edit distance. In order to perform metric learning based on pq-grams, we propose a new differentiable parameterized distance, weighted pq-gram distance. We also propose a way to learn the proposed distance based on Large Margin Nearest Neighbors (LMNN), which is a well-studied and practical metric learning scheme. We formulate the metric learning problem as an optimization problem and use the gradient descent technique to perform metric learning. We empirically show that the proposed approach not only achieves competitive results with the state-of-the-art edit distance-based methods in various classification problems, but also solves the classification problems much more rapidly than the edit distance-based methods.",0
"This paper presents a novel approach to metric learning using ordered labeled trees (OLT) to capture hierarchical structures in data distributions. We propose the use of pq-grams as a discriminative feature representation that captures local patterns from raw data without requiring domain knowledge or expert tuning. Our method employs a variant of batch gradient descent called sequential minibatch k-gradient descent (SMBGD), which enables efficient computation and reduces overfitting by effectively utilizing mini-batch information. Experimental results demonstrate that our proposed method outperforms state-of-the-art techniques across multiple benchmark datasets, including image classification, sentiment analysis, and gene regulatory network inference tasks. Our findings suggest that OLT-based metric learning with pq-gram features can be used to effectively model complex high-dimensional data distributions and achieve improved performance on challenging machine learning problems.",1
"In this paper, we study the problem of learning Graph Convolutional Networks (GCNs) for regression. Current architectures of GCNs are limited to the small receptive field of convolution filters and shared transformation matrix for each node. To address these limitations, we propose Semantic Graph Convolutional Networks (SemGCN), a novel neural network architecture that operates on regression tasks with graph-structured data. SemGCN learns to capture semantic information such as local and global node relationships, which is not explicitly represented in the graph. These semantic relationships can be learned through end-to-end training from the ground truth without additional supervision or hand-crafted rules. We further investigate applying SemGCN to 3D human pose regression. Our formulation is intuitive and sufficient since both 2D and 3D human poses can be represented as a structured graph encoding the relationships between joints in the skeleton of a human body. We carry out comprehensive studies to validate our method. The results prove that SemGCN outperforms state of the art while using 90% fewer parameters.",0
"In recent years, there has been significant progress in computer vision tasks such as image classification and object detection using deep learning techniques, particularly convolutional neural networks (CNN). However, human pose estimation remains a challenging problem due to occlusions, varying body shapes, and complex interactions within scenes. Existing methods rely on handcrafted features or recurrent architectures that can struggle to capture global dependencies. In this work, we propose semantic graph convolutional networks for human pose regression, which leverages scene contextual relationships among keypoints by modeling their dependencies through graphs. We demonstrate state-of-the-art performance on two benchmark datasets: Human3.6M and MPI-INF-3DHP, surpassing previous methods that use conventional sequential models or graph reasoning modules. Our approach opens up new possibilities for incorporating external knowledge into deep learning frameworks, paving the way for more advanced applications in areas like virtual reality, robotics, and augmented reality.",1
"In view of the huge success of convolution neural networks (CNN) for image classification and object recognition, there have been attempts to generalize the method to general graph-structured data. One major direction is based on spectral graph theory and graph signal processing. In this paper, we study the problem from a completely different perspective, by introducing parallel flow decomposition of graphs. The essential idea is to decompose a graph into families of non-intersecting one dimensional (1D) paths, after which, we may apply a 1D CNN along each family of paths. We demonstrate that the our method, which we call GraphFlow, is able to transfer CNN architectures to general graphs. To show the effectiveness of our approach, we test our method on the classical MNIST dataset, synthetic datasets on network information propagation and a news article classification dataset.",0
"This paper presents a new graph convolutional network (GCN) architecture called GFCN which utilizes parallel flows to improve performance and efficiency over traditional GCN models. GCNs have become increasingly popular in recent years due to their ability to capture structural information from graphs using neural networks. However, existing GCN architectures suffer from limitations such as limited scalability, computational complexity, and difficulty handling large datasets. To address these issues, we propose the use of parallel flows within our proposed model, allowing us to efficiently process large amounts of data while reducing computational costs. We demonstrate that GFCN achieves state-of-the-art results across several benchmark datasets and outperforms other commonly used GCN architectures in terms of accuracy and speed. Our approach represents a significant step forward in advancing GCN technology for real-world applications.",1
"Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making them infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce the relative temporal encoding technique into HGT, which is able to capture the dynamic structural dependency with arbitrary durations. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm---HGSampling---for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9%--21% on various downstream tasks.",0
"Recent advances in deep learning have revolutionized many fields and disciplines, including computer vision, natural language processing, and speech recognition. In particular, graph neural networks (GNNs) have emerged as powerful tools for modeling complex data structures such as graphs and trees. However, existing GNN architectures often suffer from performance degradation when dealing with large graphs or datasets due to computational limitations. To address these limitations, we propose a new framework called heterogeneous graph transformer that combines the strengths of both graph convolutional networks (GCNs) and attention mechanisms to improve the efficiency and accuracy of graph neural networks. Our proposed architecture allows us to effectively capture global dependencies while reducing computational complexity through parallelization across different layers of computation. We demonstrate the effectiveness of our method on several benchmark tasks involving node classification and link prediction, outperforming state-of-the-art baselines by significant margins. Overall, our work represents an important step towards creating more efficient and accurate graph neural models capable of handling real-world applications.",1
"Neural networks for structured data like graphs have been studied extensively in recent years. To date, the bulk of research activity has focused mainly on static graphs. However, most real-world networks are dynamic since their topology tends to change over time. Predicting the evolution of dynamic graphs is a task of high significance in the area of graph mining. Despite its practical importance, the task has not been explored in depth so far, mainly due to its challenging nature. In this paper, we propose a model that predicts the evolution of dynamic graphs. Specifically, we use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graphs. Then, we employ a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology. We evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets. Results demonstrate the effectiveness of the proposed model.",0
"Artificial Intelligence (AI) has been used extensively in natural language processing tasks such as sentiment analysis, machine translation, question answering, among others. In many applications, however, dynamic graphs play a crucial role where graph evolution over time plays an important part. For example, stock market analysis often relies on understanding how social networks change in response to news articles posted online, while disease prediction models can benefit from tracking the spread of illnesses through contact networks of individuals. Motivated by these applications, we present EvoNet: A novel neural network architecture that predicts how dynamic graphs evolve over time using only their current states. Our method treats each node as having a continuous state vector representation which captures both static attributes of the node and dynamic features related to changes of edges incident on them over time. We then design two separate components within our model - one responsible for learning edge dynamics conditioned solely on the initial snapshot, and another component focused on capturing self-dynamics of individual nodes. Both components interact non-linearly in order to output predictions for graph snapshots at later times. Experiments conducted across several datasets demonstrate significant improvements achieved through our approach over existing baselines. They showcase compelling accuracy gains upwards of nearly +15% when forecasting future edges over short time horizons as well as qualitative advantages in terms of predicting node attribute changes such as node degrees. Our contributions enable researchers and practitioners alike to build more accurate models of real-world systems whose behaviors can be naturally represented via dynamic graphs. By enabling better forecasts into these system properties, valuable insights become accessible through improved decision making processes across industries impacted by technology today.",1
"Learning generative models for graph-structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.",0
"This paper presents a novel approach for generating graphs that have permutation invariant properties. We use score-based generative modeling techniques to learn the probability distribution over possible graphs given some constraints on their structure. Our method is able to generate graphs that can capture a wide range of statistical patterns found in real-world networks while preserving symmetry under graph automorphisms. The key contributions of our work include: A new algorithm based on optimal transport theory which enables efficient learning of permutation invariant graph distributions from data; An improved scoring function to measure the quality of generated graphs that takes into account both structural similarity as well as permutation invariance properties; An empirical study showing the effectiveness of our approach across multiple applications ranging from biological network analysis to social network analysis and computer vision tasks like graph matching. Our results demonstrate the potential for using permutation invariant graphs as a powerful tool in modeling complex systems that naturally exhibit such symmetries, providing more accurate predictions and insights than traditional non-permutation invariant methods. This research paves the way towards developing new algorithms and models for analyzing large scale networks by capturing their inherent symmetric structures, ultimately leading to a better understanding of these complex phenomena.",1
"Detecting communities on graphs has received significant interest in recent literature. Current state-of-the-art community embedding approach called \textit{ComE} tackles this problem by coupling graph embedding with community detection. Considering the success of hyperbolic representations of graph-structured data in last years, an ongoing challenge is to set up a hyperbolic approach for the community detection problem. The present paper meets this challenge by introducing a Riemannian equivalent of \textit{ComE}. Our proposed approach combines hyperbolic embeddings with Riemannian K-means or Riemannian mixture models to perform community detection. We illustrate the usefulness of this framework through several experiments on real-world social networks and comparisons with \textit{ComE} and recent hyperbolic-based classification approaches.",0
"In recent years, node embedding methods have become increasingly popular as a tool for representing high-dimensional graphs in lower dimensions while preserving their structural properties. These techniques rely on the idea that graph nodes can be represented by fixed-length vectors, which capture both local and global relationships within the network. Despite their success, however, many existing approaches suffer from several limitations, including computational complexity, sensitivity to parameter choices, and difficulties in capturing hierarchical structures. In response to these challenges, we propose a novel framework called hyperbolic node embedding (HNE) that leverages non-Euclidean geometry to model complex networks more effectively. Our approach builds upon the successful hyperbolic random walk algorithm and uses a variational autoencoder architecture to learn embeddings. We demonstrate through extensive experiments across different datasets and applications that HNE achieves superior performance compared to state-of-the-art baselines while being computationally efficient and robust to parameter settings. Furthermore, our method offers new insights into the nature of real-world networks by providing meaningful interpretations of learned embeddings. Our work opens up exciting opportunities for future research at the intersection of computer science, mathematics, and physics.",1
"Graph-structured data arise ubiquitously in many application domains. A fundamental problem is to quantify their similarities. Graph kernels are often used for this purpose, which decompose graphs into substructures and compare these substructures. However, most of the existing graph kernels do not have the property of scale-adaptivity, i.e., they cannot compare graphs at multiple levels of granularities. Many real-world graphs such as molecules exhibit structure at varying levels of granularities. To tackle this problem, we propose a new graph kernel called Tree++ in this paper. At the heart of Tree++ is a graph kernel called the path-pattern graph kernel. The path-pattern graph kernel first builds a truncated BFS tree rooted at each vertex and then uses paths from the root to every vertex in the truncated BFS tree as features to represent graphs. The path-pattern graph kernel can only capture graph similarity at fine granularities. In order to capture graph similarity at coarse granularities, we incorporate a new concept called super path into it. The super path contains truncated BFS trees rooted at the vertices in a path. Our evaluation on a variety of real-world graphs demonstrates that Tree++ achieves the best classification accuracy compared with previous graph kernels.",0
"""Graph kernels have proven to be a powerful tool in graph analysis due to their ability to capture structural properties of graphs and enable downstream machine learning tasks such as classification and regression. In recent years, there has been significant interest in developing novel graph kernel approaches that can effectively handle large datasets while still maintaining high accuracy. This paper presents Tree++, a truncated tree based graph kernel, which addresses these challenges by leveraging a hierarchical representation of graphs using minimum spanning trees (MSTs). Our approach constructs an MST of each input graph and then recursively builds smaller subtrees by removing nodes from the MST until a certain depth is reached. These subtree representations form the basis of our graph kernel, allowing us to efficiently compute pairwise similarities between large graphs. Extensive experimental evaluations demonstrate the effectiveness of Tree++ on both benchmark datasets and real-world applications, outperforming state-of-the-art methods in terms of efficiency, scalability, and accuracy.""",1
"Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN (Geometric Graph Convolutional Networks), to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs. Code is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.",0
"This paper presents ""Geom-GCN"", a new approach that applies graph convolutional networks (GCN) to geometric data represented as graphs. We present a unified framework incorporating both node features and geometry such as proximity and directionality into GCN. Our model takes advantage of the strengths of current GCN models which already encode local structure information well, while adding the ability to capture nonlinear relationships between nodes based on global geometric properties. Experimentally we demonstrate the superior performance of our proposed method against other state-of-the-art approaches in semi-supervised node classification tasks across various datasets. These results validate the effectiveness of using geometric information along with existing methods based solely on node attributes and edge connections. Our work opens up the possibility of applying deep learning techniques to more general types of data beyond traditional grid-based images by leveraging their underlying structures.",1
"We address the problem of merging graph and feature-space information while learning a metric from structured data. Existing algorithms tackle the problem in an asymmetric way, by either extracting vectorized summaries of the graph structure or adding hard constraints to feature-space algorithms. Following a different path, we define a metric regression scheme where we train metric-constrained linear combinations of dissimilarity matrices. The idea is that the input matrices can be pre-computed dissimilarity measures obtained from any kind of available data (e.g. node attributes or edge structure). As the model inputs are distance measures, we do not need to assume the existence of any underlying feature space. Main challenge is that metric constraints (especially positive-definiteness and sub-additivity), are not automatically respected if, for example, the coefficients of the linear combination are allowed to be negative. Both positive and sub-additive constraints are linear inequalities, but the computational complexity of imposing them scales as O(D3), where D is the size of the input matrices (i.e. the size of the data set). This becomes quickly prohibitive, even when D is relatively small. We propose a new graph-based technique for optimizing under such constraints and show that, in some cases, our approach may reduce the original computational complexity of the optimization process by one order of magnitude. Contrarily to existing methods, our scheme applies to any (possibly non-convex) metric-constrained objective function.",0
"Title: Multiple Metric Learning for Structured Data Abstract: This paper presents a novel approach to learning multiple metrics jointly from structured data using variational autoencoders (VAEs). Traditionally, VAEs have been used to learn one dominant metric through their latent space. However, real world datasets often contain multiple metrics which may overlap. Our proposed method allows us to learn these overlapping metrics while preserving the underlying structure of the original dataset. We show that our model outperforms other state-of-the-art methods on both synthetic and benchmark datasets. The results indicate that our approach provides better clustering performance by capturing more intrinsic features than traditional models. Furthermore, we demonstrate applications of multi-metric learning such as collaborative filtering and image generation. Overall, our work extends the capabilities of unsupervised machine learning and provides new opportunities for exploring complex relationships within large scale datasets. Keywords: Variational Autoencoder (VAE), Multi-Metric Learning, Strucutred Data, Unsupervised Machine Learning",1
"While learning models are typically studied for inputs in the form of a fixed dimensional feature vector, real world data is rarely found in this form. In order to meet the basic requirement of traditional learning models, structural data generally have to be converted into fix-length vectors in a handcrafted manner, which is tedious and may even incur information loss. A common form of structured data is what we term ""semantic tree-structures"", corresponding to data where rich semantic information is encoded in a compositional manner, such as those expressed in JavaScript Object Notation (JSON) and eXtensible Markup Language (XML). For tree-structured data, several learning models have been studied to allow for working directly on raw tree-structure data, However such learning models are limited to either a specific tree-topology or a specific tree-structured data format, e.g., synthetic parse trees. In this paper, we propose a novel framework for end-to-end learning on generic semantic tree-structured data of arbitrary topology and heterogeneous data types, such as data expressed in JSON, XML and so on. Motivated by the works in recursive and recurrent neural networks, we develop exemplar neural implementations of our framework for the JSON format. We evaluate our approach on several UCI benchmark datasets, including ablation and data-efficiency studies, and on a toy reinforcement learning task. Experimental results suggest that our framework yields comparable performance to use of standard models with dedicated feature-vectors in general, and even exceeds baseline performance in cases where compositional nature of the data is particularly important.   The source code for a JSON-based implementation of our framework along with experiments can be downloaded at https://github.com/EndingCredits/json2vec.",0
"In this work, we propose a framework that enables end-to-end learning on semantic tree-structured data. Our approach is motivated by the observation that many real-world datasets have complex hierarchical structures which cannot be easily captured using traditional neural network architectures such as Recurrent Neural Networks (RNNs) or Transformer models.  In our framework, we represent each instance of data as a tree where nodes correspond to different levels of abstraction, from low-level details to high-level concepts. We then use graph convolutional networks (GCNs) to process these trees, allowing us to capture hierarchical relationships within each tree.  We evaluate our method on three benchmark datasets: (i) Treetap, a synthetic dataset designed to test how well our model can learn the structure of tree-structured data; (ii) OpenIE, a widely used benchmark task for open information extraction which involves extracting facts from textual sources; (iii) Semeval Task 8, a task aimed at measuring performance in relation classification. Experimental results demonstrate significant improvements over strong baselines in all tasks. Furthermore, analysis shows that our framework is able to successfully capture the structure present in the data and leverage it to improve performance. Overall, our work highlights the benefits of incorporating structured representations into deep learning models, particularly when dealing with complex and hierarchically organized data.",1
"Paper documents are widely used as an irreplaceable channel of information in many fields, especially in financial industry, fostering a great amount of demand for systems which can convert document images into structured data representations. In this paper, we present a machine learning framework for data ingestion in document images, which processes the images uploaded by users and return fine-grained data in JSON format. Details of model architectures, design strategies, distinctions with existing solutions and lessons learned during development are elaborated. We conduct abundant experiments on both synthetic and real-world data in State Street. The experimental results indicate the effectiveness and efficiency of our methods.",0
"This paper proposes a machine learning framework for data ingestion from document images. The proposed approach involves preprocessing techniques such as image cropping, normalization, and enhancement to improve the quality of the input images. Next, feature extraction methods like Fourier descriptors, Zernike moments, and textural features are used to extract relevant features from the images. These extracted features are then fed into machine learning models like convolutional neural networks (CNNs), support vector machines (SVMs) and random forest algorithms to classify and recognize patterns in the document images. Our experimental results demonstrate that our framework achieves high accuracy and efficiency in detecting and recognizing different types of documents including IDs, checks, receipts, etc., making it suitable for various real-world applications such as automation of financial transactions, customer onboarding processes, and record management systems.",1
"Machine learning models that can exploit the inherent structure in data have gained prominence. In particular, there is a surge in deep learning solutions for graph-structured data, due to its wide-spread applicability in several fields. Graph attention networks (GAT), a recent addition to the broad class of feature learning models in graphs, utilizes the attention mechanism to efficiently learn continuous vector representations for semi-supervised learning problems. In this paper, we perform a detailed analysis of GAT models, and present interesting insights into their behavior. In particular, we show that the models are vulnerable to heterogeneous rogue nodes and hence propose novel regularization strategies to improve the robustness of GAT models. Using benchmark datasets, we demonstrate performance improvements on semi-supervised learning, using the proposed robust variant of GAT.",0
"This paper presents A Regularized Attention Mechanism (RAM) that regularizes attention mechanisms commonly used in Graph Attention Networks (GAT). RAM adds two terms to the standard GAT self-attention function: one that ensures each node attends to different regions in a graph, rather than focusing on similar nodes; another that assigns high values to attended nodes rather than spreading all weights uniformly across them. Experimental results show improvements over prior approaches on several benchmark datasets, including Pubmed, Freebase25K, Reddit, and Diffbot. We also provide analyses showing how RAM influences learned attention patterns. Code and data will be made publicly available upon acceptance. This research proposes a new method called Regularized Attention Mechanism (RAM), which modifies traditional attention mechanisms utilised in Graph Attention Networks (GAT). By adding novel mathematical functions, the authors aimed to enhance the performance of these models by improving their ability to focus on diverse parts of graphs instead of homogenous nodes, as well as increasing the weight assigned to significant nodes rather than distributing them evenly among less important ones. Experiments conducted using four distinct datasets Pubmed, Freebase25K, Reddit, and Diffbot demonstrated improved outcomes compared to preexisting methods. Further analysis was carried out to explore how RAM affects the learning process. These findings may lead to future optimisations of GAN algorithms and related systems. Supplementary materials, including code and data sets, will be available once published.",1
"Despite the phenomenal success of deep neural networks in a broad range of learning tasks, there is a lack of theory to understand the way they work. In particular, Convolutional Neural Networks (CNNs) are known to perform much better than Fully-Connected Networks (FCNs) on spatially structured data: the architectural structure of CNNs benefits from prior knowledge on the features of the data, for instance their translation invariance. The aim of this work is to understand this fact through the lens of dynamics in the loss landscape.   We introduce a method that maps a CNN to its equivalent FCN (denoted as eFCN). Such an embedding enables the comparison of CNN and FCN training dynamics directly in the FCN space. We use this method to test a new training protocol, which consists in training a CNN, embedding it to FCN space at a certain ``relax time'', then resuming the training in FCN space. We observe that for all relax times, the deviation from the CNN subspace is small, and the final performance reached by the eFCN is higher than that reachable by a standard FCN of same architecture. More surprisingly, for some intermediate relax times, the eFCN outperforms the CNN it stemmed, by combining the prior information of the CNN and the expressivity of the FCN in a complementary way. The practical interest of our protocol is limited by the very large size of the highly sparse eFCN. However, it offers interesting insights into the persistence of architectural bias under stochastic gradient dynamics. It shows the existence of some rare basins in the FCN loss landscape associated with very good generalization. These can only be accessed thanks to the CNN prior, which helps navigate the landscape during the early stages of optimization.",0
"In recent years, deep learning has revolutionized the field of computer vision by achieving state-of-the-art performance across a wide range of tasks such as object detection, segmentation, and classification. One key component of many successful deep learning models is convolutional neural networks (CNNs). Despite their popularity, however, CNNs still face significant challenges in terms of computational efficiency and interpretability. As datasets continue to grow larger and more complex, these issues become even more pronounced.  This paper explores how architecture design choices can play a crucial role in addressing some of these limitations while improving overall model performance. We focus specifically on two techniques commonly used in convolutional architectures: spatial pyramid pooling and dilated convolutions. These methods offer several advantages over traditional approaches. For example, they allow the network to capture features at multiple scales without significantly increasing computational cost. They also reduce the impact of boundary effects on feature extraction, which helps improve image quality and accuracy. Finally, these techniques often lead to more efficient representations that require fewer parameters to achieve comparable results. By incorporating these principles into our proposed architecture, we demonstrate that carefully designed convolutional frameworks can indeed find ""needles"" in large haystacks quickly and accurately - ultimately leading to better overall system behavior. While there remain important unanswered questions related to generalization under distribution shift and other factors, architectural innovations like those here promise to keep advancing the cutting edge.",1
"The richness in the content of various information networks such as social networks and communication networks provides the unprecedented potential for learning high-quality expressive representations without external supervision. This paper investigates how to preserve and extract the abundant information from graph-structured data into embedding space in an unsupervised manner. To this end, we propose a novel concept, Graphical Mutual Information (GMI), to measure the correlation between input graphs and high-level hidden representations. GMI generalizes the idea of conventional mutual information computations from vector space to the graph domain where measuring mutual information from two aspects of node features and topological structure is indispensable. GMI exhibits several benefits: First, it is invariant to the isomorphic transformation of input graphs---an inevitable constraint in many existing graph representation learning algorithms; Besides, it can be efficiently estimated and maximized by current mutual information estimation methods such as MINE; Finally, our theoretical analysis confirms its correctness and rationality. With the aid of GMI, we develop an unsupervised learning model trained by maximizing GMI between the input and output of a graph neural encoder. Considerable experiments on transductive as well as inductive node classification and link prediction demonstrate that our method outperforms state-of-the-art unsupervised counterparts, and even sometimes exceeds the performance of supervised ones.",0
"Abstract: In this work, we propose a novel approach to graph representation learning that maximizes mutual information (MI) between node representations and their corresponding class labels. Our method leverages the intuition behind MI as a measure of similarity between two distributions, where high MI implies strong association between them. To achieve this goal, we first map each input graph into a probability distribution over node features using a neural network. We then minimize the negative log likelihood of these feature embeddings with respect to the ground truth labels. By doing so, our model learns representations that capture both structural and semantic information within the data, ultimately leading to improved performance on downstream tasks such as node classification and link prediction. Extensive experimental evaluations demonstrate the effectiveness and robustness of our proposed framework compared against several state-of-the-art baselines.",1
"Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent multi-dimensional, structured data such as images. The simplest solution to solve this computationally hard problem is to decompose it into independent layer-wise subproblems. However, neuroscientific evidence would suggest inter-connecting these subproblems as in the Predictive Coding (PC) theory, which adds top-down connections between consecutive layers. In this study, a new model called 2-Layers Sparse Predictive Coding (2L-SPC) is introduced to assess the impact of this inter-layer feedback connection. In particular, the 2L-SPC is compared with a Hierarchical Lasso (Hi-La) network made out of a sequence of independent Lasso layers. The 2L-SPC and the 2-layers Hi-La networks are trained on 4 different databases and with different sparsity parameters on each layer. First, we show that the overall prediction error generated by 2L-SPC is lower thanks to the feedback mechanism as it transfers prediction error between layers. Second, we demonstrate that the inference stage of the 2L-SPC is faster to converge than for the Hi-La model. Third, we show that the 2L-SPC also accelerates the learning process. Finally, the qualitative analysis of both models dictionaries, supported by their activation probability, show that the 2L-SPC features are more generic and informative.",0
"The study investigates the effectiveness of using artificial neural networks (ANNs) as a modeling tool for hierarchical sparse coding. The authors aim to address some limitations of existing methods by incorporating top-down connections that allow feedback from higher levels of abstraction to influence lower level representations. Their findings show that including top-down connections significantly improves performance on both simulated and real-world datasets compared to traditional bottom-up models. This suggests that integrating these types of constraints can lead to more robust and efficient algorithms for tasks such as object recognition and image compression. Overall, the results highlight the potential of ANNs for representing complex patterns in data while offering insights into how different connection schemes affect their operation.",1
"Graph Neural Networks (GNN) have been shown to work effectively for modeling graph structured data to solve tasks such as node classification, link prediction and graph classification. There has been some recent progress in defining the notion of pooling in graphs whereby the model tries to generate a graph level representation by downsampling and summarizing the information present in the nodes. Existing pooling methods either fail to effectively capture the graph substructure or do not easily scale to large graphs. In this work, we propose ASAP (Adaptive Structure Aware Pooling), a sparse and differentiable pooling method that addresses the limitations of previous graph pooling architectures. ASAP utilizes a novel self-attention network along with a modified GNN formulation to capture the importance of each node in a given graph. It also learns a sparse soft cluster assignment for nodes at each layer to effectively pool the subgraphs to form the pooled graph. Through extensive experiments on multiple datasets and theoretical analysis, we motivate our choice of the components used in ASAP. Our experimental results show that combining existing GNN architectures with ASAP leads to state-of-the-art results on multiple graph classification benchmarks. ASAP has an average improvement of 4%, compared to current sparse hierarchical state-of-the-art method.",0
"In recent years, graph representation learning has become increasingly important in many areas of artificial intelligence, including natural language processing and computer vision. One key challenge in this field is how to effectively pool nodes in a graph to produce representations that capture both local and global structure. This paper presents a new approach called Adaptive Structure Aware Pooling (ASAP) which addresses this problem by combining spectral clustering techniques with attention mechanisms to adaptively identify meaningful substructures within graphs. Our method allows for more effective capturing of hierarchies present in real data while also improving the robustness of node embeddings across datasets. We demonstrate the effectiveness of our approach on several benchmark tasks and show that ASAP outperforms existing methods in terms of accuracy and stability. Overall, our work represents an important step forward in developing powerful yet flexible tools for graph representation learning.",1
"Classifiers built with neural networks handle large-scale high dimensional data, such as facial images from computer vision, extremely well while traditional statistical methods often fail miserably. In this paper, we attempt to understand this empirical success in high dimensional classification by deriving the convergence rates of excess risk. In particular, a teacher-student framework is proposed that assumes the Bayes classifier to be expressed as ReLU neural networks. In this setup, we obtain a sharp rate of convergence, i.e., $\tilde{O}_d(n^{-2/3})$, for classifiers trained using either 0-1 loss or hinge loss. This rate can be further improved to $\tilde{O}_d(n^{-1})$ when the data distribution is separable. Here, $n$ denotes the sample size. An interesting observation is that the data dimension only contributes to the $\log(n)$ term in the above rates. This may provide one theoretical explanation for the empirical successes of deep neural networks in high dimensional classification, particularly for structured data.",0
"Abstract: In recent years, deep neural network classifiers have become very popular due to their ability to learn complex representations from large amounts of data. However, understanding the generalization performance of these models remains challenging. One of the main difficulties lies in quantifying how fast such classifiers converge to the true risk minimizer as more training data becomes available. This work studies the convergence behavior of several widely used deep neural network architectures including ResNet, VGG, and AlexNet on CIFAR-10, SVHN, ImageNet datasets by considering different ways of generating pseudo label based teacher student framework . Our results show that sharp rates of convergence can indeed occur for certain architectures, and we establish bounds on the rate at which this occurs under mild assumptions. Furthermore ,our methodology allows us to identify architectures where this type of convergence is less likely to hold and thus may benefit from additional regularization techniques . Overall ,this study provides insights into the fundamental nature of overfitting versus underfitting and helps to develop more robust deep learning algorithms",1
"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it. Most of the existing approaches to handle such graphs suffer from over-parameterization and are restricted to learning representations of nodes only. In this paper, we propose CompGCN, a novel Graph Convolutional framework which jointly embeds both nodes and relations in a relational graph. CompGCN leverages a variety of entity-relation composition operations from Knowledge Graph Embedding techniques and scales with the number of relations. It also generalizes several of the existing multi-relational GCN methods. We evaluate our proposed method on multiple tasks such as node classification, link prediction, and graph classification, and achieve demonstrably superior results. We make the source code of CompGCN available to foster reproducible research.",0
"Artificial intelligence (AI) has come a long way since its early days as a novelty technology used only by researchers and enthusiasts. Today, AI permeates every aspect of society, from personal devices like smartphones and virtual assistants, to businesses that use machine learning algorithms to automate processes and gain insights into consumer behavior. One area where AI continues to make significant advances is in computer vision, which focuses on teaching machines how to interpret visual data such as images and videos. In particular, graph convolutional networks have emerged as a powerful tool for processing multi-relational graphs, which are ubiquitous in many real-world applications ranging from bioinformatics to social network analysis. These models leverage graph structures and learn nonlinear node representations directly from raw input data, making them well suited for handling complex interdependencies among variables.  In recent years, several variations of graph convolutional networks have been proposed, each addressing specific shortcomings of their predecessors. For instance, some methods rely heavily on strong supervision signals during training while others utilize graph pooling layers that can cause information loss and reduce accuracy. Meanwhile, other approaches require careful preprocessing of graph inputs, limiting their applicability to new datasets without extensive tuning. Despite these advancements, there remains a need for more flexible and efficient solutions capable of tackling diverse tasks across multiple domains. To this end, we introduce composition-based multi-relational graph convolutional networks (MGNNs), which integrate graph composition operations within standard neural network architectures to achieve improved performance while maintaining simplicity and scalability. Our experiments demonstrate that MGNNs outperform state-of-the-art baselines on several benchmark datasets, offering evidence of their effectiveness in handling complex relationships among nodes. As a result, our work represents a significant step forward in the field o",1
"We propose a novel spectral convolutional neural network (CNN) model on graph structured data, namely Distributed Feedback-Looped Networks (DFNets). This model is incorporated with a robust class of spectral graph filters, called feedback-looped filters, to provide better localization on vertices, while still attaining fast convergence and linear memory requirements. Theoretically, feedback-looped filters can guarantee convergence w.r.t. a specified error bound, and be applied universally to any graph without knowing its structure. Furthermore, the propagation rule of this model can diversify features from the preceding layers to produce strong gradient flows. We have evaluated our model using two benchmark tasks: semi-supervised document classification on citation networks and semi-supervised entity classification on a knowledge graph. The experimental results show that our model considerably outperforms the state-of-the-art methods in both benchmark tasks over all datasets.",0
"Deep learning approaches have become very popular for processing graph data due to their ability to effectively encode structural information. In particular, convolutional neural networks (CNN) have been successfully applied to graphs and have achieved state-of-the-art performance on many tasks. However, traditional spectral CNNs suffer from limited capacity and slow convergence due to using fixed filters. Recently, adaptive filtering methods have been proposed as a solution, but they can still struggle with capturing high frequency features. In this paper, we introduce DFNets, which combine deep feature normalization with feedback-looped spectral filters to address these limitations. Our approach significantly improves both model capacity and computational efficiency by allowing each filter to learn its own spectrum based on local neighborhood information. We show that DFNet outperforms existing methods across several benchmark datasets, demonstrating its effectiveness for processing complex graph structures.",1
"We present Spectral Inference Networks, a framework for learning eigenfunctions of linear operators by stochastic optimization. Spectral Inference Networks generalize Slow Feature Analysis to generic symmetric operators, and are closely related to Variational Monte Carlo methods from computational physics. As such, they can be a powerful tool for unsupervised representation learning from video or graph-structured data. We cast training Spectral Inference Networks as a bilevel optimization problem, which allows for online learning of multiple eigenfunctions. We show results of training Spectral Inference Networks on problems in quantum mechanics and feature learning for videos on synthetic datasets. Our results demonstrate that Spectral Inference Networks accurately recover eigenfunctions of linear operators and can discover interpretable representations from video in a fully unsupervised manner.",0
"Spectral inference networks (SINs) bring together deep learning and spectral graph theory techniques by encoding both types of knowledge within a single architecture. By simultaneously combining these two approaches, SINs can achieve state-of-the-art performance on challenging machine learning tasks while offering several advantages over previous methods. Specifically, we show that SINs have better stability properties than traditional deep models under certain adversarial attacks and outperform them in terms of generalization performance. We illustrate our results through extensive numerical experiments, demonstrating SINs high effectiveness and strong robustness in practice. Overall, our work represents a significant step forward in understanding how different forms of representation  discrete graphs versus continuous functions  interact during learning. This research lays the foundation for further exploration into unified frameworks between these approaches.",1
"Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations.   We propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are $10,000$ times smaller, while at the same time achieving the state-of-the-art predictive performance.",0
"One potential abstract that meets your criteria could read as follows: ""This paper presents a novel method for training latent variable models on structured data using discriminative embeddings. These models have proven to be powerful tools for analyzing complex datasets by finding hidden relationships and patterns within them. However, existing methods struggle with capturing all aspects of real-world problems due to their restrictive assumptions and limitations. In contrast, our approach uses discriminative embeddings to train these models in a more flexible manner while still maintaining their interpretability. Experimental results demonstrate significant improvements over state-of-the-art algorithms across multiple tasks."" This abstract effectively summarizes the content of the paper without including the title or any specific jargon that might confuse readers who lack specialized knowledge of machine learning and computer science topics.",1
"Semantic segmentation with deep learning has achieved great progress in classifying the pixels in the image. However, the local location information is usually ignored in the high-level feature extraction by the deep learning, which is important for image semantic segmentation. To avoid this problem, we propose a graph model initialized by a fully convolutional network (FCN) named Graph-FCN for image semantic segmentation. Firstly, the image grid data is extended to graph structure data by a convolutional network, which transforms the semantic segmentation problem into a graph node classification problem. Then we apply graph convolutional network to solve this graph node classification problem. As far as we know, it is the first time that we apply the graph convolutional network in image semantic segmentation. Our method achieves competitive performance in mean intersection over union (mIOU) on the VOC dataset(about 1.34% improvement), compared to the original FCN model.",0
"Heres your new prompt: Write an abstract (150-300 words) summarizing the content of the paper Graph-FCN for Image Semantic Segmentation Here we go again! Our team has developed a novel approach called Graph-FCN that significantly improves the accuracy and speed of image semantic segmentation. Our method uses graph convolutional neural networks (GCNNs) to model spatial relationships between pixels, which allows us to handle complex scenes with irregular boundaries. In addition, we use feature pyramid network (FPN) techniques to fuse features from different resolutions, resulting in a more comprehensive representation of each pixel. We evaluate our method on several benchmark datasets and demonstrate superior performance compared to state-of-the-art methods. Overall, Graph-FCN represents an important step forward in advancing the field of image semantic segmentation. Would you like me to add citations/references? I could either just give numbers in square brackets [x] next to relevant sentences; or alternatively make up some fake names for authors and journals where those papers would likely appear given their domain area if real ones existed  such as Zhang et al [2022][IEEE Transactions on Neural Networks and Learning Systems]. Or perhaps theres another format altogether that might work better here? If so let me know and I can adjust my response accordingly.",1
"Graph Neural Networks (GNNs), which generalize deep neural networks to graph-structured data, have drawn considerable attention and achieved state-of-the-art performance in numerous graph related tasks. However, existing GNN models mainly focus on designing graph convolution operations. The graph pooling (or downsampling) operations, that play an important role in learning hierarchical representations, are usually overlooked. In this paper, we propose a novel graph pooling operator, called Hierarchical Graph Pooling with Structure Learning (HGP-SL), which can be integrated into various graph neural network architectures. HGP-SL incorporates graph pooling and structure learning into a unified module to generate hierarchical representations of graphs. More specifically, the graph pooling operation adaptively selects a subset of nodes to form an induced subgraph for the subsequent layers. To preserve the integrity of graph's topological information, we further introduce a structure learning mechanism to learn a refined graph structure for the pooled graph at each layer. By combining HGP-SL operator with graph neural networks, we perform graph level representation learning with focus on graph classification task. Experimental results on six widely used benchmarks demonstrate the effectiveness of our proposed model.",0
"""This paper presents a new approach"" etc. Also don't start by saying In this paper we describe etc."" Here is my attempt:",1
"Graph neural networks (GNNs) are an emerging model for learning graph embeddings and making predictions on graph structured data. However, robustness of graph neural networks is not yet well-understood. In this work, we focus on node structural identity predictions, where a representative GNN model is able to achieve near-perfect accuracy. We also show that the same GNN model is not robust to addition of structural noise, through a controlled dataset and set of experiments. Finally, we show that under the right conditions, graph-augmented training is capable of significantly improving robustness to structural noise.",0
"Here you go!  Graph neural networks (GNNs) have emerged as powerful tools for processing graph data, achieving state-of-the-art performance on several tasks such as node classification and link prediction. However, despite their impressive results, GNNs remain vulnerable to structural noise, which can significantly impair their performance. In this work, we aim to investigate how robust GNNs are to different types of structural noise present in real-world graphs. We evaluate the effectiveness of three popular benchmarking datasets under various levels of synthetic noise generated using four common noise strategies. Our experiments reveal that GNNs exhibit varying degrees of resilience across datasets and noise conditions. In particular, we find that increasing connectivity density increases noise tolerance overall but decreases the impact of certain types of noise, while other properties such as degree distribution play less significant roles. These insights provide valuable guidance for practitioners seeking to apply GNNs in noisy environments and inspire future research into developing more robust models. By shedding light on the relationship between noise resistance and dataset characteristics, our study contributes new perspectives on improving the stability and reliability of GNNs.",1
"In the last decade, many diverse advances have occurred in the field of information extraction from data. Information extraction in its simplest form takes place in computing environments, where structured data can be extracted through a series of queries. The continuous expansion of quantities of data have therefore provided an opportunity for knowledge extraction (KE) from a textual document (TD). A typical problem of this kind is the extraction of common characteristics and knowledge from a group of TDs, with the possibility to group such similar TDs in a process known as clustering. In this paper we present a technique for such KE among a group of TDs related to the common characteristics and meaning of their content. Our technique is based on the Spearman's Rank Correlation Coefficient (SRCC), for which the conducted experiments have proven to be comprehensive measure to achieve a high-quality KE.",0
"This article presents a methodology for measuring similarity in textual data using Spearman's rank correlation coefficient. With advances in natural language processing and machine learning algorithms, there has been growing interest in analyzing large amounts of unstructured data such as documents, articles, social media posts, etc., to extract meaningful insights from them. In many cases, the analysis involves comparing two sets of texts, where one set may represent a baseline dataset, another set represents the current topic under investigation, or multiple datasets are compared against each other. To perform effective comparisons and draw conclusions based on these analyses, having a reliable measure of similarity between textual datasets becomes critical. Existing measures of similarity used in literature have limitations that make them ill-suited for measuring similarity in textual data. We demonstrate how to use Spearman's rank correlation coefficient as a measure of similarity in textual data by first converting the datasets into numerical vectors and then applying the rank correlation coefficient. Our experiments show that this method outperforms existing methods in terms of accuracy and robustness.",1
"The (variational) graph auto-encoder and its variants have been popularly used for representation learning on graph-structured data. While the encoder is often a powerful graph convolutional network, the decoder reconstructs the graph structure by only considering two nodes at a time, thus ignoring possible interactions among edges. On the other hand, structured prediction, which considers the whole graph simultaneously, is computationally expensive. In this paper, we utilize the well-known triadic closure property which is exhibited in many real-world networks. We propose the triad decoder, which considers and predicts the three edges involved in a local triad together. The triad decoder can be readily used in any graph-based auto-encoder. In particular, we incorporate this to the (variational) graph auto-encoder. Experiments on link prediction, node clustering and graph generation show that the use of triads leads to more accurate prediction, clustering and better preservation of the graph characteristics.",0
"In recent years, graph autoencoders (AEs) have emerged as powerful tools for representation learning on graphs. One key challenge faced by these models is decoding; generating novel graph structures that retain the characteristics learned during training. Existing approaches often struggle to balance reconstruction fidelity with structural coherence, resulting in either overly faithful but uninteresting copies of the input graph or overly general approximations lacking important features. This work proposes a novel triadic closure mechanism that explicitly encourages the formation of connected triplets among generated nodes, leading to more meaningful and informative outputs. We evaluate our approach on several benchmark datasets and show consistent improvements compared to state-of-the-art methods across multiple metrics. Our results demonstrate the effectiveness of incorporating higher-order structure into graph AE decoding, paving the way for more advanced applications in network science and artificial intelligence.",1
"Global pooling, such as max- or sum-pooling, is one of the key ingredients in deep neural networks used for processing images, texts, graphs and other types of structured data. Based on the recent DeepSets architecture proposed by Zaheer et al. (NIPS 2017), we introduce a Set Aggregation Network (SAN) as an alternative global pooling layer. In contrast to typical pooling operators, SAN allows to embed a given set of features to a vector representation of arbitrary size. We show that by adjusting the size of embedding, SAN is capable of preserving the whole information from the input. In experiments, we demonstrate that replacing global pooling layer by SAN leads to the improvement of classification accuracy. Moreover, it is less prone to overfitting and can be used as a regularizer.",0
"Title: ""Set Aggregation Networks: Adaptive Fusion through Task-Specific Pooling"" Authors: Xiaodong He, Zhe Cao, Jianping Shi  Abstract: In recent years, deep learning has achieved remarkable successes across multiple domains, including computer vision, natural language processing, and speech recognition. However, designing effective convolutional neural networks (CNNs) remains challenging due to the complex interplay among network architecture, data preprocessing, optimization, and evaluation metrics. This paper introduces Set Aggregation Networks (SAN), which provide a flexible framework that generalizes popular pooling operations in CNNs as task-specific adaptive fusion. SAN models learn a trainable weight matrix to encode channel relationships into each feature map. We show that the learned weights effectively capture spatial correlations and yield superior performance over standard methods on various benchmark datasets. Furthermore, our experiments demonstrate that SAN can significantly improve the tradeoff between computational efficiency and representation capacity by adjusting model complexity during training. Our findings highlight the benefits of enabling fine-grained control over the information aggregation process within CNNs, providing valuable insights towards more effective designs of these ubiquitous models. Overall, we believe this work offers promising opportunities for advancing deep learning research in diverse areas.",1
"The current paper is a study in Recurrent Neural Networks (RNN), motivated by the lack of examples simple enough so that they can be thoroughly understood theoretically, but complex enough to be realistic. We constructed an example of structured data, motivated by problems from image-to-text conversion (OCR), which requires long-term memory to decode. Our data is a simple writing system, encoding characters 'X' and 'O' as their upper halves, which is possible due to symmetry of the two characters. The characters can be connected, as in some languages using cursive, such as Arabic (abjad). The string 'XOOXXO' may be encoded as '${\vee}{\wedge}\kern-1.5pt{\wedge}{\vee}\kern-1.5pt{\vee}{\wedge}$'. It follows that we may need to know arbitrarily long past to decode a current character, thus requiring long-term memory. Subsequently we constructed an RNN capable of decoding sequences encoded in this manner. Rather than by training, we constructed our RNN ""by inspection"", i.e. we guessed its weights. This involved a sequence of steps. We wrote a conventional program which decodes the sequences as the example above. Subsequently, we interpreted the program as a neural network (the only example of this kind known to us). Finally, we generalized this neural network to discover a new RNN architecture whose instance is our handcrafted RNN. It turns out to be a 3 layer network, where the middle layer is capable of performing simple logical inferences; thus the name ""deductron"". It is demonstrated that it is possible to train our network by simulated annealing. Also, known variants of stochastic gradient descent (SGD) methods are shown to work.",0
"This paper presents the design and implementation of ""Deductron,"" a recurrent neural network (RNN) designed for solving logical reasoning problems. RNNs have shown promise in many natural language processing tasks, but their applicability to symbolic domains like logic has been less studied. We demonstrate that Deductron can solve challenging reasoning puzzles by leveraging both sequential memory and deep learning principles. Our work explores novel techniques for integrating deduction rules into RNN architectures, using data augmentation strategies tailored specifically to logical inference problems. Experiments show that our approach outperforms competitive baselines across multiple benchmark datasets, indicating the effectiveness of combining deductive reasoning with end-to-end machine learning models.",1
"Adverse drug-drug interactions (DDIs) remain a leading cause of morbidity and mortality. Identifying potential DDIs during the drug design process is critical for patients and society. Although several computational models have been proposed for DDI prediction, there are still limitations: (1) specialized design of drug representation for DDI predictions is lacking; (2) predictions are based on limited labelled data and do not generalize well to unseen drugs or DDIs; and (3) models are characterized by a large number of parameters, thus are hard to interpret. In this work, we develop a ChemicAl SubstrucTurE Representation (CASTER) framework that predicts DDIs given chemical structures of drugs.CASTER aims to mitigate these limitations via (1) a sequential pattern mining module rooted in the DDI mechanism to efficiently characterize functional sub-structures of drugs; (2) an auto-encoding module that leverages both labelled and unlabelled chemical structure data to improve predictive accuracy and generalizability; and (3) a dictionary learning module that explains the prediction via a small set of coefficients which measure the relevance of each input sub-structures to the DDI outcome. We evaluated CASTER on two real-world DDI datasets and showed that it performed better than state-of-the-art baselines and provided interpretable predictions.",0
"Caster (CAtchment SEarch Tool) can predict potential drug interactions by identifying overlapping chemical substructures present within different molecules. While current drug interaction prediction methods rely heavily on similarity searching techniques based on fingerprints and pharmacophores, our approach utilizes chemical substructure representation as the primary search key. We demonstrate the effectiveness of this method through rigorous testing against standard benchmark datasets, achieving better performance than existing state-of-the-art systems. Additionally, we discuss future directions for expanding the scope of Caster beyond just drug interactions, including the possibility of using this tool to aid in drug discovery and design processes. Overall, Caster represents a significant advancement in the field of computational chemistry and has the potential to make important contributions towards improving human health outcomes.",1
"Healthcare data continues to flourish yet a relatively small portion, mostly structured, is being utilized effectively for predicting clinical outcomes. The rich subjective information available in unstructured clinical notes can possibly facilitate higher discrimination but tends to be under-utilized in mortality prediction. This work attempts to assess the gain in performance when multiple notes that have been minimally preprocessed are used as an input for prediction. A hierarchical architecture consisting of both convolutional and recurrent layers is used to concurrently model the different notes compiled in an individual hospital stay. This approach is evaluated on predicting in-hospital mortality on the MIMIC-III dataset. On comparison to approaches utilizing structured data, it achieved higher metrics despite requiring less cleaning and preprocessing. This demonstrates the potential of unstructured data in enhancing mortality prediction and signifies the need to incorporate more raw unstructured data into current clinical prediction methods.",0
"Medical care providers often rely on laboratory results and structured medical records to make diagnostic and treatment decisions. However, these sources may not provide sufficient information to predict patient outcomes. Clinical notes can offer valuable insights into patients health status, but current methods lack sufficient precision to accurately estimate patient survival times based solely on their content. This study aimed to develop machine learning algorithms capable of analyzing unstructured textual data from electronic health records (EHRs) to assess patient morbidity and mortality risk. We utilized a large dataset containing EHR data linked to patient demographics and death certificates, which allowed us to train models using deep learning techniques such as recurrent neural networks (RNNs). By fine-tuning pre-trained language models like GPT-2, our model achieved state-of-the-art performance in predictive accuracy while exhibiting high interpretability. Ultimately, the proposed framework could aid clinicians in identifying at-risk individuals earlier and improving the effectiveness of personalized medicine efforts. Furthermore, given its generalizability across various populations, the approach shows promise for becoming a key component in future precision health initiatives.",1
"The scattering transform is a multilayered wavelet-based deep learning architecture that acts as a model of convolutional neural networks. Recently, several works have introduced generalizations of the scattering transform for non-Euclidean settings such as graphs. Our work builds upon these constructions by introducing windowed and non-windowed graph scattering transforms based upon a very general class of asymmetric wavelets. We show that these asymmetric graph scattering transforms have many of the same theoretical guarantees as their symmetric counterparts. This work helps bridge the gap between scattering and other graph neural networks by introducing a large family of networks with provable stability and invariance guarantees. This lays the groundwork for future deep learning architectures for graph-structured data that have learned filters and also provably have desirable theoretical properties.",0
"Graph neural networks (GNNs) have recently gained popularity due to their ability to model complex graph data structures such as social networks, citation graphs, protein interaction graphs, etc., in which each node can represent one entity and each edge represents some form of relation between those entities. However, traditional GNN architectures suffer from limitations related to oversmoothing, lack of robustness, and difficulty in training deep models. In this paper, we introduce asymmetric geometric scattering transforms (AGST), which allow us to design new GNN architectures that overcome these challenges by incorporating advanced mathematical tools from representation theory, harmonic analysis, and computer graphics. AGST allows for more efficient parameterization of graph convolution kernels and enables powerful regularizers to control network capacity while reducing overfitting. Our experimental results on benchmark datasets demonstrate significant improvement compared to state-of-the-art methods in terms of prediction accuracy, stability, interpretability, and scalability. This work paves the way towards a better understanding of GNNs and their use in real-world applications involving large-scale graph structured data.",1
"GAN is a deep-learning based generative approach to generate contents such as images, languages and speeches. Recently, studies have shown that GAN can also be applied to generative adversarial attack examples to fool the machine-learning models. In comparison with the previous non-learning adversarial example attack approaches, the GAN-based adversarial attack example approach can generate the adversarial samples quickly using the GAN architecture every time facing a new sample after training, but meanwhile needs to perturb the attack samples in great quantities, which results in the unpractical application in reality. To address this issue, we propose a new approach, named Few-Feature-Attack-GAN (FFA-GAN). FFA-GAN has a significant time-consuming advantage than the non-learning adversarial samples approaches and a better non-zero-features performance than the GANbased adversarial sample approaches. FFA-GAN can automatically generate the attack samples in the black-box attack through the GAN architecture instead of the evolutional algorithms or the other non-learning approaches. Besides, we introduce the mask mechanism into the generator network of the GAN architecture to optimize the constraint issue, which can also be regarded as the sparsity problem of the important features. During the training, the different weights of losses of the generator are set in the different training phases to ensure the divergence of the two above mentioned parallel networks of the generator. Experiments are made respectively on the structured data sets KDD-Cup 1999 and CIC-IDS 2017, in which the dimensions of the data are relatively low, and also on the unstructured data sets MNIST and CIFAR-10 with the data of the relatively high dimensions. The results of the experiments demonstrate the effectiveness and the robustness of our proposed approach.",0
"In general, we consider few features attacks to generate adversarial examples that can fool machine learning models. We focus on developing such attacks based on generative adversarial networks (GANs). Specifically, we propose attack approaches utilizing mask functions to control regions where different objectives contribute during training. To improve model robustness under our attacks, we offer three types of defenses: retraining the model using transformed data; enhancing models' discriminators; and adding regularization terms. For all these defense methods, we provide insights into why they work based on analysis of the generated adversarial examples, as well as detailed evaluations across multiple datasets and tasks demonstrating their effectiveness compared to state-of-the-art methods. Our contributions lead to improved resilience for machine learning models against realistically occurring adversaries, making them more suitable for widespread deployment. Finally, our approach provides further evidence towards understanding the limits of existing techniques designed to defend against adversarial perturbation attacks and highlights challenges remaining in designing effective mitigation strategies.",1
"Human action recognition from skeleton data, fueled by the Graph Convolutional Network (GCN), has attracted lots of attention, due to its powerful capability of modeling non-Euclidean structure data. However, many existing GCN methods provide a pre-defined graph and fix it through the entire network, which can loss implicit joint correlations. Besides, the mainstream spectral GCN is approximated by one-order hop, thus higher-order connections are not well involved. Therefore, huge efforts are required to explore a better GCN architecture. To address these problems, we turn to Neural Architecture Search (NAS) and propose the first automatically designed GCN for skeleton-based action recognition. Specifically, we enrich the search space by providing multiple dynamic graph modules after fully exploring the spatial-temporal correlations between nodes. Besides, we introduce multiple-hop modules and expect to break the limitation of representational capacity caused by one-order approximation. Moreover, a sampling- and memory-efficient evolution strategy is proposed to search an optimal architecture for this task. The resulted architecture proves the effectiveness of the higher-order approximation and the dynamic graph modeling mechanism with temporal interactions, which is barely discussed before. To evaluate the performance of the searched model, we conduct extensive experiments on two very large scaled datasets and the results show that our model gets the state-of-the-art results.",0
"In this work we propose learning graph convolutional networks (GCNs) on top of skeleton data representations for human action recognition. Our approach involves applying GCN operations directly onto edge-weighted graphs constructed from skeleton sequences. We use randomized search over possible hyperparameter settings to learn optimal models, which achieves state-of-the-art accuracy among methods that do not require additional input modalities like RGB video frames. Importantly, our approach is efficient enough for realtime operation and can run at 64 frames per second using consumer hardware. By contrast, many recent state-of-the-art methods rely upon expensive computation involving larger datasets or more powerful GPUs. Thus, our results highlight the effectiveness of jointly designing learning architectures and optimizing their parameters through automated means across a wide range of problem domains. We plan to explore transferring knowledge gained here into other fields where complex relational reasoning abilities may prove advantageous, including natural language processing or even game playing agents.  -- Would you recommend reading the paper associated with the given abstract? -- I think so! It looks interesting and well thought out. What kind of tasks or problems are you interested in?",1
"Machine learning on graph structured data has attracted much research interest due to its ubiquity in real world data. However, how to efficiently represent graph data in a general way is still an open problem. Traditional methods use handcraft graph features in a tabular form but suffer from the defects of domain expertise requirement and information loss. Graph representation learning overcomes these defects by automatically learning the continuous representations from graph structures, but they require abundant training labels, which are often hard to fulfill for graph-level prediction problems. In this work, we demonstrate that, if available, the domain expertise used for designing handcraft graph features can improve the graph-level representation learning when training labels are scarce. Specifically, we proposed a multi-task knowledge distillation method. By incorporating network-theory-based graph metrics as auxiliary tasks, we show on both synthetic and real datasets that the proposed multi-task learning method can improve the prediction performance of the original learning task, especially when the training data size is small.",0
"Learning graph representations has gained increasing attention due to their wide range of applications across many domains such as computer vision, natural language processing, bioinformatics and finance. Recent advances have shown that graphs can encode complex structures which capture important patterns present in the data beyond traditional node features. In this work we focus on multi-scale graph learning and propose two novel methods using knowledge distillation from pre-trained models. Our first method utilizes self-supervised pre-training, while our second method uses semi-supervised fine-tuning using multi-source annotations. We showcase the effectiveness of our approach by applying them to four benchmark datasets. Our results demonstrate that both proposed methods yield consistent performance improvement over several baselines. These improvements open up new possibilities for graph representation learning in the broader community.",1
"Deep generative priors offer powerful models for complex-structured data, such as images, audio, and text. Using these priors in inverse problems typically requires estimating the input and/or hidden signals in a multi-layer deep neural network from observation of its output. While these approaches have been successful in practice, rigorous performance analysis is complicated by the non-convex nature of the underlying optimization problems. This paper presents a novel algorithm, Multi-Layer Vector Approximate Message Passing (ML-VAMP), for inference in multi-layer stochastic neural networks. ML-VAMP can be configured to compute maximum a priori (MAP) or approximate minimum mean-squared error (MMSE) estimates for these networks. We show that the performance of ML-VAMP can be exactly predicted in a certain high-dimensional random limit. Furthermore, under certain conditions, ML-VAMP yields estimates that achieve the minimum (i.e., Bayes-optimal) MSE as predicted by the replica method. In this way, ML-VAMP provides a computationally efficient method for multi-layer inference with an exact performance characterization and testable conditions for optimality in the large-system limit.",0
"Abstract: This paper presents a novel approach to inference in high dimensions using deep generative priors. We demonstrate that by leveraging recent advances in deep learning, we can improve upon traditional methods such as Markov Chain Monte Carlo (MCMC) simulation. Our method uses a neural network to learn a deep generative model of the data distribution which can then be used to perform Bayesian inference. By doing so, we achieve better sampling efficiency and faster convergence compared to MCMC simulation. Furthermore, our method allows for automatic regularization through the use of a sparsity prior on the weights of the neural network. Our results show that our method outperforms state-of-the-art MCMC techniques across a variety of challenging problems in high dimensions.",1
"Some recent studies have suggested using GANs for numeric data generation such as to generate data for completing the imbalanced numeric data. Considering the significant difference between the dimensions of the numeric data and images, as well as the strong correlations between features of numeric data, the conventional GANs normally face an overfitting problem, consequently leads to an ill-conditioning problem in generating numeric and structured data. This paper studies the constrained network structures between generator G and discriminator D in WGAN, designs several structures including isomorphic, mirror and self-symmetric structures. We evaluates the performances of the constrained WGANs in data augmentations, taking the non-constrained GANs and WGANs as the baselines. Experiments prove the constrained structures have been improved in 17/20 groups of experiments. In twenty experiments on four UCI Machine Learning Repository datasets, Australian Credit Approval data, German Credit data, Pima Indians Diabetes data and SPECT heart data facing five conventional classifiers. Especially, Isomorphic WGAN is the best in 15/20 experiments. Finally, we theoretically proves that the effectiveness of constrained structures by the directed graphic model (DGM) analysis.",0
"This study investigates the effectiveness of using constrained network structures for training generative adversarial networks (WGANs) on numeric data generation tasks. We compare performance metrics such as FID scores across different network architectures, including those with and without explicit regularization terms. We found that while some constraints improved stability during optimization, others led to overfitting and poorer results overall. Our findings contribute new insights into the design space of WGAN architecture search and suggest novel directions for future work towards developing more effective GAN models capable of generating high quality numeric datasets. The full paper provides details on our experimental methodology and additional analysis.",1
"We introduce an improved unsupervised clustering protocol specially suited for large-scale structured data. The protocol follows three steps: a dimensionality reduction of the data, a density estimation over the low dimensional representation of the data, and a final segmentation of the density landscape. For the dimensionality reduction step we introduce a parallelized implementation of the well-known t-Stochastic Neighbouring Embedding (t-SNE) algorithm that significantly alleviates some inherent limitations, while improving its suitability for large datasets. We also introduce a new adaptive Kernel Density Estimation particularly coupled with the t-SNE framework in order to get accurate density estimates out of the embedded data, and a variant of the rainfalling watershed algorithm to identify clusters within the density landscape. The whole mapping protocol is wrapped in the bigMap R package, together with visualization and analysis tools to ease the qualitative and quantitative assessment of the clustering.",0
"Title: Large Scale Visualization Techniques Using Big Data Clustering Algorithms and t-SNE Embedding  Abstract: Big data visualization has become increasingly important as large datasets continue to grow and require efficient analysis techniques. In order to effectively analyze these complex datasets, researchers often utilize clustering algorithms that group similar elements together based on shared characteristics. Additionally, dimensionality reduction methods such as UMAP or t-Distributed Stochastic Neighbor Embedding (t-SNE) can be used to map high-dimensional data onto lower dimensions while retaining relevant features. However, traditional mapping techniques may struggle to scale up to larger datasets due to their reliance on serial computing. To address this issue, we propose the use of parallel computing to accelerate large scale data mapping using the popular technique of t-SNE embedding. Our approach relies on distributing the computation across multiple cores and GPUs in parallel, allowing for faster processing times without compromising accuracy. We evaluate our method using several real world datasets from diverse domains, including image classification, biology, neuroscience, text analysis and recommender systems, demonstrating significant speed ups and scalability improvements. By leveraging advanced computational resources, our framework enables interactive exploration of massive datasets through intuitive scatter plots, providing new insights into the underlying structure of the data. Overall, our work offers a valuable tool for practitioners working with big data and sets a foundation for future advancements in parallelized data mapping.",1
"We propose and analyze a method for semi-supervised learning from partially-labeled network-structured data. Our approach is based on a graph signal recovery interpretation under a clustering hypothesis that labels of data points belonging to the same well-connected subset (cluster) are similar valued. This lends naturally to learning the labels by total variation (TV) minimization, which we solve by applying a recently proposed primal-dual method for non-smooth convex optimization. The resulting algorithm allows for a highly scalable implementation using message passing over the underlying empirical graph, which renders the algorithm suitable for big data applications. By applying tools of compressed sensing, we derive a sufficient condition on the underlying network structure such that TV minimization recovers clusters in the empirical graph of the data. In particular, we show that the proposed primal-dual method amounts to maximizing network flows over the empirical graph of the dataset. Moreover, the learning accuracy of the proposed algorithm is linked to the set of network flows between data points having known labels. The effectiveness and scalability of our approach is verified by numerical experiments.",0
"This could describe several different topics, so I would need some more specific prompts on what kind of data you have in mind (images? texts?) and which particular aspect(s) of semi-supervised learning you want to cover. Are there any other requirements/conventions for the abstract I should be aware of beyond length? Would you like me to write one in APA style using keywords from your prompt as ""keyword sets""? Or should I assume this will be published at arXiv where informal language might be more appropriate? Also, should I discuss related work in more detail than usual for such a brief summary, since it could very well fit within this word range if done concisely enough? For example, what about including concrete applications of SSL relevant to network-structured data analysis that can give the reader context and motivate why they may care about our proposed methods? Please provide additional guidance, thank you!",1
"We consider a family of problems that are concerned about making predictions for the majority of unlabeled, graph-structured data samples based on a small proportion of labeled samples. Relational information among the data samples, often encoded in the graph/network structure, is shown to be helpful for these semi-supervised learning tasks. However, conventional graph-based regularization methods and recent graph neural networks do not fully leverage the interrelations between the features, the graph, and the labels. In this work, we propose a flexible generative framework for graph-based semi-supervised learning, which approaches the joint distribution of the node features, labels, and the graph structure. Borrowing insights from random graph models in network science literature, this joint distribution can be instantiated using various distribution families. For the inference of missing labels, we exploit recent advances of scalable variational inference techniques to approximate the Bayesian posterior. We conduct thorough experiments on benchmark datasets for graph-based semi-supervised learning. Results show that the proposed methods outperform the state-of-the-art models in most settings.",0
"This abstract presents our new graph-based semi-supervised learning framework that uses flexibly defined graphs to model complex relationships among data points. Our method builds on prior work by introducing multiple flexible layers into the graph construction process. This allows us to capture different types of relationships between data points, such as low-dimensional embeddings, high-dimensional representations, or even semantic features extracted from text. By leveraging these multi-layered connections, we can improve performance across a variety of tasks, including image classification, natural language processing, and other machine learning domains. We demonstrate the effectiveness of our approach through extensive experiments and ablation studies. Overall, our framework provides a powerful tool for tackling challenging problems in semisupervised learning where standard methods fall short.",1
"In recent years there has been a rapid increase in classification methods on graph structured data. Both in graph kernels and graph neural networks, one of the implicit assumptions of successful state-of-the-art models was that incorporating graph isomorphism features into the architecture leads to better empirical performance. However, as we discover in this work, commonly used data sets for graph classification have repeating instances which cause the problem of isomorphism bias, i.e. artificially increasing the accuracy of the models by memorizing target information from the training set. This prevents fair competition of the algorithms and raises a question of the validity of the obtained results. We analyze 54 data sets, previously extensively used for graph-related tasks, on the existence of isomorphism bias, give a set of recommendations to machine learning practitioners to properly set up their models, and open source new data sets for the future experiments.",0
"Title: Understanding Isomorphism Bias in Graph Data Sets ---------------------------------------------------------- In recent years, graph data sets have become increasingly popular due to their ability to represent complex relationships between objects. However, working with such data can lead to issues related to isomorphism bias - a phenomenon where similar structures are treated as equivalent even if they arise from different sources. This paper seeks to deepen our understanding of isomorphism bias by examining its causes, consequences, and potential solutions. We begin by analyzing how graph representations can obscure underlying structure and introduce opportunities for bias. Next, we present case studies that illustrate the impacts of isomorphism bias on downstream applications ranging from social network analysis to scientific discovery. Finally, we discuss strategies for mitigating isomorphism bias through techniques like normalization, contextualization, and visualization. By exploring these concepts and providing concrete examples, we aim to support researchers in making informed choices when working with graph data sets.",1
"Graph Neural Networks (GNNs) have become a topic of intense research recently due to their powerful capability in high-dimensional classification and regression tasks for graph-structured data. However, as GNNs typically define the graph convolution by the orthonormal basis for the graph Laplacian, they suffer from high computational cost when the graph size is large. This paper introduces Haar basis which is a sparse and localized orthonormal system for a coarse-grained chain on graph. The graph convolution under Haar basis, called Haar convolution, can be defined accordingly for GNNs. The sparsity and locality of the Haar basis allow Fast Haar Transforms (FHTs) on graph, by which a fast evaluation of Haar convolution between graph data and filters can be achieved. We conduct experiments on GNNs equipped with Haar convolution, which demonstrates state-of-the-art results on graph-based regression and node classification tasks.",0
"This paper presents an efficient method for applying Haar transforms on graphs using Graph Neural Networks (GNNs). Haar transforms are mathematical operations that allow for quick feature extraction from data, but have been traditionally limited to grid-based representations like images. GNNs enable learning on graph-structured data by iteratively updating node features based on their neighborhood information. By combining these two concepts, we can effectively perform high-quality feature extraction on complex, non-grid data represented as graphs. We demonstrate the effectiveness of our approach through extensive experiments, comparing its performance against state-of-the-art baselines across multiple tasks and datasets. Our results show significant improvements over existing methods, validating the utility of fast Haar transformations for graph neural networks. Overall, our work expands the range of applicability of the Haar transform beyond traditional image processing, providing a powerful tool for understanding and manipulating complex graph structures.",1
"Graph structured data are abundant in the real world. Among different graph types, directed acyclic graphs (DAGs) are of particular interest to machine learning researchers, as many machine learning models are realized as computations on DAGs, including neural networks and Bayesian networks. In this paper, we study deep generative models for DAGs, and propose a novel DAG variational autoencoder (D-VAE). To encode DAGs into the latent space, we leverage graph neural networks. We propose an asynchronous message passing scheme that allows encoding the computations on DAGs, rather than using existing simultaneous message passing schemes to encode local graph structures. We demonstrate the effectiveness of our proposed DVAE through two tasks: neural architecture search and Bayesian network structure learning. Experiments show that our model not only generates novel and valid DAGs, but also produces a smooth latent space that facilitates searching for DAGs with better performance through Bayesian optimization.",0
"This is the first version of my AI Language model:  D-VAE: A Variational Autoencoder for Directed Acyclic Graphs. In this paper, we present a novel method for representing and learning representations of directed acyclic graphs (DAGs) using variational autoencoders (VAEs). Our approach, called Directed VAE (or D-VAE), addresses some key limitations of traditional VAEs on graph structured data by exploiting the directionality inherent in many real world graphs. We show that our model can learn meaningful and interpretable representations through qualitative and quantitative evaluation on several benchmark datasets including citation networks, social networks, and protein interaction graphs. Additionally, we demonstrate how these learned representations can be used for node classification tasks, achieving state-of-the-art results on several benchmark datasets. Overall, our work represents an important step towards enabling effective machine learning techniques on complex, relational domains such as bioinformatics and social network analysis.  Here is a revised abstract:  This paper presents a new deep learning framework called Directed VAE (D-VAE) tailored to represent and analyze directed acyclic graphs (DAGs). DAGs are ubiquitous across diverse fields but remain challenging due to their intricate topology, making them difficult to capture effectively. Our proposed method leverages variational autoencoders (VAEs) to embed the structural characteristics into low-dimensional latent spaces while respecting their intrinsic directivity. Extensive experiments validate the effectiveness and efficiency of our algorithm on four benchmark datasets from different application areas (citation networks, social networks, and protein interactions). The learned features greatly improve task performance over baseline models on all datasets tested herein. Therefore, our work extends existing research frontiers and helps facilitate applications in complex domains ranging from biology to computer science via scalable yet accurate analytic tools.",1
"Learning from graph-structured data is an important task in machine learning and artificial intelligence, for which Graph Neural Networks (GNNs) have shown great promise. Motivated by recent advances in geometric representation learning, we propose a novel GNN architecture for learning representations on Riemannian manifolds with differentiable exponential and logarithmic maps. We develop a scalable algorithm for modeling the structural properties of graphs, comparing Euclidean and hyperbolic geometry. In our experiments, we show that hyperbolic GNNs can lead to substantial improvements on various benchmark datasets.",0
"Graph neural networks (GNNs) have gained increasing attention due to their effectiveness in processing graph structured data such as social network graphs, knowledge graphs, chemistry molecular structures, and so on. Meanwhile, hyperbolic geometry has recently been applied to GNNs as a more expressive embedding space than Euclidean spaces. In this paper, we propose Hyperbolic Graph Neural Networks (HGNNs), which leverage the power of both hyperbolic geometry and deep learning techniques. We present two variants: HGNN-I that operates directly on vertex attributes under a global coordinate system, and HGNN-II that models local neighborhood relationships within the Poincar ball model. Both variants achieve state-of-the-art performance across three benchmark datasets covering node classification and link prediction tasks. Extensive analysis demonstrates that hyperbolicity indeed helps capture nonlinearity in complex relationship dependencies among nodes. Our work showscase new possibilities brought by hyperbolic geometry in developing advanced machine intelligence systems for graph analytics.",1
"Development of metrics for structural data-generating mechanisms is fundamental in machine learning and the related fields. In this paper, we give a general framework to construct metrics on random nonlinear dynamical systems, defined with the Perron-Frobenius operators in vector-valued reproducing kernel Hilbert spaces (vvRKHSs). We employ vvRKHSs to design mathematically manageable metrics and also to introduce operator-valued kernels, which enables us to handle randomness in systems. Our metric provides an extension of the existing metrics for deterministic systems, and gives a specification of the kernel maximal mean discrepancy of random processes. Moreover, by considering the time-wise independence of random processes, we clarify a connection between our metric and the independence criteria with kernels such as Hilbert-Schmidt independence criteria. We empirically illustrate our metric with synthetic data, and evaluate it in the context of the independence test for random processes. We also evaluate the performance with real time seris datas via clusering tasks.",0
"In the study of random dynamical systems, metric considerations play a crucial role due to their ability to characterize various properties such as stability, sensitivity, mixing, ergodicity, etc. These metrics can be expressed through reproducing kernels which allow one to associate infinite dimensional spaces of functions with feature maps obtained from sampled data. This work focuses on vector valued reproducing kernel Hilbert spaces (RKHS) and the development of appropriate metrics that account for both the randomness in the system dynamics and the multi-dimensional nature of the state variables. We propose two new classes of metrics: The first is based on a novel regularized estimate of the Gram matrix while the second employs distances between RKHS embeddings of the distribution of initial conditions. By leveraging these metrics we aim at providing insights into nonlinear stochastic evolutions governed by high-dimensional ordinary differential equations (ODEs). Furthermore, our analysis highlights interesting connections between Lyapunov exponents, decay rates of correlations, and spectral norms of operators acting on tensor product spaces. Our findings showcase how these metrics yield valuable perspectives into complex dynamic systems under uncertainty. As a consequence, they hold great potential to impact numerous applications where real world measurements contend with intrinsic variability. Ultimately, this research represents a step forward towards developing principled frameworks for studying and designing robust controllers within challenging uncertain contexts.",1
"Convolutional neural networks are nowadays witnessing a major success in different pattern recognition problems. These learning models were basically designed to handle vectorial data such as images but their extension to non-vectorial and semi-structured data (namely graphs with variable sizes, topology, etc.) remains a major challenge, though a few interesting solutions are currently emerging. In this paper, we introduce MLGCN; a novel spectral Multi-Laplacian Graph Convolutional Network. The main contribution of this method resides in a new design principle that learns graph-laplacians as convex combinations of other elementary laplacians each one dedicated to a particular topology of the input graphs. We also introduce a novel pooling operator, on graphs, that proceeds in two steps: context-dependent node expansion is achieved, followed by a global average pooling; the strength of this two-step process resides in its ability to preserve the discrimination power of nodes while achieving permutation invariance. Experiments conducted on SBU and UCF-101 datasets, show the validity of our method for the challenging task of action recognition.",0
This would need more information than I have been trained on to write an appropriate abstract as you requested. Perhaps you can provide me the research article so that I may analyze and summarize it?,1
"Data samples collected for training machine learning models are typically assumed to be independent and identically distributed (iid). Recent research has demonstrated that this assumption can be problematic as it simplifies the manifold of structured data. This has motivated different research areas such as data poisoning, model improvement, and explanation of machine learning models. In this work, we study the influence of a sample on determining the intrinsic topological features of its underlying manifold. We propose the Shapley Homology framework, which provides a quantitative metric for the influence of a sample of the homology of a simplicial complex. By interpreting the influence as a probability measure, we further define an entropy which reflects the complexity of the data manifold. Our empirical studies show that when using the 0-dimensional homology, on neighboring graphs, samples with higher influence scores have more impact on the accuracy of neural networks for determining the graph connectivity and on several regular grammars whose higher entropy values imply more difficulty in being learned.",0
"This abstract presents a novel approach for analyzing the influence of input samples on the predictions made by neural networks. The proposed method leverages homology theory and is based on a new topological descriptor called ""Shapley Homology,"" which captures how each sample contributes to the final output. Our experimental results demonstrate that Shapley Homology can effectively identify key features responsible for shaping model predictions, thus providing insights into their robustness and interpretability. Overall, our work represents a significant advancement towards understanding black box models and holds promising applications in fields such as computer vision and natural language processing.",1
"Efforts are underway to study ways via which the power of deep neural networks can be extended to non-standard data types such as structured data (e.g., graphs) or manifold-valued data (e.g., unit vectors or special matrices). Often, sizable empirical improvements are possible when the geometry of such data spaces are incorporated into the design of the model, architecture, and the algorithms. Motivated by neuroimaging applications, we study formulations where the data are {\em sequential manifold-valued measurements}. This case is common in brain imaging, where the samples correspond to symmetric positive definite matrices or orientation distribution functions. Instead of a recurrent model which poses computational/technical issues, and inspired by recent results showing the viability of dilated convolutional models for sequence prediction, we develop a dilated convolutional neural network architecture for this task. On the technical side, we show how the modules needed in our network can be derived while explicitly taking the Riemannian manifold structure into account. We show how the operations needed can leverage known results for calculating the weighted Fr\'{e}chet Mean (wFM). Finally, we present scientific results for group difference analysis in Alzheimer's disease (AD) where the groups are derived using AD pathology load: here the model finds several brain fiber bundles that are related to AD even when the subjects are all still cognitively healthy.",0
"Recently, convolutional neural networks (CNN) have emerged as powerful models for processing sequential data such as speech signals or video frames. However, most previous studies on CNN architectures assume that each time step produces only one scalar output, whereas many applications generate multi-dimensional outputs at every stage. This paper presents a novel architecture called dilated convolutional neural network (DCNN), which effectively captures spatial dependencies in manifold-valued sequences by utilizing dilated convolution kernels. Our DCNN model applies multiple dilations with different rates at every layer, allowing for flexible capture of temporal dynamics. Furthermore, we introduce several strategies to mitigate overfitting due to increased model complexity. We evaluate our method using three challenging datasets: synthetic trajectories from randomly moving agents, American Sign Language videos, and speech sounds from TIMIT Acoustic-Phonetics Continuous Speech Database. Extensive experiments demonstrate remarkable improvement compared to baseline methods. In addition, our DCNN outperforms other recent deep learning approaches on all three tasks, proving its effectiveness in handling high-dimensional sequential data. Our code can be found online to facilitate future research.",1
"In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data. Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs. This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to state-of-the-art models.",0
"Graphs have been widely studied from a static perspective, but more recently continuous graphs have gained interest as they can capture evolution over time. In fact, many real world systems naturally fall into this framework: communication networks evolve due to node additions/removals; social interactions form dynamic graphs as friendships change over time; biological neurons change their connections throughout life. Continuous graph signals lie at the core of capturing these dynamics, where traditional signal processing methods may fail since classical Fourier analysis no longer applies directly on them. We propose a new framework that generalizes graph filtering techniques to the continuous case by focusing on analyzing the flow instead of the signal itself. By leveraging ideas rooted in both graph theory and geometric mechanics we formalize a notion of differentiability adapted to such nonlinear structures. This enables us to develop a novel set of filter equations encompassing static graph counterparts like the popular Chebyshev and Butterworth filters, while achieving better performance than classical Euler discretization schemes. Applications to problems related to consensus and opinion formation, spread of diseases, and data assimilation validate our approach across several domains. Finally, we analyze convergence rates under specific conditions for first order flows (Lie groups). Overall, we provide a versatile toolkit applicable in numerous fields dealing with continuous graph signals, paving the road towards addressing otherwise intractable problems.",1
"Processing an input signal that contains arbitrary structures, e.g., superpixels and point clouds, remains a big challenge in computer vision. Linear diffusion, an effective model for image processing, has been recently integrated with deep learning algorithms. In this paper, we propose to learn pairwise relations among data points in a global fashion to improve semantic segmentation with arbitrarily-structured data, through spatial generalized propagation networks (SGPN). The network propagates information on a group of graphs, which represent the arbitrarily-structured data, through a learned, linear diffusion process. The module is flexible to be embedded and jointly trained with many types of networks, e.g., CNNs. We experiment with semantic segmentation networks, where we use our propagation module to jointly train on different data -- images, superpixels and point clouds. We show that SGPN consistently improves the performance of both pixel and point cloud segmentation, compared to networks that do not contain this module. Our method suggests an effective way to model the global pairwise relations for arbitrarily-structured data.",0
"This paper presents a new approach to deep learning called ""Learning Propagation for Arbitrarily-Structured Data"" (LPASD). LPASD is designed to handle complex datasets that may have multiple levels of structure, such as images with text annotations or multi-table relational databases.  Traditional machine learning methods often struggle with handling data from different modalities and at different scales, leading to lower performance than desired. In contrast, LPASD uses a two-step process where the model first learns to generate synthetic training examples before fine-tuning on real data. This allows the model to learn more quickly and achieve better results, even when dealing with incomplete or noisy data.  The effectiveness of LPASD is demonstrated through experiments on several benchmark datasets across diverse domains, including computer vision, natural language processing, and knowledge representation. Results show significant improvement over state-of-the-art methods, demonstrating the promise of LPASD for tackling challenging problems involving arbitrarily-structured data.  Overall, LPASD offers a powerful tool for addressing many real-world applications that involve complex data structures, paving the way for further advances in artificial intelligence and related fields.",1
"Metadata are general characteristics of the data in a well-curated and condensed format, and have been proven to be useful for decision making, knowledge discovery, and also heterogeneous data organization of biobank. Among all data types in the biobank, pathology is the key component of the biobank and also serves as the gold standard of diagnosis. To maximize the utility of biobank and allow the rapid progress of biomedical science, it is essential to organize the data with well-populated pathology metadata. However, manual annotation of such information is tedious and time-consuming. In the study, we develop a multimodal multitask learning framework to predict four major slide-level metadata of pathology images. The framework learns generalizable representations across tissue slides, pathology reports, and case-level structured data. We demonstrate improved performance across all four tasks with the proposed method compared to a single modal single task baseline on two test sets, one external test set from a distinct data source (TCGA) and one internal held-out test set (TTH). In the test sets, the performance improvements on the averaged area under receiver operating characteristic curve across the four tasks are 16.48% and 9.05% on TCGA and TTH, respectively. Such pathology metadata prediction system may be adopted to mitigate the effort of expert annotation and ultimately accelerate the data-driven research by better utilization of the pathology biobank.",0
"In recent years, pathological image analysis has become increasingly important due to its potential to enhance diagnostic accuracy and clinical decision making. However, accurately annotating these images requires significant time and effort from human experts, which can limit their availability and lead to inconsistencies in annotation quality. To address this challenge, we propose a novel approach that combines multimodal representation learning with multi-task prediction to automatically predict biobank metadata for histopathological whole slide images (WSIs). Our method leverages multiple modalities such as image patches, embedding features, and textual descriptions of tissue samples obtained from different sources, including clinical records and digital slides. By utilizing multitask learning, our model learns to jointly predict several distinct tasks related to the biobank metadata, allowing us to leverage valuable side information during training. We demonstrate that our proposed framework significantly outperforms state-of-the-art methods on two public datasets, achieving high levels of accuracy across multiple tasks. Overall, our work represents an important step towards automating WSI annotations by developing robust representations capable of capturing critical relationships among diverse data modalities.",1
"Machine learning and deep learning have gained popularity and achieved immense success in Drug discovery in recent decades. Historically, machine learning and deep learning models were trained on either structural data or chemical properties by separated model. In this study, we proposed an architecture training simultaneously both type of data in order to improve the overall performance. Given the molecular structure in the form of SMILES notation and their label, we generated the SMILES-based feature matrix and molecular descriptors. These data were trained on a deep learning model which was also integrated with the Attention mechanism to facilitate training and interpreting. Experiments showed that our model could raise the performance of prediction comparing to the reference. With the maximum MCC 0.58 and AUC 90% by cross-validation on EGFR inhibitors dataset, our architecture was outperforming the referring model. We also successfully integrated Attention mechanism into our model, which helped to interpret the contribution of chemical structures on bioactivity.",0
"This paper presents a deep learning architecture that utilizes multiple input types to predict biological activity, specifically targeting the prediction of epidermal growth factor receptor (EGFR) inhibitor activities. Our proposed approach leverages attention mechanisms to weigh the importance of different inputs and improve model performance on imbalanced datasets. To evaluate our method, we conducted experiments on two benchmark datasets and compared our results against several state-of-the-art models. Our experimental results show that our attention-based multi-input deep learning architecture outperforms existing approaches, demonstrating superior accuracy and robustness. Furthermore, our analysis suggests that incorporating diverse sources of data can enhance biological activity predictions. Overall, these findings hold significant implications for drug discovery research, where accurate predictions of pharmacological properties could greatly benefit from more advanced machine learning methods.",1
"Graph Convolutional Neural Networks (GCNNs) are generalizations of CNNs to graph-structured data, in which convolution is guided by the graph topology. In many cases where graphs are unavailable, existing methods manually construct graphs or learn task-driven adaptive graphs. In this paper, we propose Graph Learning Neural Networks (GLNNs), which exploit the optimization of graphs (the adjacency matrix in particular) from both data and tasks. Leveraging on spectral graph theory, we propose the objective of graph learning from a sparsity constraint, properties of a valid adjacency matrix as well as a graph Laplacian regularizer via maximum a posteriori estimation. The optimization objective is then integrated into the loss function of the GCNN, which adapts the graph topology to not only labels of a specific task but also the input data. Experimental results show that our proposed GLNN outperforms state-of-the-art approaches over widely adopted social network datasets and citation network datasets for semi-supervised classification.",0
"In recent years there has been significant interest in developing machine learning models that can effectively handle large amounts of unlabelled data while still achieving high levels of accuracy on supervised tasks. One approach that has gained popularity is semi-supervised learning (SSL), which leverages both labeled and unlabeled data to improve model performance. However, traditional SSL methods often assume a fixed graph structure, which may not capture complex relationships between nodes in the network. To address this limitation, we propose a novel method called structure-adaptive graph learning (SAGL) for robust SSL. Our algorithm utilizes an adaptive graph regularization term to update the edge weights in real-time during training. By doing so, SAGL is able to learn more accurate and informative connections among nodes, resulting in improved model performance on several benchmark datasets. We evaluate our method against state-of-the art baselines and show that SAGL consistently outperforms them across a wide range of settings. Overall, our results demonstrate the effectiveness of adaptive graphs in SSL and highlight the potential applications of SAGL in domains such as computer vision and natural language processing where handling large amounts of unlabeled data is critical.",1
"Graph neural networks (GNN) has been successfully applied to operate on the graph-structured data. Given a specific scenario, rich human expertise and tremendous laborious trials are usually required to identify a suitable GNN architecture. It is because the performance of a GNN architecture is significantly affected by the choice of graph convolution components, such as aggregate function and hidden dimension. Neural architecture search (NAS) has shown its potential in discovering effective deep architectures for learning tasks in image and language modeling. However, existing NAS algorithms cannot be directly applied to the GNN search problem. First, the search space of GNN is different from the ones in existing NAS work. Second, the representation learning capacity of GNN architecture changes obviously with slight architecture modifications. It affects the search efficiency of traditional search methods. Third, widely used techniques in NAS such as parameter sharing might become unstable in GNN.   To bridge the gap, we propose the automated graph neural networks (AGNN) framework, which aims to find an optimal GNN architecture within a predefined search space. A reinforcement learning based controller is designed to greedily validate architectures via small steps. AGNN has a novel parameter sharing strategy that enables homogeneous architectures to share parameters, based on a carefully-designed homogeneity definition. Experiments on real-world benchmark datasets demonstrate that the GNN architecture identified by AGNN achieves the best performance, comparing with existing handcrafted models and tradistional search methods.",0
"Effectively leveraging graph neural networks (GNN) requires identifying appropriate architectures that can accurately capture complex relationships within the given data. Recent advances have shown the feasibility of automating this process via neural architecture search (NAS). In this work, we propose Auto-GNN - a NAS algorithm specifically designed for GNNs. We build upon existing gradient-based optimization methods while introducing novel regularization techniques that maintain stability during training and prevent overfitting. Our approach significantly outperforms existing handcrafted GNN models across multiple real-world benchmark datasets, demonstrating the effectiveness of our framework in discovering high-quality, automatic GNN architectures. By providing a general method for uncovering optimal GNN architectures from raw input graphs, Auto-GNN enables more efficient use of computational resources and drives further progress in areas reliant on these technologies such as computer vision, natural language processing, and knowledge representation.",1
The paper discusses a pooling mechanism to induce subsampling in graph structured data and introduces it as a component of a graph convolutional neural network. The pooling mechanism builds on the Non-Negative Matrix Factorization (NMF) of a matrix representing node adjacency and node similarity as adaptively obtained through the vertices embedding learned by the model. Such mechanism is applied to obtain an incrementally coarser graph where nodes are adaptively pooled into communities based on the outcomes of the non-negative factorization. The empirical analysis on graph classification benchmarks shows how such coarsening process yields significant improvements in the predictive performance of the model with respect to its non-pooled counterpart.,0
"In recent years, graph convolutional neural networks (GCNNs) have emerged as powerful models for learning on graph data structures such as social network graphs and biological knowledge graphs. However, these models often struggle with overfitting due to their deep architecture and limited capacity. To tackle this issue, many techniques have been proposed to simplify the structure of the input graph before feeding it into GCNNs. One popular technique is called ""node pooling"", which involves aggregating nodes from different partitions of the graph into mini-batches to reduce computation costs while preserving important features.  This work proposes a novel non-negative factorization approach to node pooling in GCNNs. By leveraging the latent structure of the graph represented by decompositions such as the Singular Value Decomposition (SVD), our method can efficiently capture both local and global dependencies within the network. We show that our algorithm outperforms other state-of-the-art pooling methods on several benchmark datasets across multiple metrics including accuracy and clustering coefficient recovery. Further, we demonstrate how our approach can improve robustness against adversarial attacks by applying our method to real world large scale social network data sets. Our results indicate that incorporating the SVD decomposition can provide significant improvements to performance under certain types of attack where edge modification is the primary mechanism used to poison or manipulate edges within the graph. Overall, this study provides insights into new ways to leverage matrix decomposition based algorithms in the development of more resilient machine learning systems for graph structured data",1
"Learning graph-structured data with graph neural networks (GNNs) has been recently emerging as an important field because of its wide applicability in bioinformatics, chemoinformatics, social network analysis and data mining. Recent GNN algorithms are based on neural message passing, which enables GNNs to integrate local structures and node features recursively. However, past GNN algorithms based on 1-hop neighborhood neural message passing are exposed to a risk of loss of information on local structures and relationships. In this paper, we propose Neighborhood Edge AggregatoR (NEAR), a novel framework that aggregates relations between the nodes in the neighborhood via edges. NEAR, which can be orthogonally combined with previous GNN algorithms, gives integrated information that describes which nodes in the neighborhood are connected. Therefore, GNNs combined with NEAR reflect each node's local structure beyond the nodes themselves. Experimental results on multiple graph classification tasks show that our algorithm achieves state-of-the-art results.",0
"Here we describe a model that uses graph convolutional neural networks (GCNNs) on graphs where the vertices are labeled as either positive or negative examples. Our approach, called ""NEAR,"" builds upon previous work by training two separate models on neighborhood aggregates generated from the original graph using different edge weight functions. This allows us to capture both local features present near each vertex, as well as global properties like connectedness and clustering coefficients. We demonstrate the effectiveness of our method on several benchmark datasets, achieving state-of-the-art results across many tasks including node classification, link prediction, and anomaly detection. Further analysis shows that the predictions made by our algorithm can often reveal important structural insights into the underlying networks themselves. All code used in experiments is open source and available online. Overall, these results suggest that NEAR represents a powerful new tool for working with complex network data, with potential applications in fields ranging from social science to engineering to computer security.",1
"Existing deep learning models may encounter great challenges in handling graph structured data. In this paper, we introduce a new deep learning model for graph data specifically, namely the deep loopy neural network. Significantly different from the previous deep models, inside the deep loopy neural network, there exist a large number of loops created by the extensive connections among nodes in the input graph data, which makes model learning an infeasible task. To resolve such a problem, in this paper, we will introduce a new learning algorithm for the deep loopy neural network specifically. Instead of learning the model variables based on the original model, in the proposed learning algorithm, errors will be back-propagated through the edges in a group of extracted spanning trees. Extensive numerical experiments have been done on several real-world graph datasets, and the experimental results demonstrate the effectiveness of both the proposed model and the learning algorithm in handling graph data.",0
"This paper proposes the Deep Loopy Neural Network (DLNN) model which aims at improving graph structured data representation learning by incorporating loop structures into neural networks. The main contribution of DLNN lies in three aspects: i) a deep neural network framework that is able to capture multiple loops within graphs; ii) two types of loop structures implemented with different activation functions to enforce non-linearity; iii) a novel training method called loop backpropagation where each layer takes responsibility only for one full loop at most, ensuring both efficiency and accuracy. Experiments on node classification demonstrate that our proposed model outperforms state-of-the-art models significantly across datasets from four domains, including social media, citation networks, biology and protein structures. Meanwhile, case studies verify our analysis of model interpretability and validity through visualizing the learned representations via t-SNE and UMAP techniques respectively. Our findings suggest DLNN as a promising solution to boost the performance of graph structured data representation learning.",1
"Non-linear kernel methods can be approximated by fast linear ones using suitable explicit feature maps allowing their application to large scale problems. We investigate how convolution kernels for structured data are composed from base kernels and construct corresponding feature maps. On this basis we propose exact and approximative feature maps for widely used graph kernels based on the kernel trick. We analyze for which kernels and graph properties computation by explicit feature maps is feasible and actually more efficient. In particular, we derive approximative, explicit feature maps for state-of-the-art kernels supporting real-valued attributes including the GraphHopper and graph invariant kernels. In extensive experiments we show that our approaches often achieve a classification accuracy close to the exact methods based on the kernel trick, but require only a fraction of their running time. Moreover, we propose and analyze algorithms for computing random walk, shortest-path and subgraph matching kernels by explicit and implicit feature maps. Our theoretical results are confirmed experimentally by observing a phase transition when comparing running time with respect to label diversity, walk lengths and subgraph size, respectively.",0
"In recent years, graph kernels have emerged as powerful tools for analyzing structured data such as graphs and trees. These kernels can capture complex relationships between objects by representing them as graphs and computing similarities between their topological structures. However, existing graph kernel methods suffer from limitations in terms of scalability and interpretability due to their reliance on explicit feature maps that extract handcrafted features from input graphs. On the other extreme, implicit feature maps have been shown to achieve better performance but lack interpretability because they learn features directly from raw inputs without any human intervention.  In this work, we propose a unified framework that bridges the gap between explicit and implicit feature maps of graph kernels. Our approach relies on meta learning which enables us to train models that can learn to generalize across different types of tasks and datasets. We demonstrate how our method improves upon state-of-the-art results on several benchmark datasets while providing interpretable insights into the characteristics of input graphs. Further analysis shows that our method learns high quality features that closely align with human intuition, making it applicable to real world applications where interpretation and explainability play crucial roles. Overall, our study provides new perspectives on designing more effective graph kernels with improved performance and transparency.",1
"Many machine learning models can be attacked with adversarial examples, i.e. inputs close to correctly classified examples that are classified incorrectly. However, most research on adversarial attacks to date is limited to vectorial data, in particular image data. In this contribution, we extend the field by introducing adversarial edit attacks for tree-structured data with potential applications in medicine and automated program analysis. Our approach solely relies on the tree edit distance and a logarithmic number of black-box queries to the attacked classifier without any need for gradient information. We evaluate our approach on two programming and two biomedical data sets and show that many established tree classifiers, like tree-kernel-SVMs and recursive neural networks, can be attacked effectively.",0
"Abstract: In many applications including natural language processing and computer vision, tree structures serve as a compact representation of data that can facilitate efficient computation. However, these trees are vulnerable to adversarial attacks where slight modifications can lead to incorrect outputs without being detectable by humans or existing defenses such as integral robustness (e.g., distance bounds) checks. We propose two novel methods termed NaryAttack and MultiTargetAttack to generate such adversarial edit attacks on trees. For example, we show that using NaryAttack one can modify up to half of the edges in a dependency parse tree such that a pretrained model produces any target string among all possible dependencies while maintaining high semantic similarity scores over unmodified substructures. Motivated by human feedback analysis and our evaluations against models trained via reinforcement learning from human preferences, we further show that most adversarial edits remain imperceptible even after multiple rounds of inspection aimed at detecting them; thus, they pose serious threats to applications relying on black-box prediction quality measures. Our work highlights the limitations of current metrics used in practice and encourages future research toward more comprehensive evaluation of machine learning systems based on structured representations.",1
"Recent works reveal that network embedding techniques enable many machine learning models to handle diverse downstream tasks on graph structured data. However, as previous methods usually focus on learning embeddings for a single network, they can not learn representations transferable on multiple networks. Hence, it is important to design a network embedding algorithm that supports downstream model transferring on different networks, known as domain adaptation. In this paper, we propose a novel Domain Adaptive Network Embedding framework, which applies graph convolutional network to learn transferable embeddings. In DANE, nodes from multiple networks are encoded to vectors via a shared set of learnable parameters so that the vectors share an aligned embedding space. The distribution of embeddings on different networks are further aligned by adversarial learning regularization. In addition, DANE's advantage in learning transferable network embedding can be guaranteed theoretically. Extensive experiments reflect that the proposed framework outperforms other state-of-the-art network embedding baselines in cross-network domain adaptation tasks.",0
"In recent years, domain adaptation has emerged as a key challenge in natural language processing (NLP), particularly in tasks such as sentiment analysis, text classification, machine translation, and question answering that involve different domains or contexts. In order to address this problem, we propose a novel method called Domain Adaptive Network Embedding (DANE) which can effectively learn domain-specific representations by aligning distributions across multiple sources. Our approach first extracts sentence embeddings using pre-trained models like BERT, RoBERTa, or ALBERT on source data, then adapts them to target domains through adversarial training with a discriminator network. We evaluate our model against several strong baselines on several benchmark datasets from diverse NLP tasks and show consistent improvements over existing methods. Our results demonstrate the effectiveness of DANE at solving complex domain adaptation problems while maintaining high performance.",1
Kernels for structured data are commonly obtained by decomposing objects into their parts and adding up the similarities between all pairs of parts measured by a base kernel. Assignment kernels are based on an optimal bijection between the parts and have proven to be an effective alternative to the established convolution kernels. We explore how the base kernel can be learned as part of the classification problem. We build on the theory of valid assignment kernels derived from hierarchies defined on the parts. We show that the weights of this hierarchy can be optimized via multiple kernel learning. We apply this result to learn vertex similarities for the Weisfeiler-Lehman optimal assignment kernel for graph classification. We present first experimental results which demonstrate the feasibility and effectiveness of the approach.,0
"In recent years, graph kernel methods have become increasingly popular due to their ability to capture structural information in graphs through various mathematical techniques. Among these methods, the Weisfeiler-Lehman (WL) test has been shown to be particularly effective in characterizing graphs, especially those that exhibit some form of nonlinearity or hierarchy. However, there remains a challenge in designing assignment kernels based on WL tests that can properly capture complex graph structures while remaining computationally tractable.  This work addresses this issue by introducing deep Weisfeiler-Lehman assignment kernels (dWLAKs), which are defined as linear combinations of multiple kernels learned from different levels of the WL hierarchy. By leveraging techniques from machine learning, we demonstrate how multiple kernel learning can effectively select appropriate features at each level of the hierarchy and integrate them into a unified representation that captures complex graph patterns. Furthermore, we provide theoretical analysis of our proposed dWLAK methodology, showing that it admits desirable properties such as positive definiteness and symmetry.  We evaluate our approach using extensive experiments across various benchmark datasets, demonstrating significant improvements over state-of-the-art graph kernel methods, both in terms of classification accuracy and computational efficiency. Our results highlight the effectiveness of combining deep representations derived from the WL hierarchy with multiple kernel learning in tackling challenging graph mining tasks.  Overall, this study represents an important step toward developing more powerful graph kernel methods capable of handling complex real-world data sets. The development of efficient yet expressive graph descriptors holds great potential in many fields, including computer vision, bioinformatics, social network analysis, and natural language processing. With further advancements along these lines, we expect to see even more exciting applications emerge i",1
"The recent proliferation of publicly available graph-structured data has sparked an interest in machine learning algorithms for graph data. Since most traditional machine learning algorithms assume data to be tabular, embedding algorithms for mapping graph data to real-valued vector spaces has become an active area of research. Existing graph embedding approaches are based purely on structural information and ignore any semantic information from the underlying domain. In this paper, we demonstrate that semantic information can play a useful role in computing graph embeddings. Specifically, we present a framework for devising embedding strategies aware of domain-specific interpretations of graph nodes and edges, and use knowledge of downstream machine learning tasks to identify relevant graph substructures. Using two real-life domains, we show that our framework yields embeddings that are simple to implement and yet achieve equal or greater accuracy in machine learning tasks compared to domain independent approaches.",0
"Graph node embeddings have become increasingly important in recent years due to their ability to capture structural information from large networks such as social media platforms, online communities, and biological systems. In particular, domain-aware random walks (RWs) have proven to be effective at generating high quality embeddings that captures both local and global properties of graphs while preserving their intrinsic structure. However, one major challenge with existing methods is ensuring fairness across different domains by taking into account contextual biases present within each graph. This work presents a novel methodology called Domain-Aware Biased RW (DBRW), which addresses these limitations and offers improved performance compared to state-of-the-art baselines. Our proposed approach achieves this goal through a combination of targeted random walk exploration and efficient bias estimation techniques. We evaluate our method on several benchmark datasets and demonstrate its effectiveness in terms of accuracy and robustness under varying conditions. Overall, DBRW provides researchers with a powerful toolkit for generating high quality graph node embeddings with enhanced domain awareness.",1
"In this paper we propose Structuring AutoEncoders (SAE). SAEs are neural networks which learn a low dimensional representation of data which are additionally enriched with a desired structure in this low dimensional space. While traditional Autoencoders have proven to structure data naturally they fail to discover semantic structure that is hard to recognize in the raw data. The SAE solves the problem by enhancing a traditional Autoencoder using weak supervision to form a structured latent space. In the experiments we demonstrate, that the structured latent space allows for a much more efficient data representation for further tasks such as classification for sparsely labeled data, an efficient choice of data to label, and morphing between classes. To demonstrate the general applicability of our method, we show experiments on the benchmark image datasets MNIST, Fashion-MNIST, DeepFashion2 and on a dataset of 3D human shapes.",0
"Autoencoders have proven to be effective tools in deep learning applications such as image compression and generation, dimensionality reduction, and anomaly detection. However, their performance can often be improved by structuring them appropriately. In this paper, we discuss several approaches to structuring autoencoders, including variational autoencoders (VAEs) and adversarially trained autoencoders (AAEs). We explore how these structures impact the performance of autoencoders on different tasks and evaluate their effectiveness compared to unstructured autoencoders. Our results show that structured autoencoders can significantly improve model performance, particularly in tasks involving generating high quality images and recovering lost data. Additionally, we highlight some limitations of current structured autoencoder models and provide suggestions for future research directions in this area. Overall, our work demonstrates the potential benefits of using structured autoencoders for deep learning applications and contributes to the growing body of literature on this topic.",1
"Architectures for sparse hierarchical representation learning have recently been proposed for graph-structured data, but so far assume the absence of edge features in the graph. We close this gap and propose a method to pool graphs with edge features, inspired by the hierarchical nature of chemistry. In particular, we introduce two types of pooling layers compatible with an edge-feature graph-convolutional architecture and investigate their performance for molecules relevant to drug discovery on a set of two classification and two regression benchmark datasets of MoleculeNet. We find that our models significantly outperform previous benchmarks on three of the datasets and reach state-of-the-art results on the fourth benchmark, with pooling improving performance for three out of four tasks, keeping performance stable on the fourth task, and generally speeding up the training process.",0
"In recent years, machine learning has been applied successfully to solve problems in fields such as image recognition and natural language processing. However, these models often require large amounts of data and computational resources which can make them difficult to use in applications where such resources may be limited, such as drug discovery. Molecular graph theory offers the potential for efficient and accurate modeling of complex chemical structures but traditional machine learning approaches struggle with their intrinsically high dimensionality and sparsity. This work addresses this challenge by proposing a new method based on sparse hierarchical representation (SHR) of molecules in combination with convolutional neural networks (CNNs). By leveraging both global and local structural features through a hierarchy of subgraphs we can achieve state-of-the art performance on several benchmark datasets while requiring substantially less training data than other methods. Our results demonstrate the feasibility of applying deep learning techniques to molecular design tasks under constraints on data availability and computation cost.",1
"Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.",0
"This paper presents a study of overparameterized neural networks trained using stochastic gradient descent (SGD) on structured data. Despite their increased complexity, these models have shown promising results in several domains, including natural language processing and computer vision. However, learning such models remains challenging due to issues such as vanishing gradients and plateaus during training. We investigate the use of SGD, which has proven effective in handling high-dimensional datasets and achieving faster convergence rates compared to batch gradient descent methods. Additionally, we explore the effectiveness of utilizing structured prior knowledge in model development by incorporating domain-specific constraints into the optimization process. Our experiments demonstrate that models trained through our proposed method achieve superior performance in comparison to those trained without structural guidance and existing state-of-the-art approaches in related tasks. Overall, our findings contribute towards understanding how to effectively train complex deep learning architectures, particularly in cases where large amounts of labeled data may not be available.",1
"Many common sequential data sources, such as source code and natural language, have a natural tree-structured representation. These trees can be generated by fitting a sequence to a grammar, yielding a hierarchical ordering of the tokens in the sequence. This structure encodes a high degree of syntactic information, making it ideal for problems such as grammar correction. However, little work has been done to develop neural networks that can operate on and exploit tree-structured data. In this paper we present the Tree-Transformer \textemdash{} a novel neural network architecture designed to translate between arbitrary input and output trees. We applied this architecture to correction tasks in both the source code and natural language domains. On source code, our model achieved an improvement of $25\%$ $\text{F}0.5$ over the best sequential method. On natural language, we achieved comparable results to the most complex state of the art systems, obtaining a $10\%$ improvement in recall on the CoNLL 2014 benchmark and the highest to date $\text{F}0.5$ score on the AESW benchmark of $50.43$.",0
"Here is a possible abstract that meets your specifications. Please note that I have made some assumptions based on common practices in computer science papers. You may wish to adjust these assumptions before incorporating my suggested text into your final document. --- This paper describes a new method for correcting errors in tree-structured data using deep learning techniques. The proposed approach leverages state-of-the-art transformers, which have been shown to perform well on natural language processing tasks. To adapt transformers to work with trees, we introduce Tree-Transformer, a novel neural network architecture tailored for handling structured input such as XML documents. We evaluate our model on two real-world datasets containing errors in XML elements, attributes, and relationships between nodes, finding that it outperforms several baselines by significant margins. Our contributions include both the methodology and codebase, making our technique easily accessible to researchers and practitioners working on similar problems. We hope that our approach can help improve the reliability and quality of large-scale applications dealing with structured data repositories.",1
"Graph structured data provide two-fold information: graph structures and node attributes. Numerous graph-based algorithms rely on both information to achieve success in supervised tasks, such as node classification and link prediction. However, node attributes could be missing or incomplete, which significantly deteriorates the performance. The task of node attribute generation aims to generate attributes for those nodes whose attributes are completely unobserved. This task benefits many real-world problems like profiling, node classification and graph data augmentation. To tackle this task, we propose a deep adversarial learning based method to generate node attributes; called node attribute neural generator (NANG). NANG learns a unifying latent representation which is shared by both node attributes and graph structures and can be translated to different modalities. We thus use this latent representation as a bridge to convert information from one modality to another. We further introduce practical applications to quantify the performance of node attribute generation. Extensive experiments are conducted on four real-world datasets and the empirical results show that node attributes generated by the proposed method are high-qualitative and beneficial to other applications. The datasets and codes are available online.",0
"This paper presents new algorithms that solve both problems. We begin by presenting a novel algorithm for generating node attributes on graphs. This algorithm allows users to specify any desired attribute distribution, and then assigns those distributions randomly to nodes while preserving their structure (such as clusters) as closely as possible. Second, we show how these node attributes can be used to guide graph visualization. Specifically, we use node color to represent one such attribute, and demonstrate several techniques for choosing colors based on different criteria. Finally, we evaluate our approach through several case studies, demonstrating that it leads to more informative and effective visualizations than previous methods. Our work has important implications for network analysis and visualization across many domains, including social science, biology, computer systems, and beyond.",1
"Graph Convolutional Networks (GCNs) are a class of general models that can learn from graph structured data. Despite being general, GCNs are admittedly inferior to convolutional neural networks (CNNs) when applied to vision tasks, mainly due to the lack of domain knowledge that is hardcoded into CNNs, such as spatially oriented translation invariant filters. However, a great advantage of GCNs is the ability to work on irregular inputs, such as superpixels of images. This could significantly reduce the computational cost of image reasoning tasks. Another key advantage inherent to GCNs is the natural ability to model multirelational data. Building upon these two promising properties, in this work, we show best practices for designing GCNs for image classification; in some cases even outperforming CNNs on the MNIST, CIFAR-10 and PASCAL image datasets.",0
"This paper presents a novel approach to image classification using hierarchical multigraph networks (HMN). HMN is a new architecture that exploits graph representations to capture high-level semantic relationships between objects within images. By leveraging both global contextual relationships across different levels of abstraction as well as local, fine-grained details at each level, we demonstrate significant improvements over traditional methods on several benchmark datasets. Our method utilizes a multi-task framework, allowing us to jointly learn from multiple object detection tasks while optimizing parameters efficiently. Experimental results show that our model achieves state-of-the-art performance compared to current approaches used by top participants in the COCO object detection challenge. We believe that these advancements open up exciting opportunities for future research into scalable and interpretable deep learning models that leverage rich relational structures.",1
"Finding the biomarkers associated with ASD is helpful for understanding the underlying roots of the disorder and can lead to earlier diagnosis and more targeted treatment. A promising approach to identify biomarkers is using Graph Neural Networks (GNNs), which can be used to analyze graph structured data, i.e. brain networks constructed by fMRI. One way to interpret important features is through looking at how the classification probability changes if the features are occluded or replaced. The major limitation of this approach is that replacing values may change the distribution of the data and lead to serious errors. Therefore, we develop a 2-stage pipeline to eliminate the need to replace features for reliable biomarker interpretation. Specifically, we propose an inductive GNN to embed the graphs containing different properties of task-fMRI for identifying ASD and then discover the brain regions/sub-graphs used as evidence for the GNN classifier. We first show GNN can achieve high accuracy in identifying ASD. Next, we calculate the feature importance scores using GNN and compare the interpretation ability with Random Forest. Finally, we run with different atlases and parameters, proving the robustness of the proposed method. The detected biomarkers reveal their association with social behaviors. We also show the potential of discovering new informative biomarkers. Our pipeline can be generalized to other graph feature importance interpretation problems.",0
"In recent years, task-based functional magnetic resonance imaging (tfMRI) has become increasingly important as a tool for identifying neural biomarkers that can aid in diagnosing neurological disorders such as Alzheimers disease and Parkinsons disease. However, interpreting these biomarkers remains challenging due to their complex, multidimensional nature. Here we present a novel method for analyzing tfMRI data using graph convolutional networks (GCN). GCNs have previously been shown to excel at processing spatially structured data like brain connectivity graphs, but until now there has been no attempt to apply them to task fMRI data analysis. Our results demonstrate that GCNs are capable of accurately extracting relevant features from the raw dataset which significantly improve performance on downstream diagnostic classification tasks over standard approaches. We believe our work opens up exciting possibilities for advancing how we interpret task-fMRI biomarkers and potentially improving patient outcomes across a wide range of conditions.",1
"Co-Clustering, the problem of simultaneously identifying clusters across multiple aspects of a data set, is a natural generalization of clustering to higher-order structured data. Recent convex formulations of bi-clustering and tensor co-clustering, which shrink estimated centroids together using a convex fusion penalty, allow for global optimality guarantees and precise theoretical analysis, but their computational properties have been less well studied. In this note, we present three efficient operator-splitting methods for the convex co-clustering problem: a standard two-block ADMM, a Generalized ADMM which avoids an expensive tensor Sylvester equation in the primal update, and a three-block ADMM based on the operator splitting scheme of Davis and Yin. Theoretical complexity analysis suggests, and experimental evidence confirms, that the Generalized ADMM is far more efficient for large problems.",0
"The field of matrix factorization has seen many techniques proposed for obtaining low rank approximations of data matrices, with one popular approach being the use of alternating least squares (ALS) optimization. However, recent work has shown that methods based on splitting can perform as well or better than existing ALS-based approaches across a range of applications. Motivated by these results, we present a novel framework for solving bi-clustering problems that extends previous work on splitting algorithms for co-clustering problems. We introduce two new families of algorithms, referred to as Bregman Lagrangian splitting (BLS) and proximal gradient descent splitting (PGDS). For each family of algorithms, we propose both exact and inexact variants that offer different tradeoffs between computational efficiency and accuracy. Experimental evaluation using real and synthetic datasets shows that our BLS and PGDS frameworks outperform current state-of-the-art methods for convex bi-clustering problems. Our work demonstrates the effectiveness of splitting methods for solving challenging clustering problems, particularly those involving nonconvex formulations or nonsmooth regularizers.",1
"We present a versatile formulation of the convolution operation that we term a ""mapped convolution."" The standard convolution operation implicitly samples the pixel grid and computes a weighted sum. Our mapped convolution decouples these two components, freeing the operation from the confines of the image grid and allowing the kernel to process any type of structured data. As a test case, we demonstrate its use by applying it to dense inference on spherical data. We perform an in-depth study of existing spherical image convolution methods and propose an improved sampling method for equirectangular images. Then, we discuss the impact of data discretization when deriving a sampling function, highlighting drawbacks of the cube map representation for spherical data. Finally, we illustrate how mapped convolutions enable us to convolve directly on a mesh by projecting the spherical image onto a geodesic grid and training on the textured mesh. This method exceeds the state of the art for spherical depth estimation by nearly 17%. Our findings suggest that mapped convolutions can be instrumental in expanding the application scope of convolutional neural networks.",0
"In recent years, deep learning has seen explosive growth thanks to advances such as convolutional neural networks (CNNs), which have been successfully applied to image recognition tasks. CNNs leverage the local connectivity properties of grid-based data, such as images, by using shared weights among nearby neurons. However, these architectures often suffer from slow parameter sharing due to the large receptive fields required to capture global context, resulting in high memory usage and computational cost. To address these challenges, we propose mapped convolutions - a novel architecture that learns both feature maps and their spatial locations, allowing for flexible computation of convolution on arbitrary graphs. We empirically demonstrate that our approach achieves state-of-the-art performance across multiple benchmark datasets while significantly reducing model size and complexity. Our framework enables efficient computation over irregular domains and holds promise for future developments in computer vision, graphics, and other domains where graph structure plays a key role. This work represents an important step towards realizing expressive generalization via end-to-end trainable models in artificial intelligence.",1
"Neural machine translation models are used to automatically generate a document from given source code since this can be regarded as a machine translation task. Source code summarization is one of the components for automatic document generation, which generates a summary in natural language from given source code. This suggests that techniques used in neural machine translation, such as Long Short-Term Memory (LSTM), can be used for source code summarization. However, there is a considerable difference between source code and natural language: Source code is essentially {\em structured}, having loops and conditional branching, etc. Therefore, there is some obstacle to apply known machine translation models to source code.   Abstract syntax trees (ASTs) capture these structural properties and play an important role in recent machine learning studies on source code. Tree-LSTM is proposed as a generalization of LSTMs for tree-structured data. However, there is a critical issue when applying it to ASTs: It cannot handle a tree that contains nodes having an arbitrary number of children and their order simultaneously, which ASTs generally have such nodes. To address this issue, we propose an extension of Tree-LSTM, which we call \emph{Multi-way Tree-LSTM} and apply it for source code summarization. As a result of computational experiments, our proposal achieved better results when compared with several state-of-the-art techniques.",0
"Incorporating source code summarization techniques into software engineering tools can significantly aid developers in understanding complex systems by reducing the effort required to navigate them. Although recent advances have been made in developing such methods using machine learning models, most of these approaches still struggle with accurately capturing important features within large and/or deeply nested structures. To address this challenge, we propose the use of extended tree-LSTM (eTreeLSTM) architectures which enable better representation of hierarchical relationships in source code files. Our experimental evaluation shows that eTreeLSTMs outperform state-of-the-art baseline models across multiple metrics, demonstrating their effectiveness in generating accurate and concise code summaries. Overall, our study represents an important step towards enhancing developer productivity by providing efficient means for navigating code bases.",1
"Extracting key information from documents, such as receipts or invoices, and preserving the interested texts to structured data is crucial in the document-intensive streamline processes of office automation in areas that includes but not limited to accounting, financial, and taxation areas. To avoid designing expert rules for each specific type of document, some published works attempt to tackle the problem by learning a model to explore the semantic context in text sequences based on the Named Entity Recognition (NER) method in the NLP field. In this paper, we propose to harness the effective information from both semantic meaning and spatial distribution of texts in documents. Specifically, our proposed model, Convolutional Universal Text Information Extractor (CUTIE), applies convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations. We further explore the effect of employing different structures of convolutional neural network and propose a fast and portable structure. We demonstrate the effectiveness of the proposed method on a dataset with up to $4,484$ labelled receipts, without any pre-training or post-processing, achieving state of the art performance that is much better than the NER based methods in terms of either speed and accuracy. Experimental results also demonstrate that the proposed CUTIE model being able to achieve good performance with a much smaller amount of training data.",0
"Here at OpenAI we present to you ""CUTIE: Learning to Understand Documents with Convolutional Universal Text Information Extractor"". While there have been many methods proposed to extract relevant pieces of text from documents, CUTIE represents something new by using deep learning techniques to achieve state-of-the art performance on several benchmark datasets. We show how our model can effectively learn to identify objects in images, despite only receiving as input raw text from surrounding documents. In addition, we demonstrate that even though the training process was performed on one dataset, CUTIE generalizes well across multiple other datasets with no additional fine tuning required - allowing for quick deployment into applications such as OCR or question answering systems without any extra effort. This work contributes to the field of natural language processing by providing strong baseline results which future models can attempt to beat. If your interested in finding out more details, please refer to the attached PDF!",1
"Compared with shallow domain adaptation, recent progress in deep domain adaptation has shown that it can achieve higher predictive performance and stronger capacity to tackle structural data (e.g., image and sequential data). The underlying idea of deep domain adaptation is to bridge the gap between source and target domains in a joint space so that a supervised classifier trained on labeled source data can be nicely transferred to the target domain. This idea is certainly intuitive and powerful, however, limited theoretical understandings have been developed to support its underpinning principle. In this paper, we have provided a rigorous framework to explain why it is possible to close the gap of the target and source domains in the joint space. More specifically, we first study the loss incurred when performing transfer learning from the source to the target domain. This provides a theory that explains and generalizes existing work in deep domain adaptation which was mainly empirical. This enables us to further explain why closing the gap in the joint space can directly minimize the loss incurred for transfer learning between the two domains. To our knowledge, this offers the first theoretical result that characterizes a direct bound on the joint space and the gain of transfer learning via deep domain adaptation",0
"This paper presents some theoretical findings on deep domain adaptation that shed light on how machine learning models can be adapted to new domains quickly and efficiently. The authors analyze existing approaches and identify key limitations, then propose novel methods for overcoming these challenges. They demonstrate that their approach outperforms state-of-the-art techniques by significant margins across a variety of tasks and datasets. Overall, this research advances our understanding of deep domain adaption and has important implications for a wide range of applications, from computer vision to natural language processing. Keywords: Domain adaptation, deep learning, transfer learning, convolutional neural networks, recurrent neural networks.",1
"Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redefining the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying downsampling to graphs is still difficult to perform and has room for improvement. In this paper, we propose a graph pooling method based on self-attention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters.",0
"In this paper, we propose a new method for graph pooling called self-attention graph pooling (SAGP). Traditional methods for graph pooling use fixed rules that may not capture important structural properties of the input graphs, leading to suboptimal results. SAGP instead uses attention mechanisms to dynamically weight the importance of different nodes and edges within each graph, allowing it to focus on the most salient features. This enables SAGP to better preserve the overall structure and properties of the original graphs during pooling, making it more effective than previous methods. We evaluate the performance of our approach using several benchmark datasets and demonstrate that SAGP achieves state-of-the-art results across a variety of tasks including node classification, edge prediction, and graph generation. Overall, our work shows the effectiveness of using attention mechanisms for graph pooling and sets the stage for future research exploring these ideas further.",1
"For many structured learning tasks, the data annotation process is complex and costly. Existing annotation schemes usually aim at acquiring completely annotated structures, under the common perception that partial structures are of low quality and could hurt the learning process. This paper questions this common perception, motivated by the fact that structures consist of interdependent sets of variables. Thus, given a fixed budget, partly annotating each structure may provide the same level of supervision, while allowing for more structures to be annotated. We provide an information theoretic formulation for this perspective and use it, in the context of three diverse structured learning tasks, to show that learning from partial structures can sometimes outperform learning from complete ones. Our findings may provide important insights into structured data annotation schemes and could support progress in learning protocols for structured tasks.",0
"This paper presents an examination of partial versus complete solutions in problem-solving and decision making. We investigate how different types of solutions can impact outcomes and discuss factors that may influence whether partial or complete solutions are preferred. Our analysis suggests that while complete solutions can lead to optimal results, they may not always be feasible or desirable. On the other hand, partial solutions can offer benefits such as flexibility, adaptability, and simplicity but may sacrifice some degree of effectiveness. Ultimately, we argue that there may be no one-size-fits-all solution and the choice between partial or complete solutions should depend on the specific situation at hand.",1
"Machine-learning driven safety-critical autonomous systems, such as self-driving cars, must be able to detect situations where its trained model is not able to make a trustworthy prediction. Often viewed as a black-box, it is non-obvious to determine when a model will make a safe decision and when it will make an erroneous, perhaps life-threatening one. Prior work on novelty detection deal with highly structured data and do not translate well to dynamic, real-world situations. This paper proposes a multi-step framework for the detection of novel scenarios in vision-based autonomous systems by leveraging information learned by the trained prediction model and a new image similarity metric. We demonstrate the efficacy of this method through experiments on a real-world driving dataset as well as on our in-house indoor racing environment.",0
"In recent years, novelty detection has become increasingly important as a tool for anomaly detection. Anomalies can occur due to several reasons including errors in data entry, new types of attacks on systems, faults in sensors, etc. They should be identified quickly so that appropriate action could be taken. Convolutional neural networks (CNN) have been used previously for detecting anomalies by learning features from normal training data and then identifying deviations from these learned features. However, these methods often require large amounts of labeled data which may not always be available. To address this issue, we propose using saliency maps generated by CNN models during inference to identify anomalies. This approach requires no extra labels other than those required for the base model itself. Our experiments show that our method outperforms previous state-of-the art methods in terms of accuracy while requiring less amount of training data. We believe that network saliency provides a powerful tool for detecting anomalies without the need for expensive annotations. -----",1
"Graph data widely exist in many high-impact applications. Inspired by the success of deep learning in grid-structured data, graph neural network models have been proposed to learn powerful node-level or graph-level representation. However, most of the existing graph neural networks suffer from the following limitations: (1) there is limited analysis regarding the graph convolution properties, such as seed-oriented, degree-aware and order-free; (2) the node's degree-specific graph structure is not explicitly expressed in graph convolution for distinguishing structure-aware node neighborhoods; (3) the theoretical explanation regarding the graph-level pooling schemes is unclear.   To address these problems, we propose a generic degree-specific graph neural network named DEMO-Net motivated by Weisfeiler-Lehman graph isomorphism test that recursively identifies 1-hop neighborhood structures. In order to explicitly capture the graph topology integrated with node attributes, we argue that graph convolution should have three properties: seed-oriented, degree-aware, order-free. To this end, we propose multi-task graph convolution where each task represents node representation learning for nodes with a specific degree value, thus leading to preserving the degree-specific graph structure. In particular, we design two multi-task learning methods: degree-specific weight and hashing functions for graph convolution. In addition, we propose a novel graph-level pooling/readout scheme for learning graph representation provably lying in a degree-specific Hilbert kernel space. The experimental results on several node and graph classification benchmark data sets demonstrate the effectiveness and efficiency of our proposed DEMO-Net over state-of-the-art graph neural network models.",0
"This paper presents a novel graph neural network (GNN) model called DEMO-Net specifically designed for node classification and graph classification tasks. Unlike traditional GNNs that operate on entire graphs as input, our approach utilizes degree-specific message passing, which allows information exchange among nodes based on their degrees of connectivity. By doing so, we effectively reduce computational complexity without sacrificing accuracy. Through comprehensive experiments across multiple benchmark datasets, DEMO-Net consistently outperforms state-of-the-art baselines while using fewer parameters. Additionally, ablation studies demonstrate the effectiveness of each component in our method, validating the importance of degree-specific message passing in realizing high performance under resource constraints. Our findings indicate potential applications in scenarios where computation resources are limited, such as edge computing and mobile devices.",1
"Recently, graph neural networks (GNNs) have proved to be suitable in tasks on unstructured data. Particularly in tasks as community detection, node classification, and link prediction. However, most GNN models still operate with static relationships. We propose the Graph Learning Network (GLN), a simple yet effective process to learn node embeddings and structure prediction functions. Our model uses graph convolutions to propose expected node features, and predict the best structure based on them. We repeat these steps recursively to enhance the prediction and the embeddings.",0
"In recent years, there has been significant interest in developing algorithms that can learn from data without requiring explicit labels or supervision. One promising approach to addressing this challenge is through graph learning networks (GLNs), which are structures that capture relationships between pairs of objects in a dataset. These GLNs enable efficient computation of distances between points in high-dimensional spaces, allowing for more accurate model training and better generalization performance across different tasks. This paper presents a new algorithm called Graph Learning Network (GLN) that leverages these properties to improve upon existing methods for structure learning. Our proposed method outperforms state-of-the-art approaches on multiple benchmark datasets while offering competitive runtime performance. We also demonstrate the versatility of our method by applying it to several real-world applications, including semi-supervised classification, anomaly detection, and domain adaptation.",1
"We present RL-VAE, a graph-to-graph variational autoencoder that uses reinforcement learning to decode molecular graphs from latent embeddings. Methods have been described previously for graph-to-graph autoencoding, but these approaches require sophisticated decoders that increase the complexity of training and evaluation (such as requiring parallel encoders and decoders or non-trivial graph matching). Here, we repurpose a simple graph generator to enable efficient decoding and generation of molecular graphs.",0
"Here is a draft:  In recent years, machine learning has been applied successfully to problems such as image recognition, natural language processing, and speech recognition. One important task that remains difficult is drug discovery; traditional methods rely heavily on human intuition and expertise, but these can only go so far due to their limited ability to process vast amounts of data quickly and accurately. To address this challenge, we propose using molecular graph embeddings, which represent molecules as high-dimensional vectors in Euclidean space. However, existing algorithms suffer from several drawbacks, including the tendency to overfit small datasets, sensitivity to hyperparameter tuning, difficulty scaling up to larger models, and limited interpretability. In response, we develop a novel reinforcement learning algorithm that addresses each of these issues simultaneously while improving the accuracy of embedding generation across multiple benchmark datasets by at least four percentage points relative to the current state-of-the-art method. We hope our work provides a foundation for future progress toward automating drug design with deep learning techniques.",1
"Graph Neural Networks (GNNs) have proven to be successful in many classification tasks, outperforming previous state-of-the-art methods in terms of accuracy. However, accuracy alone is not enough for high-stakes decision making. Decision makers want to know the likelihood that a specific GNN prediction is correct. For this purpose, obtaining calibrated models is essential. In this work, we perform an empirical evaluation of the calibration of state-of-the-art GNNs on multiple datasets. Our experiments show that GNNs can be calibrated in some datasets but also badly miscalibrated in others, and that state-of-the-art calibration methods are helpful but do not fix the problem.",0
"While graph neural networks (GNNs) have shown great potential in many applications, there has been growing concern that they may be overhyped due to their strong performance on benchmark datasets which may no longer reflect real-world conditions. In our paper, we investigate whether GNNs are indeed miscalibrated by comparing their predictions against ground truth labels in multiple domains. Our results suggest that while GNNs perform well on benchmark datasets, they often struggle when faced with more complex tasks and real-world data, leading to significant calibration issues. We provide insights into why these calibration problems occur and discuss possible solutions to address them. Ultimately, our findings highlight the importance of evaluating GNN models beyond benchmark datasets to ensure that they deliver accurate and reliable predictions in practice.",1
"This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures. By doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces. The work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions. The result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format.",0
"This is a fascinating research paper that delves into the approximation capabilities of artificial neural networks (ANNs) on two distinct spaces: probability measure spaces and tree-structured data domains. In recent years, there has been growing interest in the use of deep learning methods, such as ANNs, to approximate functions defined over complex and high-dimensional input spaces. However, there are still many open questions regarding the performance and limitations of these models, particularly in settings where the underlying structure of the problem may not align well with traditional feedforward architectures. The authors investigate how ANNs can effectively learn continuous maps from a space of probability measures onto itself - a task relevant in applications including density estimation and generative modelling. They show that under mild assumptions on the model architecture, ANNs with random weights can achieve arbitrary precision in approximating any Lipschitz function on this domain, making them universal approximators in this setting. Next, the paper turns to consider the case of tree-structured data, which arise frequently in natural language processing tasks like text generation and sentiment analysis. Here the authors develop new techniques to design ANNs tailored specifically for tree structured inputs and prove that they can match the accuracy achieved by state-of-the-art recurrent neural network based approaches. Overall, this work represents important progress towards understanding the strengths and limitations of ANNs as tools for solving challenging problems across diverse domains. Its findings have broad implications for applied machine learning practice and further theoretical explorations.",1
"Most of the successful deep neural network architectures are structured, often consisting of elements like convolutional neural networks and gated recurrent neural networks. Recently, graph neural networks have been successfully applied to graph structured data such as point cloud and molecular data. These networks often only consider pairwise dependencies, as they operate on a graph structure. We generalize the graph neural network into a factor graph neural network (FGNN) in order to capture higher order dependencies. We show that FGNN is able to represent Max-Product Belief Propagation, an approximate inference algorithm on probabilistic graphical models; hence it is able to do well when Max-Product does well. Promising results on both synthetic and real datasets demonstrate the effectiveness of the proposed model.",0
"Factor graphs have emerged as a powerful graphical representation technique for probabilistic inference problems including but not limited to computer vision applications such as object recognition, image segmentation, pose estimation, 3D reconstruction etc. They model uncertainty in variables by their probability distributions using conditional independence relationships between random variables represented by nodes or factors, which correspond to probability functions, while edges represent constraints that factorize these probability functions. Exact inference in high dimensional factor graphs is computationally intractable and approximate methods like Markov Chain Monte Carlo (MCMC) techniques can suffer from slow convergence. Recently deep learning has been used extensively as a tool to solve large scale non linear optimization problems especially in computer vision tasks. In this work we investigate the use of neural networks to learn factorized approximations of the joint probability distribution from observations without any assumption on the form of the potential function, this leads us naturally to define Variational autoencoders (VAE), deep determinantal point processes, generative adversarial imitation learning (GAIL) as special cases of our framework depending on how we choose to regularize the network. We also experimentally demonstrate on real world datasets that our method achieves state of art results with faster training times as compared to existing methods .",1
"Recently, a method [7] was proposed to generate contrastive explanations for differentiable models such as deep neural networks, where one has complete access to the model. In this work, we propose a method, Model Agnostic Contrastive Explanations Method (MACEM), to generate contrastive explanations for \emph{any} classification model where one is able to \emph{only} query the class probabilities for a desired input. This allows us to generate contrastive explanations for not only neural networks, but models such as random forests, boosted trees and even arbitrary ensembles that are still amongst the state-of-the-art when learning on structured data [13]. Moreover, to obtain meaningful explanations we propose a principled approach to handle real and categorical features leading to novel formulations for computing pertinent positives and negatives that form the essence of a contrastive explanation. A detailed treatment of the different data types of this nature was not performed in the previous work, which assumed all features to be positive real valued with zero being indicative of the least interesting value. We part with this strong implicit assumption and generalize these methods so as to be applicable across a much wider range of problem settings. We quantitatively and qualitatively validate our approach over 5 public datasets covering diverse domains.",0
"This paper presents a novel approach called model agnostic contrastive explanations (MACE) that enables humans to interpret machine learning models trained on structured data. Traditionally, interpreting machine learning models has been challenging due to their black box nature, which often requires domain experts to make sense of complex mathematical representations. MACE addresses this problem by providing intuitive explanations that are grounded in human understanding. We showcase how our method can be applied to different types of tasks such as image classification, natural language processing, and graph representation learning problems. Our experimental results demonstrate that MACE improves over baseline methods in terms of both quantitative metrics and user studies. This work provides new opportunities for explainability research in artificial intelligence, enabling broader adoption of machine learning applications in critical domains such as healthcare, finance, and public policy.",1
"Bottom-Up Hidden Tree Markov Model is a highly expressive model for tree-structured data. Unfortunately, it cannot be used in practice due to the intractable size of its state-transition matrix. We propose a new approximation which lies on the Tucker factorisation of tensors. The probabilistic interpretation of such approximation allows us to define a new probabilistic model for tree-structured data. Hence, we define the new approximated model and we derive its learning algorithm. Then, we empirically assess the effective power of the new model evaluating it on two different tasks. In both cases, our model outperforms the other approximated model known in the literature.",0
"This work presents Bayesian Tensor Factorisation (BTF), a novel method for learning bottom-up hidden tree Markov models from data. These models are widely used in natural language processing tasks such as dependency parsing, constituency parsing, semantic role labelling, and machine translation. However, current approaches rely on heuristics that may struggle to capture complex relationships between elements in the model, resulting in suboptimal performance.  Our proposed approach uses tensor factorisation techniques to learn the parameters of these models in a principled manner, allowing us to overcome some of the limitations of existing methods. We employ variational inference to approximate the posterior distribution over the factors given the observed data, which enables efficient computation even for large datasets. Experimental results demonstrate the effectiveness of our approach compared to state-of-the-art methods across multiple NLP tasks, including both intrinsic evaluation metrics like perplexity and extrinsic evaluation metrics like F1 scores. Our framework provides a flexible and extensible foundation for future research in this domain. Overall, we believe that our contributions significantly advance the understanding and application of bottom-up HTMMs for NLP problems.",1
"Much of the recent work on learning molecular representations has been based on Graph Convolution Networks (GCN). These models rely on local aggregation operations and can therefore miss higher-order graph properties. To remedy this, we propose Path-Augmented Graph Transformer Networks (PAGTN) that are explicitly built on longer-range dependencies in graph-structured data. Specifically, we use path features in molecular graphs to create global attention layers. We compare our PAGTN model against the GCN model and show that our model consistently outperforms GCNs on molecular property prediction datasets including quantum chemistry (QM7, QM8, QM9), physical chemistry (ESOL, Lipophilictiy) and biochemistry (BACE, BBBP).",0
"Artificial intelligence (AI) has made significant advancements over the past decade, thanks largely to deep learning architectures such as transformers which have achieved state-of-the-art results across many natural language processing tasks. In recent years, there has been growing interest in developing models that can integrate both structure and semantics to better capture relationships within and between sentences. One approach that achieves this integration is through graph representations where tokens represent nodes and edges represent relations. However, training transformer networks on graphs remains challenging due to computational constraints arising from the quadratic scaling of messages with respect to the number of neighbours. This paper presents path augmentation, a novel method that extends traditional attention mechanisms by first adding paths derived from the underlying graph structure during training, before applying selfattention. Experimental evaluation shows our model outperforms existing methods across multiple benchmark datasets, demonstrating the effectiveness of integrating structural information into AI models trained with large-scale data resources. We conclude by discussing future research directions towards even more scalable approaches to leveraging graph structures and other forms of background knowledge.",1
"Graph neural networks (GNNs) have achieved lots of success on graph-structured data. In the light of this, there has been increasing interest in studying their representation power. One line of work focuses on the universal approximation of permutation-invariant functions by certain classes of GNNs, and another demonstrates the limitation of GNNs via graph isomorphism tests.   Our work connects these two perspectives and proves their equivalence. We further develop a framework of the representation power of GNNs with the language of sigma-algebra, which incorporates both viewpoints. Using this framework, we compare the expressive power of different classes of GNNs as well as other methods on graphs. In particular, we prove that order-2 Graph G-invariant networks fail to distinguish non-isomorphic regular graphs with the same degree. We then extend them to a new architecture, Ring-GNNs, which succeeds on distinguishing these graphs and provides improvements on real-world social network datasets.",0
"Graph neural networks (GNNs) have been shown to effectively model complex relationships between nodes in graphs and achieve state-of-the-art results in tasks such as node classification, link prediction, and graph classification. However, little attention has been paid to their potential role in graph matching problems like graph isomorphism testing. In our work, we demonstrate that GNNs can learn exact embeddings that encode all relevant structural properties of input graphs up to graph automorphism and propose novel architectures based on randomized linear algebra techniques to reduce computational complexity without affecting model expressivity. We establish theoretical foundations by proving under certain conditions that approximate GNN models using these architectures can solve graph isomorphism testing exactly in polynomial time, whereas existing solutions rely on exponential brute-force algorithms. Finally, through extensive experiments across numerous real-world datasets and benchmark collections, we validate both the effectiveness and efficiency gains achieved by our methods over existing alternatives, offering compelling evidence of the strong promise of utilizing GNNs for solving graph matching challenges beyond standard downstream graph learning tasks.  Keywords: Graph Neural Networks; Graph Isomorphism Testing; Function Approximation; Randomized Linear Algebra Techniques; Efficient Architectures.",1
"We propose GraphNVP, the first invertible, normalizing flow-based molecular graph generation model. We decompose the generation of a graph into two steps: generation of (i) an adjacency tensor and (ii) node attributes. This decomposition yields the exact likelihood maximization on graph-structured data, combined with two novel reversible flows. We empirically demonstrate that our model efficiently generates valid molecular graphs with almost no duplicated molecules. In addition, we observe that the learned latent space can be used to generate molecules with desired chemical properties.",0
"GraphNVP is a novel flow-based generative model that generates molecular graphs using invertible transformations. The authors propose a variational autoencoder architecture composed of two types of neural networks: one for encoding input graphs into latent space representations and another for decoding these representations back to graph space. This approach enables efficient computation and allows easy sampling from the underlying distribution over graphs. The performance of the proposed method is evaluated on several benchmark datasets, including randomly generated as well as real chemical compounds. Results show that GraphNVP outperforms existing state-of-the-art models, achieving high accuracy across multiple evaluation metrics such as precision, recall, and F1 score. The work presents a significant contribution to the field by providing a new architecture capable of generating valid and diverse molecules directly in structural form, without any intermediate textual representation. Overall, GraphNVP has the potential to revolutionize drug discovery research by enabling automation of chemists routine tasks related to exploring synthetic routes, property prediction, and optimizing structures.",1
"Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.",0
"SetTransformer is based on the observation that natural language processing tasks often require models which can process sets of inputs (strings) as first class objects. Unfortunately, current deep learning architectures typically operate over scalars or sequences, so they aren't easily adapted to these problems. In practice, one solution has been to vectorize sets into high-dimensional representations whose structure cannot be meaningfully interpreted; another is to split sets apart into individual elements before feeding them through the model. However, neither approach is ideal because they destroy the combinatorial nature of sets and often lose valuable contextual information in the process. Motivated by this challenge, we present SetTransformer, a novel attention mechanism designed specifically for permutation invariant neural network models operating directly on sets. Our method uses a selfattentive layer and multiheaded dot product attention to provide multiple parallel attentions among set elements with arbitrary positions. This allows our model to capture both local interactions within subsets of input elements and global dependencies across the entire set. We evaluate SetTransformer on several benchmark datasets and show that our architecture significantly outperforms prior state-of-the-art methods using strong baselines like BERT4Rec and GraphBert. Finally, we demonstrate that our learned features transfer effectively to downstream classification tasks, highlighting their effectiveness at capturing semantic relationships between set elements.",1
"Graph neural networks have become one of the most important techniques to solve machine learning problems on graph-structured data. Recent work on vertex classification proposed deep and distributed learning models to achieve high performance and scalability. However, we find that the feature vectors of benchmark datasets are already quite informative for the classification task, and the graph structure only provides a means to denoise the data. In this paper, we develop a theoretical framework based on graph signal processing for analyzing graph neural networks. Our results indicate that graph neural networks only perform low-pass filtering on feature vectors and do not have the non-linear manifold learning property. We further investigate their resilience to feature noise and propose some insights on GCN-based graph neural network design.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful tools for handling structured data on graphs such as social networks, molecular structures, and knowledge graphs. Despite their successes, there has been growing concern that GNN models suffer from overfitting due to their repeated application of nonlinear transformations. In practice, these concerns have led researchers to apply regularization techniques like dropout and weight decay to constrain model complexity. However, these methods can only do so much to address the underlying issue of oversmoothing, where high-frequency features are averaged out during message passing.  This work seeks to shed light on the fundamental limitations of GNNs by examining them through the lens of signal processing. Specifically, we show that popular GNN architectures can be understood as linear filters applied repeatedly along eigenvectors corresponding to small eigenvalues of the graph Laplacian. Moreover, these filters turn out to be low-pass filters that attenuate high-frequency information. While this filtering effect is desirable when dealing with noise, it may hinder performance if important discriminative patterns reside in those frequencies.  To demonstrate the impact of these findings, we conduct experiments using three benchmark datasets commonly used to evaluate GNN models (Cora, Citeseer, Pubmed). Our results suggest that current state-of-the-art GNNs already operate close to capacity for many tasks and underscore the importance of incorporating domain knowledge when designing effective GNN architectures. Finally, we discuss potential future directions for research aimed at balancing high frequency preservation while retaining benefits afforded by low-pass filters. Overall, our insights contribute towards a better understanding of GNN models, their strengths and weaknesses, and provide guidance on choosing appropriate graph representations for real-world applications.",1
"Auto-encoders have emerged as a successful framework for unsupervised learning. However, conventional auto-encoders are incapable of utilizing explicit relations in structured data. To take advantage of relations in graph-structured data, several graph auto-encoders have recently been proposed, but they neglect to reconstruct either the graph structure or node attributes. In this paper, we present the graph attention auto-encoder (GATE), a neural network architecture for unsupervised representation learning on graph-structured data. Our architecture is able to reconstruct graph-structured inputs, including both node attributes and the graph structure, through stacked encoder/decoder layers equipped with self-attention mechanisms. In the encoder, by considering node attributes as initial node representations, each layer generates new representations of nodes by attending over their neighbors' representations. In the decoder, we attempt to reverse the encoding process to reconstruct node attributes. Moreover, node representations are regularized to reconstruct the graph structure. Our proposed architecture does not need to know the graph structure upfront, and thus it can be applied to inductive learning. Our experiments demonstrate competitive performance on several node classification benchmark datasets for transductive and inductive tasks, even exceeding the performance of supervised learning baselines in most cases.",0
"""Graph Attention Auto-encoders: A Comprehensive Overview"" Abstract Deep learning techniques have been applied in diverse fields ranging from computer vision and natural language processing (NLP) to bioinformatics and game playing. In recent years, graph data has become increasingly important due to the rise of social media platforms, complex networks like brain connectomes, citation graphs, protein interactions, among others, thus creating a need for new architectures capable of handling structured data represented as graphs. Here we provide an overview of state-of-the art models that handle nonlinear relationships through self attention mechanism in autoencoder frameworks, specifically designed to work on graph structured data. We compare their strengths and limitations focusing on three main aspects related to these models: encoder design, decoding process, and attention mechanisms. Finally, applications using graph attention autoencoders are discussed briefly along with future research directions. Keywords: Graph Attention Auto-encoders, Self Attention Mechanism, Encoding Decoding, Graph Neural Networks",1
"We present batch virtual adversarial training (BVAT), a novel regularization method for graph convolutional networks (GCNs). BVAT addresses the shortcoming of GCNs that do not consider the smoothness of the model's output distribution against local perturbations around the input. We propose two algorithms, sample-based BVAT and optimization-based BVAT, which are suitable to promote the smoothness of the model for graph-structured data by either finding virtual adversarial perturbations for a subset of nodes far from each other or generating virtual adversarial perturbations for all nodes with an optimization process. Extensive experiments on three citation network datasets Cora, Citeseer and Pubmed and a knowledge graph dataset Nell validate the effectiveness of the proposed method, which establishes state-of-the-art results in the semi-supervised node classification tasks.",0
"Abstract:  Graph convolutional networks (GCNs) have been widely used for graph data analysis tasks such as node classification, link prediction, and recommendation systems. However, training GCNs can be challenging due to their sensitivity to noise and outliers in the data. In this work, we propose batch virtual adversarial training (VAT) as a method to improve the robustness and performance of GCNs on graph datasets. Our approach leverages the concept of adversarial examples to regularize the network during training and make it more resilient to input perturbations. We conduct experiments on several benchmark datasets and show that our proposed method significantly improves the accuracy and stability of the trained models compared to existing approaches. This study contributes to the field of graph neural networks by providing a new tool for enhancing model robustness and achieving better generalization performance.",1
"The construction of a meaningful graph topology plays a crucial role in the effective representation, processing, analysis and visualization of structured data. When a natural choice of the graph is not readily available from the data sets, it is thus desirable to infer or learn a graph topology from the data. In this tutorial overview, we survey solutions to the problem of graph learning, including classical viewpoints from statistics and physics, and more recent approaches that adopt a graph signal processing (GSP) perspective. We further emphasize the conceptual similarities and differences between classical and GSP-based graph inference methods, and highlight the potential advantage of the latter in a number of theoretical and practical scenarios. We conclude with several open issues and challenges that are keys to the design of future signal processing and machine learning algorithms for learning graphs from data.",0
"In recent years, there has been growing interest in learning graphical models from data. Graphs provide a powerful tool for capturing complex relationships between variables and can offer valuable insights into many domains including natural language processing, computer vision, and neuroscience. However, inferring graph structures from noisy and high dimensional data remains a challenging task due to their combinatorial nature and uncertainty associated with limited sample sizes. To address these issues, we propose a framework that exploits the connection between graph learning and signal processing, termed as Signal Representation Graph (SRG). SRG represents each variable as a mixture of latent signals which are encoded using dictionaries learned jointly with the graph structure. By positing these signals on the vertices and edges of the graph, we formulate the graph inference problem as a linear regression where we aim to recover both the dictionary atoms and sparse coefficients associated with each vertex. We show how this perspective allows us to leverage techniques such as compressed sensing and sparse coding to learn graphs effectively even in low sample settings. Experiments demonstrate the effectiveness of our approach compared to existing methods across several benchmark datasets. This work highlights the potential benefits of exploring the intersection of graph theory and signal processing for discovering meaningful representations of real world phenomena.",1
"Graph neural networks, which generalize deep neural network models to graph structured data, have attracted increasing attention in recent years. They usually learn node representations by transforming, propagating and aggregating node features and have been proven to improve the performance of many graph related tasks such as node classification and link prediction. To apply graph neural networks for the graph classification task, approaches to generate the \textit{graph representation} from node representations are demanded. A common way is to globally combine the node representations. However, rich structural information is overlooked. Thus a hierarchical pooling procedure is desired to preserve the graph structure during the graph representation learning. There are some recent works on hierarchically learning graph representation analogous to the pooling step in conventional convolutional neural (CNN) networks. However, the local structural information is still largely neglected during the pooling process. In this paper, we introduce a pooling operator $\pooling$ based on graph Fourier transform, which can utilize the node features and local structures during the pooling process. We then design pooling layers based on the pooling operator, which are further combined with traditional GCN convolutional layers to form a graph neural network framework $\m$ for graph classification. Theoretical analysis is provided to understand $\pooling$ from both local and global perspectives. Experimental results of the graph classification task on $6$ commonly used benchmarks demonstrate the effectiveness of the proposed framework.",0
"Graph Convolutional Networks (GCN) have emerged as powerful tools for processing graph data by leveraging convolutional filters over graph structures. However, due to their high computational complexity, GCNs can become prohibitively expensive when applied to large graphs or deep models. In order to address these issues, we propose EigenPooling, a novel method that approximates the eigendecomposition required in traditional GCN layers without resorting to matrix factorization. Our approach relies on iterative sampling from the graph Laplacian and approximate matrix compression techniques, resulting in significant memory savings while preserving accuracy. We demonstrate through extensive experiments on several benchmark datasets that our proposed method significantly reduces computation time and memory usage compared to state-of-the-art methods, while maintaining competitive model performance. These results highlight the potential of EigenPooling for enabling larger and more complex GCN architectures for real-world applications involving massive graphs. Overall, our work represents a step towards efficient deep learning on graphs at scale.",1
"Benchmark data sets are an indispensable ingredient of the evaluation of graph-based machine learning methods. We release a new data set, compiled from International Planning Competitions (IPC), for benchmarking graph classification, regression, and related tasks. Apart from the graph construction (based on AI planning problems) that is interesting in its own right, the data set possesses distinctly different characteristics from popularly used benchmarks. The data set, named IPC, consists of two self-contained versions, grounded and lifted, both including graphs of large and skewedly distributed sizes, posing substantial challenges for the computation of graph models such as graph kernels and graph neural networks. The graphs in this data set are directed and the lifted version is acyclic, offering the opportunity of benchmarking specialized models for directed (acyclic) structures. Moreover, the graph generator and the labeling are computer programmed; thus, the data set may be extended easily if a larger scale is desired. The data set is accessible from \url{https://github.com/IBM/IPC-graph-data}.",0
"Abstract: In recent years there has been significant interest in developing machine learning methods for graph structured data, due largely to the increasing availability of networked data on the web (e.g., social networks) and advances in information retrieval techniques that have led to large scale extraction of relational data from documents. We present IPC (Information Pairs Corpus), which contains large collections of binary classification tasks associated with labeled graphs (representing directed relationships among entities). Our evaluation shows strong results across multiple algorithms using our benchmark, indicating its suitability as a standardized dataset in future work comparing models on predictive performance over relational information and structure.",1
"Performing machine learning on structured data is complicated by the fact that such data does not have vectorial form. Therefore, multiple approaches have emerged to construct vectorial representations of structured data, from kernel and distance approaches to recurrent, recursive, and convolutional neural networks. Recent years have seen heightened attention in this demanding field of research and several new approaches have emerged, such as metric learning on structured data, graph convolutional neural networks, and recurrent decoder networks for structured data. In this contribution, we provide an high-level overview of the state-of-the-art in representation learning and embeddings for structured data across a wide range of machine learning fields.",0
"In recent years, representation learning has emerged as a powerful tool for modeling complex data distributions in many domains. One important challenge in representation learning is how to deal with structured input data such as graphs, trees, or tables that have intrinsic hierarchical or relational structure. This paper presents a new framework for embedding structured data into continuous vector spaces while preserving their original structure. We showcase two main applications: (i) predictive models on graphs, where we introduce Graph2Vec, a novel approach based on random walks that captures both local neighborhood properties and global graph topology; (ii) natural language processing tasks that involve tree structures by developing Tree2Vec, which recursively performs depth-first search for each node to explore its subtree context. Our results demonstrate substantial improvements over strong baselines on several benchmark datasets across different task types including link prediction, classification, clustering, sentiment analysis, and machine translation. Furthermore, our methods can easily scale up to large graphs and trees with billions of nodes and edges thanks to their efficiency, scalability, and low memory footprints. Overall, we believe this work offers a promising direction towards unifying representation learning techniques for various forms of structured data and advancing the state-of-the-art in multiple areas of artificial intelligence.",1
