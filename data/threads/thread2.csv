"The throughput of electron microscopes has increased significantly in recent years, enabling detailed analysis of cell morphology and ultrastructure. Analysis of neural circuits at single-synapse resolution remains the flagship target of this technique, but applications to cell and developmental biology are also starting to emerge at scale. The amount of data acquired in such studies makes manual instance segmentation, a fundamental step in many analysis pipelines, impossible. While automatic segmentation approaches have improved significantly thanks to the adoption of convolutional neural networks, their accuracy still lags behind human annotations and requires additional manual proof-reading. A major hindrance to further improvements is the limited field of view of the segmentation networks preventing them from exploiting the expected cell morphology or other prior biological knowledge which humans use to inform their segmentation decisions. In this contribution, we show how such domain-specific information can be leveraged by expressing it as long-range interactions in a graph partitioning problem known as the lifted multicut problem. Using this formulation, we demonstrate significant improvement in segmentation accuracy for three challenging EM segmentation problems from neuroscience and cell biology.",0
"In recent years, the speed at which electron microscopes operate has increased considerably, allowing for in-depth examination of cell morphology and ultrastructure. Although the primary aim of this technique remains the analysis of neural circuits at single-synapse resolution, it is now also being used in cell and developmental biology on a larger scale. The volume of data generated in these studies renders manual instance segmentation impractical. Despite improvements in automatic segmentation methods, which employ convolutional neural networks, their accuracy still trails behind that of human annotations and necessitates additional manual verification. The restricted field of view of segmentation networks is a significant barrier to further enhancements since they cannot exploit the anticipated cell morphology or other prior biological knowledge that humans use to inform their segmentation decisions. This article demonstrates how such domain-specific data can be employed by expressing it as long-range interactions in a graph partitioning problem known as the lifted multicut problem. We showcase this formulation's effectiveness by achieving a significant improvement in segmentation accuracy for three challenging EM segmentation issues in neuroscience and cell biology.",1
"We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance's graph, which are solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to the negative weight edges attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows, to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and allows for massive parallelization.",0
"Our approach to graph partitioning for image segmentation involves utilizing correlation clustering (CC) as an integer linear program (ILP). To optimize this ILP efficiently, we restructured it to allow for Benders decomposition, a well-known technique in operations research. By breaking down the optimization process into multiple subproblems, each corresponding to a node in the CC instance's graph, our method can solve these subproblems in parallel. Each subproblem is responsible for enforcing cycle inequalities related to the negative weight edges connected to its corresponding node in the CC instance. Additionally, we incorporated Magnanti-Wong Benders rows into our formulation to speed up the optimization process. Our Benders decomposition technique is a promising approach to accelerating optimization for CC and allows for significant parallelization.",1
"Clustering is one of the major roles in data mining that is widely application in pattern recognition and image segmentation. Fuzzy C-means (FCM) is the most used clustering algorithm that proven efficient, fast and easy to implement, however, FCM uses the Euclidean distance that often leads to clustering errors, especially when handling multidimensional and noisy data. In the last few years, many distances metric have been proposed by researchers to improve the performance of the FCM algorithms, and the majority of researchers propose weighted distance. In this paper, we proposed Canberra Weighted Distance to improved performance of the FCM algorithm. The experimental result using the UCI data set show the proposed method is superior to the original method and other clustering methods.",0
"Data mining involves various roles, and clustering is a crucial one that has numerous applications in pattern recognition and image segmentation. Fuzzy C-means (FCM) is a widely used clustering algorithm that is efficient, fast, and simple to implement. However, its use of Euclidean distance can result in clustering errors, particularly when dealing with noisy and multidimensional data. Over the years, researchers have proposed various distance metrics to enhance the performance of FCM algorithms, with most suggesting weighted distance. This article introduces the Canberra Weighted Distance as a means of improving the FCM algorithm's performance. The results of experiments using the UCI data set indicate that the proposed approach outperforms the original method and other clustering techniques.",1
"Separating and labeling each instance of a nucleus (instance-aware segmentation) is the key challenge in segmenting single cell nuclei on fluorescence microscopy images. Deep Neural Networks can learn the implicit transformation of a nuclear image into a probability map indicating the class membership of each pixel (nucleus or background), but the use of post-processing steps to turn the probability map into a labeled object mask is error-prone. This especially accounts for nuclear images of tissue sections and nuclear images across varying tissue preparations. In this work, we aim to evaluate the performance of state-of-the-art deep learning architectures to segment nuclei in fluorescence images of various tissue origins and sample preparation types without post-processing. We compare architectures that operate on pixel to pixel translation and an architecture that operates on object detection and subsequent locally applied segmentation. In addition, we propose a novel strategy to create artificial images to extend the training set. We evaluate the influence of ground truth annotation quality, image scale and segmentation complexity on segmentation performance. Results show that three out of four deep learning architectures (U-Net, U-Net with ResNet34 backbone, Mask R-CNN) can segment fluorescent nuclear images on most of the sample preparation types and tissue origins with satisfactory segmentation performance. Mask R-CNN, an architecture designed to address instance aware segmentation tasks, outperforms other architectures. Equal nuclear mean size, consistent nuclear annotations and the use of artificially generated images result in overall acceptable precision and recall across different tissues and sample preparation types.",0
"Segmenting single cell nuclei on fluorescence microscopy images presents a major challenge in instance-aware segmentation, which involves separating and labeling each nucleus. Although Deep Neural Networks can learn to transform nuclear images into probability maps, converting these maps into labeled object masks using post-processing steps can be error-prone, particularly for nuclear images across varying tissue preparations. This study aims to assess the effectiveness of state-of-the-art deep learning architectures in segmenting nuclei in fluorescence images from various tissue origins and preparation types, without post-processing. The study compares architectures that translate pixel-to-pixel with one that detects objects and applies segmentation locally. Additionally, the study proposes a novel approach to creating artificial images to expand the training set. The study evaluates the impact of ground truth annotation quality, image scale, and segmentation complexity on segmentation performance. Results indicate that three out of four deep learning architectures exhibit satisfactory segmentation performance on most sample preparation types and tissue origins, including U-Net, U-Net with ResNet34 backbone, and Mask R-CNN. Mask R-CNN, which is designed for instance-aware segmentation tasks, outperforms other architectures. Maintaining equal nuclear mean size, consistent nuclear annotations, and utilizing artificially generated images result in overall acceptable precision and recall across different tissues and sample preparation types.",1
"We propose a novel semi-supervised image segmentation method that simultaneously optimizes a supervised segmentation and an unsupervised reconstruction objectives. The reconstruction objective uses an attention mechanism that separates the reconstruction of image areas corresponding to different classes. The proposed approach was evaluated on two applications: brain tumor and white matter hyperintensities segmentation. Our method, trained on unlabeled and a small number of labeled images, outperformed supervised CNNs trained with the same number of images and CNNs pre-trained on unlabeled data. In ablation experiments, we observed that the proposed attention mechanism substantially improves segmentation performance. We explore two multi-task training strategies: joint training and alternating training. Alternating training requires fewer hyperparameters and achieves a better, more stable performance than joint training. Finally, we analyze the features learned by different methods and find that the attention mechanism helps to learn more discriminative features in the deeper layers of encoders.",0
"Our new approach to image segmentation is unique because it combines both supervised segmentation and unsupervised reconstruction objectives, utilizing an attention mechanism to separate image areas by class. We tested this method on two different applications: identifying brain tumors and white matter hyperintensities. Our model, trained with both labeled and unlabeled images, outperformed supervised CNNs trained with the same amount of data, as well as CNNs pre-trained on unlabeled data. We found that the attention mechanism significantly improved segmentation performance and experimented with two different training strategies: joint and alternating. Alternating training required fewer hyperparameters and produced better, more consistent results. Lastly, we analyzed the features learned by our different methods and found that the attention mechanism helped to create more distinctive features in the deeper layers of encoders.",1
"Deep learning methods have achieved promising performance in many areas, but they are still struggling with noisy-labeled images during the training process. Considering that the annotation quality indispensably relies on great expertise, the problem is even more crucial in the medical image domain. How to eliminate the disturbance from noisy labels for segmentation tasks without further annotations is still a significant challenge. In this paper, we introduce our label quality evaluation strategy for deep neural networks automatically assessing the quality of each label, which is not explicitly provided, and training on clean-annotated ones. We propose a solution for network automatically evaluating the relative quality of the labels in the training set and using good ones to tune the network parameters. We also design an overfitting control module to let the network maximally learn from the precise annotations during the training process. Experiments on the public biomedical image segmentation dataset have proved the method outperforms baseline methods and retains both high accuracy and good generalization at different noise levels.",0
"Despite the impressive performance of deep learning in many fields, the issue of noisy-labeled images during training remains a challenge. This challenge is particularly critical in the medical image domain, where high expertise is essential for accurate annotations. Eliminating noise from labels for segmentation tasks without additional annotations is still a significant challenge. This paper presents our label quality evaluation strategy for deep neural networks that automatically assesses label quality and trains on clean annotations. Our proposed solution involves evaluating the relative quality of labels in the training set and using good ones to tune network parameters. We also introduce an overfitting control module to maximize learning from precise annotations during training. Our experiments using a public biomedical image segmentation dataset have demonstrated that our method outperforms baseline methods, achieving high accuracy and good generalization at different noise levels.",1
"Most progress in semantic segmentation reports on daytime images taken under favorable illumination conditions. We instead address the problem of semantic segmentation of nighttime images and improve the state-of-the-art, by adapting daytime models to nighttime without using nighttime annotations. Moreover, we design a new evaluation framework to address the substantial uncertainty of semantics in nighttime images. Our central contributions are: 1) a curriculum framework to gradually adapt semantic segmentation models from day to night via labeled synthetic images and unlabeled real images, both for progressively darker times of day, which exploits cross-time-of-day correspondences for the real images to guide the inference of their labels; 2) a novel uncertainty-aware annotation and evaluation framework and metric for semantic segmentation, designed for adverse conditions and including image regions beyond human recognition capability in the evaluation in a principled fashion; 3) the Dark Zurich dataset, which comprises 2416 unlabeled nighttime and 2920 unlabeled twilight images with correspondences to their daytime counterparts plus a set of 151 nighttime images with fine pixel-level annotations created with our protocol, which serves as a first benchmark to perform our novel evaluation. Experiments show that our guided curriculum adaptation significantly outperforms state-of-the-art methods on real nighttime sets both for standard metrics and our uncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals that selective invalidation of predictions can lead to better results on data with ambiguous content such as our nighttime benchmark and profit safety-oriented applications which involve invalid inputs.",0
"The majority of advances in semantic segmentation have been made on daytime images that have been captured in favorable lighting conditions. In contrast, our focus is on the challenge of semantic segmentation of images taken at night. We aim to enhance the current state-of-the-art by adapting daytime models to nighttime conditions without the use of annotations specific to nighttime. Additionally, we have created a new evaluation framework to address the ambiguity of semantics in nighttime images. Our significant contributions include: 1) a curriculum framework that gradually adapts semantic segmentation models from day to night using a combination of labeled synthetic images and unlabeled real images that become progressively darker throughout the day. We use cross-time-of-day correspondences to guide the inference of labels for the real images. 2) a unique uncertainty-aware annotation and evaluation framework and metric for semantic segmentation, specifically designed for challenging conditions, and includes image regions that are beyond human recognition capability in a systematic manner. 3) We have created the Dark Zurich dataset, which includes 2416 unlabeled nighttime images, 2920 unlabeled twilight images that correspond to their daytime counterparts, and a set of 151 nighttime images with finely detailed pixel-level annotations that were created using our protocol. This dataset serves as the first benchmark for our novel evaluation. Our experiments show that our guided curriculum adaptation outperforms existing methods significantly, both for standard metrics and our uncertainty-aware metric on real nighttime sets. Furthermore, our uncertainty-aware evaluation reveals that selective invalidation of predictions can lead to better results on data with ambiguous content such as our nighttime benchmark. This approach can be particularly useful for safety-oriented applications, which involve invalid inputs.",1
"Accurate segmentation of the optic disc (OD) and cup (OC)in fundus images from different datasets is critical for glaucoma disease screening. The cross-domain discrepancy (domain shift) hinders the generalization of deep neural networks to work on different domain datasets.In this work, we present an unsupervised domain adaptation framework,called Boundary and Entropy-driven Adversarial Learning (BEAL), to improve the OD and OC segmentation performance, especially on the ambiguous boundary regions. In particular, our proposed BEAL frame-work utilizes the adversarial learning to encourage the boundary prediction and mask probability entropy map (uncertainty map) of the target domain to be similar to the source ones, generating more accurate boundaries and suppressing the high uncertainty predictions of OD and OC segmentation. We evaluate the proposed BEAL framework on two public retinal fundus image datasets (Drishti-GS and RIM-ONE-r3), and the experiment results demonstrate that our method outperforms the state-of-the-art unsupervised domain adaptation methods. Codes will be available at https://github.com/EmmaW8/BEAL.",0
"For successful screening of glaucoma disease, precise segmentation of the optic disc (OD) and cup (OC) in fundus images from various datasets is crucial. Nonetheless, the generalization of deep neural networks across different domains is hindered by the cross-domain discrepancy (domain shift). This paper proposes an unsupervised domain adaptation framework, called Boundary and Entropy-driven Adversarial Learning (BEAL), to enhance the OD and OC segmentation performance, particularly on the ambiguous boundary regions. The BEAL framework employs adversarial learning to encourage similarity between the boundary prediction and mask probability entropy map of the target domain and the source ones, producing more accurate boundaries and suppressing high uncertainty predictions of OD and OC segmentation. The proposed method is evaluated on two public retinal fundus image datasets (Drishti-GS and RIM-ONE-r3), and the results demonstrate superior performance to state-of-the-art unsupervised domain adaptation methods. Code for the BEAL framework is available at https://github.com/EmmaW8/BEAL.",1
"Deep convolutional neural networks (CNNs) are state-of-the-art for semantic image segmentation, but typically require many labeled training samples. Obtaining 3D segmentations of medical images for supervised training is difficult and labor intensive. Motivated by classical approaches for joint segmentation and registration we therefore propose a deep learning framework that jointly learns networks for image registration and image segmentation. In contrast to previous work on deep unsupervised image registration, which showed the benefit of weak supervision via image segmentations, our approach can use existing segmentations when available and computes them via the segmentation network otherwise, thereby providing the same registration benefit. Conversely, segmentation network training benefits from the registration, which essentially provides a realistic form of data augmentation. Experiments on knee and brain 3D magnetic resonance (MR) images show that our approach achieves large simultaneous improvements of segmentation and registration accuracy (over independently trained networks) and allows training high-quality models with very limited training data. Specifically, in a one-shot-scenario (with only one manually labeled image) our approach increases Dice scores (%) over an unsupervised registration network by 2.7 and 1.8 on the knee and brain images respectively.",0
"Current state-of-the-art for semantic image segmentation is achieved by deep convolutional neural networks (CNNs); however, these networks typically need numerous labeled training samples. Acquiring 3D segmentations of medical images for supervised training is challenging and laborious. Thus, inspired by classical methods for joint segmentation and registration, a deep learning framework is proposed in this study that learns networks for image registration and segmentation jointly. Unlike prior work on deep unsupervised image registration that demonstrated the usefulness of weak supervision via image segmentations, our approach can utilize existing segmentations when possible and calculate them through the segmentation network otherwise, thereby providing the same registration benefit. Conversely, segmentation network training is enhanced by the registration, which provides a realistic type of data augmentation. Experiments conducted on 3D magnetic resonance (MR) images of the knee and brain demonstrate that our approach leads to substantial simultaneous improvements in segmentation and registration accuracy compared to independently trained networks, allowing the creation of high-quality models with minimal training data. Specifically, in a one-shot-scenario (with just one manually labeled image), our approach improves Dice scores (%) over an unsupervised registration network by 2.7 and 1.8 on the knee and brain images, respectively.",1
"Segmentation is a fundamental task in medical image analysis. However, most existing methods focus on primary region extraction and ignore edge information, which is useful for obtaining accurate segmentation. In this paper, we propose a generic medical segmentation method, called Edge-aTtention guidance Network (ET-Net), which embeds edge-attention representations to guide the segmentation network. Specifically, an edge guidance module is utilized to learn the edge-attention representations in the early encoding layers, which are then transferred to the multi-scale decoding layers, fused using a weighted aggregation module. The experimental results on four segmentation tasks (i.e., optic disc/cup and vessel segmentation in retinal images, and lung segmentation in chest X-Ray and CT images) demonstrate that preserving edge-attention representations contributes to the final segmentation accuracy, and our proposed method outperforms current state-of-the-art segmentation methods. The source code of our method is available at https://github.com/ZzzJzzZ/ETNet.",0
"Medical image analysis requires segmentation as a fundamental task. However, most current methods only focus on primary region extraction and disregard edge information, which is crucial for precise segmentation. This study introduces a novel medical segmentation technique, named Edge-aTtention guidance Network (ET-Net), that incorporates edge-attention representations to guide the segmentation network. The edge guidance module learns the edge-attention representations in the early encoding layers and transfers them to the multi-scale decoding layers, which are then combined using a weighted aggregation module. The effectiveness of our approach is demonstrated through experiments on four segmentation tasks, including optic disc/cup and vessel segmentation in retinal images, as well as lung segmentation in chest X-Ray and CT images. Our method produces more accurate segmentation results than current state-of-the-art techniques, and the source code is available at https://github.com/ZzzJzzZ/ETNet.",1
"Shape instantiation which predicts the 3D shape of a dynamic target from one or more 2D images is important for real-time intra-operative navigation. Previously, a general shape instantiation framework was proposed with manual image segmentation to generate a 2D Statistical Shape Model (SSM) and with Kernel Partial Least Square Regression (KPLSR) to learn the relationship between the 2D and 3D SSM for 3D shape prediction. In this paper, the two-stage shape instantiation is improved to be one-stage. PointOutNet with 19 convolutional layers and three fully-connected layers is used as the network structure and Chamfer distance is used as the loss function to predict the 3D target point cloud from a single 2D image. With the proposed one-stage shape instantiation algorithm, a spontaneous image-to-point cloud training and inference can be achieved. A dataset from 27 Right Ventricle (RV) subjects, indicating 609 experiments, were used to validate the proposed one-stage shape instantiation algorithm. An average point cloud-to-point cloud (PC-to-PC) error of 1.72mm has been achieved, which is comparable to the PLSR-based (1.42mm) and KPLSR-based (1.31mm) two-stage shape instantiation algorithm.",0
"Real-time intra-operative navigation requires the prediction of a dynamic target's 3D shape from one or more 2D images. A previous framework proposed a general shape instantiation approach that involved manual image segmentation to generate a 2D Statistical Shape Model (SSM) and Kernel Partial Least Square Regression (KPLSR) to predict the 3D shape from the 2D SSM. However, this paper presents an improved one-stage shape instantiation method that uses PointOutNet with 19 convolutional layers and three fully-connected layers as the network structure and Chamfer distance as the loss function to predict the 3D target point cloud from a single 2D image. This allows for spontaneous image-to-point cloud training and inference. The proposed algorithm was validated on a dataset of 27 Right Ventricle (RV) subjects, consisting of 609 experiments, achieving an average point cloud-to-point cloud (PC-to-PC) error of 1.72mm, comparable to the two-stage shape instantiation algorithms based on PLSR (1.42mm) and KPLSR (1.31mm).",1
"This work addresses the task of open world semantic segmentation using RGBD sensing to discover new semantic classes over time. Although there are many types of objects in the real-word, current semantic segmentation methods make a closed world assumption and are trained only to segment a limited number of object classes. Towards a more open world approach, we propose a novel method that incrementally learns new classes for image segmentation. The proposed system first segments each RGBD frame using both color and geometric information, and then aggregates that information to build a single segmented dense 3D map of the environment. The segmented 3D map representation is a key component of our approach as it is used to discover new object classes by identifying coherent regions in the 3D map that have no semantic label. The use of coherent region in the 3D map as a primitive element, rather than traditional elements such as surfels or voxels, also significantly reduces the computational complexity and memory use of our method. It thus leads to semi-real-time performance at {10.7}Hz when incrementally updating the dense 3D map at every frame. Through experiments on the NYUDv2 dataset, we demonstrate that the proposed method is able to correctly cluster objects of both known and unseen classes. We also show the quantitative comparison with the state-of-the-art supervised methods, the processing time of each step, and the influences of each component.",0
"This study focuses on open world semantic segmentation using RGBD sensing in order to identify new semantic classes over time. While there are numerous objects in the real world, current semantic segmentation techniques assume a closed world and are only trained to segment a limited number of object classes. To adopt a more open world approach, the authors introduce a new method that gradually learns new classes for image segmentation. Initially, the proposed system segments each RGBD frame using both color and geometric information, and then combines that data to create a single segmented dense 3D map of the environment. The segmented 3D map representation is a crucial part of the approach as it is used to detect new object classes by identifying coherent regions in the 3D map that are without semantic labels. By using coherent regions in the 3D map as a primitive element instead of traditional elements such as surfels or voxels, computational complexity and memory usage are significantly reduced, resulting in semi-real-time performance of 10.7 Hz when incrementally updating the dense 3D map at each frame. Experiments on the NYUDv2 dataset demonstrate that the proposed method can accurately cluster objects of both known and unseen classes. Furthermore, the authors provide a quantitative comparison with state-of-the-art supervised methods, as well as information on the processing time of each step and the impact of each component.",1
"The task of medical image segmentation commonly involves an image reconstruction step to convert acquired raw data to images before any analysis. However, noises, artifacts and loss of information due to the reconstruction process are almost inevitable, which compromises the final performance of segmentation. We present a novel learning framework that performs magnetic resonance brain image segmentation directly from k-space data. The end-to-end framework consists of a unique task-driven attention module that recurrently utilizes intermediate segmentation estimation to facilitate image-domain feature extraction from the raw data, thus closely bridging the reconstruction and the segmentation tasks. In addition, to address the challenge of manual labeling, we introduce a novel workflow to generate labeled training data for segmentation by exploiting imaging modality simulators and digital phantoms. Extensive experimental results show that the proposed method outperforms several state-of-the-art methods.",0
"Medical image segmentation typically involves reconstructing acquired raw data into images before analysis, but this can introduce noise, artifacts, and information loss that can compromise the segmentation's quality. Our new learning framework directly segments magnetic resonance brain images from k-space data, avoiding the reconstruction step. It includes a task-driven attention module that uses intermediate segmentation estimates to extract image-domain features from raw data, effectively linking the reconstruction and segmentation tasks. To address the challenge of manual labeling, we generate labeled training data using imaging modality simulators and digital phantoms. Our experiments show that our method outperforms several state-of-the-art approaches.",1
"Convolutional Neural Network (CNN) based image segmentation has made great progress in recent years. However, video object segmentation remains a challenging task due to its high computational complexity. Most of the previous methods employ a two-stream CNN framework to handle spatial and motion features separately. In this paper, we propose an end-to-end encoder-decoder style 3D CNN to aggregate spatial and temporal information simultaneously for video object segmentation. To efficiently process video, we propose 3D separable convolution for the pyramid pooling module and decoder, which dramatically reduces the number of operations while maintaining the performance. Moreover, we also extend our framework to video action segmentation by adding an extra classifier to predict the action label for actors in videos. Extensive experiments on several video datasets demonstrate the superior performance of the proposed approach for action and object segmentation compared to the state-of-the-art.",0
"Recent years have seen significant advancements in image segmentation with Convolutional Neural Networks (CNNs). However, video object segmentation remains a challenging task due to its high computational complexity. Prior methods have employed a two-stream CNN framework to handle spatial and motion features separately. This paper proposes an end-to-end encoder-decoder style 3D CNN to simultaneously aggregate spatial and temporal information for video object segmentation. To enhance processing efficiency, the authors propose 3D separable convolution for the pyramid pooling module and decoder, which reduces the number of operations while maintaining performance. Furthermore, the authors extend their framework to video action segmentation by including an extra classifier to predict action labels for actors in videos. The proposed approach for action and object segmentation outperforms state-of-the-art methods, as demonstrated through extensive experiments on various video datasets.",1
"Training deep convolutional neural networks usually requires a large amount of labeled data. However, it is expensive and time-consuming to annotate data for medical image segmentation tasks. In this paper, we present a novel uncertainty-aware semi-supervised framework for left atrium segmentation from 3D MR images. Our framework can effectively leverage the unlabeled data by encouraging consistent predictions of the same input under different perturbations. Concretely, the framework consists of a student model and a teacher model, and the student model learns from the teacher model by minimizing a segmentation loss and a consistency loss with respect to the targets of the teacher model. We design a novel uncertainty-aware scheme to enable the student model to gradually learn from the meaningful and reliable targets by exploiting the uncertainty information. Experiments show that our method achieves high performance gains by incorporating the unlabeled data. Our method outperforms the state-of-the-art semi-supervised methods, demonstrating the potential of our framework for the challenging semi-supervised problems.",0
"The process of training deep convolutional neural networks typically necessitates a significant quantity of labeled data, which can be costly and time-consuming to obtain for medical image segmentation tasks. In this article, we introduce a new uncertainty-aware semi-supervised system for the segmentation of the left atrium from 3D MR images. Our approach can effectively employ unlabeled data by promoting consistent predictions of the same input under various perturbations. Our system comprises a student model and a teacher model, with the student model learning from the teacher model by minimizing segmentation and consistency losses with respect to the targets of the teacher model. To allow the student model to gradually learn from the meaningful and reliable targets by utilizing uncertainty information, we have devised an innovative uncertainty-aware scheme. As demonstrated by our experiments, our technique achieves substantial performance improvements by integrating the unlabeled data. Our method surpasses the existing semi-supervised methods, indicating the potential of our system for challenging semi-supervised issues.",1
"Semantic Segmentation is an important module for autonomous robots such as self-driving cars. The advantage of video segmentation approaches compared to single image segmentation is that temporal image information is considered, and their performance increases due to this. Hence, single image segmentation approaches are extended by recurrent units such as convolutional LSTM (convLSTM) cells, which are placed at suitable positions in the basic network architecture. However, a major critique of video segmentation approaches based on recurrent neural networks is their large parameter count and their computational complexity, and so, their inference time of one video frame takes up to 66 percent longer than their basic version. Inspired by the success of the spatial and depthwise separable convolutional neural networks, we generalize these techniques for convLSTMs in this work, so that the number of parameters and the required FLOPs are reduced significantly. Experiments on different datasets show that the segmentation approaches using the proposed, modified convLSTM cells achieve similar or slightly worse accuracy, but are up to 15 percent faster on a GPU than the ones using the standard convLSTM cells. Furthermore, a new evaluation metric is introduced, which measures the amount of flickering pixels in the segmented video sequence.",0
"Autonomous robots, such as self-driving cars, rely heavily on Semantic Segmentation. Video segmentation approaches offer an advantage over single image segmentation because they consider temporal image information, which results in improved performance. Recurrent units like convolutional LSTM cells are utilized to extend single image segmentation approaches and are placed strategically in the basic network architecture. However, a critical issue with video segmentation approaches based on recurrent neural networks is their large parameter count and computational complexity, leading to an inference time that takes 66 percent longer than their basic version. To address this problem, we have generalized the successful techniques of spatial and depthwise separable convolutional neural networks for convLSTMs in this study, resulting in a significant reduction in the number of parameters and required FLOPs. Experiments on various datasets have demonstrated that the proposed modified convLSTM cells achieve comparable or slightly worse accuracy but are up to 15 percent faster on a GPU than those using standard convLSTM cells. Additionally, we have introduced a new evaluation metric that quantifies the amount of flickering pixels in the segmented video sequence.",1
"Over the last two decades, deep learning has transformed the field of computer vision. Deep convolutional networks were successfully applied to learn different vision tasks such as image classification, image segmentation, object detection and many more. By transferring the knowledge learned by deep models on large generic datasets, researchers were further able to create fine-tuned models for other more specific tasks. Recently this idea was applied for regressing the absolute camera pose from an RGB image. Although the resulting accuracy was sub-optimal, compared to classic feature-based solutions, this effort led to a surge of learning-based pose estimation methods. Here, we review deep learning approaches for camera pose estimation. We describe key methods in the field and identify trends aiming at improving the original deep pose regression solution. We further provide an extensive cross-comparison of existing learning-based pose estimators, together with practical notes on their execution for reproducibility purposes. Finally, we discuss emerging solutions and potential future research directions.",0
"In the past 20 years, computer vision has been revolutionized by deep learning. Deep convolutional networks have been used to learn various visual tasks, such as image classification, object detection, and image segmentation. By applying the knowledge gained from large-scale generic datasets, researchers have been able to develop models for more specific tasks. One such task is estimating the absolute camera pose from an RGB image, which has led to the development of learning-based pose estimation methods. Despite lower accuracy compared to traditional feature-based solutions, these efforts have sparked a surge in research and development of deep learning approaches for camera pose estimation. In this article, we present a comprehensive review of these approaches, highlighting key methods and trends aimed at improving the original deep pose regression solution. We also provide practical guidance for reproducibility and discuss emerging solutions and potential future research directions.",1
"In this paper, an stereo-based traversability analysis approach for all terrains in off-road mobile robotics, e.g. Unmanned Ground Vehicles (UGVs) is proposed. This approach reformulates the problem of terrain traversability analysis into two main problems: (1) 3D terrain reconstruction and (2) terrain all surfaces detection and analysis. The proposed approach is using stereo camera for perception and 3D reconstruction of the terrain. In order to detect all the existing surfaces in the 3D reconstructed terrain as superpixel surfaces (i.e. segments), an image segmentation technique is applied using geometry-based features (pixel-based surface normals). Having detected all the surfaces, Superpixel Surface Traversability Analysis approach (SSTA) is applied on all of the detected surfaces (superpixel segments) in order to classify them based on their traversability index. The proposed SSTA approach is based on: (1) Superpixel surface normal and plane estimation, (2) Traversability analysis using superpixel surface planes. Having analyzed all the superpixel surfaces based on their traversability, these surfaces are finally classified into five main categories as following: traversable, semi-traversable, non-traversable, unknown and undecided.",0
"This paper presents a stereo-based approach for analyzing the traversability of all terrains in off-road mobile robotics, specifically Unmanned Ground Vehicles (UGVs). The approach involves two main problems: 3D terrain reconstruction and detection/analysis of all surfaces. Stereo cameras are used to perceive and reconstruct the terrain, while an image segmentation technique using geometry-based features is applied to detect all surfaces as superpixel segments. These segments are then analyzed using the Superpixel Surface Traversability Analysis (SSTA) approach, which involves estimating superpixel surface normals and planes, and analyzing their traversability. The resulting surfaces are classified into five categories: traversable, semi-traversable, non-traversable, unknown, and undecided.",1
"To improve the classification performance in the context of hyperspectral image processing, many works have been developed based on two common strategies, namely the spatial-spectral information integration and the utilization of neural networks. However, both strategies typically require more training data than the classical algorithms, aggregating the shortage of labeled samples. In this letter, we propose a novel framework that organically combines the spectrum-based deep metric learning model and the conditional random field algorithm. The deep metric learning model is supervised by the center loss to produce spectrum-based features that gather more tightly in Euclidean space within classes. The conditional random field with Gaussian edge potentials, which is firstly proposed for image segmentation tasks, is introduced to give the pixel-wise classification over the hyperspectral image by utilizing both the geographical distances between pixels and the Euclidean distances between the features produced by the deep metric learning model. The proposed framework is trained by spectral pixels at the deep metric learning stage and utilizes the half handcrafted spatial features at the conditional random field stage. This settlement alleviates the shortage of training data to some extent. Experiments on two real hyperspectral images demonstrate the advantages of the proposed method in terms of both classification accuracy and computation cost.",0
"Numerous techniques have been developed to enhance the classification performance of hyperspectral image processing. Two common strategies include the integration of spatial-spectral information and the use of neural networks. However, these approaches typically require more training data than classical algorithms, which poses a challenge due to the lack of labeled samples. In this study, we suggest a new framework that combines a deep metric learning model based on spectra and a conditional random field algorithm. The deep metric learning model uses center loss to generate spectrum-based features that are more closely grouped in Euclidean space within classes. The conditional random field algorithm, which was initially designed for image segmentation tasks, is introduced to enable pixel-wise classification across the hyperspectral image by incorporating both the distance between pixels and the distance between features generated by the deep metric learning model. The proposed framework is trained using spectral pixels in the deep metric learning stage and utilizes half handcrafted spatial features in the conditional random field stage, thereby easing the shortage of training data to some extent. Experimental results on two real hyperspectral images demonstrate that the proposed method offers advantages in terms of both classification accuracy and computation cost.",1
"Deep neural networks have achieved tremendous success in various fields including medical image segmentation. However, they have long been criticized for being a black-box, in that interpretation, understanding and correcting architectures is difficult as there is no general theory for deep neural network design. Previously, precision learning was proposed to fuse deep architectures and traditional approaches. Deep networks constructed in this way benefit from the original known operator, have fewer parameters, and improved interpretability. However, they do not yield state-of-the-art performance in all applications. In this paper, we propose to analyze deep networks using known operators, by adopting a divide-and-conquer strategy to replace network components, whilst retaining its performance. The task of retinal vessel segmentation is investigated for this purpose. We start with a high-performance U-Net and show by step-by-step conversion that we are able to divide the network into modules of known operators. The results indicate that a combination of a trainable guided filter and a trainable version of the Frangi filter yields a performance at the level of U-Net (AUC 0.974 vs. 0.972) with a tremendous reduction in parameters (111,536 vs. 9,575). In addition, the trained layers can be mapped back into their original algorithmic interpretation and analyzed using standard tools of signal processing.",0
"Despite their success in various fields, including medical image segmentation, deep neural networks have faced criticism for their black-box nature, making interpretation, understanding, and correction of architectures difficult due to a lack of a general theory for their design. While precision learning has been proposed as a solution to fuse deep architectures with traditional approaches, these networks do not always achieve state-of-the-art performance. To address this, we propose a divide-and-conquer strategy to replace network components with known operators while retaining performance, focusing on the task of retinal vessel segmentation. By step-by-step conversion of a high-performance U-Net, we demonstrate that a combination of a trainable guided filter and a trainable version of the Frangi filter yields comparable performance with a significant reduction in parameters, allowing for analysis using standard tools of signal processing.",1
"Deep learning approaches have achieved state-of-the-art performance in cardiac magnetic resonance (CMR) image segmentation. However, most approaches have focused on learning image intensity features for segmentation, whereas the incorporation of anatomical shape priors has received less attention. In this paper, we combine a multi-task deep learning approach with atlas propagation to develop a shape-constrained bi-ventricular segmentation pipeline for short-axis CMR volumetric images. The pipeline first employs a fully convolutional network (FCN) that learns segmentation and landmark localisation tasks simultaneously. The architecture of the proposed FCN uses a 2.5D representation, thus combining the computational advantage of 2D FCNs networks and the capability of addressing 3D spatial consistency without compromising segmentation accuracy. Moreover, the refinement step is designed to explicitly enforce a shape constraint and improve segmentation quality. This step is effective for overcoming image artefacts (e.g. due to different breath-hold positions and large slice thickness), which preclude the creation of anatomically meaningful 3D cardiac shapes. The proposed pipeline is fully automated, due to network's ability to infer landmarks, which are then used downstream in the pipeline to initialise atlas propagation. We validate the pipeline on 1831 healthy subjects and 649 subjects with pulmonary hypertension. Extensive numerical experiments on the two datasets demonstrate that our proposed method is robust and capable of producing accurate, high-resolution and anatomically smooth bi-ventricular 3D models, despite the artefacts in input CMR volumes.",0
"State-of-the-art performance in cardiac magnetic resonance (CMR) image segmentation has been achieved through deep learning approaches. However, these approaches have mainly focused on learning image intensity features for segmentation, with less attention given to the incorporation of anatomical shape priors. In this study, we developed a shape-constrained bi-ventricular segmentation pipeline for short-axis CMR volumetric images by combining a multi-task deep learning approach with atlas propagation. The pipeline utilizes a fully convolutional network (FCN) that simultaneously learns segmentation and landmark localisation tasks, using a 2.5D representation for addressing 3D spatial consistency without compromising segmentation accuracy. The refinement step enforces a shape constraint to improve segmentation quality and overcome image artefacts. The proposed pipeline is fully automated, with landmarks inferred by the network and used downstream to initialise atlas propagation. The pipeline was validated on 1831 healthy subjects and 649 subjects with pulmonary hypertension, demonstrating its robustness and capability of producing accurate, high-resolution, and anatomically smooth bi-ventricular 3D models despite artefacts in input CMR volumes.",1
"The machine learning community has been overwhelmed by a plethora of deep learning based approaches. Many challenging computer vision tasks such as detection, localization, recognition and segmentation of objects in unconstrained environment are being efficiently addressed by various types of deep neural networks like convolutional neural networks, recurrent networks, adversarial networks, autoencoders and so on. While there have been plenty of analytical studies regarding the object detection or recognition domain, many new deep learning techniques have surfaced with respect to image segmentation techniques. This paper approaches these various deep learning techniques of image segmentation from an analytical perspective. The main goal of this work is to provide an intuitive understanding of the major techniques that has made significant contribution to the image segmentation domain. Starting from some of the traditional image segmentation approaches, the paper progresses describing the effect deep learning had on the image segmentation domain. Thereafter, most of the major segmentation algorithms have been logically categorized with paragraphs dedicated to their unique contribution. With an ample amount of intuitive explanations, the reader is expected to have an improved ability to visualize the internal dynamics of these processes.",0
"A multitude of deep learning approaches have inundated the machine learning community, effectively tackling challenging computer vision tasks such as object detection, localization, recognition, and segmentation in unconstrained environments through various types of deep neural networks. Although numerous analytical studies have been conducted on object detection and recognition, the emergence of new deep learning techniques for image segmentation has garnered attention. This paper offers an analytical perspective on the various deep learning techniques for image segmentation, with the objective of providing readers with an intuitive understanding of the major techniques that have made significant contributions to this domain. The paper begins with traditional image segmentation approaches before delving into the impact of deep learning on this field. Major segmentation algorithms are logically categorized with dedicated paragraphs discussing their unique contributions, with ample intuitive explanations provided to enhance readers' visualization of the internal dynamics of these processes.",1
"Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. Here, we propose a new two-stream CNN architecture for semantic segmentation that explicitly wires shape information as a separate processing branch, i.e. shape stream, that processes information in parallel to the classical stream. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines.",0
"The latest techniques for image segmentation use a deep CNN to process color, shape, and texture information together, which may not be optimal since these types of information are different and important for recognition. We propose a new two-stream CNN architecture that separates shape information into its own processing branch, called the shape stream, which runs parallel to the classical stream. The architecture uses new gate connections to remove noise and help the shape stream focus only on boundary-related information. This allows us to use a shallow architecture for the shape stream that works on image-level resolution. Our experiments demonstrate that this approach produces sharper predictions around object boundaries and significantly improves performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, showing improvements of 2% and 4% over strong baselines in mask (mIoU) and boundary (F-score) quality, respectively.",1
"Semantic segmentation is a crucial task in biomedical image processing, which recent breakthroughs in deep learning have allowed to improve. However, deep learning methods in general are not yet widely used in practice since they require large amount of data for training complex models. This is particularly challenging for biomedical images, because data and ground truths are a scarce resource. Annotation efforts for biomedical images come with a real cost, since experts have to manually label images at pixel-level on samples usually containing many instances of the target anatomy (e.g. in histology samples: neurons, astrocytes, mitochondria, etc.). In this paper we provide a framework for Deep Active Learning applied to a real-world scenario. Our framework relies on the U-Net architecture and overall uncertainty measure to suggest which sample to annotate. It takes advantage of the uncertainty measure obtained by taking Monte Carlo samples while using Dropout regularization scheme. Experiments were done on spinal cord and brain microscopic histology samples to perform a myelin segmentation task. Two realistic small datasets of 14 and 24 images were used, from different acquisition settings (Serial Block-Face Electron Microscopy and Transmitting Electron Microscopy) and showed that our method reached a maximum Dice value after adding 3 uncertainty-selected samples to the initial training set, versus 15 randomly-selected samples, thereby significantly reducing the annotation effort. We focused on a plausible scenario and showed evidence that this straightforward implementation achieves a high segmentation performance with very few labelled samples. We believe our framework may benefit any biomedical researcher willing to obtain fast and accurate image segmentation on their own dataset. The code is freely available at https://github.com/neuropoly/deep-active-learning.",0
"Recent advancements in deep learning have greatly enhanced the significance of semantic segmentation in biomedical image processing. However, deep learning techniques are not widely utilized in practice due to the requirement of vast amounts of data to train complex models. This is especially challenging in the case of biomedical images, as the data and ground truths are scarce and the annotation process is costly and time-consuming. In this paper, we propose a Deep Active Learning framework that utilizes the U-Net architecture and an overall uncertainty measure to suggest which sample to annotate. We conduct experiments on two small datasets of spinal cord and brain microscopic histology samples and show that our method achieves high segmentation performance with very few labeled samples. Our framework reduces annotation effort significantly and can benefit any biomedical researcher seeking fast and accurate image segmentation on their dataset. The code for our framework is available on GitHub.",1
"Although numerous improvements have been made in the field of image segmentation using convolutional neural networks, the majority of these improvements rely on training with larger datasets, model architecture modifications, novel loss functions, and better optimizers. In this paper, we propose a new segmentation performance boosting paradigm that relies on optimally modifying the network's input instead of the network itself. In particular, we leverage the gradients of a trained segmentation network with respect to the input to transfer it to a space where the segmentation accuracy improves. We test the proposed method on three publicly available medical image segmentation datasets: the ISIC 2017 Skin Lesion Segmentation dataset, the Shenzhen Chest X-Ray dataset, and the CVC-ColonDB dataset, for which our method achieves improvements of 5.8%, 0.5%, and 4.8% in the average Dice scores, respectively.",0
"Despite the various advancements made in the field of image segmentation using convolutional neural networks, most of the improvements rely on bigger datasets, modifications to the model architecture, novel loss functions, and more efficient optimizers. This paper introduces a new approach to enhancing segmentation performance, which involves optimizing the network's input instead of the network itself. Specifically, we utilize the trained segmentation network's gradients in relation to the input to transfer it to a space where segmentation accuracy is enhanced. We evaluate our proposal on three publicly available medical image segmentation datasets, namely the ISIC 2017 Skin Lesion Segmentation dataset, the Shenzhen Chest X-Ray dataset, and the CVC-ColonDB dataset, where our approach achieves an average Dice score improvement of 5.8%, 0.5%, and 4.8%, respectively.",1
"Selective segmentation involves incorporating user input to partition an image into foreground and background, by discriminating between objects of a similar type. Typically, such methods involve introducing additional constraints to generic segmentation approaches. However, we show that this is often inconsistent with respect to common assumptions about the image. The proposed method introduces a new fitting term that is more useful in practice than the Chan-Vese framework. In particular, the idea is to define a term that allows for the background to consist of multiple regions of inhomogeneity. We provide comparitive experimental results to alternative approaches to demonstrate the advantages of the proposed method, broadening the possible application of these methods.",0
"Selective segmentation utilizes user input to divide an image into foreground and background by distinguishing between similar objects. This method typically involves imposing more restrictions on standard segmentation techniques. However, our research demonstrates that this approach can be at odds with common assumptions about the image. Instead, we suggest a new fitting term that is more practical than the Chan-Vese framework. Specifically, we propose a term that enables the background to consist of multiple regions of non-uniformity. Through comparative experiments, we demonstrate the superiority of our method over alternative approaches, expanding the potential uses of these techniques.",1
"In the recent years, convolutional neural networks have transformed the field of medical image analysis due to their capacity to learn discriminative image features for a variety of classification and regression tasks. However, successfully learning these features requires a large amount of manually annotated data, which is expensive to acquire and limited by the available resources of expert image analysts. Therefore, unsupervised, weakly-supervised and self-supervised feature learning techniques receive a lot of attention, which aim to utilise the vast amount of available data, while at the same time avoid or substantially reduce the effort of manual annotation. In this paper, we propose a novel way for training a cardiac MR image segmentation network, in which features are learnt in a self-supervised manner by predicting anatomical positions. The anatomical positions serve as a supervisory signal and do not require extra manual annotation. We demonstrate that this seemingly simple task provides a strong signal for feature learning and with self-supervised learning, we achieve a high segmentation accuracy that is better than or comparable to a U-net trained from scratch, especially at a small data setting. When only five annotated subjects are available, the proposed method improves the mean Dice metric from 0.811 to 0.852 for short-axis image segmentation, compared to the baseline U-net.",0
"Convolutional neural networks have revolutionized medical image analysis by enabling the learning of discriminative image features for classification and regression tasks. However, acquiring the vast amount of annotated data required for successful feature learning is expensive and limited by the availability of expert image analysts. Consequently, there is growing interest in unsupervised, weakly-supervised, and self-supervised feature learning techniques that aim to leverage available data while reducing manual annotation efforts. This paper presents a novel self-supervised approach for training a cardiac MR image segmentation network using anatomical position prediction as a supervisory signal. This approach achieves high segmentation accuracy, outperforming a U-net trained from scratch, particularly with limited data. The proposed method improves the mean Dice metric from 0.811 to 0.852 for short-axis image segmentation, compared to the baseline U-net, with only five annotated subjects.",1
"With the recent advances in complex networks theory, graph-based techniques for image segmentation has attracted great attention recently. In order to segment the image into meaningful connected components, this paper proposes an image segmentation general framework using complex networks based community detection algorithms. If we consider regions as communities, using community detection algorithms directly can lead to an over-segmented image. To address this problem, we start by splitting the image into small regions using an initial segmentation. The obtained regions are used for building the complex network. To produce meaningful connected components and detect homogeneous communities, some combinations of color and texture based features are employed in order to quantify the regions similarities. To sum up, the network of regions is constructed adaptively to avoid many small regions in the image, and then, community detection algorithms are applied on the resulting adaptive similarity matrix to obtain the final segmented image. Experiments are conducted on Berkeley Segmentation Dataset and four of the most influential community detection algorithms are tested. Experimental results have shown that the proposed general framework increases the segmentation performances compared to some existing methods.",0
"Recently, graph-based techniques for image segmentation have gained significant attention due to the advancements in complex networks theory. This paper proposes a general framework for image segmentation using complex networks-based community detection algorithms to segment images into meaningful connected components. However, using community detection algorithms directly on regions can lead to an over-segmented image. To solve this problem, the paper suggests splitting the image into small regions using an initial segmentation and then employing color and texture-based features to quantify region similarities to detect homogeneous communities. The network of regions is constructed adaptively to avoid many small regions in the image, and community detection algorithms are applied on the resulting adaptive similarity matrix to obtain the final segmented image. Experiments conducted on the Berkeley Segmentation Dataset using four of the most influential community detection algorithms have shown that the proposed framework outperforms some existing methods.",1
"In this paper we test the use of a deep learning approach to automatically count Wandering Albatrosses in Very High Resolution (VHR) satellite imagery. We use a dataset of manually labelled imagery provided by the British Antarctic Survey to train and develop our methods. We employ a U-Net architecture, designed for image segmentation, to simultaneously classify and localise potential albatrosses. We aid training with the use of the Focal Loss criterion, to deal with extreme class imbalance in the dataset. Initial results achieve peak precision and recall values of approximately 80%. Finally we assess the model's performance in relation to inter-observer variation, by comparing errors against an image labelled by multiple observers. We conclude model accuracy falls within the range of human counters. We hope that the methods will streamline the analysis of VHR satellite images, enabling more frequent monitoring of a species which is of high conservation concern.",0
"The objective of this study is to evaluate the effectiveness of deep learning techniques in counting Wandering Albatrosses using Very High Resolution (VHR) satellite imagery. The methodology involves using a U-Net architecture for image segmentation to classify and locate potential albatrosses, trained on a dataset provided by the British Antarctic Survey. To address class imbalance, the Focal Loss criterion is employed during training. The initial results indicate a peak precision and recall rate of around 80%. Furthermore, the model's performance is compared to that of multiple observers to assess inter-observer variation. The study concludes that the accuracy of the model is comparable to that of human counters. The proposed method is expected to enhance the analysis of VHR satellite images, making it easier to monitor a species that is of significant conservation importance.",1
"Extracting texts of various size and shape from images containing multiple objects is an important problem in many contexts, especially, in connection to e-commerce, augmented reality assistance system in natural scene, etc. The existing works (based on only CNN) often perform sub-optimally when the image contains regions of high entropy having multiple objects. This paper presents an end-to-end text detection strategy combining a segmentation algorithm and an ensemble of multiple text detectors of different types to detect text in every individual image segments independently. The proposed strategy involves a super-pixel based image segmenter which splits an image into multiple regions. A convolutional deep neural architecture is developed which works on each of the segments and detects texts of multiple shapes, sizes, and structures. It outperforms the competing methods in terms of coverage in detecting texts in images especially the ones where the text of various types and sizes are compacted in a small region along with various other objects. Furthermore, the proposed text detection method along with a text recognizer outperforms the existing state-of-the-art approaches in extracting text from high entropy images. We validate the results on a dataset consisting of product images on an e-commerce website.",0
"The detection of text in images that contain multiple objects is a significant issue in various contexts, particularly in relation to e-commerce and augmented reality assistance systems in natural scenes. CNN-based methods that are currently available often perform inadequately when images contain regions of high entropy with several objects. This study introduces an end-to-end text detection strategy that combines a segmentation algorithm with multiple text detectors of different types to independently detect text in each image segment. The proposed approach involves a super-pixel-based image segmenter that divides an image into several regions. A convolutional deep neural architecture is developed to identify texts of different shapes, sizes, and structures in each segment, outperforming other methods in detecting texts in images that contain various types and sizes of text that are compacted with other objects in a limited region. Moreover, the proposed text detection method, along with a text recognizer, surpasses existing state-of-the-art methods in extracting text from high entropy images. The outcomes of this research are validated using a dataset of product images on an e-commerce website.",1
"Advances in the image-based diagnostics of complex biological and manufacturing processes have brought unsupervised image segmentation to the forefront of enabling automated, on the fly decision making. However, most existing unsupervised segmentation approaches are either computationally complex or require manual parameter selection (e.g., flow capacities in max-flow/min-cut segmentation). In this work, we present a fully unsupervised segmentation approach using a continuous max-flow formulation over the image domain while optimally estimating the flow parameters from the image characteristics. More specifically, we show that the maximum a posteriori estimate of the image labels can be formulated as a continuous max-flow problem given the flow capacities are known. The flow capacities are then iteratively obtained by employing a novel Markov random field prior over the image domain. We present theoretical results to establish the posterior consistency of the flow capacities. We compare the performance of our approach on two real-world case studies including brain tumor image segmentation and defect identification in additively manufactured components using electron microscopic images. Comparative results with several state-of-the-art supervised as well as unsupervised methods suggest that the present method performs statistically similar to the supervised methods, but results in more than 90% improvement in the Dice score when compared to the state-of-the-art unsupervised methods.",0
"Recently, advancements in image-based diagnostics for complex biological and manufacturing processes have brought unsupervised image segmentation to the forefront of enabling automated decision making. However, most existing unsupervised segmentation methods are either computationally complex or require manual parameter selection. In this study, we introduce a fully unsupervised segmentation approach utilizing a continuous max-flow formulation over the image domain, while optimally estimating flow parameters from image characteristics. By employing a novel Markov random field prior over the image domain, we obtain flow capacities iteratively, establishing their posterior consistency through theoretical results. We compare the performance of our approach on two real-world case studies, including brain tumor image segmentation and defect identification in additively manufactured components using electron microscopic images. Results show that our method performs statistically similar to supervised methods, but with over 90% improvement in the Dice score compared to state-of-the-art unsupervised methods.",1
"Convolutional neural networks (CNNs) show outstanding performance in many image processing problems, such as image recognition, object detection and image segmentation. Semantic segmentation is a very challenging task that requires recognizing, understanding what's in the image in pixel level. Though the state of the art has been greatly improved by CNNs, there is no explicit connections between prediction of neighbouring pixels. That is, spatial regularity of the segmented objects is still a problem for CNNs. In this paper, we propose a method to add spatial regularization to the segmented objects. In our method, the spatial regularization such as total variation (TV) can be easily integrated into CNN network. It can help CNN find a better local optimum and make the segmentation results more robust to noise. We apply our proposed method to Unet and Segnet, which are well established CNNs for image segmentation, and test them on WBC, CamVid and SUN-RGBD datasets, respectively. The results show that the regularized networks not only could provide better segmentation results with regularization effect than the original ones but also have certain robustness to noise.",0
"CNNs are highly effective in solving various image processing problems, including image recognition, object detection, and image segmentation. However, semantic segmentation is a complex task that requires recognizing and understanding the image at a pixel level. Despite CNNs' advanced performance, there are no explicit connections between neighboring pixels' predictions, leading to spatial regularity issues in segmented objects. In this study, we propose a method to integrate spatial regularization, such as total variation (TV), into CNN networks to address this issue. This approach assists in finding a better local optimum and enhances the segmentation results' resilience to noise. Our method is applied to established CNNs for image segmentation, including Unet and Segnet, and tested on WBC, CamVid, and SUN-RGBD datasets. Our results demonstrate that the regularized networks not only provide better segmentation outcomes but also have a certain level of noise robustness.",1
"Deep learning has been used as a powerful tool for various tasks in computer vision, such as image segmentation, object recognition and data generation. A key part of end-to-end training is designing the appropriate encoder to extract specific features from the input data. However, few encoders maintain the topological properties of data, such as connection structures and global contours. In this paper, we introduce a Voronoi Diagram encoder based on convex set distance (CSVD) and apply it in edge encoding. The boundaries of Voronoi cells is related to detected edges of structures and contours. The CSVD model improves contour extraction in CNN and structure generation in GAN. We also show the experimental results and demonstrate that the proposed model has great potentiality in different visual problems where topology information should be involved.",0
"Various tasks in computer vision, such as image segmentation, object recognition, and data generation, have benefited from the use of deep learning as a powerful tool. To achieve end-to-end training, it is essential to design an appropriate encoder that can extract specific features from input data. However, most encoders fail to maintain the topological properties of data, including connection structures and global contours. This paper introduces a Voronoi Diagram encoder, which is based on convex set distance (CSVD) and is applied in edge encoding. The Voronoi cell boundaries are related to detected edges of structures and contours. The CSVD model enhances contour extraction in CNN and structure generation in GAN. Experimental results demonstrate that the proposed model has significant potential in addressing various visual problems that require topology information.",1
"To evaluate their performance, existing dehazing approaches generally rely on distance measures between the generated image and its corresponding ground truth. Despite its ability to produce visually good images, using pixel-based or even perceptual metrics do not guarantee, in general, that the produced image is fit for being used as input for low-level computer vision tasks such as segmentation. To overcome this weakness, we are proposing a novel end-to-end approach for image dehazing, fit for being used as input to an image segmentation procedure, while maintaining the visual quality of the generated images. Inspired by the success of Generative Adversarial Networks (GAN), we propose to optimize the generator by introducing a discriminator network and a loss function that evaluates segmentation quality of dehazed images. In addition, we make use of a supplementary loss function that verifies that the visual and the perceptual quality of the generated image are preserved in hazy conditions. Results obtained using the proposed technique are appealing, with a favorable comparison to state-of-the-art approaches when considering the performance of segmentation algorithms on the hazy images.",0
"Commonly, existing methods for dehazing assess their effectiveness by measuring the distance between the produced image and its corresponding ground truth. However, even though the images may appear visually pleasing, relying on pixel-based or perceptual metrics may not guarantee that they are suitable for low-level computer vision tasks like segmentation. To address this issue, we propose an end-to-end approach for dehazing images that can be used as input for segmentation while maintaining visual quality. Inspired by the success of Generative Adversarial Networks (GAN), we introduce a discriminator network and a loss function that assesses the segmentation quality of dehazed images. Additionally, we utilize a supplementary loss function to ensure that the visual and perceptual quality of the image remains intact in hazy conditions. Our proposed technique produces promising results, outperforming state-of-the-art methods in terms of segmentation performance on hazy images.",1
"Semantic segmentation is essentially important to biomedical image analysis. Many recent works mainly focus on integrating the Fully Convolutional Network (FCN) architecture with sophisticated convolution implementation and deep supervision. In this paper, we propose to decompose the single segmentation task into three subsequent sub-tasks, including (1) pixel-wise image segmentation, (2) prediction of the class labels of the objects within the image, and (3) classification of the scene the image belonging to. While these three sub-tasks are trained to optimize their individual loss functions of different perceptual levels, we propose to let them interact by the task-task context ensemble. Moreover, we propose a novel sync-regularization to penalize the deviation between the outputs of the pixel-wise segmentation and the class prediction tasks. These effective regularizations help FCN utilize context information comprehensively and attain accurate semantic segmentation, even though the number of the images for training may be limited in many biomedical applications. We have successfully applied our framework to three diverse 2D/3D medical image datasets, including Robotic Scene Segmentation Challenge 18 (ROBOT18), Brain Tumor Segmentation Challenge 18 (BRATS18), and Retinal Fundus Glaucoma Challenge (REFUGE18). We have achieved top-tier performance in all three challenges.",0
"The importance of semantic segmentation in biomedical image analysis cannot be overstated. Recent works have primarily focused on incorporating sophisticated convolution implementation and deep supervision into the Fully Convolutional Network (FCN) architecture. However, this paper proposes a different approach by dividing the single segmentation task into three sub-tasks: pixel-wise image segmentation, object class label prediction, and scene classification. Each sub-task is optimized through its individual loss functions, but they are allowed to interact through the task-task context ensemble. Furthermore, a novel sync-regularization is suggested to minimize the deviation between the outputs of the pixel-wise segmentation and the class prediction tasks. These regularizations enable FCN to make the most of context information and produce accurate semantic segmentation, even with limited training data in biomedical applications. The proposed framework is successfully applied to three different medical image datasets, namely ROBOT18, BRATS18, and REFUGE18, where it emerges as a top-performing method.",1
"One of the key drawbacks of 3D convolutional neural networks for segmentation is their memory footprint, which necessitates compromises in the network architecture in order to fit into a given memory budget. Motivated by the RevNet for image classification, we propose a partially reversible U-Net architecture that reduces memory consumption substantially. The reversible architecture allows us to exactly recover each layer's outputs from the subsequent layer's ones, eliminating the need to store activations for backpropagation. This alleviates the biggest memory bottleneck and enables very deep (theoretically infinitely deep) 3D architectures. On the BraTS challenge dataset, we demonstrate substantial memory savings. We further show that the freed memory can be used for processing the whole field-of-view (FOV) instead of patches. Increasing network depth led to higher segmentation accuracy while growing the memory footprint only by a very small fraction, thanks to the partially reversible architecture.",0
"The memory footprint of 3D convolutional neural networks for segmentation is a significant drawback, as it requires compromising the network architecture to fit within a given memory budget. To address this issue, we propose a partially reversible U-Net architecture inspired by the RevNet for image classification, which substantially reduces memory consumption. With this reversible architecture, storing activations for backpropagation is unnecessary, and each layer's outputs can be precisely recovered from the subsequent layer's ones. This eliminates the biggest memory bottleneck, enabling very deep 3D architectures. We demonstrate significant memory savings on the BraTS challenge dataset, freeing up memory that can be utilized for processing the entire field-of-view instead of patches. Moreover, the partially reversible architecture allows for increased network depth and higher segmentation accuracy without a major increase in memory footprint.",1
"This paper presents a novel unsupervised domain adaptation framework, called Synergistic Image and Feature Adaptation (SIFA), to effectively tackle the problem of domain shift. Domain adaptation has become an important and hot topic in recent studies on deep learning, aiming to recover performance degradation when applying the neural networks to new testing domains. Our proposed SIFA is an elegant learning diagram which presents synergistic fusion of adaptations from both image and feature perspectives. In particular, we simultaneously transform the appearance of images across domains and enhance domain-invariance of the extracted features towards the segmentation task. The feature encoder layers are shared by both perspectives to grasp their mutual benefits during the end-to-end learning procedure. Without using any annotation from the target domain, the learning of our unified model is guided by adversarial losses, with multiple discriminators employed from various aspects. We have extensively validated our method with a challenging application of cross-modality medical image segmentation of cardiac structures. Experimental results demonstrate that our SIFA model recovers the degraded performance from 17.2% to 73.0%, and outperforms the state-of-the-art methods by a significant margin.",0
"The purpose of this article is to introduce a new approach to unsupervised domain adaptation called Synergistic Image and Feature Adaptation (SIFA), which aims to address the issue of domain shift effectively. Domain adaptation has emerged as a key topic in recent research on deep learning, as it seeks to overcome performance degradation when applying neural networks to new testing domains. The SIFA framework is a sophisticated learning model that combines image and feature adaptations in a synergistic manner. It focuses on transforming the appearance of images across domains while enhancing the domain-invariance of the extracted features for segmentation tasks. The feature encoder layers are shared by both perspectives to maximize the benefits of end-to-end learning. The model is trained using adversarial losses guided by multiple discriminators from various aspects, without requiring any annotation from the target domain. The efficacy of our approach has been extensively tested using cross-modality medical image segmentation of cardiac structures, demonstrating a significant improvement in performance compared to state-of-the-art methods, with our SIFA model increasing performance from 17.2% to 73.0%.",1
"Deep learning based models, generally, require a large number of samples for appropriate training, a requirement that is difficult to satisfy in the medical field. This issue can usually be avoided with a proper initialization of the weights. On the task of medical image segmentation in general, two techniques are oftentimes employed to tackle the training of a deep network $f_T$. The first one consists in reusing some weights of a network $f_S$ pre-trained on a large scale database ($e.g.$ ImageNet). This procedure, also known as $transfer$ $learning$, happens to reduce the flexibility when it comes to new network design since $f_T$ is constrained to match some parts of $f_S$. The second commonly used technique consists in working on image patches to benefit from the large number of available patches. This paper brings together these two techniques and propose to train $arbitrarily$ $designed$ $networks$ that segment an image in one forward pass, with a focus on relatively small databases. An experimental work have been carried out on the tasks of retinal blood vessel segmentation and the optic disc one, using four publicly available databases. Furthermore, three types of network are considered, going from a very light weighted network to a densely connected one. The final results show the efficiency of the proposed framework along with state of the art results on all the databases.",0
"In the medical field, it is challenging to meet the large sample requirement for appropriate training of deep learning models. However, this issue can be mitigated by initializing weights correctly. When training deep network $f_T$ for medical image segmentation, two techniques are commonly used. The first method involves reusing weights from a network $f_S$ pre-trained on a large-scale database like ImageNet, which limits the flexibility of designing a new network. The second technique involves working on image patches to benefit from the availability of numerous patches. This paper proposes combining these two techniques to train arbitrarily designed networks for image segmentation in one forward pass, with a focus on relatively small databases. The experimental work conducted on retinal blood vessel segmentation and the optic disc shows that the proposed framework is efficient and yields state-of-the-art results on all databases, using three network types ranging from lightweight to densely connected.",1
"3D image segmentation is one of the most important and ubiquitous problems in medical image processing. It provides detailed quantitative analysis for accurate disease diagnosis, abnormal detection, and classification. Currently deep learning algorithms are widely used in medical image segmentation, most algorithms trained models with full annotated datasets. However, obtaining medical image datasets is very difficult and expensive, and full annotation of 3D medical image is a monotonous and time-consuming work. Partially labelling informative slices in 3D images will be a great relief of manual annotation. Sample selection strategies based on active learning have been proposed in the field of 2D image, but few strategies focus on 3D images. In this paper, we propose a sparse annotation strategy based on attention-guided active learning for 3D medical image segmentation. Attention mechanism is used to improve segmentation accuracy and estimate the segmentation accuracy of each slice. The comparative experiments with three different strategies using datasets from the developing human connectome project (dHCP) show that, our strategy only needs 15% to 20% annotated slices in brain extraction task and 30% to 35% annotated slices in tissue segmentation task to achieve comparative results as full annotation.",0
"Medical image processing involves a crucial problem of 3D image segmentation, which facilitates accurate disease diagnosis, abnormal detection, and classification. Although deep learning algorithms are widely employed in medical image segmentation, obtaining fully annotated datasets is challenging and expensive, and manually annotating 3D medical images is tedious and time-consuming. To ease the burden of manual annotation, informative slices in 3D images can be partially labeled. While active learning-based sample selection strategies have been proposed for 2D images, few focus on 3D images. This paper presents an attention-guided active learning approach for 3D medical image segmentation, which employs an attention mechanism to improve segmentation accuracy and estimate the accuracy of each slice. Using datasets from the dHCP, our approach achieves results comparable to full annotation with only 15% to 20% annotated slices in brain extraction and 30% to 35% annotated slices in tissue segmentation.",1
"In this paper, a neural architecture search (NAS) framework is proposed for 3D medical image segmentation, to automatically optimize a neural architecture from a large design space. Our NAS framework searches the structure of each layer including neural connectivities and operation types in both of the encoder and decoder. Since optimizing over a large discrete architecture space is difficult due to high-resolution 3D medical images, a novel stochastic sampling algorithm based on a continuous relaxation is also proposed for scalable gradient based optimization. On the 3D medical image segmentation tasks with a benchmark dataset, an automatically designed architecture by the proposed NAS framework outperforms the human-designed 3D U-Net, and moreover this optimized architecture is well suited to be transferred for different tasks.",0
"This paper proposes a framework for neural architecture search (NAS) that is specifically designed for 3D medical image segmentation. The aim is to automatically optimize the neural architecture from a large design space. The framework searches the structure of each layer, including neural connectivities and operation types, in both the encoder and decoder. However, optimizing over a large discrete architecture space is challenging due to the high-resolution 3D medical images. To overcome this, a novel stochastic sampling algorithm is proposed based on continuous relaxation, which allows for scalable gradient-based optimization. Results show that the automatically designed architecture outperforms the human-designed 3D U-Net on 3D medical image segmentation tasks using a benchmark dataset. Additionally, this optimized architecture can be easily transferred for different tasks.",1
"Segmentation algorithms are prone to make topological errors on fine-scale structures, e.g., broken connections. We propose a novel method that learns to segment with correct topology. In particular, we design a continuous-valued loss function that enforces a segmentation to have the same topology as the ground truth, i.e., having the same Betti number. The proposed topology-preserving loss function is differentiable and we incorporate it into end-to-end training of a deep neural network. Our method achieves much better performance on the Betti number error, which directly accounts for the topological correctness. It also performs superiorly on other topology-relevant metrics, e.g., the Adjusted Rand Index and the Variation of Information. We illustrate the effectiveness of the proposed method on a broad spectrum of natural and biomedical datasets.",0
"The algorithms used for segmentation often make errors in fine-scale structures such as broken connections. To address this issue, we present a new approach that learns to segment while maintaining accurate topology. Specifically, we have developed a continuous-valued loss function that ensures the segmentation has the same Betti number as the ground truth. This topology-preserving loss function is differentiable and is integrated into the end-to-end training of a deep neural network. Our method performs significantly better in terms of Betti number error, which is a direct measure of topological correctness, as well as other relevant metrics such as the Adjusted Rand Index and the Variation of Information. We demonstrate the effectiveness of our approach on a variety of natural and biomedical datasets.",1
"Recent advances in deep learning for medical image segmentation demonstrate expert-level accuracy. However, in clinically realistic environments, such methods have marginal performance due to differences in image domains, including different imaging protocols, device vendors and patient populations. Here we consider the problem of domain generalization, when a model is trained once, and its performance generalizes to unseen domains. Intuitively, within a specific medical imaging modality the domain differences are smaller relative to natural images domain variability. We rethink data augmentation for medical 3D images and propose a deep stacked transformations (DST) approach for domain generalization. Specifically, a series of n stacked transformations are applied to each image in each mini-batch during network training to account for the contribution of domain-specific shifts in medical images. We comprehensively evaluate our method on three tasks: segmentation of whole prostate from 3D MRI, left atrial from 3D MRI, and left ventricle from 3D ultrasound. We demonstrate that when trained on a small source dataset, (i) on average, DST models on unseen datasets degrade only by 11% (Dice score change), compared to the conventional augmentation (degrading 39%) and CycleGAN-based domain adaptation method (degrading 25%); (ii) when evaluation on the same domain, DST is also better albeit only marginally. (iii) When training on large-sized data, DST on unseen domains reaches performance of state-of-the-art fully supervised models. These findings establish a strong benchmark for the study of domain generalization in medical imaging, and can be generalized to the design of robust deep segmentation models for clinical deployment.",0
"Although recent advancements in deep learning have enabled medical image segmentation to achieve expert-level precision, these techniques often perform poorly in realistic clinical settings due to differences in image domains, which may include variations in imaging protocols, device vendors, and patient populations. To address this challenge of domain generalization, we propose a deep stacked transformations (DST) approach to data augmentation for medical 3D images. By applying a series of n stacked transformations to each image in each mini-batch during network training, we can account for the contribution of domain-specific shifts in medical images. Our method is evaluated on three tasks: segmentation of whole prostate from 3D MRI, left atrial from 3D MRI, and left ventricle from 3D ultrasound. We find that DST models, when trained on a small source dataset, degrade only by 11% on average when evaluated on unseen datasets, compared to conventional augmentation (degrading 39%) and CycleGAN-based domain adaptation (degrading 25%). Moreover, when training on large-sized data, DST on unseen domains outperforms state-of-the-art fully supervised models. These results provide a promising benchmark for domain generalization in medical imaging and offer insights into the development of robust deep segmentation models for clinical deployment.",1
"Adversarial learning has been proven to be effective for capturing long-range and high-level label consistencies in semantic segmentation. Unique to medical imaging, capturing 3D semantics in an effective yet computationally efficient way remains an open problem. In this study, we address this computational burden by proposing a novel projective adversarial network, called PAN, which incorporates high-level 3D information through 2D projections. Furthermore, we introduce an attention module into our framework that helps for a selective integration of global information directly from our segmentor to our adversarial network. For the clinical application we chose pancreas segmentation from CT scans. Our proposed framework achieved state-of-the-art performance without adding to the complexity of the segmentor.",0
"The effectiveness of adversarial learning in semantic segmentation has been proven to capture high-level label consistencies and long-range patterns. However, capturing 3D semantics in medical imaging remains a challenge due to computational inefficiency. To address this issue, we propose a projective adversarial network, PAN, which incorporates high-level 3D information through 2D projections. Additionally, an attention module is introduced to selectively integrate global information from the segmentor to the adversarial network, reducing computational burden. We tested our framework on pancreas segmentation from CT scans and achieved state-of-the-art performance without increasing the complexity of the segmentor.",1
"Encoder-decoder architectures are widely adopted for medical image segmentation tasks. With the lateral skip connection, the models can obtain and fuse both semantic and resolution information in deep layers to achieve more accurate segmentation performance. However, in many applications (e.g., blurry boundary images), these models often cannot precisely locate complex boundaries and segment tiny isolated parts. To solve this challenging problem, we firstly analyze why simple skip connections are not enough to help accurately locate indistinct boundaries and argue that it is due to the fuzzy information in the skip connection provided in the encoder layers. Then we propose a semantic-guided encoder feature learning strategy to learn both high resolution and rich semantic encoder features so that we can more accurately locate the blurry boundaries, which can also enhance the network by selectively learning discriminative features. Besides, we further propose a soft contour constraint mechanism to model the blurry boundary detection. Experimental results on real clinical datasets show that our proposed method can achieve state-of-the-art segmentation accuracy, especially for the blurry regions. Further analysis also indicates that our proposed network components indeed contribute to the improvement of performance. Experiments on additional datasets validate the generalization ability of our proposed method.",0
"The use of encoder-decoder architectures is widespread in medical image segmentation tasks. Incorporating lateral skip connections allows for the fusion of both semantic and resolution information in deep layers, leading to more precise segmentation performance. However, these models often struggle with accurately locating complex boundaries and segmenting small, isolated parts, especially in blurry images. To address this issue, we examine the limitations of simple skip connections and attribute them to the vague information provided by the encoder layers. As a solution, we propose a semantic-guided encoder feature learning approach that teaches the network to identify high-resolution and semantically rich encoder features. This approach enhances the accuracy of boundary detection and enables the network to learn more discriminative features. Additionally, we introduce a soft contour constraint mechanism to model blurry boundary detection. Our experimental results demonstrate that our proposed strategy achieves state-of-the-art segmentation accuracy, particularly in blurry regions. Further analysis confirms that our network components contribute significantly to performance improvement. We also validate the generalization ability of our method with experiments on additional datasets.",1
"Real-time semantic image segmentation on platforms subject to size, weight and power (SWaP) constraints is a key area of interest for air surveillance and inspection. In this work, we propose MAVNet: a small, light-weight, deep neural network for real-time semantic segmentation on micro Aerial Vehicles (MAVs). MAVNet, inspired by ERFNet, features 400 times fewer parameters and achieves comparable performance with some reference models in empirical experiments. Our model achieves a trade-off between speed and accuracy, achieving up to 48 FPS on an NVIDIA 1080Ti and 9 FPS on the NVIDIA Jetson Xavier when processing high resolution imagery. Additionally, we provide two novel datasets that represent challenges in semantic segmentation for real-time MAV tracking and infrastructure inspection tasks and verify MAVNet on these datasets. Our algorithm and datasets are made publicly available.",0
"Real-time semantic image segmentation is of significant interest for air surveillance and inspection, particularly when platforms are subject to size, weight, and power (SWaP) constraints. Our research proposes MAVNet, a deep neural network that is small, lightweight, and optimized for real-time semantic segmentation on micro Aerial Vehicles (MAVs). Inspired by ERFNet, our model is designed with 400 times fewer parameters yet delivers comparable performance to some reference models in empirical experiments. We have achieved a balance between speed and accuracy, with our model processing high-resolution imagery at up to 48 FPS on an NVIDIA 1080Ti and 9 FPS on the NVIDIA Jetson Xavier. Additionally, we have created two innovative datasets that present challenges in semantic segmentation for real-time MAV tracking and infrastructure inspection tasks, which we have used to validate our algorithm's effectiveness. Our algorithm and datasets are publicly available.",1
"In this work, we explore the issue of the inter-annotator agreement for training and evaluating automated segmentation of skin lesions. We explore what different degrees of agreement represent, and how they affect different use cases for segmentation. We also evaluate how conditioning the ground truths using different (but very simple) algorithms may help to enhance agreement and may be appropriate for some use cases. The segmentation of skin lesions is a cornerstone task for automated skin lesion analysis, useful both as an end-result to locate/detect the lesions and as an ancillary task for lesion classification. Lesion segmentation, however, is a very challenging task, due not only to the challenge of image segmentation itself but also to the difficulty in obtaining properly annotated data. Detecting accurately the borders of lesions is challenging even for trained humans, since, for many lesions, those borders are fuzzy and ill-defined. Using lesions and annotations from the ISIC Archive, we estimate inter-annotator agreement for skin-lesion segmentation and propose several simple procedures that may help to improve inter-annotator agreement if used to condition the ground truths.",0
"The objective of this study is to analyze inter-annotator agreement regarding the automated segmentation of skin lesions. We examine the implications of varying degrees of agreement and their impact on segmentation applications. Additionally, we assess the effectiveness of conditioning ground truths using basic algorithms to enhance agreement and determine the suitability of such procedures for certain use cases. Automated skin lesion analysis relies heavily on accurate lesion segmentation, which serves as both a means of locating and detecting lesions and as an ancillary task for lesion classification. However, lesion segmentation is a complex undertaking that poses challenges for image segmentation and obtaining properly annotated data. Even for trained professionals, accurately detecting lesion borders can be difficult due to their fuzzy and indistinct nature. By utilizing lesions and annotations from the ISIC Archive, we estimate inter-annotator agreement for skin-lesion segmentation and propose simple procedures to improve agreement by conditioning the ground truths.",1
"Volumetric image segmentation with convolutional neural networks (CNNs) encounters several challenges, which are specific to medical images. Among these challenges are large volumes of interest, high class imbalances, and difficulties in learning shape representations. To tackle these challenges, we propose to improve over traditional CNN-based volumetric image segmentation through point-wise classification of point clouds. The sparsity of point clouds allows processing of entire image volumes, balancing highly imbalanced segmentation problems, and explicitly learning an anatomical shape. We build upon PointCNN, a neural network proposed to process point clouds, and propose here to jointly encode shape and volumetric information within the point cloud in a compact and computationally effective manner. We demonstrate how this approach can then be used to refine CNN-based segmentation, which yields significantly improved results in our experiments on the difficult task of peripheral nerve segmentation from magnetic resonance neurography images. By synthetic experiments, we further show the capability of our approach in learning an explicit anatomical shape representation.",0
"Convolutional neural networks (CNNs) face various challenges when it comes to volumetric image segmentation, particularly in the medical field. These challenges include dealing with large volumes of interest, imbalanced classes, and difficulty in learning shape representations. To address these challenges, we propose a point-wise classification approach to improve upon traditional CNN-based volumetric image segmentation. The sparsity of point clouds enables processing of entire image volumes, balancing highly imbalanced segmentation problems, and explicitly learning anatomical shape. We use PointCNN as a basis for our neural network to process point clouds, jointly encoding shape and volumetric information in a compact and computationally efficient manner. Our approach can refine CNN-based segmentation, resulting in significantly improved results in peripheral nerve segmentation from magnetic resonance neurography images. Synthetic experiments also demonstrate the capability of our approach in learning explicit anatomical shape representations.",1
"Deep learning (DL) approaches are state-of-the-art for many medical image segmentation tasks. They offer a number of advantages: they can be trained for specific tasks, computations are fast at test time, and segmentation quality is typically high. In contrast, previously popular multi-atlas segmentation (MAS) methods are relatively slow (as they rely on costly registrations) and even though sophisticated label fusion strategies have been proposed, DL approaches generally outperform MAS. In this work, we propose a DL-based label fusion strategy (VoteNet) which locally selects a set of reliable atlases whose labels are then fused via plurality voting. Experiments on 3D brain MRI data show that by selecting a good initial atlas set MAS with VoteNet significantly outperforms a number of other label fusion strategies as well as a direct DL segmentation approach. We also provide an experimental analysis of the upper performance bound achievable by our method. While unlikely achievable in practice, this bound suggests room for further performance improvements. Lastly, to address the runtime disadvantage of standard MAS, all our results make use of a fast DL registration approach.",0
"For numerous medical image segmentation tasks, deep learning (DL) techniques are the current leading method. The benefits of DL include the ability to train for specific tasks, fast computations at test time, and high segmentation quality. In contrast, multi-atlas segmentation (MAS) methods, once popular, are relatively slow due to costly registrations and while advanced label fusion approaches have been proposed, DL approaches typically outperform MAS. This paper presents a DL-based label fusion strategy called VoteNet, which selects a group of dependable atlases that are fused through plurality voting. Trials using 3D brain MRI data indicate that MAS with VoteNet outperforms other label fusion strategies and direct DL segmentation approaches. The paper also analyzes the maximum performance level achievable by the method, which while not practical, suggests potential for improvement. Lastly, to address the runtime disadvantage of standard MAS, a fast DL registration approach is employed in all results.",1
"Optical coherence tomography (OCT) is a non-invasive imaging modality which is widely used in clinical ophthalmology. OCT images are capable of visualizing deep retinal layers which is crucial for early diagnosis of retinal diseases. In this paper, we describe a comprehensive open-access database containing more than 500 highresolution images categorized into different pathological conditions. The image classes include Normal (NO), Macular Hole (MH), Age-related Macular Degeneration (AMD), Central Serous Retinopathy (CSR), and Diabetic Retinopathy (DR). The images were obtained from a raster scan protocol with a 2mm scan length and 512x1024 pixel resolution. We have also included 25 normal OCT images with their corresponding ground truth delineations which can be used for an accurate evaluation of OCT image segmentation. In addition, we have provided a user-friendly GUI which can be used by clinicians for manual (and semi-automated) segmentation.",0
"The use of Optical Coherence Tomography (OCT) in clinical ophthalmology has become widespread due to its non-invasive nature. OCT images have the ability to display deep retinal layers, which is crucial in the early detection of retinal diseases. This paper introduces a comprehensive open-access database that contains over 500 high-resolution images that have been categorized into various pathological conditions such as Normal (NO), Macular Hole (MH), Age-related Macular Degeneration (AMD), Central Serous Retinopathy (CSR), and Diabetic Retinopathy (DR). The images were obtained through a raster scan protocol with a 2mm scan length and 512x1024 pixel resolution. To ensure accurate evaluation of OCT image segmentation, 25 normal OCT images with corresponding ground truth delineations have been included. Moreover, a user-friendly GUI has been provided to enable clinicians to manually or semi-automatically segment the images.",1
"Methods that move towards less supervised scenarios are key for image segmentation, as dense labels demand significant human intervention. Generally, the annotation burden is mitigated by labeling datasets with weaker forms of supervision, e.g. image-level labels or bounding boxes. Another option are semi-supervised settings, that commonly leverage a few strong annotations and a huge number of unlabeled/weakly-labeled data. In this paper, we revisit semi-supervised segmentation schemes and narrow down significantly the annotation budget (in terms of total labeling time of the training set) compared to previous approaches. With a very simple pipeline, we demonstrate that at low annotation budgets, semi-supervised methods outperform by a wide margin weakly-supervised ones for both semantic and instance segmentation. Our approach also outperforms previous semi-supervised works at a much reduced labeling cost. We present results for the Pascal VOC benchmark and unify weakly and semi-supervised approaches by considering the total annotation budget, thus allowing a fairer comparison between methods.",0
"Less supervised methods are crucial for image segmentation as dense labeling requires significant human involvement. To reduce the annotation burden, datasets can be labeled with weaker forms of supervision such as image-level labels or bounding boxes. Another option is to use semi-supervised settings where a few strong annotations are combined with a large number of unlabeled or weakly-labeled data. In this study, we focus on semi-supervised segmentation schemes and significantly reduce the annotation budget compared to previous approaches. Our simple pipeline shows that at low annotation budgets, semi-supervised methods outperform weakly-supervised ones for both semantic and instance segmentation. We also outperform previous semi-supervised works at a much lower labeling cost. Results are presented for the Pascal VOC benchmark and we unify weakly and semi-supervised approaches by considering the total annotation budget, allowing for a fairer comparison between methods.",1
"We present an end-to-end learned algorithm for seeded segmentation. Our method is based on the Random Walker algorithm, where we predict the edge weights of the underlying graph using a convolutional neural network. This can be interpreted as learning context-dependent diffusivities for a linear diffusion process. Besides calculating the exact gradient for optimizing these diffusivities, we also propose simplifications that sparsely sample the gradient and still yield competitive results. The proposed method achieves the currently best results on a seeded version of the CREMI neuron segmentation challenge.",0
"Our algorithm for seeded segmentation is learned end-to-end and relies on the Random Walker approach. We use a convolutional neural network to predict the edge weights of the graph underlying the algorithm. This allows us to learn diffusivities that are dependent on context for a linear diffusion process. We not only calculate the precise gradient for optimizing these diffusivities, but we also suggest simplifications that yield competitive outcomes while sparsely sampling the gradient. With our method, we have achieved the most favorable results to date on the seeded CREMI neuron segmentation challenge.",1
"In response to the growing importance of geospatial data, its analysis including semantic segmentation becomes an increasingly popular task in computer vision today. Convolutional neural networks are powerful visual models that yield hierarchies of features and practitioners widely use them to process remote sensing data. When performing remote sensing image segmentation, multiple instances of one class with precisely defined boundaries are often the case, and it is crucial to extract those boundaries accurately. The accuracy of segments boundaries delineation influences the quality of the whole segmented areas explicitly. However, widely-used segmentation loss functions such as BCE, IoU loss or Dice loss do not penalize misalignment of boundaries sufficiently. In this paper, we propose a novel loss function, namely a differentiable surrogate of a metric accounting accuracy of boundary detection. We can use the loss function with any neural network for binary segmentation. We performed validation of our loss function with various modifications of UNet on a synthetic dataset, as well as using real-world data (ISPRS Potsdam, INRIA AIL). Trained with the proposed loss function, models outperform baseline methods in terms of IoU score.",0
"The analysis of geospatial data, including semantic segmentation, has become increasingly important in computer vision. Convolutional neural networks are commonly used to process remote sensing data due to their ability to produce hierarchical features. Accurately detecting the boundaries of multiple instances of one class in remote sensing image segmentation is crucial for obtaining high-quality segmented areas. However, commonly used segmentation loss functions do not adequately penalize misalignment of boundaries. To address this issue, we propose a novel loss function that accounts for the accuracy of boundary detection. This loss function can be used with any neural network for binary segmentation and was validated using both synthetic and real-world datasets. Models trained with our proposed loss function outperformed baseline methods in terms of IoU score.",1
"Automatic segmentation of organs-at-risk (OAR) in computed tomography (CT) is an essential part of planning effective treatment strategies to combat lung and esophageal cancer. Accurate segmentation of organs surrounding tumours helps account for the variation in position and morphology inherent across patients, thereby facilitating adaptive and computer-assisted radiotherapy. Although manual delineation of OARs is still highly prevalent, it is prone to errors due to complex variations in the shape and position of organs across patients, and low soft tissue contrast between neighbouring organs in CT images. Recently, deep convolutional neural networks (CNNs) have gained tremendous traction and achieved state-of-the-art results in medical image segmentation. In this paper, we propose a deep learning framework to segment OARs in thoracic CT images, specifically for the: heart, esophagus, trachea and aorta. Our approach employs dilated convolutions and aggregated residual connections in the bottleneck of a U-Net styled network, which incorporates global context and dense information. Our method achieved an overall Dice score of 91.57% on 20 unseen test samples from the ISBI 2019 SegTHOR challenge.",0
"Segmentation of organs-at-risk (OAR) in computed tomography (CT) is crucial for planning effective treatment strategies to combat lung and esophageal cancer. Accurate segmentation of organs surrounding tumors helps to consider the variation in position and morphology inherent across patients, thereby facilitating adaptive and computer-assisted radiotherapy. However, manual delineation of OARs is still prevalent and prone to errors due to complex variations in the shape and position of organs across patients and low soft tissue contrast in CT images. Recently, deep convolutional neural networks (CNNs) have achieved state-of-the-art results in medical image segmentation. This paper proposes a deep learning framework to segment OARs in thoracic CT images, specifically for the heart, esophagus, trachea, and aorta. Our approach employs dilated convolutions and aggregated residual connections in a U-Net styled network, incorporating global context and dense information. Our method achieved an overall Dice score of 91.57% on 20 unseen test samples from the ISBI 2019 SegTHOR challenge.",1
"Automated design of neural network architectures tailored for a specific task is an extremely promising, albeit inherently difficult, avenue to explore. While most results in this domain have been achieved on image classification and language modelling problems, here we concentrate on dense per-pixel tasks, in particular, semantic image segmentation using fully convolutional networks. In contrast to the aforementioned areas, the design choices of a fully convolutional network require several changes, ranging from the sort of operations that need to be used---e.g., dilated convolutions---to a solving of a more difficult optimisation problem. In this work, we are particularly interested in searching for high-performance compact segmentation architectures, able to run in real-time using limited resources. To achieve that, we intentionally over-parameterise the architecture during the training time via a set of auxiliary cells that provide an intermediate supervisory signal and can be omitted during the evaluation phase. The design of the auxiliary cell is emitted by a controller, a neural network with the fixed structure trained using reinforcement learning. More crucially, we demonstrate how to efficiently search for these architectures within limited time and computational budgets. In particular, we rely on a progressive strategy that terminates non-promising architectures from being further trained, and on Polyak averaging coupled with knowledge distillation to speed-up the convergence. Quantitatively, in 8 GPU-days our approach discovers a set of architectures performing on-par with state-of-the-art among compact models on the semantic segmentation, pose estimation and depth prediction tasks. Code will be made available here: https://github.com/drsleep/nas-segm-pytorch",0
"Exploring the automated design of neural network architectures for specific tasks is a promising but challenging approach. While current progress has been made in image classification and language modelling, our focus is on dense per-pixel tasks, specifically semantic image segmentation using fully convolutional networks. Unlike other areas, designing a fully convolutional network requires various changes, such as using dilated convolutions and solving a more complex optimization problem. Our objective is to search for high-performance compact segmentation architectures that can run in real-time using limited resources. To achieve this, we intentionally over-parameterize the architecture during training with auxiliary cells that provide an intermediate supervisory signal, which can be omitted during evaluation. A neural network controller with a fixed structure is trained using reinforcement learning to design the auxiliary cell. We also demonstrate an efficient method to search for architectures within limited time and computational budgets, using a progressive strategy to terminate non-promising architectures and Polyak averaging with knowledge distillation to speed up convergence. Our approach discovers a set of architectures that perform on-par with state-of-the-art among compact models on semantic segmentation, pose estimation, and depth prediction tasks in just 8 GPU-days. Code will be available at https://github.com/drsleep/nas-segm-pytorch.",1
"Since acquiring pixel-wise annotations for training convolutional neural networks for semantic image segmentation is time-consuming, weakly supervised approaches that only require class tags have been proposed. In this work, we propose another form of supervision, namely image captions as they can be found on the Internet. These captions have two advantages. They do not require additional curation as it is the case for the clean class tags used by current weakly supervised approaches and they provide textual context for the classes present in an image. To leverage such textual context, we deploy a multi-modal network that learns a joint embedding of the visual representation of the image and the textual representation of the caption. The network estimates text activation maps (TAMs) for class names as well as compound concepts, i.e. combinations of nouns and their attributes. The TAMs of compound concepts describing classes of interest substantially improve the quality of the estimated class activation maps which are then used to train a network for semantic segmentation. We evaluate our method on the COCO dataset where it achieves state of the art results for weakly supervised image segmentation.",0
"Weakly supervised approaches for training convolutional neural networks for semantic image segmentation have been proposed to avoid the time-consuming process of acquiring pixel-wise annotations. In this study, we introduce image captions as a new form of supervision, which offer two advantages: they do not require extra curation like clean class tags used in current weakly supervised methods, and they provide textual context for image classes. To effectively utilize this textual context, we use a multi-modal network that learns a joint embedding of the visual and textual representations of an image. The network estimates text activation maps (TAMs) for class names and compound concepts, i.e., combinations of nouns and their attributes. The TAMs of compound concepts describing classes of interest improve the quality of the estimated class activation maps, which are then used to train a network for semantic segmentation. Our method achieves state of the art results for weakly supervised image segmentation on the COCO dataset.",1
"Machine learning methods have achieved good performance and been widely applied in various real-world applications. They can learn the model adaptively and be better fit for special requirements of different tasks. Generally, a good machine learning system is composed of plentiful training data, a good model training process, and an accurate inference. Many factors can affect the performance of the machine learning process, among which the diversity of the machine learning process is an important one. The diversity can help each procedure to guarantee a total good machine learning: diversity of the training data ensures that the training data can provide more discriminative information for the model, diversity of the learned model (diversity in parameters of each model or diversity among different base models) makes each parameter/model capture unique or complement information and the diversity in inference can provide multiple choices each of which corresponds to a specific plausible local optimal result. Even though the diversity plays an important role in machine learning process, there is no systematical analysis of the diversification in machine learning system. In this paper, we systematically summarize the methods to make data diversification, model diversification, and inference diversification in the machine learning process, respectively. In addition, the typical applications where the diversity technology improved the machine learning performance have been surveyed, including the remote sensing imaging tasks, machine translation, camera relocalization, image segmentation, object detection, topic modeling, and others. Finally, we discuss some challenges of the diversity technology in machine learning and point out some directions in future work.",0
"Various real-world applications have been benefitting from the effective performance of machine learning methods. These methods have the ability to learn models adaptively and cater to the unique demands of different tasks. A good machine learning system comprises a large amount of training data, an efficient model training process, and accurate inference. Diverse aspects of the machine learning process can impact its performance, with diversity being a crucial factor. Diversity can facilitate a good machine learning process by ensuring that the training data provides more informative details, the learned model captures unique or complementary information, and the inference provides several alternatives that correspond to specific optimal results. However, there is a lack of systematic analysis of the diversification process in machine learning systems. This paper aims to summarize the methods used to diversify the data, model, and inference in the machine learning process, respectively. Moreover, it surveys typical applications that have benefitted from diversity technology, including remote sensing imaging tasks, machine translation, camera relocalization, image segmentation, object detection, topic modeling, and others. Finally, it discusses the challenges associated with diversity technology in machine learning and suggests future directions for research.",1
"Pap smear testing has been widely used for detecting cervical cancers based on the morphology properties of cell nuclei in microscopic image. An accurate nuclei segmentation could thus improve the success rate of cervical cancer screening. In this work, a method of automated cervical nuclei segmentation using Deformable Multipath Ensemble Model (D-MEM) is proposed. The approach adopts a U-shaped convolutional network as a backbone network, in which dense blocks are used to transfer feature information more effectively. To increase the flexibility of the model, we then use deformable convolution to deal with different nuclei irregular shapes and sizes. To reduce the predictive bias, we further construct multiple networks with different settings, which form an ensemble model. The proposed segmentation framework has achieved state-of-the-art accuracy on Herlev dataset with Zijdenbos similarity index (ZSI) of 0.933, and has the potential to be extended for solving other medical image segmentation tasks.",0
"The detection of cervical cancer through Pap smear testing has been widely used by examining the morphology properties of cell nuclei in microscopic images. For the success rate of cervical cancer screening to improve, an accurate nuclei segmentation is necessary. As a result, an automated cervical nuclei segmentation method utilizing Deformable Multipath Ensemble Model (D-MEM) has been proposed. The approach utilizes a U-shaped convolutional network as a backbone network with dense blocks to transfer feature information more effectively. To accommodate various nuclei shapes and sizes, the model employs deformable convolution to increase its flexibility. Multiple networks with diverse settings are also constructed to form an ensemble model, reducing predictive bias. The proposed segmentation framework has achieved state-of-the-art accuracy on the Herlev dataset, as indicated by a Zijdenbos similarity index (ZSI) of 0.933. Furthermore, this model can potentially be extended to tackle other medical image segmentation tasks.",1
"This paper proposes the first, known to us, open source presentation attack detection (PAD) solution to distinguish between authentic iris images (possibly wearing clear contact lenses) and irises with textured contact lenses. This software can serve as a baseline in various PAD evaluations, and also as an open-source platform with an up-to-date reference method for iris PAD. The software is written in C++ and Python and uses only open source resources, such as OpenCV. This method does not incorporate iris image segmentation, which may be problematic for unknown fake samples. Instead, it makes a best guess to localize the rough position of the iris. The PAD-related features are extracted with the Binary Statistical Image Features (BSIF), which are classified by an ensemble of classifiers incorporating support vector machine, random forest and multilayer perceptron. The models attached to the current software have been trained with the NDCLD'15 database and evaluated on the independent datasets included in the LivDet-Iris 2017 competition. The software implements the functionality of retraining the classifiers with any database of authentic and attack images. The accuracy of the current version offered with this paper exceeds 99% when tested on subject-disjoint subsets of NDCLD'15, and oscillates around 85% when tested on the LivDet-Iris 2017 benchmarks, which is on par with the results obtained by the LivDet-Iris 2017 winner.",0
"In this paper, a new open source solution for presentation attack detection (PAD) is proposed. This solution is capable of identifying authentic iris images, including those with clear or textured contact lenses. The software has multiple uses, including serving as a benchmark for PAD evaluations and providing an open source platform for iris PAD. It is written in C++ and Python and uses only open source resources like OpenCV. Unlike other methods, this solution does not rely on iris image segmentation and instead estimates the rough position of the iris. It uses Binary Statistical Image Features (BSIF) to extract PAD-related features, which are classified by an ensemble of support vector machine, random forest, and multilayer perceptron classifiers. The models are trained with the NDCLD'15 database and evaluated on the LivDet-Iris 2017 competition datasets. The software can also retrain the classifiers with any authentic or attack image database. The accuracy of this solution exceeds 99% when tested on subject-disjoint subsets of NDCLD'15 and around 85% on the LivDet-Iris 2017 benchmarks, which is comparable to the results achieved by the competition winner.",1
"What did it feel like to walk through a city from the past? In this work, we describe Nostalgin (Nostalgia Engine), a method that can faithfully reconstruct cities from historical images. Unlike existing work in city reconstruction, we focus on the task of reconstructing 3D cities from historical images. Working with historical image data is substantially more difficult, as there are significantly fewer buildings available and the details of the camera parameters which captured the images are unknown. Nostalgin can generate a city model even if there is only a single image per facade, regardless of viewpoint or occlusions. To achieve this, our novel architecture combines image segmentation, rectification, and inpainting. We motivate our design decisions with experimental analysis of individual components of our pipeline, and show that we can improve on baselines in both speed and visual realism. We demonstrate the efficacy of our pipeline by recreating two 1940s Manhattan city blocks. We aim to deploy Nostalgin as an open source platform where users can generate immersive historical experiences from their own photos.",0
"How would it feel to stroll through a city from the past? Our team has developed Nostalgin (Nostalgia Engine), a technique that can recreate historical cities with remarkable accuracy. Our approach differs from previous city reconstruction methods as we concentrate on generating 3D models from historical images. Reconstructing cities from historical image data presents unique challenges, given the limited number of buildings available and the lack of information about the camera parameters used to capture the images. However, Nostalgin can produce a city model even with only one image per building facade, regardless of the viewpoint or occlusions. Our innovative architecture combines image segmentation, rectification, and inpainting to achieve this goal. We have tested each component of our pipeline and demonstrated that our design decisions improve speed and visual realism compared to existing methods. We have also successfully recreated two Manhattan city blocks from the 1940s to showcase the effectiveness of our pipeline. Our ultimate aim is to make Nostalgin an open source platform that allows users to create immersive historical experiences using their own photos.",1
"Most of the semantic segmentation approaches have been developed for single image segmentation, and hence, video sequences are currently segmented by processing each frame of the video sequence separately. The disadvantage of this is that temporal image information is not considered, which improves the performance of the segmentation approach. One possibility to include temporal information is to use recurrent neural networks. However, there are only a few approaches using recurrent networks for video segmentation so far. These approaches extend the encoder-decoder network architecture of well-known segmentation approaches and place convolutional LSTM layers between encoder and decoder. However, in this paper it is shown that this position is not optimal, and that other positions in the network exhibit better performance. Nowadays, state-of-the-art segmentation approaches rarely use the classical encoder-decoder structure, but use multi-branch architectures. These architectures are more complex, and hence, it is more difficult to place the recurrent units at a proper position. In this work, the multi-branch architectures are extended by convolutional LSTM layers at different positions and evaluated on two different datasets in order to find the best one. It turned out that the proposed approach outperforms the pure CNN-based approach for up to 1.6 percent.",0
"Currently, most semantic segmentation methods are designed for single image segmentation, resulting in video sequences being segmented by processing each frame separately. However, this approach neglects temporal image information that could enhance segmentation performance. Recurrent neural networks offer a solution to incorporate temporal information, but there are limited approaches using them for video segmentation. These approaches typically add convolutional LSTM layers between the encoder and decoder of the encoder-decoder network architecture. However, recent state-of-the-art segmentation approaches use multi-branch architectures, making it more challenging to place recurrent units optimally. This study extends multi-branch architectures by adding convolutional LSTM layers at various positions and evaluates their performance on two datasets to identify the best approach. The proposed method outperforms the pure CNN-based approach by up to 1.6 percent.",1
"Recent progress in biomedical image segmentation based on deep convolutional neural networks (CNNs) has drawn much attention. However, its vulnerability towards adversarial samples cannot be overlooked. This paper is the first one that discovers that all the CNN-based state-of-the-art biomedical image segmentation models are sensitive to adversarial perturbations. This limits the deployment of these methods in safety-critical biomedical fields. In this paper, we discover that global spatial dependencies and global contextual information in a biomedical image can be exploited to defend against adversarial attacks. To this end, non-local context encoder (NLCE) is proposed to model short- and long range spatial dependencies and encode global contexts for strengthening feature activations by channel-wise attention. The NLCE modules enhance the robustness and accuracy of the non-local context encoding network (NLCEN), which learns robust enhanced pyramid feature representations with NLCE modules, and then integrates the information across different levels. Experiments on both lung and skin lesion segmentation datasets have demonstrated that NLCEN outperforms any other state-of-the-art biomedical image segmentation methods against adversarial attacks. In addition, NLCE modules can be applied to improve the robustness of other CNN-based biomedical image segmentation methods.",0
"The recent advancements in deep convolutional neural networks (CNNs) for biomedical image segmentation have gained significant attention, but its vulnerability to adversarial samples cannot be ignored. This paper highlights the sensitivity of all CNN-based biomedical image segmentation models to adversarial perturbations, which restricts their usage in safety-critical biomedical fields. By exploiting global spatial dependencies and contextual information in biomedical images, this paper proposes a non-local context encoder (NLCE) to defend against adversarial attacks. The NLCE modules model short- and long-range spatial dependencies, encode global contexts, and enhance feature activations using channel-wise attention, thereby improving the robustness and accuracy of the non-local context encoding network (NLCEN). The NLCEN learns robust enhanced pyramid feature representations with NLCE modules and integrates the information across different levels. The experiments conducted on lung and skin lesion segmentation datasets demonstrate that NLCEN outperforms other state-of-the-art biomedical image segmentation methods against adversarial attacks. Additionally, the NLCE modules can enhance the robustness of other CNN-based biomedical image segmentation methods.",1
"It is important to find the target as soon as possible for search and rescue operations. Surveillance camera systems and unmanned aerial vehicles (UAVs) are used to support search and rescue. Automatic object detection is important because a person cannot monitor multiple surveillance screens simultaneously for 24 hours. Also, the object is often too small to be recognized by the human eye on the surveillance screen. This study used UAVs around the Port of Houston and fixed surveillance cameras to build an automatic target detection system that supports the US Coast Guard (USCG) to help find targets (e.g., person overboard). We combined image segmentation, enhancement, and convolution neural networks to reduce detection time to detect small targets. We compared the performance between the auto-detection system and the human eye. Our system detected the target within 8 seconds, but the human eye detected the target within 25 seconds. Our systems also used synthetic data generation and data augmentation techniques to improve target detection accuracy. This solution may help the search and rescue operations of the first responders in a timely manner.",0
"Swift identification of the target is crucial in search and rescue missions, hence the use of surveillance camera systems and unmanned aerial vehicles (UAVs) to aid the process. Detecting the object automatically is necessary due to the limitations of human monitoring, as one cannot keep a watchful eye on multiple surveillance screens for 24 hours, and the target may be too small to be identified by the naked eye. This research involved the implementation of a target detection system that leveraged UAVs and fixed surveillance cameras around the Port of Houston to assist the US Coast Guard (USCG) in locating targets like individuals in distress. By utilizing image segmentation, enhancement, and convolution neural networks, the detection time for small targets was reduced. A comparison between the performance of the auto-detection system and the human eye showed that our system identified the target within 8 seconds, while the human eye took 25 seconds. Additionally, synthetic data generation and data augmentation techniques were employed to enhance target detection accuracy. This solution could aid first responders in conducting search and rescue operations promptly.",1
"Autonomous Micro Aerial Vehicles (MAVs) gained tremendous attention in recent years. Autonomous flight in indoor requires a dense depth map for navigable space detection which is the fundamental component for autonomous navigation. In this paper, we address the problem of reconstructing dense depth while a drone is hovering (small camera motion) in indoor scenes using already estimated cameras and sparse point cloud obtained from a vSLAM. We start by segmenting the scene based on sudden depth variation using sparse 3D points and introduce a patch-based local plane fitting via energy minimization which combines photometric consistency and co-planarity with neighbouring patches. The method also combines a plane sweep technique for image segments having almost no sparse point for initialization. Experiments show, the proposed method produces better depth for indoor in artificial lighting condition, low-textured environment compared to earlier literature in small motion.",0
"Autonomous Micro Aerial Vehicles (MAVs) have attracted significant attention in recent years due to their potential applications. Autonomous navigation in indoor environments requires a dense depth map to detect navigable space. In this study, we aim to address the issue of generating a dense depth map when a drone is hovering in an indoor setting using sparse point clouds and previously estimated cameras. The method involves segmenting the area based on sudden depth variation using sparse 3D points and implementing a patch-based local plane fitting technique via energy minimization to combine photometric consistency and co-planarity with neighbouring patches. Furthermore, a plane sweep technique is used for image segments that lack sparse points for initialization. The proposed method outperforms previous literature in small motion, producing better depth for indoor environments with artificial lighting conditions and low-textured surroundings based on experimental results.",1
"In the last few years, Deep Learning (DL) has been showing superior performance in different modalities of biomedical image analysis. Several DL architectures have been proposed for classification, segmentation, and detection tasks in medical imaging and computational pathology. In this paper, we propose a new DL architecture, the NABLA-N network, with better feature fusion techniques in decoding units for dermoscopic image segmentation tasks. The NABLA-N network has several advances for segmentation tasks. First, this model ensures better feature representation for semantic segmentation with a combination of low to high-level feature maps. Second, this network shows better quantitative and qualitative results with the same or fewer network parameters compared to other methods. In addition, the Inception Recurrent Residual Convolutional Neural Network (IRRCNN) model is used for skin cancer classification. The proposed NABLA-N network and IRRCNN models are evaluated for skin cancer segmentation and classification on the benchmark datasets from the International Skin Imaging Collaboration 2018 (ISIC-2018). The experimental results show superior performance on segmentation tasks compared to the Recurrent Residual U-Net (R2U-Net). The classification model shows around 87% testing accuracy for dermoscopic skin cancer classification on ISIC2018 dataset.",0
"Over the past few years, Deep Learning (DL) has demonstrated exceptional performance in various biomedical image analysis modalities. A multitude of DL architectures have been suggested for medical imaging and computational pathology tasks such as detection, segmentation, and classification. This paper presents a novel DL architecture, the NABLA-N network, which utilizes improved feature fusion techniques in decoding units for dermoscopic image segmentation tasks. The NABLA-N network offers numerous advantages for segmentation tasks including enhanced feature representation for semantic segmentation through a combination of low to high-level feature maps. Furthermore, this model exhibits better quantitative and qualitative results with the same or fewer network parameters than other methods. Additionally, the skin cancer classification is performed using the Inception Recurrent Residual Convolutional Neural Network (IRRCNN) model. Both the proposed NABLA-N network and the IRRCNN models are evaluated on the benchmark datasets from the International Skin Imaging Collaboration 2018 (ISIC-2018) for skin cancer segmentation and classification. The experimental outcomes demonstrate superior segmentation task performance compared to the Recurrent Residual U-Net (R2U-Net), while the classification model exhibits an approximately 87% testing accuracy for dermoscopic skin cancer classification on the ISIC2018 dataset.",1
"In this paper, we propose a novel iterative convolution-thresholding method (ICTM) that is applicable to a range of variational models for image segmentation. A variational model usually minimizes an energy functional consisting of a fidelity term and a regularization term. In the ICTM, the interface between two different segment domains is implicitly represented by their characteristic functions. The fidelity term is then usually written as a linear functional of the characteristic functions and the regularized term is approximated by a functional of characteristic functions in terms of heat kernel convolution. This allows us to design an iterative convolution-thresholding method to minimize the approximate energy. The method is simple, efficient and enjoys the energy-decaying property. Numerical experiments show that the method is easy to implement, robust and applicable to various image segmentation models.",0
"A new iterative convolution-thresholding method (ICTM) is proposed in this research article. The ICTM can be utilized for a range of variational models for image segmentation. These models typically minimize an energy functional comprising a fidelity term and a regularization term. The ICTM represents the interface between two distinct segment domains through their characteristic functions. The fidelity term is then expressed as a linear functional of the characteristic functions. Meanwhile, the regularized term is estimated via a functional of characteristic functions using heat kernel convolution. As a result, an iterative convolution-thresholding method can be developed to decrease the approximate energy. This procedure is uncomplicated, effective, and possesses the energy-decaying property. Experimental data demonstrates that the approach is easy to implement, resilient, and suitable for various image segmentation models.",1
"Domain adaptation for semantic image segmentation is very necessary since manually labeling large datasets with pixel-level labels is expensive and time consuming. Existing domain adaptation techniques either work on limited datasets, or yield not so good performance compared with supervised learning. In this paper, we propose a novel bidirectional learning framework for domain adaptation of segmentation. Using the bidirectional learning, the image translation model and the segmentation adaptation model can be learned alternatively and promote to each other. Furthermore, we propose a self-supervised learning algorithm to learn a better segmentation adaptation model and in return improve the image translation model. Experiments show that our method is superior to the state-of-the-art methods in domain adaptation of segmentation with a big margin. The source code is available at https://github.com/liyunsheng13/BDL.",0
"Due to the high cost and time required for manually labeling large datasets with pixel-level labels, domain adaptation for semantic image segmentation is crucial. However, current domain adaptation techniques have limitations such as working with limited datasets or producing subpar results compared to supervised learning. To address this, we present a new bidirectional learning framework for segmentation domain adaptation. This framework involves alternating between learning an image translation model and a segmentation adaptation model, which mutually promote each other. Additionally, our self-supervised learning algorithm enhances the segmentation adaptation model and improves the image translation model in return. Our experiments demonstrate that our method outperforms state-of-the-art domain adaptation techniques for segmentation by a significant margin. The source code is available at https://github.com/liyunsheng13/BDL.",1
"X-Ray image enhancement, along with many other medical image processing applications, requires the segmentation of images into bone, soft tissue, and open beam regions. We apply a machine learning approach to this problem, presenting an end-to-end solution which results in robust and efficient inference. Since medical institutions frequently do not have the resources to process and label the large quantity of X-Ray images usually needed for neural network training, we design an end-to-end solution for small datasets, while achieving state-of-the-art results. Our implementation produces an overall accuracy of 92%, F1 score of 0.92, and an AUC of 0.98, surpassing classical image processing techniques, such as clustering and entropy based methods, while improving upon the output of existing neural networks used for segmentation in non-medical contexts. The code used for this project is available online.",0
"To enhance X-Ray images and perform other medical image processing tasks, it is necessary to segment the images into bone, soft tissue, and open beam regions. We have developed a machine learning approach that provides an end-to-end solution, resulting in efficient and reliable inferences. Due to the limited resources available in medical institutions for processing and labeling a large number of X-Ray images for neural network training, we have created an end-to-end solution for small datasets while achieving state-of-the-art results. Our implementation has an overall accuracy of 92%, F1 score of 0.92, and an AUC of 0.98, surpassing traditional image processing techniques like clustering and entropy-based methods, and improving upon the output of existing neural networks used for segmentation in non-medical contexts. The code for this project is available online.",1
"Segmenting objects in images and separating sound sources in audio are challenging tasks, in part because traditional approaches require large amounts of labeled data. In this paper we develop a neural network model for visual object segmentation and sound source separation that learns from natural videos through self-supervision. The model is an extension of recently proposed work that maps image pixels to sounds. Here, we introduce a learning approach to disentangle concepts in the neural networks, and assign semantic categories to network feature channels to enable independent image segmentation and sound source separation after audio-visual training on videos. Our evaluations show that the disentangled model outperforms several baselines in semantic segmentation and sound source separation.",0
"The tasks of segmenting objects in images and separating sound sources in audio are difficult due to the need for large amounts of labeled data in traditional approaches. In this article, we present a neural network model that addresses these challenges by utilizing self-supervision while learning from natural videos. Building upon the previous work of mapping image pixels to sounds, we propose a learning approach that disentangles concepts in the neural network. Our approach assigns semantic categories to network feature channels, enabling independent image segmentation and sound source separation after audio-visual training on videos. Our evaluations demonstrate that the disentangled model surpasses several baselines in semantic segmentation and sound source separation.",1
"Cardiac image segmentation is a critical process for generating personalized models of the heart and for quantifying cardiac performance parameters. Several convolutional neural network (CNN) architectures have been proposed to segment the heart chambers from cardiac cine MR images. Here we propose a multi-task learning (MTL)-based regularization framework for cardiac MR image segmentation. The network is trained to perform the main task of semantic segmentation, along with a simultaneous, auxiliary task of pixel-wise distance map regression. The proposed distance map regularizer is a decoder network added to the bottleneck layer of an existing CNN architecture, facilitating the network to learn robust global features. The regularizer block is removed after training, so that the original number of network parameters does not change. We show that the proposed regularization method improves both binary and multi-class segmentation performance over the corresponding state-of-the-art CNN architectures on two publicly available cardiac cine MRI datasets, obtaining average dice coefficient of 0.84$\pm$0.03 and 0.91$\pm$0.04, respectively. Furthermore, we also demonstrate improved generalization performance of the distance map regularized network on cross-dataset segmentation, showing as much as 42% improvement in myocardium Dice coefficient from 0.56$\pm$0.28 to 0.80$\pm$0.14.",0
"Generating personalized heart models and quantifying cardiac performance parameters require accurate cardiac image segmentation. Convolutional neural network (CNN) architectures have been proposed for segmenting heart chambers from cardiac cine MR images, but we propose a new approach using a multi-task learning (MTL)-based regularization framework. Our network performs semantic segmentation while also utilizing an auxiliary task of pixel-wise distance map regression. The distance map regularizer is a decoder network added to the bottleneck layer of the CNN architecture to improve global feature learning. After training, the regularizer block is removed to maintain the original number of network parameters. Our regularization method improves both binary and multi-class segmentation performance on two publicly available cardiac cine MRI datasets, achieving average dice coefficients of 0.840.03 and 0.910.04, respectively. Additionally, our approach demonstrates improved generalization performance on cross-dataset segmentation with up to a 42% improvement in myocardium Dice coefficient from 0.560.28 to 0.800.14.",1
"Crack is one of the most common road distresses which may pose road safety hazards. Generally, crack detection is performed by either certified inspectors or structural engineers. This task is, however, time-consuming, subjective and labor-intensive. In this paper, we propose a novel road crack detection algorithm based on deep learning and adaptive image segmentation. Firstly, a deep convolutional neural network is trained to determine whether an image contains cracks or not. The images containing cracks are then smoothed using bilateral filtering, which greatly minimizes the number of noisy pixels. Finally, we utilize an adaptive thresholding method to extract the cracks from road surface. The experimental results illustrate that our network can classify images with an accuracy of 99.92%, and the cracks can be successfully extracted from the images using our proposed thresholding algorithm.",0
"The most common road distress, crack, can create road safety hazards. Typically, detecting cracks is done by certified inspectors or structural engineers, but this process is time-consuming, labor-intensive, and subjective. Therefore, we propose a new approach to detecting road cracks using deep learning and adaptive image segmentation. Initially, a deep convolutional neural network is trained to identify images that contain cracks. The images with cracks are then smoothed using bilateral filtering to reduce the number of noisy pixels. Finally, an adaptive thresholding method is employed to extract the cracks from the road surface. Our experimental results demonstrate that our network can accurately classify images at a rate of 99.92%, and our proposed thresholding algorithm can successfully extract the cracks from the images.",1
"The quality of images captured in outdoor environments can be affected by poor weather conditions such as fog, dust, and atmospheric scattering of other particles. This problem can bring extra challenges to high-level computer vision tasks like image segmentation and object detection. However, previous studies on image dehazing suffer from a huge computational workload and corruption of the original image, such as over-saturation and halos. In this paper, we present a novel image dehazing approach based on the optical model for haze images and regularized optimization. Specifically, we convert the non-convex, bilinear problem concerning the unknown haze-free image and light transmission distribution to a convex, linear optimization problem by estimating the atmosphere light constant. Our method is further accelerated by introducing a multilevel Haar wavelet transform. The optimization, instead, is applied to the low frequency sub-band decomposition of the original image. This dimension reduction significantly improves the processing speed of our method and exhibits the potential for real-time applications. Experimental results show that our approach outperforms state-of-the-art dehazing algorithms in terms of both image reconstruction quality and computational efficiency. For implementation details, source code can be publicly accessed via http://github.com/JiaxiHe/Image-and-Video-Dehazing.",0
"When capturing images outdoors, weather conditions like fog, dust, and atmospheric scattering can affect their quality. This can pose challenges for computer vision tasks such as image segmentation and object detection. However, previous attempts to solve this problem have been hindered by high computational requirements and image distortion issues like over-saturation and halos. In this study, we introduce a novel image dehazing method that uses the optical model for haze images and regularized optimization. By estimating the atmosphere light constant, we convert the non-convex problem of the unknown haze-free image and light transmission distribution to a convex, linear optimization problem. Our approach is also accelerated by utilizing a multilevel Haar wavelet transform and applying optimization to the low frequency sub-band decomposition of the original image, reducing processing time significantly. Our method outperforms other dehazing algorithms in terms of image quality and computational efficiency, and the source code is available at http://github.com/JiaxiHe/Image-and-Video-Dehazing for implementation details.",1
"In order to create an image segmentation method robust to lighting changes, two novel homogeneity criteria of an image region were studied. Both were defined using the Logarithmic Image Processing (LIP) framework whose laws model lighting changes. The first criterion estimates the LIP-additive homogeneity and is based on the LIP-additive law. It is theoretically insensitive to lighting changes caused by variations of the camera exposure-time or source intensity. The second, the LIP-multiplicative homogeneity criterion, is based on the LIP-multiplicative law and is insensitive to changes due to variations of the object thickness or opacity. Each criterion is then applied in Revol and Jourlin's (1997) region growing method which is based on the homogeneity of an image region. The region growing method becomes therefore robust to the lighting changes specific to each criterion. Experiments on simulated and on real images presenting lighting variations prove the robustness of the criteria to those variations. Compared to a state-of the art method based on the image component-tree, ours is more robust. These results open the way to numerous applications where the lighting is uncontrolled or partially controlled.",0
"To develop a dependable image segmentation approach that can withstand lighting changes, two innovative homogeneity criteria were examined. These criteria were formulated utilizing the Logarithmic Image Processing (LIP) framework, which utilizes laws that simulate lighting changes. The first criterion measures the LIP-additive homogeneity and is founded on the LIP-additive law. It is theoretically unaffected by lighting changes induced by alterations in the camera exposure-time or source intensity. The second criterion, called the LIP-multiplicative homogeneity criterion, is established on the LIP-multiplicative law and is unaffected by changes arising from variations in the object thickness or opacity. Both criteria are applied in Revol and Jourlin's (1997) region growing method, which focuses on the homogeneity of an image region. As a result, the region growing method becomes resistant to the lighting changes unique to each criterion. Experiments conducted on simulated and real images displaying lighting variations confirm the criteria's resistance to such changes. In comparison to a state-of-the-art method based on the image component-tree, our approach is more dependable. These results pave the way for numerous applications where lighting is uncontrolled or partly controlled.",1
"Single Image Super Resolution (SISR) techniques based on Super Resolution Convolutional Neural Networks (SRCNN) are applied to micro-computed tomography ({\mu}CT) images of sandstone and carbonate rocks. Digital rock imaging is limited by the capability of the scanning device resulting in trade-offs between resolution and field of view, and super resolution methods tested in this study aim to compensate for these limits. SRCNN models SR-Resnet, Enhanced Deep SR (EDSR), and Wide-Activation Deep SR (WDSR) are used on the Digital Rock Super Resolution 1 (DRSRD1) Dataset of 4x downsampled images, comprising of 2000 high resolution (800x800) raw micro-CT images of Bentheimer sandstone and Estaillades carbonate. The trained models are applied to the validation and test data within the dataset and show a 3-5 dB rise in image quality compared to bicubic interpolation, with all tested models performing within a 0.1 dB range. Difference maps indicate that edge sharpness is completely recovered in images within the scope of the trained model, with only high frequency noise related detail loss. We find that aside from generation of high-resolution images, a beneficial side effect of super resolution methods applied to synthetically downgraded images is the removal of image noise while recovering edgewise sharpness which is beneficial for the segmentation process. The model is also tested against real low-resolution images of Bentheimer rock with image augmentation to account for natural noise and blur. The SRCNN method is shown to act as a preconditioner for image segmentation under these circumstances which naturally leads to further future development and training of models that segment an image directly. Image restoration by SRCNN on the rock images is of significantly higher quality than traditional methods and suggests SRCNN methods are a viable processing step in a digital rock workflow.",0
"This study explores the application of Single Image Super Resolution (SISR) techniques to micro-computed tomography ({\mu}CT) images of sandstone and carbonate rocks using Super Resolution Convolutional Neural Networks (SRCNN). As the resolution of digital rock imaging is limited by the capabilities of scanning devices, the study aims to compensate for these limits by testing three SRCNN models - SR-Resnet, Enhanced Deep SR (EDSR), and Wide-Activation Deep SR (WDSR) - on the Digital Rock Super Resolution 1 (DRSRD1) Dataset of 4x downsampled images. The trained models are applied to validation and test data within the dataset and show a 3-5 dB increase in image quality compared to bicubic interpolation, with all models performing similarly. The study finds that aside from generating high-resolution images, super resolution methods applied to synthetically downgraded images can also remove image noise while recovering edgewise sharpness, which is beneficial for the segmentation process. The SRCNN method is also tested on real low-resolution images of Bentheimer rock and is shown to act as a preconditioner for image segmentation, suggesting that further development and training of models that segment an image directly is possible. Overall, the results suggest that SRCNN methods are a viable processing step in a digital rock workflow and produce significantly higher quality image restoration compared to traditional methods.",1
"Automated digital histopathology image segmentation is an important task to help pathologists diagnose tumors and cancer subtypes. For pathological diagnosis of cancer subtypes, pathologists usually change the magnification of whole-slide images (WSI) viewers. A key assumption is that the importance of the magnifications depends on the characteristics of the input image, such as cancer subtypes. In this paper, we propose a novel semantic segmentation method, called Adaptive-Weighting-Multi-Field-of-View-CNN (AWMF-CNN), that can adaptively use image features from images with different magnifications to segment multiple cancer subtype regions in the input image. The proposed method aggregates several expert CNNs for images of different magnifications by adaptively changing the weight of each expert depending on the input image. It leverages information in the images with different magnifications that might be useful for identifying the subtypes. It outperformed other state-of-the-art methods in experiments.",0
"The segmentation of histopathology images is a crucial task that aids pathologists in identifying cancer subtypes and tumors. Pathologists often adjust the magnification of whole-slide image viewers to make a pathological diagnosis of cancer subtypes. The significance of the magnification is believed to vary with the properties of the input image, such as cancer subtypes. This article proposes a new semantic segmentation technique known as Adaptive-Weighting-Multi-Field-of-View-CNN (AWMF-CNN), which can flexibly utilize image features from images with distinct magnifications to segment various cancer subtype regions in the input image. The proposed technique combines various expert CNNs for images of different magnifications by dynamically adjusting the weight of each specialist based on the input image. It employs information from images with various magnifications that could be useful in identifying the subtypes. In experiments, the proposed technique outperformed other cutting-edge techniques.",1
"We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation.",0
"The task of panoptic segmentation (PS) is introduced and examined in this study. PS combines the previously separate tasks of semantic segmentation (assigning a label to each pixel) and instance segmentation (detecting and segmenting each object instance) to generate a comprehensive and cohesive scene segmentation. This is an important advancement for real-world vision systems. Although early computer vision work dealt with similar image/scene parsing tasks, they are not currently popular, potentially due to a lack of appropriate metrics or recognition challenges. To address this, a new metric called panoptic quality (PQ) is proposed, which measures performance for all classes in a clear and unified manner. By utilizing this metric, the study investigates both human and machine performance for PS on three established datasets, revealing intriguing insights into the task. The ultimate goal of this work is to rekindle interest in a more integrated approach to image segmentation within the community.",1
"We address interactive full image annotation, where the goal is to accurately segment all object and stuff regions in an image. We propose an interactive, scribble-based annotation framework which operates on the whole image to produce segmentations for all regions. This enables sharing scribble corrections across regions, and allows the annotator to focus on the largest errors made by the machine across the whole image. To realize this, we adapt Mask-RCNN into a fast interactive segmentation framework and introduce an instance-aware loss measured at the pixel-level in the full image canvas, which lets predictions for nearby regions properly compete for space. Finally, we compare to interactive single object segmentation on the COCO panoptic dataset. We demonstrate that our interactive full image segmentation approach leads to a 5% IoU gain, reaching 90% IoU at a budget of four extreme clicks and four corrective scribbles per region.",0
"Our focus is on accurately segmenting all object and stuff regions in an image through interactive full image annotation. To achieve this, we have developed a scribble-based annotation framework that works on the entire image, allowing for scribble corrections to be shared across regions. This approach enables the annotator to concentrate on the largest errors made by the machine throughout the entire image. We have adapted Mask-RCNN to create a quick interactive segmentation framework and introduced an instance-aware loss measured at the pixel-level in the full image canvas. This loss ensures that predictions for nearby regions compete for space appropriately. Our interactive full image segmentation approach has been tested against interactive single object segmentation on the COCO panoptic dataset and has yielded a 5% IoU gain, achieving 90% IoU with only four extreme clicks and four corrective scribbles per region.",1
"Binary segmentation of volumetric images of porous media is a crucial step towards gaining a deeper understanding of the factors governing biogeochemical processes at minute scales. Contemporary work primarily revolves around primitive techniques based on global or local adaptive thresholding that have known common drawbacks in image segmentation. Moreover, absence of a unified benchmark prohibits quantitative evaluation, which further clouds the impact of existing methodologies. In this study, we tackle the issue on both fronts. Firstly, by drawing parallels with natural image segmentation, we propose a novel, and automatic segmentation technique, 3D Quantum Cuts (QCuts-3D) grounded on a state-of-the-art spectral clustering technique. Secondly, we curate and present a publicly available dataset of 68 multiphase volumetric images of porous media with diverse solid geometries, along with voxel-wise ground truth annotations for each constituting phase. We provide comparative evaluations between QCuts-3D and the current state-of-the-art over this dataset across a variety of evaluation metrics. The proposed systematic approach achieves a 26% increase in AUROC while achieving a substantial reduction of the computational complexity of the state-of-the-art competitors. Moreover, statistical analysis reveals that the proposed method exhibits significant robustness against the compositional variations of porous media.",0
"The segmentation of volumetric images of porous media into binary form is a crucial step towards understanding the factors that govern biogeochemical processes at small scales. However, current techniques based on global or local adaptive thresholding suffer from common drawbacks in image segmentation, and the absence of a benchmark makes quantitative evaluation difficult. This study addresses these issues by proposing a novel, automatic segmentation technique called QCuts-3D, which draws parallels with natural image segmentation and uses a state-of-the-art spectral clustering technique. Additionally, a publicly available dataset of 68 multiphase volumetric images of porous media with ground truth annotations is presented and used to compare QCuts-3D to the current state-of-the-art. The proposed approach achieves a 26% increase in AUROC and reduces computational complexity compared to current methods. Statistical analysis also shows that the method is robust against compositional variations of porous media.",1
"We consider the problem of referring image segmentation. Given an input image and a natural language expression, the goal is to segment the object referred by the language expression in the image. Existing works in this area treat the language expression and the input image separately in their representations. They do not sufficiently capture long-range correlations between these two modalities. In this paper, we propose a cross-modal self-attention (CMSA) module that effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the input image. In addition, we propose a gated multi-level fusion module to selectively integrate self-attentive cross-modal features corresponding to different levels in the image. This module controls the information flow of features at different levels. We validate the proposed approach on four evaluation datasets. Our proposed approach consistently outperforms existing state-of-the-art methods.",0
"The article discusses image segmentation and the issue of referring to specific objects in an image using natural language expressions. Previous research in this area has treated the language and image separately in their representations, which fails to take into account the connections between the two modalities. To address this issue, the authors propose a cross-modal self-attention module that can effectively capture long-range dependencies between linguistic and visual features. The model can focus on important words in the language expression and significant regions in the image. The authors also introduce a gated multi-level fusion module that selectively integrates self-attentive cross-modal features at different levels in the image. The proposed approach outperforms existing methods on four evaluation datasets.",1
"Cloud based medical image analysis has become popular recently due to the high computation complexities of various deep neural network (DNN) based frameworks and the increasingly large volume of medical images that need to be processed. It has been demonstrated that for medical images the transmission from local to clouds is much more expensive than the computation in the clouds itself. Towards this, 3D image compression techniques have been widely applied to reduce the data traffic. However, most of the existing image compression techniques are developed around human vision, i.e., they are designed to minimize distortions that can be perceived by human eyes. In this paper we will use deep learning based medical image segmentation as a vehicle and demonstrate that interestingly, machine and human view the compression quality differently. Medical images compressed with good quality w.r.t. human vision may result in inferior segmentation accuracy. We then design a machine vision oriented 3D image compression framework tailored for segmentation using DNNs. Our method automatically extracts and retains image features that are most important to the segmentation. Comprehensive experiments on widely adopted segmentation frameworks with HVSMR 2016 challenge dataset show that our method can achieve significantly higher segmentation accuracy at the same compression rate, or much better compression rate under the same segmentation accuracy, when compared with the existing JPEG 2000 method. To the best of the authors' knowledge, this is the first machine vision guided medical image compression framework for segmentation in the clouds.",0
"The popularity of cloud-based medical image analysis has increased due to the complex computations required by deep neural network (DNN) frameworks and the growing volume of medical images that need to be processed. It has been found that the cost of transmitting medical images from local machines to the cloud is higher than the cost of computation in the cloud. Therefore, 3D image compression techniques have been widely used to reduce data traffic. However, existing compression techniques are designed to minimize distortions perceived by the human eye, and not necessarily best for machine vision. This paper focuses on deep learning-based medical image segmentation and shows that machine and human vision perceive compression quality differently. Compression quality that is good for human vision may result in inferior segmentation accuracy. Hence, a machine vision-oriented 3D image compression framework is proposed, which is tailored for segmentation using DNNs. The proposed method automatically extracts and retains the most important image features for segmentation, achieving significantly higher segmentation accuracy at the same compression rate than existing methods. This is the first machine vision-guided medical image compression framework for segmentation in the clouds.",1
"Morphological reconstruction (MR) is often employed by seeded image segmentation algorithms such as watershed transform and power watershed as it is able to filter seeds (regional minima) to reduce over-segmentation. However, MR might mistakenly filter meaningful seeds that are required for generating accurate segmentation and it is also sensitive to the scale because a single-scale structuring element is employed. In this paper, a novel adaptive morphological reconstruction (AMR) operation is proposed that has three advantages. Firstly, AMR can adaptively filter useless seeds while preserving meaningful ones. Secondly, AMR is insensitive to the scale of structuring elements because multiscale structuring elements are employed. Finally, AMR has two attractive properties: monotonic increasingness and convergence that help seeded segmentation algorithms to achieve a hierarchical segmentation. Experiments clearly demonstrate that AMR is useful for improving algorithms of seeded image segmentation and seed-based spectral segmentation. Compared to several state-of-the-art algorithms, the proposed algorithms provide better segmentation results requiring less computing time. Source code is available at https://github.com/SUST-reynole/AMR.",0
"Seeded image segmentation algorithms like watershed transform and power watershed often use Morphological Reconstruction (MR) to reduce over-segmentation by filtering seeds (regional minima). However, MR can mistakenly filter important seeds needed for accurate segmentation and is scale-sensitive due to the use of a single-scale structuring element. This paper introduces an Adaptive Morphological Reconstruction (AMR) operation with three benefits. Firstly, it can adaptively filter unnecessary seeds while retaining meaningful ones. Secondly, it is insensitive to the scale of structuring elements as it employs multiscale structuring elements. Thirdly, it has two attractive properties: monotonic increasingness and convergence, which help seeded segmentation algorithms achieve hierarchical segmentation. Experiments show that AMR improves seeded image segmentation and seed-based spectral segmentation algorithms and provides better segmentation results with less computing time than several state-of-the-art algorithms. The source code for AMR is available at https://github.com/SUST-reynole/AMR.",1
"Image segmentation is an important task in many medical applications. Methods based on convolutional neural networks attain state-of-the-art accuracy; however, they typically rely on supervised training with large labeled datasets. Labeling medical images requires significant expertise and time, and typical hand-tuned approaches for data augmentation fail to capture the complex variations in such images.   We present an automated data augmentation method for synthesizing labeled medical images. We demonstrate our method on the task of segmenting magnetic resonance imaging (MRI) brain scans. Our method requires only a single segmented scan, and leverages other unlabeled scans in a semi-supervised approach. We learn a model of transformations from the images, and use the model along with the labeled example to synthesize additional labeled examples. Each transformation is comprised of a spatial deformation field and an intensity change, enabling the synthesis of complex effects such as variations in anatomy and image acquisition procedures. We show that training a supervised segmenter with these new examples provides significant improvements over state-of-the-art methods for one-shot biomedical image segmentation. Our code is available at https://github.com/xamyzhao/brainstorm.",0
"In medical applications, image segmentation is a crucial task. Although convolutional neural networks yield high accuracy, they typically require supervised training with large labeled datasets, which is time-consuming and requires expertise. Additionally, conventional data augmentation methods are unable to capture complex variations in medical images. To address these issues, we present an automatic data augmentation technique for generating labeled medical images, which we apply to segmenting MRI brain scans. Our approach only requires one segmented scan and uses other unlabeled scans in a semi-supervised manner. By learning a model of transformations from the images, we can synthesize additional labeled examples by combining the model and the labeled example. Each transformation involves a spatial deformation field and an intensity change, allowing us to synthesize intricate effects like variations in anatomy and image acquisition procedures. We demonstrate that training a supervised segmenter with these new examples yields significant improvements over state-of-the-art one-shot biomedical image segmentation methods. Interested parties may access our code at https://github.com/xamyzhao/brainstorm.",1
"Referring object detection and referring image segmentation are important tasks that require joint understanding of visual information and natural language. Yet there has been evidence that current benchmark datasets suffer from bias, and current state-of-the-art models cannot be easily evaluated on their intermediate reasoning process. To address these issues and complement similar efforts in visual question answering, we build CLEVR-Ref+, a synthetic diagnostic dataset for referring expression comprehension. The precise locations and attributes of the objects are readily available, and the referring expressions are automatically associated with functional programs. The synthetic nature allows control over dataset bias (through sampling strategy), and the modular programs enable intermediate reasoning ground truth without human annotators.   In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we also propose IEP-Ref, a module network approach that significantly outperforms other models on our dataset. In particular, we present two interesting and important findings using IEP-Ref: (1) the module trained to transform feature maps into segmentation masks can be attached to any intermediate module to reveal the entire reasoning process step-by-step; (2) even if all training data has at least one object referred, IEP-Ref can correctly predict no-foreground when presented with false-premise referring expressions. To the best of our knowledge, this is the first direct and quantitative proof that neural modules behave in the way they are intended.",0
"The comprehension of referring object detection and image segmentation requires a joint understanding of visual information and natural language. However, there is evidence that current benchmark datasets suffer from bias and current state-of-the-art models lack an easily evaluated intermediate reasoning process. To address these issues and complement similar efforts in visual question answering, a synthetic diagnostic dataset called CLEVR-Ref+ has been built. This dataset provides precise locations and attributes of objects, and the referring expressions are automatically associated with functional programs. The synthetic nature allows control over dataset bias and the modular programs enable intermediate reasoning ground truth without human annotators. In addition, a module network approach called IEP-Ref has been proposed, which outperforms other models on the CLEVR-Ref+ dataset. IEP-Ref also reveals two interesting and important findings: the transformation of feature maps into segmentation masks can be attached to any intermediate module to reveal the entire reasoning process step-by-step, and even false-premise referring expressions can be correctly predicted as no-foreground. This is the first direct and quantitative proof that neural modules behave as intended.",1
"Recently, Neural Architecture Search (NAS) has successfully identified neural network architectures that exceed human designed ones on large-scale image classification. In this paper, we study NAS for semantic image segmentation. Existing works often focus on searching the repeatable cell structure, while hand-designing the outer network structure that controls the spatial resolution changes. This choice simplifies the search space, but becomes increasingly problematic for dense image prediction which exhibits a lot more network level architectural variations. Therefore, we propose to search the network level structure in addition to the cell level structure, which forms a hierarchical architecture search space. We present a network level search space that includes many popular designs, and develop a formulation that allows efficient gradient-based architecture search (3 P100 GPU days on Cityscapes images). We demonstrate the effectiveness of the proposed method on the challenging Cityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our architecture searched specifically for semantic image segmentation, attains state-of-the-art performance without any ImageNet pretraining.",0
"The discovery of neural network architectures that surpass those designed by humans for large-scale image classification has been achieved by Neural Architecture Search (NAS). This study investigates the use of NAS for semantic image segmentation. Prior research has concentrated on searching for the repeatable cell structure, while leaving the outer network structure, which manages spatial resolution changes, to be hand-designed. Although this simplifies the search space, it is problematic for dense image prediction, which has a more varied architectural structure. To address this, we propose a hierarchical architecture search space that includes both the cell level and network level structure. We present a network level search space with various designs and a formulation that allows efficient gradient-based architecture search. Our proposed method, Auto-DeepLab, successfully achieved state-of-the-art performance on the Cityscapes, PASCAL VOC 2012, and ADE20K datasets without pretraining on ImageNet.",1
"The prevalence of skin melanoma is rapidly increasing as well as the recorded death cases of its patients. Automatic image segmentation tools play an important role in providing standardized computer-assisted analysis for skin melanoma patients. Current state-of-the-art segmentation methods are based on fully convolutional neural networks, which utilize an encoder-decoder approach. However, these methods produce coarse segmentation masks due to the loss of location information during the encoding layers. Inspired by Pyramid Scene Parsing Network (PSP-Net), we propose an encoder-decoder model that utilizes pyramid pooling modules in the deep skip connections which aggregate the global context and compensate for the lost spatial information. We trained and validated our approach using ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection grand challenge dataset. Our approach showed a validation accuracy with a Jaccard index of 0.837, which outperforms U-Net. We believe that with this reported reliable accuracy, this method can be introduced for clinical practice.",0
"The incidence of skin melanoma is on the rise, and there has been an increase in death cases among patients. Automatic image segmentation tools are crucial for computer-assisted analysis for those affected by skin melanoma. The latest segmentation methods use fully convolutional neural networks that follow an encoder-decoder approach. However, these methods produce imprecise segmentation masks due to the loss of location information during the encoding process. We propose a new model that uses pyramid pooling modules in the deep skip connections to compensate for the lost spatial information. Inspired by Pyramid Scene Parsing Network (PSP-Net), we trained and validated our approach using the ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection grand challenge dataset. Our approach outperformed U-Net, with a Jaccard index of 0.837, and we believe that it can be introduced for clinical practice based on its reliable accuracy.",1
"In applications of supervised learning applied to medical image segmentation, the need for large amounts of labeled data typically goes unquestioned. In particular, in the case of brain anatomy segmentation, hundreds or thousands of weakly-labeled volumes are often used as training data. In this paper, we first observe that for many brain structures, a small number of training examples, (n=9), weakly labeled using Freesurfer 6.0, plus simple data augmentation, suffice as training data to achieve high performance, achieving an overall mean Dice coefficient of $0.84 \pm 0.12$ compared to Freesurfer over 28 brain structures in T1-weighted images of $\approx 4000$ 9-10 year-olds from the Adolescent Brain Cognitive Development study. We then examine two varieties of heteroscedastic network as a method for improving classification results. An existing proposal by Kendall and Gal, which uses Monte-Carlo inference to learn to predict the variance of each prediction, yields an overall mean Dice of $0.85 \pm 0.14$ and showed statistically significant improvements over 25 brain structures. Meanwhile a novel heteroscedastic network which directly learns the probability that an example has been mislabeled yielded an overall mean Dice of $0.87 \pm 0.11$ and showed statistically significant improvements over all but one of the brain structures considered. The loss function associated to this network can be interpreted as performing a form of learned label smoothing, where labels are only smoothed where they are judged to be uncertain.",0
"The usual practice in supervised medical image segmentation using machine learning involves the use of large amounts of labeled data. For brain anatomy segmentation, hundreds or thousands of weakly-labeled volumes are commonly utilized for training. However, this study presents a different approach by demonstrating that only a few training examples (n=9), weakly labeled using Freesurfer 6.0 and with the aid of simple data augmentation, are sufficient to achieve high performance. Specifically, an overall mean Dice coefficient of $0.84 \pm 0.12$ was achieved compared to Freesurfer over 28 brain structures in T1-weighted images of around 4000 9-10 year-olds from the Adolescent Brain Cognitive Development study. The study also explores two types of heteroscedastic network to improve classification results. The first, proposed by Kendall and Gal, uses Monte-Carlo inference to predict the variance of each prediction and yielded an overall mean Dice of $0.85 \pm 0.14$, showing significant improvements over 25 brain structures. The second, a novel heteroscedastic network, directly learns the probability of mislabeling and achieved an overall mean Dice of $0.87 \pm 0.11$, showing significant improvements over all but one of the brain structures considered. The loss function of this network can be interpreted as a form of learned label smoothing, where uncertain labels are smoothed.",1
"We propose an active learning approach to image segmentation that exploits geometric priors to speed up and streamline the annotation process. It can be applied for both background-foreground and multi-class segmentation tasks in 2D images and 3D image volumes. Our approach combines geometric smoothness priors in the image space with more traditional uncertainty measures to estimate which pixels or voxels are the most informative, and thus should to be annotated next. For multi-class settings, we additionally introduce two novel criteria for uncertainty. In the 3D case, we use the resulting uncertainty measure to select voxels lying on a planar patch, which makes batch annotation much more convenient for the end user compared to the setting where voxels are randomly distributed in a volume. The planar patch is found using a branch-and-bound algorithm that looks for a 2D patch in a 3D volume where the most informative instances are located. We evaluate our approach on Electron Microscopy and Magnetic Resonance image volumes, as well as on regular images of horses and faces. We demonstrate a substantial performance increase over other approaches thanks to the use of geometric priors.",0
"Our proposed approach to image segmentation utilizes geometric priors to enhance and accelerate the annotation process. This technique is applicable for both 2D and 3D image volumes, and can be used for background-foreground and multi-class segmentation tasks. Our method combines geometric smoothness priors with traditional uncertainty measures to determine the most informative pixels or voxels for annotation. For multi-class settings, we introduce two new uncertainty criteria. In 3D, we employ an uncertainty measure to select voxels on a planar patch, which simplifies batch annotation for users. A branch-and-bound algorithm finds the planar patch containing the most informative instances. Our approach is evaluated on various images, including Electron Microscopy and Magnetic Resonance image volumes, as well as regular images of horses and faces. Our method outperforms other techniques by leveraging geometric priors.",1
"Since the generative neural networks have made a breakthrough in the image generation problem, lots of researches on their applications have been studied such as image restoration, style transfer and image completion. However, there has been few research generating objects in uncontrolled real-world environments. In this paper, we propose a novel approach for vehicle image generation in real-world scenes. Using a subnetwork based on a precedent work of image completion, our model makes the shape of an object. Details of objects are trained by an additional colorization and refinement subnetwork, resulting in a better quality of generated objects. Unlike many other works, our method does not require any segmentation layout but still makes a plausible vehicle in the image. We evaluate our method by using images from Berkeley Deep Drive (BDD) and Cityscape datasets, which are widely used for object detection and image segmentation problems. The adequacy of the generated images by the proposed method has also been evaluated using a widely utilized object detection algorithm and the FID score.",0
"Numerous studies have explored the potential applications of generative neural networks since they have been successful in solving the image generation problem. These studies have focused on areas such as image restoration, style transfer, and image completion. However, there has been limited research on generating objects in uncontrolled, real-world environments. Our paper proposes a unique method for generating vehicle images in real-world scenes. Our model uses a subnetwork based on a previous image completion work to create the shape of the object, and an additional colorization and refinement subnetwork trains the object's details, resulting in higher-quality generated objects. Unlike other methods, our approach does not require any segmentation layout, yet still produces convincing vehicle images. We evaluate our method on images from the BDD and Cityscape datasets, which are commonly used for object detection and image segmentation, while also assessing the generated images' quality using an object detection algorithm and the FID score.",1
"The design and performance of computer vision algorithms are greatly influenced by the hardware on which they are implemented. CPUs, multi-core CPUs, FPGAs and GPUs have inspired new algorithms and enabled existing ideas to be realized. This is notably the case with GPUs, which has significantly changed the landscape of computer vision research through deep learning. As the end of Moores law approaches, researchers and hardware manufacturers are exploring alternative hardware computing paradigms. Quantum computers are a very promising alternative and offer polynomial or even exponential speed-ups over conventional computing for some problems. This paper presents a novel approach to image segmentation that uses new quantum computing hardware. Segmentation is formulated as a graph cut problem that can be mapped to the quantum approximate optimization algorithm (QAOA). This algorithm can be implemented on current and near-term quantum computers. Encouraging results are presented on artificial and medical imaging data. This represents an important, practical step towards leveraging quantum computers for computer vision.",0
"The hardware used to implement computer vision algorithms greatly affects their design and performance. CPUs, multi-core CPUs, FPGAs, and GPUs have inspired and enabled new and existing algorithms, particularly GPUs which have revolutionized computer vision research through deep learning. As Moore's law nears its end, researchers and hardware manufacturers are exploring alternative computing paradigms, such as quantum computers which offer significant speed-ups for some problems. In this paper, a novel approach to image segmentation is presented, using quantum computing hardware. The segmentation problem is formulated as a graph cut problem which can be mapped to the quantum approximate optimization algorithm (QAOA), an algorithm that can be used on current and near-future quantum computers. Encouraging results are shown for artificial and medical imaging data, representing a practical and important step towards utilizing quantum computers for computer vision.",1
"Liver lesion segmentation is a difficult yet critical task for medical image analysis. Recently, deep learning based image segmentation methods have achieved promising performance, which can be divided into three categories: 2D, 2.5D and 3D, based on the dimensionality of the models. However, 2.5D and 3D methods can have very high complexity and 2D methods may not perform satisfactorily. To obtain competitive performance with low complexity, in this paper, we propose a Feature-fusion Encoder-Decoder Network (FED-Net) based 2D segmentation model to tackle the challenging problem of liver lesion segmentation from CT images. Our feature fusion method is based on the attention mechanism, which fuses high-level features carrying semantic information with low-level features having image details. Additionally, to compensate for the information loss during the upsampling process, a dense upsampling convolution and a residual convolutional structure are proposed. We tested our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge and achieved competitive results compared with other state-of-the-art methods.",0
"Medical image analysis requires the challenging task of liver lesion segmentation. Deep learning methods have shown promise in achieving accurate segmentation, with 2D, 2.5D, and 3D models being the most commonly used. However, 2.5D and 3D models can be overly complex, while 2D models may not provide satisfactory results. To address this issue, we propose the Feature-fusion Encoder-Decoder Network (FED-Net) based on a 2D segmentation model. Our approach employs an attention mechanism to fuse high-level semantic features with low-level image details. We also introduce a dense upsampling convolution and a residual convolutional structure to compensate for information loss during the upsampling process. We evaluated our method on the MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge dataset and achieved highly competitive results compared to other state-of-the-art methods.",1
"We propose a method to classify cardiac pathology based on a novel approach to extract image derived features to characterize the shape and motion of the heart. An original semi-supervised learning procedure, which makes efficient use of a large amount of non-segmented images and a small amount of images segmented manually by experts, is developed to generate pixel-wise apparent flow between two time points of a 2D+t cine MRI image sequence. Combining the apparent flow maps and cardiac segmentation masks, we obtain a local apparent flow corresponding to the 2D motion of myocardium and ventricular cavities. This leads to the generation of time series of the radius and thickness of myocardial segments to represent cardiac motion. These time series of motion features are reliable and explainable characteristics of pathological cardiac motion. Furthermore, they are combined with shape-related features to classify cardiac pathologies. Using only nine feature values as input, we propose an explainable, simple and flexible model for pathology classification. On ACDC training set and testing set, the model achieves 95% and 94% respectively as classification accuracy. Its performance is hence comparable to that of the state-of-the-art. Comparison with various other models is performed to outline some advantages of our model.",0
"Our proposed technique utilizes a new approach to extract image-based features for identifying cardiac abnormalities. We have developed an innovative semi-supervised learning process that efficiently uses both non-segmented and manually-segmented images to generate pixel-wise apparent flow between two time points in a 2D+t cine MRI image sequence. This enables us to obtain a local apparent flow that corresponds to the 2D motion of the myocardium and ventricular cavities, which we then use to create time series of myocardial segment radius and thickness to represent cardiac motion. These features are reliable and easily explainable, making them ideal for identifying pathological cardiac motion when combined with other shape-related features. Our model is simple, flexible, and explainable, requiring only nine feature values as input. In testing, our model achieved a classification accuracy of 95% and 94% on the ACDC training and testing sets, respectively, comparable to the state-of-the-art. We have also compared our model to various other models, highlighting some of its advantages.",1
"This paper presents a new approach for relatively accurate brain region of interest (ROI) detection from dynamic susceptibility contrast (DSC) perfusion magnetic resonance (MR) images of a human head with abnormal brain anatomy. Such images produce problems for automatic brain segmentation algorithms, and as a result, poor perfusion ROI detection affects both quantitative measurements and visual assessment of perfusion data. In the proposed approach image segmentation is based on CUSUM filter usage that was adapted to be applicable to process DSC perfusion MR images. The result of segmentation is a binary mask of brain ROI that is generated via usage of brain boundary location. Each point of the boundary between the brain and surrounding tissues is detected as a change-point by CUSUM filter. Proposed adopted CUSUM filter operates by accumulating the deviations between the observed and expected intensities of image points at the time of moving on a trajectory. Motion trajectory is created by the iterative change of movement direction inside the background region in order to reach brain region, and vice versa after boundary crossing. Proposed segmentation approach was evaluated with Dice index comparing obtained results to the reference standard. Manually marked brain region pixels (reference standard), as well as visual inspection of detected with CUSUM filter usage brain ROI, were provided by experienced radiologists. The results showed that proposed approach is suitable to be used for brain ROI detection from DSC perfusion MR images of a human head with abnormal brain anatomy and can, therefore, be applied in the DSC perfusion data analysis.",0
"This article introduces a novel technique for accurately detecting regions of interest (ROIs) in dynamic susceptibility contrast (DSC) perfusion magnetic resonance (MR) images of human brains with abnormal anatomy. The automatic segmentation algorithms used in such images often encounter difficulties, leading to inaccurate perfusion ROI detection and affecting both quantitative measurements and visual assessment of perfusion data. The proposed approach employs the CUSUM filter, which has been adapted to process DSC perfusion MR images, to segment the images and generate a binary mask of brain ROIs based on the boundary location of the brain. The filter detects the change-points between the brain and surrounding tissues and accumulates deviations between observed and expected intensities as it moves along a trajectory. The approach was evaluated using the Dice index and compared to manually marked brain region pixels and visual inspection by experienced radiologists. The results indicate that the proposed approach is suitable for detecting brain ROIs in DSC perfusion MR images of human brains with abnormal anatomy and can be applied in perfusion data analysis.",1
"Over the past few years, deep learning techniques have achieved tremendous success in many visual understanding tasks such as object detection, image segmentation, and caption generation. Despite this thriving in computer vision and natural language processing, deep learning has not yet shown significant impact in robotics. Due to the gap between theory and application, there are many challenges when applying the results of deep learning to the real robotic systems. In this study, our long-term goal is to bridge the gap between computer vision and robotics by developing visual methods that can be used in real robots. In particular, this work tackles two fundamental visual problems for autonomous robotic manipulation: affordance detection and fine-grained action understanding. Theoretically, we propose different deep architectures to further improves the state of the art in each problem. Empirically, we show that the outcomes of our proposed methods can be applied in real robots and allow them to perform useful manipulation tasks.",0
"In recent years, deep learning has accomplished remarkable feats in various visual recognition tasks like object detection, image segmentation, and captioning. Despite its success in computer vision and natural language processing, deep learning has not made a significant impact in robotics. The disparity between theory and practical application poses several challenges when implementing deep learning results in actual robotic systems. This research aims to bridge this gap by developing visual techniques that can be employed in real robots. The study focuses on two key visual problems for autonomous robotic manipulation: affordance detection and fine-grained action understanding. The proposed deep architectures aim to enhance the existing state of the art in each problem theoretically. The practical application of the proposed methods in real robots demonstrates their effectiveness in performing useful manipulation tasks.",1
"In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.   The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters",0
"Presented in this paper is a novel mobile architecture, MobileNetV2, which enhances the performance of mobile models across a variety of tasks and benchmarks while accommodating diverse model sizes. We introduce an innovative framework called SSDLite, demonstrating efficient means of applying mobile models to object detection. Moreover, we illustrate how to create mobile semantic segmentation models through a reduced version of DeepLabv3, known as Mobile DeepLabv3. The MobileNetV2 architecture is founded on an inverted residual structure, utilizing narrow bottleneck layers for the input and output of the residual block in contrast to traditional residual models that use expanded representations in the input. To filter features in the intermediate expansion layer, MobileNetV2 employs lightweight depthwise convolutions. Omitting non-linearities in the narrow layers is crucial to maintaining representational power, enhancing performance, and providing a design intuition. Furthermore, our approach facilitates the separation of the input/output domains from the expressiveness of the transformation, allowing for further analysis. We evaluate our performance on Imagenet classification, COCO object detection, and VOC image segmentation, analyzing the trade-offs between accuracy, number of operations measured by multiply-adds (MAdd), and number of parameters.",1
"In this paper, we investigate how to learn a suitable representation of satellite image time series in an unsupervised manner by leveraging large amounts of unlabeled data. Additionally , we aim to disentangle the representation of time series into two representations: a shared representation that captures the common information between the images of a time series and an exclusive representation that contains the specific information of each image of the time series. To address these issues, we propose a model that combines a novel component called cross-domain autoencoders with the variational autoencoder (VAE) and generative ad-versarial network (GAN) methods. In order to learn disentangled representations of time series, our model learns the multimodal image-to-image translation task. We train our model using satellite image time series from the Sentinel-2 mission. Several experiments are carried out to evaluate the obtained representations. We show that these disentangled representations can be very useful to perform multiple tasks such as image classification, image retrieval, image segmentation and change detection.",0
"The purpose of our research is to explore the unsupervised learning of a suitable representation for satellite image time series, utilizing vast amounts of unlabeled data. Our goal is to separate the representation of time series into two parts: a shared representation that captures the common information between the images and an exclusive representation that contains the specific information of each image. To accomplish this, we propose a model that combines cross-domain autoencoders with VAE and GAN methods. Our model learns the multimodal image-to-image translation task to obtain disentangled representations of time series. We train our model using Sentinel-2 mission satellite image time series and conduct various experiments to evaluate the obtained representations. Our results demonstrate the usefulness of these disentangled representations in multiple tasks, including image classification, image retrieval, image segmentation, and change detection.",1
"Image segmentation plays an essential role in medicine for both diagnostic and interventional tasks. Segmentation approaches are either manual, semi-automated or fully-automated. Manual segmentation offers full control over the quality of the results, but is tedious, time consuming and prone to operator bias. Fully automated methods require no human effort, but often deliver sub-optimal results without providing users with the means to make corrections. Semi-automated approaches keep users in control of the results by providing means for interaction, but the main challenge is to offer a good trade-off between precision and required interaction. In this paper we present a deep learning (DL) based semi-automated segmentation approach that aims to be a ""smart"" interactive tool for region of interest delineation in medical images. We demonstrate its use for segmenting multiple organs on computed tomography (CT) of the abdomen. Our approach solves some of the most pressing clinical challenges: (i) it requires only one to a few user clicks to deliver excellent 2D segmentations in a fast and reliable fashion; (ii) it can generalize to previously unseen structures and ""corner cases""; (iii) it delivers results that can be corrected quickly in a smart and intuitive way up to an arbitrary degree of precision chosen by the user and (iv) ensures high accuracy. We present our approach and compare it to other techniques and previous work to show the advantages brought by our method.",0
"In medicine, image segmentation is crucial for both diagnostic and interventional tasks. Segmentation methods can be manual, semi-automated, or fully automated. Manual segmentation provides control over the quality of results but is time-consuming and prone to operator bias. Fully automated methods are effortless but often deliver sub-optimal results without user correction. Semi-automated approaches allow users to interact with the results, but balancing precision and required interaction can be challenging. This paper presents a deep learning-based semi-automated segmentation approach that serves as a smart interactive tool for region of interest delineation in medical images. The approach requires only one to a few user clicks and delivers excellent 2D segmentations quickly and reliably. It can generalize to previously unseen structures and corner cases, and results can be corrected quickly and intuitively to an arbitrary degree of user-chosen precision. The approach ensures high accuracy and addresses pressing clinical challenges. The paper presents the approach, compares it to other techniques and previous work, and highlights the advantages brought by this method.",1
"Dilated Convolutions have been shown to be highly useful for the task of image segmentation. By introducing gaps into convolutional filters, they enable the use of larger receptive fields without increasing the original kernel size. Even though this allows for the inexpensive capturing of features at different scales, the structure of the dilated convolutional filter leads to a loss of information. We hypothesise that inexpensive modifications to Dilated Convolutional Neural Networks, such as additional averaging layers, could overcome this limitation. In this project we test this hypothesis by evaluating the effect of these modifications for a state-of-the art image segmentation system and compare them to existing approaches with the same objective. Our experiments show that our proposed methods improve the performance of dilated convolutions for image segmentation. Crucially, our modifications achieve these results at a much lower computational cost than previous smoothing approaches.",0
"Dilated Convolutions are proven to be highly beneficial in image segmentation. They create gaps in convolutional filters, allowing for larger receptive fields without increasing the kernel size. This captures features at different scales inexpensively, but the dilated convolutional filter structure leads to information loss. We propose that Dilated Convolutional Neural Networks can be modified inexpensively with additional averaging layers to overcome this limitation. Our project tests this hypothesis by comparing our modifications to existing approaches for a state-of-the-art image segmentation system. Our experiments demonstrate that our methods improve the performance of dilated convolutions for image segmentation while maintaining a lower computational cost than previous smoothing approaches.",1
"Plant root research can provide a way to attain stress-tolerant crops that produce greater yield in a diverse array of conditions. Phenotyping roots in soil is often challenging due to the roots being difficult to access and the use of time consuming manual methods. Rhizotrons allow visual inspection of root growth through transparent surfaces. Agronomists currently manually label photographs of roots obtained from rhizotrons using a line-intersect method to obtain root length density and rooting depth measurements which are essential for their experiments. We investigate the effectiveness of an automated image segmentation method based on the U-Net Convolutional Neural Network (CNN) architecture to enable such measurements. We design a data-set of 50 annotated Chicory (Cichorium intybus L.) root images which we use to train, validate and test the system and compare against a baseline built using the Frangi vesselness filter. We obtain metrics using manual annotations and line-intersect counts. Our results on the held out data show our proposed automated segmentation system to be a viable solution for detecting and quantifying roots. We evaluate our system using 867 images for which we have obtained line-intersect counts, attaining a Spearman rank correlation of 0.9748 and an $r^2$ of 0.9217. We also achieve an $F_1$ of 0.7 when comparing the automated segmentation to the manual annotations, with our automated segmentation system producing segmentations with higher quality than the manual annotations for large portions of the image.",0
"Research on plant roots can lead to the development of crops that can withstand stress and produce higher yields in various conditions. However, examining roots in soil can be difficult due to limited accessibility and time-consuming manual methods. Rhizotrons, which have transparent surfaces, allow for visual observation of root growth. Currently, agronomists manually label photographs of roots obtained from rhizotrons using a line-intersect method to obtain measurements that are necessary for their experiments. In this study, we investigated the effectiveness of an automated image segmentation method based on the U-Net Convolutional Neural Network architecture to enable precise measurements. We created a dataset of 50 annotated Chicory root images for training, validation, and testing. Our results indicate that our proposed automated segmentation system is a viable solution for detecting and quantifying roots. We evaluated our system using 867 images and achieved a high Spearman rank correlation and F1 score when comparing the automated segmentation to the manual annotations. The automated segmentation system also produced higher quality segmentations than manual annotations for a significant portion of the image.",1
"Superpixels have become very popular in many computer vision applications. Nevertheless, they remain underexploited since the superpixel decomposition may produce irregular and non stable segmentation results due to the dependency to the image content. In this paper, we first introduce a novel structure, a superpixel-based patch, called SuperPatch. The proposed structure, based on superpixel neighborhood, leads to a robust descriptor since spatial information is naturally included. The generalization of the PatchMatch method to SuperPatches, named SuperPatchMatch, is introduced. Finally, we propose a framework to perform fast segmentation and labeling from an image database, and demonstrate the potential of our approach since we outperform, in terms of computational cost and accuracy, the results of state-of-the-art methods on both face labeling and medical image segmentation.",0
"Despite their popularity in various computer vision applications, superpixels are not fully utilized due to their tendency to produce irregular and unstable segmentation results that depend on the image content. To address this issue, this paper introduces a new structure called SuperPatch, which is a superpixel-based patch that includes spatial information and leads to a more robust descriptor. The PatchMatch method is also generalized to SuperPatches, known as SuperPatchMatch. A framework is proposed for rapid segmentation and labeling of an image database, which outperforms state-of-the-art methods in terms of computational cost and accuracy for both face labeling and medical image segmentation.",1
"Different empirical models have been developed for cloud detection. There is a growing interest in using the ground-based sky/cloud images for this purpose. Several methods exist that perform binary segmentation of clouds. In this paper, we propose to use a deep learning architecture (U-Net) to perform multi-label sky/cloud image segmentation. The proposed approach outperforms recent literature by a large margin.",0
"Cloud detection has been approached through various empirical models, including the utilization of ground-based sky/cloud images. Multiple binary segmentation techniques have been developed for this purpose. Our study introduces a novel method utilizing a deep learning architecture, specifically U-Net, to achieve multi-label sky/cloud image segmentation. Our proposed approach has demonstrated significant superiority compared to recent literature.",1
"In this paper, we propose to tackle the problem of reducing discrepancies between multiple domains referred to as multi-source domain adaptation and consider it under the target shift assumption: in all domains we aim to solve a classification problem with the same output classes, but with labels' proportions differing across them. This problem, generally ignored in the vast majority papers on domain adaptation papers, is nevertheless critical in real-world applications, and we theoretically show its impact on the adaptation success. To address this issue, we design a method based on optimal transport, a theory that has been successfully used to tackle adaptation problems in machine learning. Our method performs multi-source adaptation and target shift correction simultaneously by learning the class probabilities of the unlabeled target sample and the coupling allowing to align two (or more) probability distributions. Experiments on both synthetic and real-world data related to satellite image segmentation task show the superiority of the proposed method over the state-of-the-art.",0
"The focus of this paper is to address the challenge of minimizing disparities between various domains, known as multi-source domain adaptation, under the assumption of target shift. The classification problem in all domains remains the same, but the label proportions differ. This issue is often overlooked in domain adaptation literature, despite being crucial in practical applications. We demonstrate its impact on the success of adaptation and propose a solution based on optimal transport, a theory widely used in machine learning for adaptation problems. Our approach tackles multi-source adaptation and target shift correction simultaneously by learning the class probabilities of the target sample and coupling to align probability distributions. Our experiments on both artificial and real-world data, specifically satellite image segmentation, show that our method outperforms the state-of-the-art.",1
"Large-scale annotation of image segmentation datasets is often prohibitively expensive, as it usually requires a huge number of worker hours to obtain high-quality results. Abundant and reliable data has been, however, crucial for the advances on image understanding tasks achieved by deep learning models. In this paper, we introduce FreeLabel, an intuitive open-source web interface that allows users to obtain high-quality segmentation masks with just a few freehand scribbles, in a matter of seconds. The efficacy of FreeLabel is quantitatively demonstrated by experimental results on the PASCAL dataset as well as on a dataset from the agricultural domain. Designed to benefit the computer vision community, FreeLabel can be used for both crowdsourced or private annotation and has a modular structure that can be easily adapted for any image dataset.",0
"The cost of annotating image segmentation datasets on a large scale is often too high due to the significant amount of labor required to achieve accurate results. Nonetheless, reliable and extensive data is essential for the advancements made in image understanding tasks by deep learning models. This article presents FreeLabel, an open-source web interface that enables users to generate high-quality segmentation masks within seconds by merely drawing some freehand scribbles. The effectiveness of FreeLabel is demonstrated by quantitative experiments carried out on both the PASCAL dataset and an agricultural dataset. Created to benefit the computer vision community, FreeLabel can be employed for either crowdsourced or private annotation and has a modular structure that can be effortlessly customized to suit any image dataset.",1
"A fully automated knee MRI segmentation method to study osteoarthritis (OA) was developed using a novel hierarchical set of random forests (RF) classifiers to learn the appearance of cartilage regions and their boundaries. A neighborhood approximation forest is used first to provide contextual feature to the second-level RF classifier that also considers local features and produces location-specific costs for the layered optimal graph image segmentation of multiple objects and surfaces (LOGISMOS) framework. Double echo steady state (DESS) MRIs used in this work originated from the Osteoarthritis Initiative (OAI) study. Trained on 34 MRIs with varying degrees of OA, the performance of the learning-based method tested on 108 MRIs showed a significant reduction in segmentation errors (\emph{p}$<$0.05) compared with the conventional gradient-based and single-stage RF-learned costs. The 3D LOGISMOS was extended to longitudinal-3D (4D) to simultaneously segment multiple follow-up visits of the same patient. As such, data from all time-points of the temporal sequence contribute information to a single optimal solution that utilizes both spatial 3D and temporal contexts. 4D LOGISMOS validation on 108 MRIs from baseline and 12 month follow-up scans of 54 patients showed a significant reduction in segmentation errors (\emph{p}$<$0.01) compared to 3D. Finally, the potential of 4D LOGISMOS was further explored on the same 54 patients using 5 annual follow-up scans demonstrating a significant improvement of measuring cartilage thickness (\emph{p}$<$0.01) compared to the sequential 3D approach.",0
"The researchers developed a new knee MRI segmentation method for studying osteoarthritis (OA) that is fully automated. This method uses a hierarchical set of random forests (RF) classifiers to learn the appearance of cartilage regions and their boundaries. The first level of the classifier uses a neighborhood approximation forest to provide contextual features, while the second level considers local features and produces location-specific costs for the layered optimal graph image segmentation of multiple objects and surfaces (LOGISMOS) framework. The method was trained on 34 MRIs with varying degrees of OA and was tested on 108 MRIs, showing a significant reduction in segmentation errors compared with conventional gradient-based and single-stage RF-learned costs. The 3D LOGISMOS was extended to longitudinal-3D (4D) to simultaneously segment multiple follow-up visits of the same patient, which showed a significant improvement in measuring cartilage thickness compared to the sequential 3D approach.",1
"We present a method to address the challenging problem of segmentation of multi-modality isointense infant brain MR images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Our method is based on context-guided, multi-stream fully convolutional networks (FCN), which after training, can directly map a whole volumetric data to its volume-wise labels. In order to alleviate the poten-tial gradient vanishing problem during training, we designed multi-scale deep supervision. Furthermore, context infor-mation was used to further improve the performance of our method. Validated on the test data of the MICCAI 2017 Grand Challenge on 6-month infant brain MRI segmentation (iSeg-2017), our method achieved an average Dice Overlap Coefficient of 95.4%, 91.6% and 89.6% for CSF, GM and WM, respectively.",0
"We introduce an approach to solve the intricate task of segmenting multi-modality isointense infant brain MR images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Our strategy employs multi-stream fully convolutional networks (FCN) guided by context, which can map volumetric data to volume-wise labels after being trained. To overcome the issue of potential gradient vanishing during training, we implemented multi-scale deep supervision. In addition, we utilized context information to enhance our method's performance. Our method was validated on the MICCAI 2017 Grand Challenge on 6-month infant brain MRI segmentation (iSeg-2017) test data and achieved 95.4%, 91.6%, and 89.6% average Dice Overlap Coefficient scores for CSF, GM, and WM, respectively.",1
"Although convolutional neural networks (CNNs) currently dominate competitions on image segmentation, for neuroimaging analysis tasks, more classical generative approaches based on mixture models are still used in practice to parcellate brains. To bridge the gap between the two, in this paper we propose a marriage between a probabilistic generative model, which has been shown to be robust to variability among magnetic resonance (MR) images acquired via different imaging protocols, and a CNN. The link is in the prior distribution over the unknown tissue classes, which are classically modelled using a Markov random field. In this work we model the interactions among neighbouring pixels by a type of recurrent CNN, which can encode more complex spatial interactions. We validate our proposed model on publicly available MR data, from different centres, and show that it generalises across imaging protocols. This result demonstrates a successful and principled inclusion of a CNN in a generative model, which in turn could be adapted by any probabilistic generative approach for image segmentation.",0
"Despite convolutional neural networks (CNNs) being the leading method for image segmentation competitions, classical generative techniques using mixture models are still prevalent in neuroimaging analysis for brain parcellation. In this paper, we propose a combination of a probabilistic generative model and a CNN to bridge the gap between the two approaches. The model incorporates a prior distribution over unknown tissue classes, traditionally modeled by a Markov random field, and a recurrent CNN for more intricate spatial interactions among neighboring pixels. We tested the model on publicly available MR data from different centers and imaging protocols and found that it can generalize across protocols. This successful integration of a CNN into a generative model can be adapted by any probabilistic generative method for image segmentation.",1
"We present an approach for fully automatic urinary bladder segmentation in CT images with artificial neural networks in this study. Automatic medical image analysis has become an invaluable tool in the different treatment stages of diseases. Especially medical image segmentation plays a vital role, since segmentation is often the initial step in an image analysis pipeline. Since deep neural networks have made a large impact on the field of image processing in the past years, we use two different deep learning architectures to segment the urinary bladder. Both of these architectures are based on pre-trained classification networks that are adapted to perform semantic segmentation. Since deep neural networks require a large amount of training data, specifically images and corresponding ground truth labels, we furthermore propose a method to generate such a suitable training data set from Positron Emission Tomography/Computed Tomography image data. This is done by applying thresholding to the Positron Emission Tomography data for obtaining a ground truth and by utilizing data augmentation to enlarge the dataset. In this study, we discuss the influence of data augmentation on the segmentation results, and compare and evaluate the proposed architectures in terms of qualitative and quantitative segmentation performance. The results presented in this study allow concluding that deep neural networks can be considered a promising approach to segment the urinary bladder in CT images.",0
"In this study, we introduce a method for automatic segmentation of the urinary bladder in CT images using artificial neural networks. Medical image analysis has become an essential tool for various stages of disease treatment, and segmentation is a crucial initial step in this process. We utilize two deep learning architectures, which are based on pre-trained classification networks and adapted for semantic segmentation. However, deep neural networks require a significant amount of training data, so we propose a method to generate a suitable dataset by applying thresholding to Positron Emission Tomography data and utilizing data augmentation techniques. We also evaluate the impact of data augmentation on segmentation results and compare and assess the proposed architectures' performance in terms of quantitative and qualitative segmentation results. Our findings demonstrate the potential of deep neural networks for accurately segmenting the urinary bladder in CT images.",1
"Medical image segmentation is an important step in medical image analysis. With the rapid development of convolutional neural network in image processing, deep learning has been used for medical image segmentation, such as optic disc segmentation, blood vessel detection, lung segmentation, cell segmentation, etc. Previously, U-net based approaches have been proposed. However, the consecutive pooling and strided convolutional operations lead to the loss of some spatial information. In this paper, we propose a context encoder network (referred to as CE-Net) to capture more high-level information and preserve spatial information for 2D medical image segmentation. CE-Net mainly contains three major components: a feature encoder module, a context extractor and a feature decoder module. We use pretrained ResNet block as the fixed feature extractor. The context extractor module is formed by a newly proposed dense atrous convolution (DAC) block and residual multi-kernel pooling (RMP) block. We applied the proposed CE-Net to different 2D medical image segmentation tasks. Comprehensive results show that the proposed method outperforms the original U-Net method and other state-of-the-art methods for optic disc segmentation, vessel detection, lung segmentation, cell contour segmentation and retinal optical coherence tomography layer segmentation.",0
"The segmentation of medical images is a crucial aspect of medical image analysis. The use of deep learning, specifically convolutional neural networks, has enabled the segmentation of medical images for various purposes, including the detection of blood vessels, optic discs, lung segmentation, and cell segmentation. While U-net based approaches have been proposed previously, the pooling and strided convolutional operations resulted in the loss of some spatial information. This paper introduces a new context encoder network known as CE-Net, which aims to capture more high-level information and retain spatial information for 2D medical image segmentation. CE-Net is composed of three main components: a feature encoder module, a context extractor, and a feature decoder module. The fixed feature extractor is a pretrained ResNet block, while the context extractor comprises a newly proposed dense atrous convolution (DAC) block and residual multi-kernel pooling (RMP) block. The proposed CE-Net was applied to various 2D medical image segmentation tasks and demonstrated superior performance compared to the U-Net method and other state-of-the-art methods in detecting optic discs, blood vessels, lung segmentation, cell contour segmentation, and retinal optical coherence tomography layer segmentation.",1
"Difficult image segmentation problems, for instance left atrium MRI, can be addressed by incorporating shape priors to find solutions that are consistent with known objects. Nonetheless, a single multivariate Gaussian is not an adequate model in cases with significant nonlinear shape variation or where the prior distribution is multimodal. Nonparametric density estimation is more general, but has a ravenous appetite for training samples and poses serious challenges in optimization, especially in high dimensional spaces. Here, we propose a maximum-a-posteriori formulation that relies on a generative image model by incorporating both local intensity and global shape priors. We use deep autoencoders to capture the complex intensity distribution while avoiding the careful selection of hand-crafted features. We formulate the shape prior as a mixture of Gaussians and learn the corresponding parameters in a high-dimensional shape space rather than pre-projecting onto a low-dimensional subspace. In segmentation, we treat the identity of the mixture component as a latent variable and marginalize it within a generalized expectation-maximization framework. We present a conditional maximization-based scheme that alternates between a closed-form solution for component-specific shape parameters that provides a global update-based optimization strategy, and an intensity-based energy minimization that translates the global notion of a nonlinear shape prior into a set of local penalties. We demonstrate our approach on the left atrial segmentation from gadolinium-enhanced MRI, which is useful in quantifying the atrial geometry in patients with atrial fibrillation.",0
"To address difficult image segmentation problems, such as left atrium MRI, incorporating shape priors to find solutions consistent with known objects can be effective. However, using a single multivariate Gaussian is inadequate in cases with significant nonlinear shape variation or multimodal prior distribution. Nonparametric density estimation is more general, but requires many training samples and poses optimization challenges, especially in high dimensional spaces. In this study, a maximum-a-posteriori formulation is proposed, relying on a generative image model that incorporates both local intensity and global shape priors. Deep autoencoders are used to capture the complex intensity distribution, avoiding the need for hand-crafted features. The shape prior is formulated as a mixture of Gaussians, with parameters learned in a high-dimensional shape space rather than a low-dimensional subspace. In segmentation, the mixture component's identity is treated as a latent variable and marginalized within a generalized expectation-maximization framework. A conditional maximization-based scheme is presented, alternating between a closed-form solution for component-specific shape parameters providing a global update-based optimization strategy and an intensity-based energy minimization. This approach is demonstrated on left atrial segmentation from gadolinium-enhanced MRI, useful in quantifying atrial geometry in patients with atrial fibrillation.",1
"Accurate segmentation of brain tissue in magnetic resonance images (MRI) is a diffcult task due to different types of brain abnormalities. Using information and features from multimodal MRI including T1, T1-weighted inversion recovery (T1-IR) and T2-FLAIR and differential geometric features including the Jacobian determinant(JD) and the curl vector(CV) derived from T1 modality can result in a more accurate analysis of brain images. In this paper, we use the differential geometric information including JD and CV as image characteristics to measure the differences between different MRI images, which represent local size changes and local rotations of the brain image, and we can use them as one CNN channel with other three modalities (T1-weighted, T1-IR and T2-FLAIR) to get more accurate results of brain segmentation. We test this method on two datasets including IBSR dataset and MRBrainS datasets based on the deep voxelwise residual network, namely VoxResNet, and obtain excellent improvement over single modality or three modalities and increases average DSC(Cerebrospinal Fluid (CSF), Gray Matter (GM) and White Matter (WM)) by about 1.5% on the well-known MRBrainS18 dataset and about 2.5% on the IBSR dataset. Moreover, we discuss that one modality combined with its JD or CV information can replace the segmentation effect of three modalities, which can provide medical conveniences for doctor to diagnose because only to extract T1-modality MRI image of patients. Finally, we also compare the segmentation performance of our method in two networks, VoxResNet and U-Net network. The results show VoxResNet has a better performance than U-Net network with our method in brain MRI segmentation. We believe the proposed method can advance the performance in brain segmentation and clinical diagnosis.",0
"Segmenting brain tissue accurately in magnetic resonance images (MRI) is a challenging task, primarily due to various types of brain abnormalities. To improve the analysis of brain images, we use information and features from multimodal MRI, such as T1, T1-IR, and T2-FLAIR, along with differential geometric features like the Jacobian determinant (JD) and the curl vector (CV) derived from the T1 modality. In this study, we utilize the differential geometric information, including JD and CV, as image characteristics to measure differences between different MRI images. These characteristics represent local size changes and local rotations of the brain image, and we use them as one CNN channel along with the other three modalities to obtain more accurate results of brain segmentation. Our method is tested on two datasets using the deep voxelwise residual network, VoxResNet, and yields a substantial improvement over single or three modalities. The average DSC (Cerebrospinal Fluid (CSF), Gray Matter (GM), and White Matter (WM)) increases by about 1.5% on the MRBrainS18 dataset and about 2.5% on the IBSR dataset. We also demonstrate that one modality combined with its JD or CV information can replace the segmentation effect of three modalities, providing medical convenience for doctors to diagnose patients. Finally, we compare the segmentation performance of our method in two networks, VoxResNet and U-Net, and show that VoxResNet performs better in brain MRI segmentation. Overall, we believe that our proposed method can enhance the performance of brain segmentation and improve clinical diagnosis.",1
"Human motion capture data has been widely used in data-driven character animation. In order to generate realistic, natural-looking motions, most data-driven approaches require considerable efforts of pre-processing, including motion segmentation and annotation. Existing (semi-) automatic solutions either require hand-crafted features for motion segmentation or do not produce the semantic annotations required for motion synthesis and building large-scale motion databases. In addition, human labeled annotation data suffers from inter- and intra-labeler inconsistencies by design. We propose a semi-automatic framework for semantic segmentation of motion capture data based on supervised machine learning techniques. It first transforms a motion capture sequence into a ``motion image'' and applies a convolutional neural network for image segmentation. Dilated temporal convolutions enable the extraction of temporal information from a large receptive field. Our model outperforms two state-of-the-art models for action segmentation, as well as a popular network for sequence modeling. Most of all, our method is very robust under noisy and inaccurate training labels and thus can handle human errors during the labeling process.",0
"The use of human motion capture data is prevalent in data-driven character animation. However, generating natural-looking motions requires significant pre-processing efforts, including motion segmentation and annotation. Current (semi-) automatic solutions either necessitate hand-crafted features for motion segmentation or do not produce the semantic annotations necessary for motion synthesis and building extensive motion databases. Additionally, human labeled annotation data is inherently inconsistent due to inter- and intra-labeler discrepancies. To address these issues, we propose a semi-automatic framework for semantic segmentation of motion capture data utilizing supervised machine learning techniques. Our approach transforms motion capture sequences into ""motion images"" and applies a convolutional neural network for image segmentation, incorporating dilated temporal convolutions for temporal information extraction. Our model surpasses two state-of-the-art models for action segmentation and a popular network for sequence modeling, and is particularly resilient to noisy and inaccurate training labels, making it suitable for handling human errors in the labeling process.",1
"The main obstacle to weakly supervised semantic image segmentation is the difficulty of obtaining pixel-level information from coarse image-level annotations. Most methods based on image-level annotations use localization maps obtained from the classifier, but these only focus on the small discriminative parts of objects and do not capture precise boundaries. FickleNet explores diverse combinations of locations on feature maps created by generic deep neural networks. It selects hidden units randomly and then uses them to obtain activation scores for image classification. FickleNet implicitly learns the coherence of each location in the feature maps, resulting in a localization map which identifies both discriminative and other parts of objects. The ensemble effects are obtained from a single network by selecting random hidden unit pairs, which means that a variety of localization maps are generated from a single image. Our approach does not require any additional training steps and only adds a simple layer to a standard convolutional neural network; nevertheless it outperforms recent comparable techniques on the Pascal VOC 2012 benchmark in both weakly and semi-supervised settings.",0
"Obtaining pixel-level information from coarse image-level annotations is the main challenge in weakly supervised semantic image segmentation. While most methods rely on classifier-generated localization maps, they often fail to capture precise boundaries and miss out on other object parts. FickleNet addresses this issue by exploring various feature map locations in deep neural networks to obtain activation scores for image classification. By randomly selecting hidden units and implicitly learning their coherence, FickleNet generates a localization map that identifies all parts of objects. Our approach is simple, requiring only a standard convolutional neural network with an added layer, and outperforms other comparable techniques on the Pascal VOC 2012 benchmark in both weakly and semi-supervised settings without any additional training steps. Additionally, our method generates a variety of localization maps from a single network through random hidden unit pair selections.",1
"Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training. Particularly, DenseNet, which connects each layer to every other layer in a feed-forward fashion, has shown impressive performances in natural image classification tasks. We propose HyperDenseNet, a 3D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path, but also between those across different paths. This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network. Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation. We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on 6-month infant data and the latter on adult images. HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks. We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning. Our code is publicly available at https://www.github.com/josedolz/HyperDenseNet.",0
"Computer vision has recently focused on dense connections as they aid gradient flow and deep supervision during training. The DenseNet, which links every layer to each other in a feed-forward manner, has demonstrated remarkable performance in natural image classification tasks. This study presents HyperDenseNet, a 3D fully convolutional neural network that extends the concept of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path but also between those across different paths. This differs from existing multi-modal CNN approaches, which depend on a single joint layer (or level of abstraction) for fusion, usually at the input or output of the network. Therefore, the proposed network has the flexibility to learn more intricate combinations between the modalities, within and between all the levels of abstraction, which significantly enhances the learning representation. HyperDenseNet outperformed many state-of-the-art segmentation networks in both the iSEG 2017 and MRBrainS 2013 multi-modal brain tissue segmentation challenges, which focus on 6-month infant data and adult images, respectively. The study further includes a comprehensive experimental analysis of feature reuse, which confirms the importance of hyper-dense connections in multi-modal representation learning. The code is available to the public at https://www.github.com/josedolz/HyperDenseNet.",1
"Supervised training a deep neural network aims to ""teach"" the network to mimic human visual perception that is represented by image-and-label pairs in the training data. Superpixelized (SP) images are visually perceivable to humans, but a conventionally trained deep learning model often performs poorly when working on SP images. To better mimic human visual perception, we think it is desirable for the deep learning model to be able to perceive not only raw images but also SP images. In this paper, we propose a new superpixel-based data augmentation (SPDA) method for training deep learning models for biomedical image segmentation. Our method applies a superpixel generation scheme to all the original training images to generate superpixelized images. The SP images thus obtained are then jointly used with the original training images to train a deep learning model. Our experiments of SPDA on four biomedical image datasets show that SPDA is effective and can consistently improve the performance of state-of-the-art fully convolutional networks for biomedical image segmentation in 2D and 3D images. Additional studies also demonstrate that SPDA can practically reduce the generalization gap.",0
"The objective of supervised training for a deep neural network is to instruct the network to imitate human vision by utilizing image-and-label pairs in the training data. Although Superpixelized (SP) images are visually comprehensible to humans, a regularly trained deep learning model does not perform well when dealing with SP images. To better replicate human visual perception, it is desirable for the deep learning model to perceive not only raw images but also SP images. This paper proposes a new superpixel-based data augmentation (SPDA) technique for training deep learning models for biomedical image segmentation. Our method generates superpixelized images by using a superpixel generation scheme on all the original training images. The SP images obtained are then used in conjunction with the original training images to train a deep learning model. Our experiments with SPDA on four biomedical image datasets indicate that SPDA is effective and consistently improves the performance of state-of-the-art fully convolutional networks for biomedical image segmentation in both 2D and 3D images. Further studies demonstrate that SPDA can practically reduce the generalization gap.",1
"Various saliency detection algorithms from color images have been proposed to mimic eye fixation or attentive object detection response of human observers for the same scenes. However, developments on hyperspectral imaging systems enable us to obtain redundant spectral information of the observed scenes from the reflected light source from objects. A few studies using low-level features on hyperspectral images demonstrated that salient object detection can be achieved. In this work, we proposed a salient object detection model on hyperspectral images by applying manifold ranking (MR) on self-supervised Convolutional Neural Network (CNN) features (high-level features) from unsupervised image segmentation task. Self-supervision of CNN continues until clustering loss or saliency maps converges to a defined error between each iteration. Finally, saliency estimations is done as the saliency map at last iteration when the self-supervision procedure terminates with convergence. Experimental evaluations demonstrated that proposed saliency detection algorithm on hyperspectral images is outperforming state-of-the-arts hyperspectral saliency models including the original MR based saliency model.",0
"Several algorithms have been proposed to detect saliency from color images, which aim to replicate the object detection response of human observers. However, the emergence of hyperspectral imaging systems has allowed the capture of additional spectral information from reflected light sources in the observed scenes. Some studies have shown that salient object detection can be achieved using low-level features on hyperspectral images. This work proposes a salient object detection model on hyperspectral images using manifold ranking (MR) and self-supervised Convolutional Neural Network (CNN) features. The CNN continues self-supervision until clustering loss or saliency maps converge to a defined error, and the final saliency estimation is based on the last iteration of the self-supervision procedure. Experimental evaluations reveal that the proposed saliency detection algorithm outperforms state-of-the-art hyperspectral saliency models, including the original MR-based saliency model.",1
"This work examines the use of a fully convolutional net (FCN) to find an image segment, given a pixel within this segment region. The net receives an image, a point in the image and a region of interest (RoI ) mask. The net output is a binary mask of the segment in which the point is located. The region where the segment can be found is contained within the input RoI mask. Full image segmentation can be achieved by running this net sequentially, region-by-region on the image, and stitching the output segments into a single segmentation map. This simple method addresses two major challenges of image segmentation: 1) Segmentation of unknown categories that were not included in the training set. 2) Segmentation of both individual object instances (things) and non-objects (stuff), such as sky and vegetation. Hence, if the pointer pixel is located within a person in a group, the net will output a mask that covers that individual person; if the pointer point is located within the sky region, the net returns the region of the sky in the image. This is true even if no example for sky or person appeared in the training set. The net was tested and trained on the COCO panoptic dataset and achieved 67% IOU for segmentation of familiar classes (that were part of the net training set) and 53% IOU for segmentation of unfamiliar classes (that were not included in the training).",0
"This study explores the use of a fully convolutional net (FCN) to locate an image segment based on a pixel within that segment's region. The FCN takes in an image, a point in the image, and a region of interest (RoI) mask. Its output is a binary mask of the segment containing the point, which is within the input RoI mask. To obtain full image segmentation, the net can be run sequentially, region-by-region, on the image, and the output segments can be stitched together to form a single segmentation map. This straightforward method addresses two major challenges of image segmentation: 1) the segmentation of unknown categories not included in the training set, and 2) the segmentation of both individual object instances and non-objects, like the sky and vegetation. As such, the net can output a mask covering an individual person if the pointer pixel is located within a person in a group, or the region of the sky in the image if the pointer point is in the sky region, even if there were no examples for these categories in the training set. The COCO panoptic dataset was used to train and test the net, which achieved a 67% IOU for familiar classes and a 53% IOU for unfamiliar classes.",1
"Dense 3D visual mapping estimates as many as possible pixel depths, for each image. This results in very dense point clouds that often contain redundant and noisy information, especially for surfaces that are roughly planar, for instance, the ground or the walls in the scene. In this paper we leverage on semantic image segmentation to discriminate which regions of the scene require simplification and which should be kept at high level of details. We propose four different point cloud simplification methods which decimate the perceived point cloud by relying on class-specific local and global statistics still maintaining more points in the proximity of class boundaries to preserve the infra-class edges and discontinuities. 3D dense model is obtained by fusing the point clouds in a 3D Delaunay Triangulation to deal with variable point cloud density. In the experimental evaluation we have shown that, by leveraging on semantics, it is possible to simplify the model and diminish the noise affecting the point clouds.",0
"The process of dense 3D visual mapping involves estimating pixel depths for each image, resulting in highly detailed point clouds that may contain redundant and noisy information, particularly for flat surfaces like walls and floors. This paper proposes the use of semantic image segmentation to identify areas of the scene that require simplification and those that should retain a high level of detail. Four point cloud simplification methods are proposed, which rely on class-specific local and global statistics to reduce the number of points while preserving infra-class edges and discontinuities. The resulting 3D dense model is obtained by fusing the point clouds in a 3D Delaunay Triangulation, which effectively addresses variable point cloud density. Experimental evaluation shows that leveraging semantics can lead to a simpler model with reduced noise in the point clouds.",1
"Segmentation of colorectal cancerous regions from 3D Magnetic Resonance (MR) images is a crucial procedure for radiotherapy which conventionally requires accurate delineation of tumour boundaries at an expense of labor, time and reproducibility. While deep learning based methods serve good baselines in 3D image segmentation tasks, small applicable patch size limits effective receptive field and degrades segmentation performance. In addition, Regions of interest (RoIs) localization from large whole volume 3D images serves as a preceding operation that brings about multiple benefits in terms of speed, target completeness, reduction of false positives. Distinct from sliding window or non-joint localization-segmentation based models, we propose a novel multitask framework referred to as 3D RoI-aware U-Net (3D RU-Net), for RoI localization and in-region segmentation where the two tasks share one backbone encoder network. With the region proposals from the encoder, we crop multi-level RoI in-region features from the encoder to form a GPU memory-efficient decoder for detailpreserving segmentation and therefore enlarged applicable volume size and effective receptive field. To effectively train the model, we designed a Dice formulated loss function for the global-to-local multi-task learning procedure. Based on the efficiency gains, we went on to ensemble models with different receptive fields to achieve even higher performance costing minor extra computational expensiveness. Extensive experiments were conducted on 64 cancerous cases with a four-fold cross-validation, and the results showed significant superiority in terms of accuracy and efficiency over conventional frameworks. In conclusion, the proposed method has a huge potential for extension to other 3D object segmentation tasks from medical images due to its inherent generalizability. The code for the proposed method is publicly available.",0
"The accurate segmentation of colorectal cancer regions from 3D Magnetic Resonance (MR) images is an essential step for radiotherapy, but it is a labor-intensive and time-consuming process that lacks reproducibility. While deep learning-based methods offer good results for 3D image segmentation tasks, their small applicable patch size limits the effective receptive field, leading to degraded segmentation performance. To address this issue, we propose a novel multitask framework, the 3D RoI-aware U-Net (3D RU-Net), which combines RoI localization and in-region segmentation in one backbone encoder network. By cropping multi-level RoI in-region features from the encoder, our method achieves detail-preserving segmentation, enlarges the applicable volume size, and increases the effective receptive field. To train the model effectively, we designed a Dice-formulated loss function for the global-to-local multi-task learning procedure. By ensembling models with different receptive fields, we achieved even higher performance at a slightly higher computational cost. Our experiments on 64 cancerous cases with a four-fold cross-validation demonstrated significant improvements in accuracy and efficiency over conventional frameworks. This proposed method has great potential for generalization to other 3D object segmentation tasks from medical images, and the code is available to the public.",1
"Optimal surface segmentation is a state-of-the-art method used for segmentation of multiple globally optimal surfaces in volumetric datasets. The method is widely used in numerous medical image segmentation applications. However, nodes in the graph based optimal surface segmentation method typically encode uniformly distributed orthogonal voxels of the volume. Thus the segmentation cannot attain an accuracy greater than a single unit voxel, i.e. the distance between two adjoining nodes in graph space. Segmentation accuracy higher than a unit voxel is achievable by exploiting partial volume information in the voxels which shall result in non-equidistant spacing between adjoining graph nodes. This paper reports a generalized graph based multiple surface segmentation method with convex priors which can optimally segment the target surfaces in an irregularly sampled space. The proposed method allows non-equidistant spacing between the adjoining graph nodes to achieve subvoxel segmentation accuracy by utilizing the partial volume information in the voxels. The partial volume information in the voxels is exploited by computing a displacement field from the original volume data to identify the subvoxel-accurate centers within each voxel resulting in non-equidistant spacing between the adjoining graph nodes. The smoothness of each surface modeled as a convex constraint governs the connectivity and regularity of the surface. We employ an edge-based graph representation to incorporate the necessary constraints and the globally optimal solution is obtained by computing a minimum s-t cut. The proposed method was validated on 10 intravascular multi-frame ultrasound image datasets for subvoxel segmentation accuracy. In all cases, the approach yielded highly accurate results. Our approach can be readily extended to higher-dimensional segmentations.",0
"The optimal surface segmentation technique is a cutting-edge approach utilized to segment multiple surfaces in volumetric datasets, which is widely used in medical image segmentation applications. However, the nodes in the graph-based optimal surface segmentation method encode uniformly distributed orthogonal voxels of the volume, resulting in a segmentation accuracy limited to a single unit voxel. To achieve a higher accuracy, partial volume information in the voxels is exploited to allow non-equidistant spacing between adjoining graph nodes. This paper presents a generalized graph-based multiple surface segmentation method with convex priors that can optimally segment target surfaces in an irregularly sampled space. The proposed method utilizes a displacement field to identify subvoxel-accurate centers within each voxel, resulting in non-equidistant spacing between adjoining graph nodes. The smoothness of each surface is modeled as a convex constraint, which governs the connectivity and regularity of the surface. An edge-based graph representation is employed to incorporate the necessary constraints, and the globally optimal solution is obtained by computing a minimum s-t cut. The proposed method was tested on 10 intravascular multi-frame ultrasound image datasets and yielded highly accurate results. This approach can be extended to higher-dimensional segmentations.",1
"The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024x2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our `learning to downsample' module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.",0
"The encoder-decoder framework is currently the leading method for offline semantic image segmentation. However, with the growing popularity of autonomous systems, there is a need for real-time computation. Our paper presents the Fast-SCNN, an above real-time semantic segmentation model that can efficiently operate on embedded devices with low memory, even on high resolution image data (1024x2048px). By incorporating a ""learning to downsample"" module, which simultaneously computes low-level features for multiple resolution branches, we combine spatial detail at high resolution with deep features at lower resolution, achieving an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. Our experiments demonstrate that large scale pre-training is not necessary, and we show even faster computation with competitive results on subsampled inputs, without any network modifications. We thoroughly validate our metric with ImageNet pre-training and the coarse labeled data of Cityscapes.",1
"Medical image segmentation being a substantial component of image processing plays a significant role to analyze gross anatomy, to locate an infirmity and to plan the surgical procedures. Segmentation of brain Magnetic Resonance Imaging (MRI) is of considerable importance for the accurate diagnosis. However, precise and accurate segmentation of brain MRI is a challenging task. Here, we present an efficient framework for segmentation of brain MR images. For this purpose, Gabor transform method is used to compute features of brain MRI. Then, these features are classified by using four different classifiers i.e., Incremental Supervised Neural Network (ISNN), K-Nearest Neighbor (KNN), Probabilistic Neural Network (PNN), and Support Vector Machine (SVM). Performance of these classifiers is investigated over different images of brain MRI and the variation in the performance of these classifiers is observed for different brain tissues. Thus, we proposed a rule-based hybrid approach to segment brain MRI. Experimental results show that the performance of these classifiers varies over each tissue MRI and the proposed rule-based hybrid approach exhibits better segmentation of brain MRI tissues.",0
"Segmenting medical images is a crucial aspect of image processing that plays a significant role in analyzing gross anatomy, identifying ailments, and planning surgical procedures. Accurately segmenting brain MRI is particularly important for precise diagnosis, but it is a challenging task. In this study, we introduce an efficient framework for segmenting brain MR images using the Gabor transform method to compute features. We then classify these features using four different classifiers: Incremental Supervised Neural Network (ISNN), K-Nearest Neighbor (KNN), Probabilistic Neural Network (PNN), and Support Vector Machine (SVM). We investigate the performance of these classifiers on various brain MRI images and observe that they perform differently for different brain tissues. Therefore, we propose a rule-based hybrid approach to segment brain MRI, which yields better results than the individual classifiers.",1
"In recent years Deep Learning has brought about a breakthrough in Medical Image Segmentation. U-Net is the most prominent deep network in this regard, which has been the most popular architecture in the medical imaging community. Despite outstanding overall performance in segmenting multimodal medical images, from extensive experimentations on challenging datasets, we found out that the classical U-Net architecture seems to be lacking in certain aspects. Therefore, we propose some modifications to improve upon the already state-of-the-art U-Net model. Hence, following the modifications we develop a novel architecture MultiResUNet as the potential successor to the successful U-Net architecture. We have compared our proposed architecture MultiResUNet with the classical U-Net on a vast repertoire of multimodal medical images. Albeit slight improvements in the cases of ideal images, a remarkable gain in performance has been attained for challenging images. We have evaluated our model on five different datasets, each with their own unique challenges, and have obtained a relative improvement in performance of 10.15%, 5.07%, 2.63%, 1.41%, and 0.62% respectively.",0
"Medical Image Segmentation has experienced a significant breakthrough thanks to the emergence of Deep Learning in recent years. The most popular architecture in the medical imaging community is U-Net, which has demonstrated exceptional overall performance in segmenting multimodal medical images. However, after extensive experimentation with challenging datasets, we have identified certain limitations in the classical U-Net architecture. To overcome these limitations, we propose modifications to enhance the already state-of-the-art U-Net model. Our novel architecture, MultiResUNet, was developed following these modifications and has the potential to succeed U-Net. We have compared MultiResUNet with U-Net on various multimodal medical images and observed significant improvements in the performance of challenging images. Our evaluation on five different datasets, each with its unique challenges, resulted in a relative improvement in performance of 10.15%, 5.07%, 2.63%, 1.41%, and 0.62%, respectively, despite only slight improvements in the case of ideal images.",1
"We present a method for highly efficient landmark detection that combines deep convolutional neural networks with well established model-based fitting algorithms. Motivated by established model-based fitting methods such as active shapes, we use a PCA of the landmark positions to allow generative modeling of facial landmarks. Instead of computing the model parameters using iterative optimization, the PCA is included in a deep neural network using a novel layer type. The network predicts model parameters in a single forward pass, thereby allowing facial landmark detection at several hundreds of frames per second. Our architecture allows direct end-to-end training of a model-based landmark detection method and shows that deep neural networks can be used to reliably predict model parameters directly without the need for an iterative optimization. The method is evaluated on different datasets for facial landmark detection and medical image segmentation. PyTorch code is freely available at https://github.com/justusschock/shapenet",0
"We have devised a technique to detect landmarks with a high level of efficiency, which involves combining deep convolutional neural networks with established model-based fitting algorithms. Using the active shapes method as our inspiration, we have employed a principal component analysis (PCA) of landmark positions to facilitate generative modeling of facial landmarks. Rather than relying on iterative optimization to calculate the model parameters, we have integrated the PCA into a deep neural network through a novel layer type. This network is capable of predicting model parameters in a single forward pass, enabling facial landmark detection at a rate of several hundred frames per second. Our architecture permits direct end-to-end training of a model-based landmark detection method and demonstrates that deep neural networks can accurately predict model parameters without the need for iterative optimization. The method has been assessed on different datasets for facial landmark detection and medical image segmentation, and the PyTorch code is freely accessible at https://github.com/justusschock/shapenet.",1
We propose a novel technique to incorporate attention within convolutional neural networks using feature maps generated by a separate convolutional autoencoder. Our attention architecture is well suited for incorporation with deep convolutional networks. We evaluate our model on benchmark segmentation datasets in skin cancer segmentation and lung lesion segmentation. Results show highly competitive performance when compared with U-Net and it's residual variant.,0
"A new approach is suggested for integrating attention into convolutional neural networks by utilizing feature maps produced by an independent convolutional autoencoder. This attention architecture is particularly compatible with deep convolutional networks. In skin cancer segmentation and lung lesion segmentation benchmark segmentation datasets, our model is examined. When compared to U-Net and its residual variation, the outcomes indicate extremely competitive performance.",1
"Deep convolutional networks have achieved the state-of-the-art for semantic image segmentation tasks. However, training these networks requires access to densely labeled images, which are known to be very expensive to obtain. On the other hand, the web provides an almost unlimited source of images annotated at the image level. How can one utilize this much larger weakly annotated set for tasks that require dense labeling? Prior work often relied on localization cues, such as saliency maps, objectness priors, bounding boxes etc., to address this challenging problem. In this paper, we propose a model that generates auxiliary labels for each image, while simultaneously forcing the output of the CNN to satisfy the mean-field constraints imposed by a conditional random field. We show that one can enforce the CRF constraints by forcing the distribution at each pixel to be close to the distribution of its neighbors. This is in stark contrast with methods that compute a recursive expansion of the mean-field distribution using a recurrent architecture and train the resultant distribution. Instead, the proposed model adds an extra loss term to the output of the CNN, and hence, is faster than recursive implementations. We achieve the state-of-the-art for weakly supervised semantic image segmentation on VOC 2012 dataset, assuming no manually labeled pixel level information is available. Furthermore, the incorporation of conditional random fields in CNN incurs little extra time during training.",0
"Semantic image segmentation tasks have been dominated by deep convolutional networks; however, acquiring densely labeled images is costly. By contrast, the web provides an abundance of images that are annotated at the image level. In the past, localization cues, such as bounding boxes and saliency maps, have been leveraged to harness this weakly annotated set for dense labeling tasks. Our paper proposes a new model that generates auxiliary labels for each image while also enforcing the mean-field constraints of a conditional random field on the output of the CNN. To achieve this, we ensure that the distribution of each pixel is similar to that of its neighbors. Unlike recursive methods that compute the mean-field distribution using a recurrent architecture, our model adds a loss term to the CNN output, making it faster. We demonstrate state-of-the-art performance for weakly supervised semantic image segmentation on the VOC 2012 dataset while incorporating conditional random fields in CNN with minimal extra training time.",1
"The instance segmentation problem intends to precisely detect and delineate objects in images. Most of the current solutions rely on deep convolutional neural networks but despite this fact proposed solutions are very diverse. Some solutions approach the problem as a network problem, where they use several networks or specialize a single network to solve several tasks. A different approach tries to solve the problem as an annotation problem, where the instance information is encoded in a mathematical representation. This work proposes a solution based in the DCME technique to solve the instance segmentation with a single segmentation network. Different from others, the segmentation network decoder is not specialized in a multi-task network. Instead, the network encoder is repurposed to classify image objects, reducing the computational cost of the solution.",0
"The primary goal of the instance segmentation problem is to accurately identify and outline objects in images. Although deep convolutional neural networks are commonly utilized, there is a wide range of proposed solutions. Some approaches use multiple networks or specialize a single network to address various tasks, while others treat the problem as an annotation issue, encoding instance information in a mathematical form. This study proposes a single segmentation network solution that employs the DCME method for instance segmentation. Unlike other methods, this approach does not involve a specialized multi-task network for the segmentation network decoder. Instead, the network encoder is adapted for image object classification, resulting in reduced computational expenses.",1
"In the deep metric learning approach to image segmentation, a convolutional net densely generates feature vectors at the pixels of an image. Pairs of feature vectors are trained to be similar or different, depending on whether the corresponding pixels belong to same or different ground truth segments. To segment a new image, the feature vectors are computed and clustered. Both empirically and theoretically, it is unclear whether or when deep metric learning is superior to the more conventional approach of directly predicting an affinity graph with a convolutional net. We compare the two approaches using brain images from serial section electron microscopy images, which constitute an especially challenging example of instance segmentation. We first show that seed-based postprocessing of the feature vectors, as originally proposed, produces inferior accuracy because it is difficult for the convolutional net to predict feature vectors that remain uniform across large objects. Then we consider postprocessing by thresholding a nearest neighbor graph followed by connected components. In this case, segmentations from a ""metric graph"" turn out to be competitive or even superior to segmentations from a directly predicted affinity graph. To explain these findings theoretically, we invoke the property that the metric function satisfies the triangle inequality. Then we show with an example where this constraint suppresses noise, causing connected components to more robustly segment a metric graph than an unconstrained affinity graph.",0
"The deep metric learning method for image segmentation involves generating feature vectors densely at image pixels using a convolutional net. These feature vectors are then trained to be similar or different, depending on whether the corresponding pixels belong to the same or different ground truth segments. To segment a new image, the feature vectors are computed and clustered. However, it is uncertain whether deep metric learning is better than directly predicting an affinity graph using a convolutional net. To compare these approaches, we use brain images from serial section electron microscopy images, which pose a challenging example of instance segmentation. The original seed-based postprocessing of feature vectors yields low accuracy, as it is challenging for the convolutional net to predict uniform feature vectors across large objects. We experiment with postprocessing by thresholding a nearest neighbor graph followed by connected components, and find that segmentations from a ""metric graph"" are competitive or even better than those from a directly predicted affinity graph. This is explained theoretically by the property that the metric function satisfies the triangle inequality, which suppresses noise and leads to more robust segmentation of a metric graph using connected components than an unconstrained affinity graph.",1
"Despite the state-of-the-art performance for medical image segmentation, deep convolutional neural networks (CNNs) have rarely provided uncertainty estimations regarding their segmentation outputs, e.g., model (epistemic) and image-based (aleatoric) uncertainties. In this work, we analyze these different types of uncertainties for CNN-based 2D and 3D medical image segmentation tasks. We additionally propose a test-time augmentation-based aleatoric uncertainty to analyze the effect of different transformations of the input image on the segmentation output. Test-time augmentation has been previously used to improve segmentation accuracy, yet not been formulated in a consistent mathematical framework. Hence, we also propose a theoretical formulation of test-time augmentation, where a distribution of the prediction is estimated by Monte Carlo simulation with prior distributions of parameters in an image acquisition model that involves image transformations and noise. We compare and combine our proposed aleatoric uncertainty with model uncertainty. Experiments with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic Resonance Images (MRI) showed that 1) the test-time augmentation-based aleatoric uncertainty provides a better uncertainty estimation than calculating the test-time dropout-based model uncertainty alone and helps to reduce overconfident incorrect predictions, and 2) our test-time augmentation outperforms a single-prediction baseline and dropout-based multiple predictions.",0
"Although deep convolutional neural networks (CNNs) have achieved impressive results in medical image segmentation, they have not adequately addressed uncertainty estimations for their outputs, including model (epistemic) and image-based (aleatoric) uncertainties. To fill this gap, we investigate these different types of uncertainties for CNN-based 2D and 3D medical image segmentation tasks, proposing a novel test-time augmentation-based aleatoric uncertainty to explore the impact of various input image transformations on the segmentation output. This approach, which has been used previously to enhance segmentation accuracy but has lacked a consistent mathematical framework, is formulated here through Monte Carlo simulation with prior distributions of parameters in an image acquisition model that considers image transformations and noise. We compare and combine our proposed aleatoric uncertainty with model uncertainty, conducting experiments with segmentation of fetal brains and brain tumors from 2D and 3D Magnetic Resonance Images (MRI). Our findings indicate that the test-time augmentation-based aleatoric uncertainty provides a better estimate of uncertainty than the test-time dropout-based model uncertainty alone, effectively reducing overconfident incorrect predictions. Furthermore, our test-time augmentation approach outperforms a single-prediction baseline and dropout-based multiple predictions.",1
"We present a novel method to explicitly incorporate topological prior knowledge into deep learning based segmentation, which is, to our knowledge, the first work to do so. Our method uses the concept of persistent homology, a tool from topological data analysis, to capture high-level topological characteristics of segmentation results in a way which is differentiable with respect to the pixelwise probability of being assigned to a given class. The topological prior knowledge consists of the sequence of desired Betti numbers of the segmentation. As a proof-of-concept we demonstrate our approach by applying it to the problem of left-ventricle segmentation of cardiac MR images of 500 subjects from the UK Biobank dataset, where we show that it improves segmentation performance in terms of topological correctness without sacrificing pixelwise accuracy.",0
"Our method introduces a new approach to integrating topological prior knowledge into deep learning-based segmentation. This is the first work of its kind, as far as we know. We utilize persistent homology, a tool from topological data analysis, to capture higher-level topological features of the segmentation results. Our approach is differentiable with respect to the pixelwise probability of being assigned to a specific class. The topological prior knowledge is comprised of the sequence of Betti numbers desired for the segmentation. To demonstrate our method's effectiveness, we apply it to the segmentation of left ventricles in cardiac MR images of 500 subjects from the UK Biobank dataset. We show that our approach enhances segmentation performance in terms of topological accuracy without compromising pixelwise accuracy.",1
"Current state-of-the-art deep learning segmentation methods have not yet made a broad entrance into the clinical setting in spite of high demand for such automatic methods. One important reason is the lack of reliability caused by models that fail unnoticed and often locally produce anatomically implausible results that medical experts would not make. This paper presents an automatic image segmentation method based on (Bayesian) dilated convolutional networks (DCNN) that generate segmentation masks and spatial uncertainty maps for the input image at hand. The method was trained and evaluated using segmentation of the left ventricle (LV) cavity, right ventricle (RV) endocardium and myocardium (Myo) at end-diastole (ED) and end-systole (ES) in 100 cardiac 2D MR scans from the MICCAI 2017 Challenge (ACDC). Combining segmentations and uncertainty maps and employing a human-in-the-loop setting, we provide evidence that image areas indicated as highly uncertain regarding the obtained segmentation almost entirely cover regions of incorrect segmentations. The fused information can be harnessed to increase segmentation performance. Our results reveal that we can obtain valuable spatial uncertainty maps with low computational effort using DCNNs.",0
"Despite the high demand for automated segmentation methods in the clinical setting, current state-of-the-art deep learning techniques have not yet been widely adopted due to their lack of reliability. These models often generate anatomically implausible results that go unnoticed, making them unsuitable for medical experts. This paper introduces an automatic image segmentation method that uses Bayesian dilated convolutional networks (DCNN) to produce segmentation masks and spatial uncertainty maps for a given input image. The method was trained and evaluated on 100 cardiac 2D MR scans from the MICCAI 2017 Challenge, specifically targeting the left ventricle (LV) cavity, right ventricle (RV) endocardium and myocardium (Myo) at end-diastole (ED) and end-systole (ES). By combining segmentations and uncertainty maps, and applying a human-in-the-loop approach, we demonstrate that highly uncertain areas correspond to regions of incorrect segmentations. This combined information can be leveraged to improve segmentation accuracy, with valuable spatial uncertainty maps obtainable through low computational effort using DCNNs.",1
"Background: The trend towards large-scale studies including population imaging poses new challenges in terms of quality control (QC). This is a particular issue when automatic processing tools, e.g. image segmentation methods, are employed to derive quantitative measures or biomarkers for later analyses. Manual inspection and visual QC of each segmentation isn't feasible at large scale. However, it's important to be able to automatically detect when a segmentation method fails so as to avoid inclusion of wrong measurements into subsequent analyses which could lead to incorrect conclusions. Methods: To overcome this challenge, we explore an approach for predicting segmentation quality based on Reverse Classification Accuracy, which enables us to discriminate between successful and failed segmentations on a per-cases basis. We validate this approach on a new, large-scale manually-annotated set of 4,800 cardiac magnetic resonance scans. We then apply our method to a large cohort of 7,250 cardiac MRI on which we have performed manual QC. Results: We report results used for predicting segmentation quality metrics including Dice Similarity Coefficient (DSC) and surface-distance measures. As initial validation, we present data for 400 scans demonstrating 99% accuracy for classifying low and high quality segmentations using predicted DSC scores. As further validation we show high correlation between real and predicted scores and 95% classification accuracy on 4,800 scans for which manual segmentations were available. We mimic real-world application of the method on 7,250 cardiac MRI where we show good agreement between predicted quality metrics and manual visual QC scores. Conclusions: We show that RCA has the potential for accurate and fully automatic segmentation QC on a per-case basis in the context of large-scale population imaging as in the UK Biobank Imaging Study.",0
"Large-scale studies that involve population imaging present new challenges in terms of quality control (QC). When automatic processing tools like image segmentation methods are used to derive quantitative measures or biomarkers for later analysis, manual inspection and visual QC of each segmentation is not practical. However, it is crucial to automatically detect when a segmentation method fails to avoid including incorrect measurements into subsequent analyses that could lead to inaccurate conclusions. To address this challenge, an approach for predicting segmentation quality based on Reverse Classification Accuracy has been explored. This approach enables discrimination between successful and failed segmentations on a per-case basis. The approach was validated on a large-scale manually-annotated set of 4,800 cardiac magnetic resonance scans and applied to a large cohort of 7,250 cardiac MRI. The results demonstrate accurate and fully automatic segmentation QC on a per-case basis in the context of large-scale population imaging, such as in the UK Biobank Imaging Study.",1
"Recently, state-of-the-art results have been achieved in semantic segmentation using fully convolutional networks (FCNs). Most of these networks employ encoder-decoder style architecture similar to U-Net and are trained with images and the corresponding segmentation maps as a pixel-wise classification task. Such frameworks only exploit class information by using the ground truth segmentation maps. In this paper, we propose a multi-task learning framework with the main aim of exploiting structural and spatial information along with the class information. We modify the decoder part of the FCN to exploit class information and the structural information as well. We intend to do this while also keeping the parameters of the network as low as possible. We obtain the structural information using either of the two ways: i) using the contour map and ii) using the distance map, both of which can be obtained from ground truth segmentation maps with no additional annotation costs. We also explore different ways in which distance maps can be computed and study the effects of different distance maps on the segmentation performance. We also experiment extensively on two different medical image segmentation applications: i.e i) using color fundus images for optic disc and cup segmentation and ii) using endoscopic images for polyp segmentation. Through our experiments, we report results comparable to, and in some cases performing better than the current state-of-the-art architectures and with an order of 2x reduction in the number of parameters.",0
"Recently, fully convolutional networks (FCNs) have achieved state-of-the-art results in semantic segmentation. These networks typically employ an encoder-decoder architecture similar to U-Net and are trained for pixel-wise classification using images and their corresponding segmentation maps. However, these frameworks only use class information from the ground truth segmentation maps. In this study, we propose a multi-task learning framework that exploits both class information and structural/spatial information. We modify the decoder part of the FCN to incorporate class and structural information while minimizing the number of network parameters. We obtain structural information from contour and distance maps, both of which are derived from ground truth segmentation maps with no additional annotation costs. We experiment with different distance map computation methods and evaluate our approach on two medical image segmentation applications: optic disc/cup segmentation using color fundus images and polyp segmentation using endoscopic images. Our results demonstrate performance comparable to, and sometimes better than, current state-of-the-art architectures with a 2x reduction in parameter count.",1
"Image-to-image translation is a long-established and a difficult problem in computer vision. In this paper we propose an adversarial based model for image-to-image translation. The regular deep neural-network based methods perform the task of image-to-image translation by comparing gram matrices and using image segmentation which requires human intervention. Our generative adversarial network based model works on a conditional probability approach. This approach makes the image translation independent of any local, global and content or style features. In our approach we use a bidirectional reconstruction model appended with the affine transform factor that helps in conserving the content and photorealism as compared to other models. The advantage of using such an approach is that the image-to-image translation is semi-supervised, independant of image segmentation and inherits the properties of generative adversarial networks tending to produce realistic. This method has proven to produce better results than Multimodal Unsupervised Image-to-image translation.",0
"The task of image-to-image translation has been a challenging problem in computer vision for a long time. Our research proposes an adversarial based model to tackle this issue. Unlike previous deep neural-network based methods that rely on gram matrices and human intervention for image segmentation, our generative adversarial network model uses a conditional probability approach that is independent of local, global, content or style features. Our approach incorporates a bidirectional reconstruction model combined with an affine transform factor to preserve content and photorealism. The advantage of our semi-supervised approach is its independence from image segmentation and its inheritance of properties from generative adversarial networks, which tend to produce realistic results. Our method has been shown to outperform Multimodal Unsupervised Image-to-image translation.",1
"A class of vision problems, less commonly studied, consists of detecting objects in imagery obtained from physics-based experiments. These objects can span in 4D (x, y, z, t) and are visible as disturbances (caused due to physical phenomena) in the image with background distribution being approximately uniform. Such objects, occasionally referred to as `events', can be considered as high energy blobs in the image. Unlike the images analyzed in conventional vision problems, very limited features are associated with such events, and their shape, size and count can vary significantly. This poses a challenge on the use of pre-trained models obtained from supervised approaches.   In this paper, we propose an unsupervised approach involving iterative clustering based segmentation (ICS) which can detect target objects (events) in real-time. In this approach, a test image is analyzed over several cycles, and one event is identified per cycle. Each cycle consists of the following steps: (1) image segmentation using a modified k-means clustering method, (2) elimination of empty (with no events) segments based on statistical analysis of each segment, (3) merging segments that overlap (correspond to same event), and (4) selecting the strongest event. These four steps are repeated until all the events have been identified. The ICS approach consists of a few hyper-parameters that have been chosen based on statistical study performed over a set of test images. The applicability of ICS method is demonstrated on several 2D and 3D test examples.",0
"A group of visual issues, which have received less attention, involves detecting objects in images obtained from experiments based on physics. These objects can exist in 4 dimensions, including x, y, z, and t, and appear as disturbances in the image due to physical phenomena, with a uniform background distribution. These objects are sometimes referred to as ""events"" and can be considered high-energy blobs within the image. Unlike images in traditional vision problems, these objects have limited features and vary in shape, size, and count, making it challenging to use pre-trained models obtained from supervised approaches. To address this issue, we propose an unsupervised approach called iterative clustering based segmentation (ICS) that can detect these events in real-time. The ICS approach involves analyzing a test image over several cycles and identifying one event per cycle. This approach includes four steps: (1) image segmentation using a modified k-means clustering method, (2) elimination of empty segments based on statistical analysis, (3) merging segments that overlap, and (4) selecting the strongest event. These steps are repeated until all events have been identified. The ICS approach has a few hyper-parameters chosen based on statistical studies performed on test images. We demonstrate the applicability of the ICS method on several 2D and 3D test examples.",1
"Ultrasound image compression by preserving speckle-based key information is a challenging task. In this paper, we introduce an ultrasound image compression framework with the ability to retain realism of speckle appearance despite achieving very high-density compression factors. The compressor employs a tissue segmentation method, transmitting segments along with transducer frequency, number of samples and image size as essential information required for decompression. The decompressor is based on a convolutional network trained to generate patho-realistic ultrasound images which convey essential information pertinent to tissue pathology visible in the images. We demonstrate generalizability of the building blocks using two variants to build the compressor. We have evaluated the quality of decompressed images using distortion losses as well as perception loss and compared it with other off the shelf solutions. The proposed method achieves a compression ratio of $725:1$ while preserving the statistical distribution of speckles. This enables image segmentation on decompressed images to achieve dice score of $0.89 \pm 0.11$, which evidently is not so accurately achievable when images are compressed with current standards like JPEG, JPEG 2000, WebP and BPG. We envision this frame work to serve as a roadmap for speckle image compression standards.",0
"Preserving speckle-based key information in ultrasound image compression is a difficult task that is addressed in this paper. Our proposed framework achieves high-density compression while maintaining the realism of speckle appearance. The compressor involves tissue segmentation and transmits essential information such as transducer frequency, number of samples, and image size required for decompression. The decompressor uses a convolutional network to generate patho-realistic ultrasound images that convey essential information about tissue pathology. We demonstrate the generalizability of the compressor by evaluating two variants and compare the quality of decompressed images with other off-the-shelf solutions using distortion losses and perception loss. The proposed method achieves a compression ratio of 725:1 while preserving the statistical distribution of speckles. This enables image segmentation on decompressed images with a dice score of 0.89  0.11, which is not achievable with current standards like JPEG, JPEG 2000, WebP, and BPG. Our framework provides a roadmap for future speckle image compression standards.",1
"The Encoder-Decoder architecture is a main stream deep learning model for biomedical image segmentation. The encoder fully compresses the input and generates encoded features, and the decoder then produces dense predictions using encoded features. However, decoders are still under-explored in such architectures. In this paper, we comprehensively study the state-of-the-art Encoder-Decoder architectures, and propose a new universal decoder, called cascade decoder, to improve semantic segmentation accuracy. Our cascade decoder can be embedded into existing networks and trained altogether in an end-to-end fashion. The cascade decoder structure aims to conduct more effective decoding of hierarchically encoded features and is more compatible with common encoders than the known decoders. We replace the decoders of state-of-the-art models with our cascade decoder for several challenging biomedical image segmentation tasks, and the considerable improvements achieved demonstrate the efficacy of our new decoding method.",0
"The Encoder-Decoder architecture is widely used in deep learning for segmenting biomedical images. The encoder compresses the input to produce encoded features, which the decoder uses to make dense predictions. However, decoders have not been extensively explored in this architecture. This study thoroughly examines the most advanced Encoder-Decoder architectures and introduces a new decoder, the cascade decoder, that enhances semantic segmentation accuracy. The cascade decoder can be integrated into existing networks and trained end-to-end. Its design improves the decoding of hierarchically encoded features and is more suitable for common encoders than existing decoders. We replaced state-of-the-art decoders with our cascade decoder for various challenging biomedical image segmentation tasks and achieved significant improvements, demonstrating the effectiveness of our new decoding technique.",1
"Photorealism is a complex concept that cannot easily be formulated mathematically. Deep Photo Style Transfer is an attempt to transfer the style of a reference image to a content image while preserving its photorealism. This is achieved by introducing a constraint that prevents distortions in the content image and by applying the style transfer independently for semantically different parts of the images. In addition, an automated segmentation process is presented that consists of a neural network based segmentation method followed by a semantic grouping step. To further improve the results a measure for image aesthetics is used and elaborated. If the content and the style image are sufficiently similar, the result images look very realistic. With the automation of the image segmentation the pipeline becomes completely independent from any user interaction, which allows for new applications.",0
"The concept of photorealism is complicated and cannot be easily expressed through mathematics. However, Deep Photo Style Transfer aims to transfer the style of a reference image to a content image while maintaining its photorealism. This is accomplished by implementing a constraint that prevents distortions in the content image and applying the style transfer separately for semantically different parts of the images. Additionally, an automated segmentation process is introduced, which includes a neural network-based segmentation method and a semantic grouping step. To enhance the outcomes, an image aesthetics measure is employed. The resulting images appear extremely realistic if the content and style images are similar enough. The automation of the image segmentation allows for new applications without any user interaction.",1
"Recent advances in deep learning methods have come to define the state-of-the-art for many medical imaging applications, surpassing even human judgment in several tasks. Those models, however, when trained to reduce the empirical risk on a single domain, fail to generalize when applied to other domains, a very common scenario in medical imaging due to the variability of images and anatomical structures, even across the same imaging modality. In this work, we extend the method of unsupervised domain adaptation using self-ensembling for the semantic segmentation task and explore multiple facets of the method on a small and realistic publicly-available magnetic resonance (MRI) dataset. Through an extensive evaluation, we show that self-ensembling can indeed improve the generalization of the models even when using a small amount of unlabelled data.",0
"Many medical imaging applications now rely on deep learning techniques, which have proven to be more effective than human judgement in certain tasks. However, these models often struggle to generalize when applied to different domains because of the variability of images and anatomical structures, even within the same imaging modality. To address this issue, we have employed self-ensembling for the semantic segmentation task, using unsupervised domain adaptation. We tested this approach on a small and realistic magnetic resonance (MRI) dataset, and found that self-ensembling can improve model generalization even when only a small amount of unlabelled data is available.",1
"In recent years, deep learning has shown performance breakthroughs in many applications, such as image detection, image segmentation, pose estimation, and speech recognition. However, this comes with a major concern: deep networks have been found to be vulnerable to adversarial examples. Adversarial examples are slightly modified inputs that are intentionally designed to cause a misclassification by the model. In the domains of images and speech, the modifications are so small that they are not seen or heard by humans, but nevertheless greatly affect the classification of the model.   Deep learning models have been successfully applied to malware detection. In this domain, generating adversarial examples is not straightforward, as small modifications to the bytes of the file could lead to significant changes in its functionality and validity. We introduce a novel loss function for generating adversarial examples specifically tailored for discrete input sets, such as executable bytes. We modify malicious binaries so that they would be detected as benign, while preserving their original functionality, by injecting a small sequence of bytes (payload) in the binary file. We applied this approach to an end-to-end convolutional deep learning malware detection model and show a high rate of detection evasion. Moreover, we show that our generated payload is robust enough to be transferable within different locations of the same file and across different files, and that its entropy is low and similar to that of benign data sections.",0
"Deep learning has exhibited remarkable performance advancements in various applications, including image detection, image segmentation, pose estimation, and speech recognition. Nevertheless, there is a significant concern that deep networks are susceptible to adversarial examples, which are slightly altered inputs intended to cause a model to misclassify. These alterations are often imperceptible to humans but can still significantly affect a model's classification. In malware detection, generating adversarial examples can be challenging because minor modifications to the file's bytes can have a significant impact on functionality and validity. To address this issue, we devised a new loss function for generating adversarial examples that target discrete input sets, such as executable bytes. By injecting a small byte sequence (payload) into a malicious binary file, we can modify it to be detected as benign while preserving its original functionality. Our approach was applied to a convolutional deep learning malware detection model, resulting in a high evasion detection rate. Additionally, we discovered that our generated payload was transferable within different file locations and across diverse files. Its entropy was also low and comparable to that of benign data sections.",1
"In order to identify and prevent tea leaf diseases effectively, convolution neural network (CNN) was used to realize the image recognition of tea disease leaves. Firstly, image segmentation and data enhancement are used to preprocess the images, and then these images were input into the network for training. Secondly, to reach a higher recognition accuracy of CNN, the learning rate and iteration numbers were adjusted frequently and the dropout was added properly in the case of over-fitting. Finally, the experimental results show that the recognition accuracy of CNN is 93.75%, while the accuracy of SVM and BP neural network is 89.36% and 87.69% respectively. Therefore, the recognition algorithm based on CNN is better in classification and can improve the recognition efficiency of tea leaf diseases effectively.",0
"To effectively detect and prevent tea leaf diseases, a convolution neural network (CNN) was utilized for recognizing images of diseased tea leaves. The process involved pre-processing the images with image segmentation and data enhancement before inputting them into the network for training. To improve the accuracy of the CNN, the learning rate and iteration numbers were frequently adjusted, and the dropout was added to prevent over-fitting. The experimental results indicated that the CNN had a recognition accuracy of 93.75%, which was higher than the accuracy of the SVM and BP neural network at 89.36% and 87.69%, respectively. Thus, the CNN-based recognition algorithm provides better classification and improves the efficiency of detecting tea leaf diseases.",1
"Interactive image segmentation is a topic of many studies in image processing. In a conventional approach, a user marks some pixels of the object(s) of interest and background, and an algorithm propagates these labels to the rest of the image. This paper presents a new graph-based method for interactive segmentation with two stages. In the first stage, nodes representing pixels are connected to their $k$-nearest neighbors to build a complex network with the small-world property to propagate the labels quickly. In the second stage, a regular network in a grid format is used to refine the segmentation on the object borders. Despite its simplicity, the proposed method can perform the task with high accuracy. Computer simulations are performed using some real-world images to show its effectiveness in both two-classes and multi-classes problems. It is also applied to all the images from the Microsoft GrabCut dataset for comparison, and the segmentation accuracy is comparable to those achieved by some state-of-the-art methods, while it is faster than them. In particular, it outperforms some recent approaches when the user input is composed only by a few ""scribbles"" draw over the objects. Its computational complexity is only linear on the image size at the best-case scenario and linearithmic in the worst case.",0
"Numerous studies have been conducted on interactive image segmentation in image processing. Traditionally, users mark certain pixels as belonging to the object(s) of interest and the background, and an algorithm propagates these labels throughout the image. This study presents a new two-stage graph-based method for interactive segmentation. In the first stage, a complex network with the small-world property is constructed by connecting pixels to their $k$-nearest neighbors to quickly propagate the labels. In the second stage, a regular grid network is used to refine the segmentation on object borders. Despite its simplicity, the proposed method achieves high accuracy and is evaluated using computer simulations on real-world images for both two-class and multi-class problems. The Microsoft GrabCut dataset is also used for comparison, and the proposed method achieves comparable accuracy to state-of-the-art methods while also being faster. Notably, it outperforms recent approaches when the user input consists of only a few ""scribbles"" on the objects. The computational complexity of the proposed method is linear on the image size in the best-case scenario and linearithmic in the worst case.",1
"We propose a new self-supervised approach to image feature learning from motion cue. This new approach leverages recent advances in deep learning in two directions: 1) the success of training deep neural network in estimating optical flow in real data using synthetic flow data; and 2) emerging work in learning image features from motion cues, such as optical flow. Building on these, we demonstrate that image features can be learned in self-supervision by first training an optical flow estimator with synthetic flow data, and then learning image features from the estimated flows in real motion data. We demonstrate and evaluate this approach on an image segmentation task. Using the learned image feature representation, the network performs significantly better than the ones trained from scratch in few-shot segmentation tasks.",0
"Our proposed method for image feature learning from motion cue involves a novel self-supervised approach. We have taken advantage of recent advancements in deep learning, specifically in two areas: 1) the successful training of deep neural networks in estimating optical flow using synthetic flow data in real scenarios, and 2) emerging research in image feature learning from motion cues, particularly optical flow. By utilizing these developments, we have shown that self-supervised learning of image features is possible through the training of an optical flow estimator with synthetic flow data, followed by the learning of image features from estimated flows in actual motion data. We have evaluated this approach on an image segmentation task and observed that the network performs significantly better than those trained from scratch in few-shot segmentation tasks, thanks to the learned image feature representation.",1
"Optimal decision making with limited or no information in stochastic environments where multiple agents interact is a challenging topic in the realm of artificial intelligence. Reinforcement learning (RL) is a popular approach for arriving at optimal strategies by predicating stimuli, such as the reward for following a strategy, on experience. RL is heavily explored in the single-agent context, but is a nascent concept in multiagent problems. To this end, I propose several principled model-free and partially model-based reinforcement learning approaches for several multiagent settings. In the realm of normative reinforcement learning, I introduce scalable extensions to Monte Carlo exploring starts for partially observable Markov Decision Processes (POMDP), dubbed MCES-P, where I expand the theory and algorithm to the multiagent setting. I first examine MCES-P with probably approximately correct (PAC) bounds in the context of multiagent setting, showing MCESP+PAC holds in the presence of other agents. I then propose a more sample-efficient methodology for antagonistic settings, MCESIP+PAC. For cooperative settings, I extend MCES-P to the Multiagent POMDP, dubbed MCESMP+PAC. I then explore the use of reinforcement learning as a methodology in searching for optima in realistic and latent model environments. First, I explore a parameterized Q-learning approach in modeling humans learning to reason in an uncertain, multiagent environment. Next, I propose an implementation of MCES-P, along with image segmentation, to create an adaptive team-based reinforcement learning technique to positively identify the presence of phenotypically-expressed water and pathogen stress in crop fields.",0
"Artificial intelligence faces a significant challenge in making optimal decisions with limited or no information in stochastic environments where multiple agents interact. Reinforcement learning is a favored method for arriving at ideal strategies through experience-based stimuli prediction, specifically in single-agent settings. However, it is a relatively new concept in multiagent contexts. The author proposes several model-free and partially model-based reinforcement learning approaches for various multiagent settings. In normative reinforcement learning, the author introduces scalable Monte Carlo exploring starts extensions for partially observable Markov Decision Processes, referred to as MCES-P, where the theory and algorithm are expanded to the multiagent setting. MCESP+PAC is examined with probably approximately correct bounds in the context of multiagent settings, demonstrating its applicability in the presence of other agents. A more sample-efficient methodology, MCESIP+PAC, is proposed for antagonistic settings, while MCES-P is extended to the Multiagent POMDP, referred to as MCESMP+PAC, for cooperative settings. The author also explores the use of reinforcement learning as a methodology in searching for optima in realistic and latent model environments. A parameterized Q-learning method is used to model human learning to reason in an uncertain, multiagent environment. The author also proposes an adaptive team-based reinforcement learning technique using MCES-P and image segmentation to positively identify the presence of phenotypically-expressed water and pathogen stress in crop fields.",1
"This paper offers three new, open-source, deep learning-based iris segmentation methods, and the methodology how to use irregular segmentation masks in a conventional Gabor-wavelet-based iris recognition. To train and validate the methods, we used a wide spectrum of iris images acquired by different teams and different sensors and offered publicly, including data taken from CASIA-Iris-Interval-v4, BioSec, ND-Iris-0405, UBIRIS, Warsaw-BioBase-Post-Mortem-Iris v2.0 (post-mortem iris images), and ND-TWINS-2009-2010 (iris images acquired from identical twins). This varied training data should increase the generalization capabilities of the proposed segmentation techniques. In database-disjoint training and testing, we show that deep learning-based segmentation outperforms the conventional (OSIRIS) segmentation in terms of Intersection over Union calculated between the obtained results and manually annotated ground-truth. Interestingly, the Gabor-based iris matching is not always better when deep learning-based segmentation is used, and is on par with the method employing Daugman's based segmentation.",0
"In this paper, we present three new iris segmentation methods that are based on deep learning and are open-source. We also describe a methodology for using irregular segmentation masks in a conventional Gabor-wavelet-based iris recognition system. To evaluate our methods, we used a diverse set of iris images that were acquired by different teams and sensors. These images were publicly available and included data from various sources such as CASIA-Iris-Interval-v4, BioSec, ND-Iris-0405, UBIRIS, Warsaw-BioBase-Post-Mortem-Iris v2.0, and ND-TWINS-2009-2010. By using such varied training data, we aimed to improve the generalization capabilities of our proposed segmentation techniques. In our experiments, we found that deep learning-based segmentation outperforms conventional (OSIRIS) segmentation in terms of Intersection over Union. However, we also observed that Gabor-based iris matching is not always superior when deep learning-based segmentation is used, and is sometimes equivalent to the method employing Daugman's based segmentation.",1
"Preparation of high-quality datasets for the urban scene understanding is a labor-intensive task, especially, for datasets designed for the autonomous driving applications. The application of the coarse ground truth (GT) annotations of these datasets without detriment to the accuracy of semantic image segmentation (by the mean intersection over union - mIoU) could simplify and speedup the dataset preparation and model fine tuning before its practical application. Here the results of the comparative analysis for semantic segmentation accuracy obtained by PSPNet deep learning architecture are presented for fine and coarse annotated images from Cityscapes dataset. Two scenarios were investigated: scenario 1 - the fine GT images for training and prediction, and scenario 2 - the fine GT images for training and the coarse GT images for prediction. The obtained results demonstrated that for the most important classes the mean accuracy values of semantic image segmentation for coarse GT annotations are higher than for the fine GT ones, and the standard deviation values are vice versa. It means that for some applications some unimportant classes can be excluded and the model can be tuned further for some classes and specific regions on the coarse GT dataset without loss of the accuracy even. Moreover, this opens the perspectives to use deep neural networks for the preparation of such coarse GT datasets.",0
"Preparing high-quality datasets for urban scene understanding, particularly for autonomous driving applications, is a time-consuming process. However, using coarse ground truth (GT) annotations in datasets can simplify and accelerate the preparation and fine-tuning of models without compromising semantic image segmentation accuracy (measured by mean intersection over union - mIoU). This study compares the accuracy of semantic segmentation using fine and coarse annotated images from the Cityscapes dataset with the PSPNet deep learning architecture. Two scenarios were examined: scenario 1 used fine GT images for both training and prediction, while scenario 2 used fine GT images for training and coarse GT images for prediction. The results indicate that for important classes, semantic image segmentation accuracy is higher using coarse GT annotations, while the opposite is true for standard deviation values. This suggests that unimportant classes can be excluded for certain applications, and deep neural networks can be used to prepare coarse GT datasets.",1
"For medical image segmentation, most fully convolutional networks (FCNs) need strong supervision through a large sample of high-quality dense segmentations, which is taxing in terms of costs, time and logistics involved. This burden of annotation can be alleviated by exploiting weak inexpensive annotations such as bounding boxes and anatomical landmarks. However, it is very difficult to \textit{a priori} estimate the optimal balance between the number of annotations needed for each supervision type that leads to maximum performance with the least annotation cost. To optimize this cost-performance trade off, we present a budget-based cost-minimization framework in a mixed-supervision setting via dense segmentations, bounding boxes, and landmarks. We propose a linear programming (LP) formulation combined with uncertainty and similarity based ranking strategy to judiciously select samples to be annotated next for optimal performance. In the results section, we show that our proposed method achieves comparable performance to state-of-the-art approaches with significantly reduced cost of annotations.",0
"To achieve medical image segmentation, most FCNs require extensive supervision through a large number of high-quality dense segmentations. This process is costly, time-consuming, and logistically challenging. However, the use of weak, affordable annotations such as bounding boxes and anatomical landmarks can alleviate this burden. Nonetheless, determining the optimal balance between the number of annotations required for each supervision type to achieve maximum performance at the lowest annotation cost is challenging. To address this issue, we introduce a budget-based cost-minimization framework that utilizes mixed supervisions such as dense segmentations, bounding boxes, and landmarks. We propose a linear programming (LP) formulation combined with uncertainty and similarity-based ranking strategies to intelligently select samples for annotation to optimize cost-performance trade-off. Our method achieves comparable performance to state-of-the-art approaches with significantly reduced annotation cost, as evidenced in the results section.",1
"Recently there has been a lot of work on pruning filters from deep convolutional neural networks (CNNs) with the intention of reducing computations.The key idea is to rank the filters based on a certain criterion (say, l1-norm) and retain only the top ranked filters. Once the low scoring filters are pruned away the remainder of the network is fine tuned and is shown to give performance comparable to the original unpruned network. In this work, we report experiments which suggest that the comparable performance of the pruned network is not due to the specific criterion chosen but due to the inherent plasticity of deep neural networks which allows them to recover from the loss of pruned filters once the rest of the filters are fine-tuned. Specifically we show counter-intuitive results wherein by randomly pruning 25-50% filters from deep CNNs we are able to obtain the same performance as obtained by using state-of-the-art pruning methods. We empirically validate our claims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also evaluate a real world scenario where a CNN trained on all 1000 ImageNet classes needs to be tested on only a small set of classes at test time (say, only animals). We create a new benchmark dataset from ImageNet to evaluate such class specific pruning and show that even here a random pruning strategy gives close to state-of-the-art performance. Unlike existing approaches which mainly focus on the task of image classification, in this work we also report results on object detection and image segmentation. We show that using a simple random pruning strategy we can achieve significant speed up in object detection (74% improvement in fps) while retaining the same accuracy as that of the original Faster RCNN model. Similarly we show that the performance of a pruned Segmentation Network (SegNet) is actually very similar to that of the original unpruned SegNet.",0
"Recently, there has been a lot of research focused on decreasing computations in deep convolutional neural networks (CNNs) by removing filters through pruning. The method involves ranking filters according to a criterion, such as l1-norm, and keeping only the top-ranked filters. After pruning out low-scoring filters, the rest of the network is fine-tuned and found to perform similarly to the original unpruned network. Our study challenges the notion that the comparable performance of pruned networks is a result of the specific criterion used. Instead, we suggest that the plasticity of deep neural networks allows them to recover from the loss of pruned filters once the rest of the filters are fine-tuned. Surprisingly, we demonstrate that randomly pruning 25-50% of filters from deep CNNs can yield the same performance as state-of-the-art pruning methods. Our claims are empirically validated through exhaustive evaluations with VGG-16 and ResNet-50. We also evaluate the effectiveness of random pruning in a real-world scenario where CNNs are trained on all 1000 ImageNet classes but tested on a limited set of classes, such as animals. A new benchmark dataset is created from ImageNet to evaluate class-specific pruning, and we show that even in this scenario, random pruning performs comparably to state-of-the-art methods. Unlike existing approaches that focus mainly on image classification, we report results on object detection and image segmentation, demonstrating that simple random pruning can significantly speed up object detection (74% improvement in fps) while maintaining the accuracy of the original Faster RCNN model. Additionally, we show that pruned Segmentation Networks (SegNets) perform similarly to the unpruned version.",1
"Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in many different 2D medical image analysis tasks. In clinical practice, however, a large part of the medical imaging data available is in 3D. This has motivated the development of 3D CNNs for volumetric image segmentation in order to benefit from more spatial context. Due to GPU memory restrictions caused by moving to fully 3D, state-of-the-art methods depend on subvolume/patch processing and the size of the input patch is usually small, limiting the incorporation of larger context information for a better performance. In this paper, we propose a novel Holistic Decomposition Convolution (HDC), for an effective and efficient semantic segmentation of volumetric images. HDC consists of a periodic down-shuffling operation followed by a conventional 3D convolution. HDC has the advantage of significantly reducing the size of the data for sub-sequential processing while using all the information available in the input irrespective of the down-shuffling factors. Results obtained from comprehensive experiments conducted on hip T1 MR images and intervertebral disc T2 MR images demonstrate the efficacy of the present approach.",0
"Many 2D medical image analysis tasks have seen state-of-the-art performance from Convolutional Neural Networks (CNNs). However, a large portion of medical imaging data in clinical practice is in 3D, leading to the development of 3D CNNs for volumetric image segmentation. Due to memory restrictions when fully transitioning to 3D, current methods rely on subvolume/patch processing with small input patch sizes, limiting performance. This paper proposes a novel approach, Holistic Decomposition Convolution (HDC), for effective and efficient semantic segmentation of volumetric images. HDC uses periodic down-shuffling followed by conventional 3D convolution, reducing data size for sub-sequential processing while utilizing all input information. Experiments on hip T1 MR images and intervertebral disc T2 MR images show the effectiveness of HDC.",1
This article suggests an algorithm of formation a training set for artificial neural network in case of image segmentation. The distinctive feature of this algorithm is that it using only one image for segmentation. The segmentation performs using three-layer perceptron. The main method of the segmentation is a method of region growing. Neural network is using for get a decision to include pixel into an area or not. Impulse noise is using for generation of a training set. Pixels damaged by noise are not related to the same region. Suggested method has been tested with help of computer experiment in automatic and interactive modes.,0
"The article proposes an algorithm for creating a training set for artificial neural network in the scenario of image segmentation. What sets this algorithm apart is its utilization of only a single image for segmentation, employing a three-layer perceptron for the segmentation process. The primary segmentation method used is region growing, with the neural network making a decision on whether or not to include a pixel in an area. The training set is generated using impulse noise, with noisy pixels being excluded from the same region. To validate the efficacy of the proposed method, computer experiments were conducted in both automatic and interactive modes.",1
"This paper tries to give a gentle introduction to deep learning in medical image processing, proceeding from theoretical foundations to applications. We first discuss general reasons for the popularity of deep learning, including several major breakthroughs in computer science. Next, we start reviewing the fundamental basics of the perceptron and neural networks, along with some fundamental theory that is often omitted. Doing so allows us to understand the reasons for the rise of deep learning in many application domains. Obviously medical image processing is one of these areas which has been largely affected by this rapid progress, in particular in image detection and recognition, image segmentation, image registration, and computer-aided diagnosis. There are also recent trends in physical simulation, modelling, and reconstruction that have led to astonishing results. Yet, some of these approaches neglect prior knowledge and hence bear the risk of producing implausible results. These apparent weaknesses highlight current limitations of deep learning. However, we also briefly discuss promising approaches that might be able to resolve these problems in the future.",0
"The aim of this paper is to introduce deep learning in medical image processing in a simple manner, covering its theoretical foundations and real-world applications. We begin by examining the reasons for the popularity of deep learning, including significant breakthroughs in computer science. We then delve into the basic concepts of neural networks and the perceptron, as well as fundamental theory that is often overlooked. This provides insight into the reasons behind the widespread use of deep learning in various application domains, including medical image processing. This field has been significantly impacted by deep learning, especially in image detection, recognition, segmentation, and computer-aided diagnosis. The paper also touches upon recent trends in physical simulation, modelling, and reconstruction, which have yielded impressive results. However, some of these approaches lack prior knowledge and may produce unrealistic outcomes. These limitations indicate the current constraints of deep learning. Nevertheless, the paper briefly explores promising approaches that may overcome these issues in the future.",1
"This paper presents a ""learning to learn"" approach to figure-ground image segmentation. By exploring webly-abundant images of specific visual effects, our method can effectively learn the visual-effect internal representations in an unsupervised manner and uses this knowledge to differentiate the figure from the ground in an image. Specifically, we formulate the meta-learning process as a compositional image editing task that learns to imitate a certain visual effect and derive the corresponding internal representation. Such a generative process can help instantiate the underlying figure-ground notion and enables the system to accomplish the intended image segmentation. Whereas existing generative methods are mostly tailored to image synthesis or style transfer, our approach offers a flexible learning mechanism to model a general concept of figure-ground segmentation from unorganized images that have no explicit pixel-level annotations. We validate our approach via extensive experiments on six datasets to demonstrate that the proposed model can be end-to-end trained without ground-truth pixel labeling yet outperforms the existing methods of unsupervised segmentation tasks.",0
"The ""learning to learn"" approach to figure-ground image segmentation is presented in this paper. The method is based on exploring webly-abundant images of specific visual effects to effectively learn the internal representations of these effects in an unsupervised manner. This knowledge is then used to differentiate between the figure and the ground in an image. To achieve this, the meta-learning process is formulated as a compositional image editing task that imitates a certain visual effect and derives the corresponding internal representation. This generative process helps establish the underlying figure-ground notion and enables the system to accomplish the intended image segmentation. Unlike existing generative methods that focus on image synthesis or style transfer, our approach offers a flexible learning mechanism to model a general concept of figure-ground segmentation from unorganized images without explicit pixel-level annotations. Extensive experiments on six datasets validate our approach, demonstrating that the proposed model can be end-to-end trained without ground-truth pixel labeling and outperforms existing unsupervised segmentation methods.",1
"Deep neural network models used for medical image segmentation are large because they are trained with high-resolution three-dimensional (3D) images. Graphics processing units (GPUs) are widely used to accelerate the trainings. However, the memory on a GPU is not large enough to train the models. A popular approach to tackling this problem is patch-based method, which divides a large image into small patches and trains the models with these small patches. However, this method would degrade the segmentation quality if a target object spans multiple patches. In this paper, we propose a novel approach for 3D medical image segmentation that utilizes the data-swapping, which swaps out intermediate data from GPU memory to CPU memory to enlarge the effective GPU memory size, for training high-resolution 3D medical images without patching. We carefully tuned parameters in the data-swapping method to obtain the best training performance for 3D U-Net, a widely used deep neural network model for medical image segmentation. We applied our tuning to train 3D U-Net with full-size images of 192 x 192 x 192 voxels in brain tumor dataset. As a result, communication overhead, which is the most important issue, was reduced by 17.1%. Compared with the patch-based method for patches of 128 x 128 x 128 voxels, our training for full-size images achieved improvement on the mean Dice score by 4.48% and 5.32 % for detecting whole tumor sub-region and tumor core sub-region, respectively. The total training time was reduced from 164 hours to 47 hours, resulting in 3.53 times of acceleration.",0
"The use of deep neural network models in medical image segmentation requires large models due to their training with high-resolution three-dimensional (3D) images. Graphics processing units (GPUs) are commonly used to speed up the training process, but their memory capacity is insufficient for the large models. To address this issue, the patch-based method is often used, which divides images into smaller patches for training. However, this method can negatively impact segmentation quality when the target object spans multiple patches. To overcome this limitation, our paper proposes a novel approach that utilizes data-swapping to enlarge effective GPU memory size for training high-resolution 3D medical images without patching. We fine-tuned the parameters of the data-swapping method for 3D U-Net, a widely used deep neural network model, and applied it to train full-size images of 192 x 192 x 192 voxels in a brain tumor dataset. Our approach reduced communication overhead by 17.1%, improved the mean Dice score by 4.48% and 5.32% for detecting whole tumor sub-region and tumor core sub-region, respectively, compared to the patch-based method for patches of 128 x 128 x 128 voxels. Additionally, the total training time was reduced from 164 hours to 47 hours, resulting in 3.53 times acceleration.",1
"Interactive image segmentation algorithms rely on the user to provide annotations as the guidance. When the task of interactive segmentation is performed on a small touchscreen device, the requirement of providing precise annotations could be cumbersome to the user. We design an efficient seed proposal method that actively proposes annotation seeds for the user to label. The user only needs to check which ones of the query seeds are inside the region of interest (ROI). We enforce the sparsity and diversity criteria on the selection of the query seeds. At each round of interaction the user is only presented with a small number of informative query seeds that are far apart from each other. As a result, we are able to derive a user friendly interaction mechanism for annotation on small touchscreen devices. The user merely has to swipe through on the ROI-relevant query seeds, which should be easy since those gestures are commonly used on a touchscreen. The performance of our algorithm is evaluated on six publicly available datasets. The evaluation results show that our algorithm achieves high segmentation accuracy, with short response time and less user feedback.",0
"Interactive image segmentation algorithms require user-provided annotations for guidance, which can be challenging on small touchscreen devices. To address this issue, we have developed an efficient seed proposal method that suggests annotation seeds for the user to label. Our method enforces sparsity and diversity criteria to select informative query seeds that are far apart from each other. This approach presents the user with a small number of relevant query seeds at each interaction round, which can be easily swiped through on the region of interest. We have evaluated our algorithm on six publicly available datasets and observed high segmentation accuracy, short response time, and minimal user feedback. Overall, our approach provides a user-friendly interaction mechanism for annotation on small touchscreen devices.",1
"We propose an end-to-end learning framework for segmenting generic objects in both images and videos. Given a novel image or video, our approach produces a pixel-level mask for all ""object-like"" regions---even for object categories never seen during training. We formulate the task as a structured prediction problem of assigning an object/background label to each pixel, implemented using a deep fully convolutional network. When applied to a video, our model further incorporates a motion stream, and the network learns to combine both appearance and motion and attempts to extract all prominent objects whether they are moving or not. Beyond the core model, a second contribution of our approach is how it leverages varying strengths of training annotations. Pixel-level annotations are quite difficult to obtain, yet crucial for training a deep network approach for segmentation. Thus we propose ways to exploit weakly labeled data for learning dense foreground segmentation. For images, we show the value in mixing object category examples with image-level labels together with relatively few images with boundary-level annotations. For video, we show how to bootstrap weakly annotated videos together with the network trained for image segmentation. Through experiments on multiple challenging image and video segmentation benchmarks, our method offers consistently strong results and improves the state-of-the-art for fully automatic segmentation of generic (unseen) objects. In addition, we demonstrate how our approach benefits image retrieval and image retargeting, both of which flourish when given our high-quality foreground maps. Code, models, and videos are at:http://vision.cs.utexas.edu/projects/pixelobjectness/",0
"Our proposed framework offers a comprehensive learning approach for segmenting objects in images and videos. Our method produces a detailed mask for all object-like regions in novel images and videos, even for categories that were not seen during training. We employ a deep fully convolutional network to assign object/background labels to each pixel, treating the task as a structured prediction problem. Our model is designed to combine both motion and appearance features to extract prominent objects, regardless of whether they are stationary or in motion. To overcome the difficulty of obtaining pixel-level annotations, we present ways to utilize weakly labeled data for training the segmentation model. Our approach involves mixing object category examples with image-level labels and boundary-level annotations for images, and bootstrapping weakly annotated videos for video segmentation. Experimental results on challenging benchmarks demonstrate a significant improvement over the state-of-the-art for fully automatic segmentation of generic objects. Moreover, our method benefits image retrieval and retargeting by providing high-quality foreground maps. Access to our code, models, and videos can be found at: http://vision.cs.utexas.edu/projects/pixelobjectness/",1
"Markov Random Fields (MRFs) are a popular model for several pattern recognition and reconstruction problems in robotics and computer vision. Inference in MRFs is intractable in general and related work resorts to approximation algorithms. Among those techniques, semidefinite programming (SDP) relaxations have been shown to provide accurate estimates while scaling poorly with the problem size and being typically slow for practical applications. Our first contribution is to design a dual ascent method to solve standard SDP relaxations that takes advantage of the geometric structure of the problem to speed up computation. This technique, named Dual Ascent Riemannian Staircase (DARS), is able to solve large problem instances in seconds. Our second contribution is to develop a second and faster approach. The backbone of this second approach is a novel SDP relaxation combined with a fast and scalable solver based on smooth Riemannian optimization. We show that this approach, named Fast Unconstrained SEmidefinite Solver (FUSES), can solve large problems in milliseconds. Contrarily to local MRF solvers, e.g., loopy belief propagation, our approaches do not require an initial guess. Moreover, we leverage recent results from optimization theory to provide per-instance sub-optimality guarantees. We demonstrate the proposed approaches in multi-class image segmentation problems. Extensive experimental evidence shows that (i) FUSES and DARS produce near-optimal solutions, attaining an objective within 0.1% of the optimum, (ii) FUSES and DARS are remarkably faster than general-purpose SDP solvers, and FUSES is more than two orders of magnitude faster than DARS while attaining similar solution quality, (iii) FUSES is faster than local search methods while being a global solver.",0
"Markov Random Fields (MRFs) are frequently used in robotics and computer vision for pattern recognition and reconstruction problems. However, inferring in MRFs is generally impractical, so approximation algorithms are often employed. One such method is semidefinite programming (SDP) relaxations, but they are slow for practical use and do not scale well. We have two contributions to overcome these limitations. Firstly, we developed a Dual Ascent Riemannian Staircase (DARS) technique that utilizes the geometric structure of the problem to solve standard SDP relaxations quickly. Secondly, we created the Fast Unconstrained SEmidefinite Solver (FUSES) technique, which combines a novel SDP relaxation with a fast, scalable solver based on smooth Riemannian optimization to solve large problems in milliseconds. Our approaches do not require an initial guess and provide per-instance sub-optimality guarantees. We tested our techniques on multi-class image segmentation problems, showing that FUSES and DARS produce near-optimal solutions within 0.1% of the optimum, are much faster than general-purpose SDP solvers, and FUSES is over two orders of magnitude faster than DARS while achieving similar solution quality. Additionally, FUSES is faster than local search methods while being a global solver.",1
"Fully convolutional deep neural networks have been asserted to be fast and precise frameworks with great potential in image segmentation. One of the major challenges in training such networks raises when data is unbalanced, which is common in many medical imaging applications such as lesion segmentation where lesion class voxels are often much lower in numbers than non-lesion voxels. A trained network with unbalanced data may make predictions with high precision and low recall, being severely biased towards the non-lesion class which is particularly undesired in most medical applications where FNs are more important than FPs. Various methods have been proposed to address this problem, more recently similarity loss functions and focal loss. In this work we trained fully convolutional deep neural networks using an asymmetric similarity loss function to mitigate the issue of data imbalance and achieve much better tradeoff between precision and recall. To this end, we developed a 3D FC-DenseNet with large overlapping image patches as input and an asymmetric similarity loss layer based on Tversky index (using Fbeta scores). We used large overlapping image patches as inputs for intrinsic and extrinsic data augmentation, a patch selection algorithm, and a patch prediction fusion strategy using B-spline weighted soft voting to account for the uncertainty of prediction in patch borders. We applied this method to MS lesion segmentation based on two different datasets of MSSEG and ISBI longitudinal MS lesion segmentation challenge, where we achieved top performance in both challenges. Our network trained with focal loss ranked first according to the ISBI challenge overall score and resulted in the lowest reported lesion false positive rate among all submitted methods. Our network trained with the asymmetric similarity loss led to the lowest surface distance and the best lesion true positive rate.",0
"Image segmentation can greatly benefit from the speed and precision of fully convolutional deep neural networks. However, training such networks with unbalanced data, as is common in medical imaging applications like lesion segmentation, can pose a challenge. Networks trained with unbalanced data may have high precision and low recall, leading to bias towards the non-lesion class and a higher number of false negatives. Several methods have been proposed to address this issue, including similarity loss functions and focal loss. In this study, we trained a 3D FC-DenseNet with large overlapping image patches and an asymmetric similarity loss layer based on the Tversky index. We used intrinsic and extrinsic data augmentation, patch selection algorithm, and patch prediction fusion strategy to account for prediction uncertainty in patch borders. Our approach achieved top performance in the MSSEG and ISBI longitudinal MS lesion segmentation challenge, with the network trained with focal loss ranking first in the ISBI challenge overall score and resulting in the lowest reported lesion false positive rate. The network trained with the asymmetric similarity loss achieved the lowest surface distance and the best lesion true positive rate.",1
"The graph Laplacian is a standard tool in data science, machine learning, and image processing. The corresponding matrix inherits the complex structure of the underlying network and is in certain applications densely populated. This makes computations, in particular matrix-vector products, with the graph Laplacian a hard task. A typical application is the computation of a number of its eigenvalues and eigenvectors. Standard methods become infeasible as the number of nodes in the graph is too large. We propose the use of the fast summation based on the nonequispaced fast Fourier transform (NFFT) to perform the dense matrix-vector product with the graph Laplacian fast without ever forming the whole matrix. The enormous flexibility of the NFFT algorithm allows us to embed the accelerated multiplication into Lanczos-based eigenvalues routines or iterative linear system solvers and even consider other than the standard Gaussian kernels. We illustrate the feasibility of our approach on a number of test problems from image segmentation to semi-supervised learning based on graph-based PDEs. In particular, we compare our approach with the Nystr\""om method. Moreover, we present and test an enhanced, hybrid version of the Nystr\""om method, which internally uses the NFFT.",0
"The graph Laplacian is commonly used in data science, machine learning, and image processing, but its corresponding matrix can be complex and densely populated in certain applications, making computations such as matrix-vector products difficult. Calculating the eigenvalues and eigenvectors of the graph Laplacian using standard methods becomes impractical with large graphs. To address this issue, we suggest using the nonequispaced fast Fourier transform (NFFT) for fast summation-based matrix-vector products with the graph Laplacian, without needing to form the entire matrix. The NFFT algorithm is highly flexible, allowing us to incorporate the accelerated multiplication into Lanczos-based eigenvalue routines, iterative linear system solvers, and even non-standard Gaussian kernels. We demonstrate the feasibility of our approach on various problems, including image segmentation and graph-based PDE semi-supervised learning, comparing it with the Nystr\""om method. Additionally, we present and test a hybrid version of the Nystr\""om method that leverages the NFFT algorithm.",1
"Weakly-supervised instance segmentation, which could greatly save labor and time cost of pixel mask annotation, has attracted increasing attention in recent years. The commonly used pipeline firstly utilizes conventional image segmentation methods to automatically generate initial masks and then use them to train an off-the-shelf segmentation network in an iterative way. However, the initial generated masks usually contains a notable proportion of invalid masks which are mainly caused by small object instances. Directly using these initial masks to train segmentation model is harmful for the performance. To address this problem, we propose a hybrid network in this paper. In our architecture, there is a principle segmentation network which is used to handle the normal samples with valid generated masks. In addition, a complementary branch is added to handle the small and dim objects without valid masks. Experimental results indicate that our method can achieve significantly performance improvement both on the small object instances and large ones, and outperforms all state-of-the-art methods.",0
"In recent years, there has been an increasing interest in weakly-supervised instance segmentation as it can significantly reduce the labor and time costs associated with pixel mask annotation. The commonly used approach involves using conventional image segmentation methods to generate initial masks, which are then iteratively used to train an off-the-shelf segmentation network. However, these initial masks often contain a significant number of invalid masks, primarily due to small object instances. Using these masks to train the segmentation model can be detrimental to performance. To overcome this challenge, we propose a hybrid network in this paper. Our architecture includes a principle segmentation network to handle normal samples with valid generated masks, and a complementary branch to handle small and dim objects without valid masks. Our experimental results demonstrate that our method substantially improves the performance for both small and large object instances, outperforming all state-of-the-art methods.",1
"Partial differential equations (PDEs) are indispensable for modeling many physical phenomena and also commonly used for solving image processing tasks. In the latter area, PDE-based approaches interpret image data as discretizations of multivariate functions and the output of image processing algorithms as solutions to certain PDEs. Posing image processing problems in the infinite dimensional setting provides powerful tools for their analysis and solution. Over the last few decades, the reinterpretation of classical image processing problems through the PDE lens has been creating multiple celebrated approaches that benefit a vast area of tasks including image segmentation, denoising, registration, and reconstruction.   In this paper, we establish a new PDE-interpretation of a class of deep convolutional neural networks (CNN) that are commonly used to learn from speech, image, and video data. Our interpretation includes convolution residual neural networks (ResNet), which are among the most promising approaches for tasks such as image classification having improved the state-of-the-art performance in prestigious benchmark challenges. Despite their recent successes, deep ResNets still face some critical challenges associated with their design, immense computational costs and memory requirements, and lack of understanding of their reasoning.   Guided by well-established PDE theory, we derive three new ResNet architectures that fall into two new classes: parabolic and hyperbolic CNNs. We demonstrate how PDE theory can provide new insights and algorithms for deep learning and demonstrate the competitiveness of three new CNN architectures using numerical experiments.",0
"Partial differential equations (PDEs) are vital for modeling various physical phenomena and are also commonly used in image processing tasks. The use of PDEs in image processing involves interpreting image data as multivariate functions and solving certain PDEs to obtain output. This approach provides powerful tools for analyzing and solving image processing problems in the infinite dimensional setting. In recent years, the application of PDEs to classical image processing problems has led to the development of several celebrated approaches that benefit tasks such as image segmentation, denoising, registration, and reconstruction. This paper proposes a new PDE-based interpretation of deep convolutional neural networks (CNNs), including ResNet, which are promising approaches for speech, image, and video data learning. Although ResNets have improved the state-of-the-art performance in prestigious benchmark challenges, they still face critical challenges related to their design, computational costs, memory requirements, and reasoning. This paper derives three new ResNet architectures guided by well-established PDE theory, falling into two new classes: parabolic and hyperbolic CNNs. The paper demonstrates how PDE theory provides new insights and algorithms for deep learning and demonstrates the competitiveness of the three new CNN architectures through numerical experiments.",1
"3D image segmentation plays an important role in biomedical image analysis. Many 2D and 3D deep learning models have achieved state-of-the-art segmentation performance on 3D biomedical image datasets. Yet, 2D and 3D models have their own strengths and weaknesses, and by unifying them together, one may be able to achieve more accurate results. In this paper, we propose a new ensemble learning framework for 3D biomedical image segmentation that combines the merits of 2D and 3D models. First, we develop a fully convolutional network based meta-learner to learn how to improve the results from 2D and 3D models (base-learners). Then, to minimize over-fitting for our sophisticated meta-learner, we devise a new training method that uses the results of the base-learners as multiple versions of ""ground truths"". Furthermore, since our new meta-learner training scheme does not depend on manual annotation, it can utilize abundant unlabeled 3D image data to further improve the model. Extensive experiments on two public datasets (the HVSMR 2016 Challenge dataset and the mouse piriform cortex dataset) show that our approach is effective under fully-supervised, semi-supervised, and transductive settings, and attains superior performance over state-of-the-art image segmentation methods.",0
"Biomedical image analysis relies heavily on 3D image segmentation. Although 2D and 3D deep learning models have achieved remarkable segmentation results on 3D biomedical image datasets, they each have their own strengths and weaknesses. Combining them may lead to more precise results. This paper proposes a new ensemble learning framework for 3D biomedical image segmentation that combines the benefits of 2D and 3D models. The approach involves developing a fully convolutional network-based meta-learner that improves the results from the base-learners. To avoid over-fitting, a new training method that uses the results of the base-learners as multiple versions of ""ground truths"" is devised. Because the meta-learner training scheme does not require manual annotation, it can use abundant unlabeled 3D image data to enhance the model. The approach is effective under fully-supervised, semi-supervised, and transductive settings, and produces superior performance compared to state-of-the-art image segmentation methods, as demonstrated by extensive experiments on two public datasets  the HVSMR 2016 Challenge dataset and the mouse piriform cortex dataset.",1
"The most recent fast and accurate image segmentation methods are built upon fully convolutional deep neural networks. In this paper, we propose new deep learning strategies for DenseNets to improve segmenting images with subtle differences in intensity values and features. We aim to segment brain tissue on infant brain MRI at about 6 months of age where white matter and gray matter of the developing brain show similar T1 and T2 relaxation times, thus appear to have similar intensity values on both T1- and T2-weighted MRI scans. Brain tissue segmentation at this age is, therefore, very challenging. To this end, we propose an exclusive multi-label training strategy to segment the mutually exclusive brain tissues with similarity loss functions that automatically balance the training based on class prevalence. Using our proposed training strategy based on similarity loss functions and patch prediction fusion we decrease the number of parameters in the network, reduce the complexity of the training process focusing the attention on less number of tasks, while mitigating the effects of data imbalance between labels and inaccuracies near patch borders. By taking advantage of these strategies we were able to perform fast image segmentation (90 seconds per 3D volume), using a network with less parameters than many state-of-the-art networks, overcoming issues such as 3Dvs2D training and large vs small patch size selection, while achieving the top performance in segmenting brain tissue among all methods tested in first and second round submissions of the isointense infant brain MRI segmentation (iSeg) challenge according to the official challenge test results. Our proposed strategy improves the training process through balanced training and by reducing its complexity while providing a trained model that works for any size input image and is fast and more accurate than many state-of-the-art methods.",0
"Recent advancements in image segmentation have relied on fully convolutional deep neural networks. Our study proposes novel deep learning techniques for DenseNets to enhance the segmentation of images with subtle variations in intensity values and features. Specifically, we aim to segment brain tissue in infant brain MRI scans at around 6 months of age, which poses a significant challenge due to the similar intensity values of white and gray matter. To address this issue, we introduce an exclusive multi-label training approach that incorporates similarity loss functions to balance the training based on class prevalence. By utilizing this strategy, we effectively reduce network parameters and simplify the training process while mitigating inaccuracies near patch borders and data imbalances between labels. Our proposed method achieved top performance in segmenting brain tissue among all methods tested in the iSeg challenge, as evidenced by official challenge test results. Overall, our study provides a faster and more accurate segmentation approach that can be applied to images of any size.",1
"Radiological imaging offers effective measurement of anatomy, which is useful in disease diagnosis and assessment. Previous study has shown that the left atrial wall remodeling can provide information to predict treatment outcome in atrial fibrillation. Nevertheless, the segmentation of the left atrial structures from medical images is still very time-consuming. Current advances in neural network may help creating automatic segmentation models that reduce the workload for clinicians. In this preliminary study, we propose automated, two-stage, three-dimensional U-Nets with convolutional neural network, for the challenging task of left atrial segmentation. Unlike previous two-dimensional image segmentation methods, we use 3D U-Nets to obtain the heart cavity directly in 3D. The dual 3D U-Net structure consists of, a first U-Net to coarsely segment and locate the left atrium, and a second U-Net to accurately segment the left atrium under higher resolution. In addition, we introduce a Contour loss based on additional distance information to adjust the final segmentation. We randomly split the data into training datasets (80 subjects) and validation datasets (20 subjects) to train multiple models, with different augmentation setting. Experiments show that the average Dice coefficients for validation datasets are around 0.91 - 0.92, the sensitivity around 0.90-0.94 and the specificity 0.99. Compared with traditional Dice loss, models trained with Contour loss in general offer smaller Hausdorff distance with similar Dice coefficient, and have less connected components in predictions. Finally, we integrate several trained models in an ensemble prediction to segment testing datasets.",0
"Radiological imaging is an effective way to measure anatomy, which can aid in diagnosing and assessing diseases. Previous research has demonstrated that left atrial wall remodeling can predict treatment outcomes in atrial fibrillation. However, segmenting left atrial structures from medical images is a time-consuming task. Advances in neural networks may help reduce the workload for clinicians by creating automatic segmentation models. In this study, we propose using automated, two-stage, three-dimensional U-Nets with convolutional neural networks to segment the left atrium. Unlike previous methods, we use 3D U-Nets to obtain the heart cavity in 3D. Our dual 3D U-Net structure includes a first U-Net to coarsely segment and locate the left atrium and a second U-Net to accurately segment it at a higher resolution. We also introduce a Contour loss based on distance information to adjust the final segmentation. We trained multiple models using different augmentation settings and split the data into training and validation datasets. Our experiments showed that models trained with Contour loss had smaller Hausdorff distances and fewer connected components in predictions compared to those trained with traditional Dice loss. We integrated several trained models in an ensemble prediction to segment testing datasets. The validation datasets had an average Dice coefficient of 0.91-0.92, a sensitivity of 0.90-0.94, and a specificity of 0.99.",1
"Many seemingly unrelated computer vision tasks can be viewed as a special case of image decomposition into separate layers. For example, image segmentation (separation into foreground and background layers); transparent layer separation (into reflection and transmission layers); Image dehazing (separation into a clear image and a haze map), and more. In this paper we propose a unified framework for unsupervised layer decomposition of a single image, based on coupled ""Deep-image-Prior"" (DIP) networks. It was shown [Ulyanov et al] that the structure of a single DIP generator network is sufficient to capture the low-level statistics of a single image. We show that coupling multiple such DIPs provides a powerful tool for decomposing images into their basic components, for a wide variety of applications. This capability stems from the fact that the internal statistics of a mixture of layers is more complex than the statistics of each of its individual components. We show the power of this approach for Image-Dehazing, Fg/Bg Segmentation, Watermark-Removal, Transparency Separation in images and video, and more. These capabilities are achieved in a totally unsupervised way, with no training examples other than the input image/video itself.",0
"A range of computer vision tasks can be considered as a type of image decomposition into distinct layers. This includes activities such as image segmentation, transparent layer separation and image dehazing. In this study, we propose a unified framework for unsupervised layer decomposition of a single image, utilizing connected ""Deep-image-Prior"" networks. Research has demonstrated that a single DIP generator network can capture the low-level statistics of a single image. By coupling multiple DIPs, we can effectively break down images into their fundamental components for various applications. This is particularly useful as the internal statistics of a mixture of layers are more complex than those of individual components. We showcase the potential of this approach for tasks such as Image-Dehazing, Fg/Bg Segmentation, Watermark-Removal and Transparency Separation in both images and videos. The technique is entirely unsupervised, requiring no training examples beyond the input image or video.",1
"While deep learning has achieved significant advances in accuracy for medical image segmentation, its benefits for deformable image registration have so far remained limited to reduced computation times. Previous work has either focused on replacing the iterative optimization of distance and smoothness terms with CNN-layers or using supervised approaches driven by labels. Our method is the first to combine the complementary strengths of global semantic information (represented by segmentation labels) and local distance metrics that help align surrounding structures. We demonstrate significant higher Dice scores (of 86.5\%) for deformable cardiac image registration compared to classic registration (79.0\%) as well as label-driven deep learning frameworks (83.4\%).",0
"Although deep learning has made significant progress in improving the accuracy of medical image segmentation, its advantages for deformable image registration have been limited to reducing computation times. Prior research has either focused on substituting CNN-layers for the iterative optimization of distance and smoothness terms or employing supervised approaches driven by labels. Our approach is novel in that it combines the strengths of global semantic information, represented by segmentation labels, and local distance metrics, which aid in aligning surrounding structures. We have demonstrated that our method yields significantly higher Dice scores (86.5\%) for deformable cardiac image registration compared to traditional registration (79.0\%) and label-driven deep learning frameworks (83.4\%).",1
"Radiologist is ""doctor's doctor"", biomedical image segmentation plays a central role in quantitative analysis, clinical diagnosis, and medical intervention. In the light of the fully convolutional networks (FCN) and U-Net, deep convolutional networks (DNNs) have made significant contributions in biomedical image segmentation applications. In this paper, based on U-Net, we propose MDUnet, a multi-scale densely connected U-net for biomedical image segmentation. we propose three different multi-scale dense connections for U shaped architectures encoder, decoder and across them. The highlights of our architecture is directly fuses the neighboring different scale feature maps from both higher layers and lower layers to strengthen feature propagation in current layer. Which can largely improves the information flow encoder, decoder and across them. Multi-scale dense connections, which means containing shorter connections between layers close to the input and output, also makes much deeper U-net possible. We adopt the optimal model based on the experiment and propose a novel Multi-scale Dense U-Net (MDU-Net) architecture with quantization. Which reduce overfitting in MDU-Net for better accuracy. We evaluate our purpose model on the MICCAI 2015 Gland Segmentation dataset (GlaS). The three multi-scale dense connections improve U-net performance by up to 1.8% on test A and 3.5% on test B in the MICCAI Gland dataset. Meanwhile the MDU-net with quantization achieves the superiority over U-Net performance by up to 3% on test A and 4.1% on test B.",0
"The role of a radiologist is often referred to as the ""doctor's doctor,"" and biomedical image segmentation is a crucial part of quantitative analysis, clinical diagnosis, and medical intervention. In recent years, deep convolutional networks (DNNs) such as fully convolutional networks (FCN) and U-Net have made significant contributions to biomedical image segmentation applications. This paper proposes MDUnet, a multi-scale densely connected U-net based on U-Net. We introduce three different multi-scale dense connections for encoder, decoder, and across U-shaped architectures, which fuse neighboring feature maps of different scales to enhance feature propagation and improve information flow. The inclusion of shorter connections between layers close to the input and output enables a much deeper U-Net. We adopt a quantization technique to reduce overfitting in MDU-Net for better accuracy and evaluate its performance on the MICCAI 2015 Gland Segmentation dataset (GlaS). The proposed MDU-Net architecture with quantization achieves superior performance over U-Net, improving its performance by up to 3% on test A and 4.1% on test B. The three multi-scale dense connections also improve U-Net performance by up to 1.8% on test A and 3.5% on test B in the MICCAI Gland dataset.",1
"Accurate segmentation of different sub-regions of gliomas including peritumoral edema, necrotic core, enhancing and non-enhancing tumor core from multimodal MRI scans has important clinical relevance in diagnosis, prognosis and treatment of brain tumors. However, due to the highly heterogeneous appearance and shape, segmentation of the sub-regions is very challenging. Recent development using deep learning models has proved its effectiveness in the past several brain segmentation challenges as well as other semantic and medical image segmentation problems. Most models in brain tumor segmentation use a 2D/3D patch to predict the class label for the center voxel and variant patch sizes and scales are used to improve the model performance. However, it has low computation efficiency and also has limited receptive field. U-Net is a widely used network structure for end-to-end segmentation and can be used on the entire image or extracted patches to provide classification labels over the entire input voxels so that it is more efficient and expect to yield better performance with larger input size. Furthermore, instead of picking the best network structure, an ensemble of multiple models, trained on different dataset or different hyper-parameters, can generally improve the segmentation performance. In this study we propose to use an ensemble of 3D U-Nets with different hyper-parameters for brain tumor segmentation. Preliminary results showed effectiveness of this model. In addition, we developed a linear model for survival prediction using extracted imaging and non-imaging features, which, despite the simplicity, can effectively reduce overfitting and regression errors.",0
"Multimodal MRI scans require accurate segmentation of different sub-regions of gliomas, such as peritumoral edema, necrotic core, and enhancing and non-enhancing tumor core. This is crucial for diagnosing, prognosing, and treating brain tumors. However, segmentation of these sub-regions is challenging due to their heterogeneous appearance and shape. Deep learning models have shown effectiveness in previous brain segmentation challenges and other image segmentation problems. Most models for brain tumor segmentation use a patch-based approach, which has limited computation efficiency and receptive field. U-Net is a widely used network structure that can be used on the entire image or extracted patches to provide classification labels and improve performance. Using an ensemble of multiple models trained on different datasets or hyper-parameters can also improve segmentation performance. In this study, we propose using an ensemble of 3D U-Nets with different hyper-parameters for brain tumor segmentation, which has shown promising results. Additionally, we developed a linear model for survival prediction using imaging and non-imaging features to reduce overfitting and regression errors.",1
"Segmentation of magnetic resonance (MR) images is a fundamental step in many medical imaging-based applications. The recent implementation of deep convolutional neural networks (CNNs) in image processing has been shown to have significant impacts on medical image segmentation. Network training of segmentation CNNs typically requires images and paired annotation data representing pixel-wise tissue labels referred to as masks. However, the supervised training of highly efficient CNNs with deeper structure and more network parameters requires a large number of training images and paired tissue masks. Thus, there is great need to develop a generalized CNN-based segmentation method which would be applicable for a wide variety of MR image datasets with different tissue contrasts. The purpose of this study was to develop and evaluate a generalized CNN-based method for fully-automated segmentation of different MR image datasets using a single set of annotated training data. A technique called cycle-consistent generative adversarial network (CycleGAN) is applied as the core of the proposed method to perform image-to-image translation between MR image datasets with different tissue contrasts. A joint segmentation network is incorporated into the adversarial network to obtain additional segmentation functionality. The proposed method was evaluated for segmenting bone and cartilage on two clinical knee MR image datasets acquired at our institution using only a single set of annotated data from a publicly available knee MR image dataset. The new technique may further improve the applicability and efficiency of CNN-based segmentation of medical images while eliminating the need for large amounts of annotated training data.",0
"Medical imaging-based applications rely on the segmentation of magnetic resonance (MR) images, which has been improved by the use of deep convolutional neural networks (CNNs). However, training efficient CNNs with more network parameters requires a large number of training images and paired tissue masks. Therefore, there is a need for a generalized CNN-based segmentation method that is applicable to various MR image datasets with different tissue contrasts. This study developed and evaluated a method using a technique called cycle-consistent generative adversarial network (CycleGAN) to translate MR image datasets with different tissue contrasts. A joint segmentation network was incorporated into the adversarial network to obtain additional segmentation functionality. The proposed method was evaluated for segmenting bone and cartilage using only a single set of annotated data from a publicly available knee MR image dataset. This new technique may improve the efficiency of CNN-based segmentation of medical images and eliminate the need for large amounts of annotated training data.",1
"Increasing the mini-batch size for stochastic gradient descent offers significant opportunities to reduce wall-clock training time, but there are a variety of theoretical and systems challenges that impede the widespread success of this technique. We investigate these issues, with an emphasis on time to convergence and total computational cost, through an extensive empirical analysis of network training across several architectures and problem domains, including image classification, image segmentation, and language modeling. Although it is common practice to increase the batch size in order to fully exploit available computational resources, we find a substantially more nuanced picture. Our main finding is that across a wide range of network architectures and problem domains, increasing the batch size beyond a certain point yields no decrease in wall-clock time to convergence for \emph{either} train or test loss. This batch size is usually substantially below the capacity of current systems. We show that popular training strategies for large batch size optimization begin to fail before we can populate all available compute resources, and we show that the point at which these methods break down depends more on attributes like model architecture and data complexity than it does directly on the size of the dataset.",0
"The potential benefits of reducing wall-clock training time by increasing the mini-batch size for stochastic gradient descent are hindered by various theoretical and systems challenges. Our study focuses on time to convergence and total computational cost, and we conduct extensive empirical analysis on network training in different architectures and problem domains such as image classification, image segmentation, and language modeling. It is common practice to increase batch size to maximize available computational resources, but our research reveals a more nuanced perspective. We find that increasing the batch size beyond a certain point does not reduce wall-clock time to convergence for either train or test loss, even though this batch size is usually below the capacity of current systems. We also demonstrate that training strategies for large batch size optimization fail before all available compute resources can be utilized, and this breakdown is more reliant on factors such as model architecture and data complexity rather than the dataset size.",1
"Convolutional neural networks (CNNs) have become increasingly popular for solving a variety of computer vision tasks, ranging from image classification to image segmentation. Recently, autonomous vehicles have created a demand for depth information, which is often obtained using hardware sensors such as Light detection and ranging (LIDAR). Although it can provide precise distance measurements, most LIDARs are still far too expensive to sell in mass-produced consumer vehicles, which has motivated methods to generate depth information from commodity automotive sensors like cameras.   In this paper, we propose an approach called Deep Sensor Cloning (DSC). The idea is to use Convolutional Neural Networks in conjunction with inexpensive sensors to replicate the 3D point-clouds that are created by expensive LIDARs. To accomplish this, we develop a new dataset (DSDepth) and a new family of CNN architectures (DSCnets). While previous tasks such as KITTI depth prediction use an interpolated RGB-D images as ground-truth for training, we instead use DSCnets to directly predict LIDAR point-clouds. When we compare the output of our models to a $75,000 LIDAR, we find that our most accurate DSCnet achieves a relative error of 5.77% using a single camera and 4.69% using stereo cameras.",0
"The use of Convolutional Neural Networks (CNNs) has grown in popularity for various computer vision tasks, from image classification to image segmentation. The need for depth information in autonomous vehicles has resulted in the use of hardware sensors like LIDAR, which are expensive and not suitable for mass-produced consumer vehicles. This has led to the development of methods to generate depth information from commodity automotive sensors such as cameras. The proposed approach in this paper, called Deep Sensor Cloning (DSC), makes use of CNNs and inexpensive sensors to replicate the 3D point-clouds created by expensive LIDARs. To achieve this, the authors develop a new dataset (DSDepth) and a new family of CNN architectures (DSCnets). Unlike previous approaches that use interpolated RGB-D images, DSCnets directly predict LIDAR point-clouds. The most accurate DSCnet achieves a relative error of 5.77% using a single camera and 4.69% using stereo cameras when compared to a $75,000 LIDAR.",1
"This paper presents a novel framework in which video/image segmentation and localization are cast into a single optimization problem that integrates information from low level appearance cues with that of high level localization cues in a very weakly supervised manner. The proposed framework leverages two representations at different levels, exploits the spatial relationship between bounding boxes and superpixels as linear constraints and simultaneously discriminates between foreground and background at bounding box and superpixel level. Different from previous approaches that mainly rely on discriminative clustering, we incorporate a foreground model that minimizes the histogram difference of an object across all image frames. Exploiting the geometric relation between the superpixels and bounding boxes enables the transfer of segmentation cues to improve localization output and vice-versa. Inclusion of the foreground model generalizes our discriminative framework to video data where the background tends to be similar and thus, not discriminative. We demonstrate the effectiveness of our unified framework on the YouTube Object video dataset, Internet Object Discovery dataset and Pascal VOC 2007.",0
"This paper introduces a fresh approach that combines video/image segmentation and localization into a single optimization problem. The method utilizes low and high-level visual cues in a weakly supervised manner. The framework uses two different representations and implements spatial relationships between bounding boxes and superpixels as linear constraints. It simultaneously distinguishes foreground and background at both the bounding box and superpixel levels. Unlike previous methods that rely on discriminative clustering, this framework includes a foreground model that minimizes the histogram difference of an object across all image frames. The geometric relation between the superpixels and bounding boxes is exploited to transfer segmentation cues to improve localization output and vice versa. The foreground model broadens the discriminative framework to video data where the background is not discriminative. The effectiveness of the method is demonstrated on several datasets.",1
"We consider the structured-output prediction problem through probabilistic approaches and generalize the ""perturb-and-MAP"" framework to more challenging weighted Hamming losses, which are crucial in applications. While in principle our approach is a straightforward marginalization, it requires solving many related MAP inference problems. We show that for log-supermodular pairwise models these operations can be performed efficiently using the machinery of dynamic graph cuts. We also propose to use double stochastic gradient descent, both on the data and on the perturbations, for efficient learning. Our framework can naturally take weak supervision (e.g., partial labels) into account. We conduct a set of experiments on medium-scale character recognition and image segmentation, showing the benefits of our algorithms.",0
"We address the problem of predicting structured outputs using probabilistic methods and extend the ""perturb-and-MAP"" framework to accommodate more complex weighted Hamming losses, which are critical in real-world applications. Although our approach is conceptually simple - involving marginalization - it entails solving numerous MAP inference problems. We demonstrate that for log-supermodular pairwise models, dynamic graph cuts can efficiently execute these operations. Moreover, we suggest leveraging double stochastic gradient descent for efficient learning, both on the data and perturbations. Our framework can handle weak supervision, such as partial labels. We evaluate our algorithms on medium-scale tasks, including character recognition and image segmentation, and observe their advantages.",1
"In this work, we propose a special cascade network for image segmentation, which is based on the U-Net networks as building blocks and the idea of the iterative refinement. The model was mainly applied to achieve higher recognition quality for the task of finding borders of the optic disc and cup, which are relevant to the presence of glaucoma. Compared to a single U-Net and the state-of-the-art methods for the investigated tasks, very high segmentation quality has been achieved without a need for increasing the volume of datasets. Our experiments include comparison with the best-known methods on publicly available databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS, and evaluation on a private data set collected in collaboration with University of California San Francisco Medical School. The analysis of the architecture details is presented, and it is argued that the model can be employed for a broad scope of image segmentation problems of similar nature.",0
"Our proposed work introduces a cascade network for image segmentation that utilizes U-Net networks as building blocks and incorporates the iterative refinement approach. This model specifically targets the identification of borders for the optic disc and cup, which are crucial for detecting glaucoma. Notably, our approach achieves superior segmentation quality compared to both single U-Net and state-of-the-art methods for this task, without requiring an increase in dataset volume. Our experiments involve a comparison with leading methods on publicly available databases (DRIONS-DB, RIM-ONE v.3, DRISHTI-GS) and evaluation on a private dataset gathered in partnership with the University of California San Francisco Medical School. We provide an analysis of the architecture details and suggest that our model can be applied to a wide range of similar image segmentation problems.",1
"In multi-organ segmentation of abdominal CT scans, most existing fully supervised deep learning algorithms require lots of voxel-wise annotations, which are usually difficult, expensive, and slow to obtain. In comparison, massive unlabeled 3D CT volumes are usually easily accessible. Current mainstream works to address the semi-supervised biomedical image segmentation problem are mostly graph-based. By contrast, deep network based semi-supervised learning methods have not drawn much attention in this field. In this work, we propose Deep Multi-Planar Co-Training (DMPCT), whose contributions can be divided into two folds: 1) The deep model is learned in a co-training style which can mine consensus information from multiple planes like the sagittal, coronal, and axial planes; 2) Multi-planar fusion is applied to generate more reliable pseudo-labels, which alleviates the errors occurring in the pseudo-labels and thus can help to train better segmentation networks. Experiments are done on our newly collected large dataset with 100 unlabeled cases as well as 210 labeled cases where 16 anatomical structures are manually annotated by four radiologists and confirmed by a senior expert. The results suggest that DMPCT significantly outperforms the fully supervised method by more than 4% especially when only a small set of annotations is used.",0
"In the field of multi-organ segmentation for abdominal CT scans, many deep learning algorithms that are fully supervised require a significant number of voxel-wise annotations. These annotations can be difficult, costly, and time-consuming to obtain. In contrast, unlabeled 3D CT volumes are more readily accessible. While there have been graph-based solutions to address the semi-supervised biomedical image segmentation problem, deep network based semi-supervised learning methods have not received much attention. Our proposed solution, Deep Multi-Planar Co-Training (DMPCT), tackles this problem in two ways: 1) the deep model is learned through a co-training approach that extracts consensus information from multiple planes, such as sagittal, coronal, and axial planes, and 2) multi-planar fusion is employed to produce more reliable pseudo-labels, which reduces errors and improves the performance of segmentation networks. We tested our method on a large dataset consisting of 100 unlabeled cases and 210 labeled cases, where 16 anatomical structures were manually annotated by four radiologists and confirmed by a senior expert. Our results show that DMPCT outperforms the fully supervised approach by more than 4% when only a small number of annotations are available.",1
"Brain image segmentation is used for visualizing and quantifying anatomical structures of the brain. We present an automated ap-proach using 2D deep residual dilated networks which captures rich context information of different tissues for the segmentation of eight brain structures. The proposed system was evaluated in the MICCAI Brain Segmentation Challenge and ranked 9th out of 22 teams. We further compared the method with traditional U-Net using leave-one-subject-out cross-validation setting on the public dataset. Experimental results shows that the proposed method outperforms traditional U-Net (i.e. 80.9% vs 78.3% in averaged Dice score, 4.35mm vs 11.59mm in averaged robust Hausdorff distance) and is computationally efficient.",0
"To visualize and quantify anatomical structures of the brain, brain image segmentation is employed. Our study introduces a 2D deep residual dilated network that automatically captures rich context information of various tissues to segment eight brain structures. The system's performance was assessed in the MICCAI Brain Segmentation Challenge, where it ranked 9th out of 22 teams. We also evaluated the method against the traditional U-Net approach using leave-one-subject-out cross-validation on a public dataset. Our experimental findings demonstrate that our proposed method outperforms the traditional U-Net approach in terms of averaged Dice score (80.9% vs 78.3%) and averaged robust Hausdorff distance (4.35mm vs 11.59mm), while also being computationally efficient.",1
"In this paper, we propose a new pre-training scheme for U-net based image segmentation. We first train the encoding arm as a localization network to predict the center of the target, before extending it into a U-net architecture for segmentation. We apply our proposed method to the problem of segmenting the optic disc from fundus photographs. Our work shows that the features learned by encoding arm can be transferred to the segmentation network to reduce the annotation burden. We propose that an approach could have broad utility for medical image segmentation, and alleviate the burden of delineating complex structures by pre-training on annotations that are much easier to acquire.",0
"The objective of this paper is to present a novel pre-training method for image segmentation using U-net. The encoding arm is initially trained as a localization network to predict the target's center, and subsequently integrated into the U-net for segmentation. The proposed methodology is applied to the segmentation of the optic disc from fundus photographs. The study demonstrates that the encoding arm's learned features can be transferred to the segmentation network, reducing the need for extensive annotations. We suggest that this approach could have widespread applicability in medical image segmentation, lessening the burden of annotating complex structures through pre-training on simpler annotations.",1
"Building detection from satellite multispectral imagery data is being a fundamental but a challenging problem mainly because it requires correct recovery of building footprints from high-resolution images. In this work, we propose a deep learning approach for building detection by applying numerous enhancements throughout the process. Initial dataset is preprocessed by 2-sigma percentile normalization. Then data preparation includes ensemble modelling where 3 models were created while incorporating OpenStreetMap data. Binary Distance Transformation (BDT) is used for improving data labeling process and the U-Net (Convolutional Networks for Biomedical Image Segmentation) is modified by adding batch normalization wrappers. Afterwards, it is explained how each component of our approach is correlated with the final detection accuracy. Finally, we compare our results with winning solutions of SpaceNet 2 competition for real satellite multispectral images of Vegas, Paris, Shanghai and Khartoum, demonstrating the importance of our solution for achieving higher building detection accuracy.",0
"Detecting buildings from satellite multispectral imagery data is a difficult task, as it requires accurately identifying building footprints from high-resolution images. In this study, we propose a deep learning method for building detection that incorporates several improvements throughout the process. Our approach involves preprocessing the initial dataset using 2-sigma percentile normalization, creating three models using ensemble modelling while integrating OpenStreetMap data, enhancing data labeling using Binary Distance Transformation (BDT), and modifying the U-Net model by adding batch normalization wrappers. We analyze how each component of our approach contributes to the final detection accuracy. We then compare our results to the winning solutions of the SpaceNet 2 competition for real satellite multispectral images of Vegas, Paris, Shanghai, and Khartoum. Our findings highlight the significance of our approach in achieving higher building detection accuracy.",1
"Hyperspectral satellite imaging attracts enormous research attention in the remote sensing community, hence automated approaches for precise segmentation of such imagery are being rapidly developed. In this letter, we share our observations on the strategy for validating hyperspectral image segmentation algorithms currently followed in the literature, and show that it can lead to over-optimistic experimental insights. We introduce a new routine for generating segmentation benchmarks, and use it to elaborate ready-to-use hyperspectral training-test data partitions. They can be utilized for fair validation of new and existing algorithms without any training-test data leakage.",0
"There is a great deal of interest in the remote sensing community regarding hyperspectral satellite imaging, which has led to the rapid development of automated techniques for accurately segmenting this type of imagery. However, we have observed that the validation strategy for hyperspectral image segmentation algorithms currently being used in the literature can result in overly optimistic experimental results. To address this issue, we propose a new approach for generating segmentation benchmarks and provide readily available training-test data partitions for hyperspectral imagery. These partitions can be used to fairly evaluate both new and existing algorithms without any issues related to training-test data leakage.",1
"The training of many existing end-to-end steering angle prediction models heavily relies on steering angles as the supervisory signal. Without learning from much richer contexts, these methods are susceptible to the presence of sharp road curves, challenging traffic conditions, strong shadows, and severe lighting changes. In this paper, we considerably improve the accuracy and robustness of predictions through heterogeneous auxiliary networks feature mimicking, a new and effective training method that provides us with much richer contextual signals apart from steering direction. Specifically, we train our steering angle predictive model by distilling multi-layer knowledge from multiple heterogeneous auxiliary networks that perform related but different tasks, e.g., image segmentation or optical flow estimation. As opposed to multi-task learning, our method does not require expensive annotations of related tasks on the target set. This is made possible by applying contemporary off-the-shelf networks on the target set and mimicking their features in different layers after transformation. The auxiliary networks are discarded after training without affecting the runtime efficiency of our model. Our approach achieves a new state-of-the-art on Udacity and Comma.ai, outperforming the previous best by a large margin of 12.8% and 52.1%, respectively. Encouraging results are also shown on Berkeley Deep Drive (BDD) dataset.",0
"Many current end-to-end steering angle prediction models rely heavily on steering angles as their main supervisory signal. However, these models are vulnerable to challenging driving conditions such as sharp road curves, strong shadows, and changes in lighting, due to their lack of exposure to more complex contexts. In this study, we propose a novel training method called heterogeneous auxiliary networks feature mimicking, which significantly enhances the accuracy and robustness of our predictions by incorporating richer contextual signals beyond just steering direction. Our approach involves training our steering angle predictive model using multi-layer knowledge distilled from several auxiliary networks that perform different, but related tasks such as image segmentation or optical flow estimation. Unlike multi-task learning, our method does not require expensive annotations of related tasks on the target set. We achieve this by applying readily available off-the-shelf networks on the target set and mimicking their features at different layers after transformation. After training, the auxiliary networks are discarded without affecting the runtime efficiency of our model. Our approach outperforms the previous state-of-the-art on Udacity and Comma.ai by a significant margin of 12.8% and 52.1%, respectively, and shows promising results on the Berkeley Deep Drive (BDD) dataset.",1
"Automatic extraction of liver and tumor from CT volumes is a challenging task due to their heterogeneous and diffusive shapes. Recently, 2D and 3D deep convolutional neural networks have become popular in medical image segmentation tasks because of the utilization of large labeled datasets to learn hierarchical features. However, 3D networks have some drawbacks due to their high cost on computational resources. In this paper, we propose a 3D hybrid residual attention-aware segmentation method, named RA-UNet, to precisely extract the liver volume of interests (VOI) and segment tumors from the liver VOI. The proposed network has a basic architecture as a 3D U-Net which extracts contextual information combining low-level feature maps with high-level ones. Attention modules are stacked so that the attention-aware features change adaptively as the network goes ""very deep"" and this is made possible by residual learning. This is the first work that an attention residual mechanism is used to process medical volumetric images. We evaluated our framework on the public MICCAI 2017 Liver Tumor Segmentation dataset and the 3DIRCADb dataset. The results show that our architecture outperforms other state-of-the-art methods. We also extend our RA-UNet to brain tumor segmentation on the BraTS2018 and BraTS2017 datasets, and the results indicate that RA-UNet achieves good performance on a brain tumor segmentation task as well.",0
"Extracting liver and tumor from CT volumes automatically is a difficult task due to their heterogeneous and diffusive shapes. Deep convolutional neural networks have become popular in medical image segmentation tasks because of the utilization of large labeled datasets to learn hierarchical features. However, 3D networks are computationally expensive. This paper proposes a 3D hybrid residual attention-aware segmentation method, called RA-UNet, to accurately extract liver volume of interests (VOI) and segment tumors from the liver VOI. The network has a basic architecture as a 3D U-Net with attention modules stacked to change adaptively as the network goes ""very deep"" through residual learning. This is the first work to use an attention residual mechanism to process medical volumetric images. The framework is evaluated on public datasets, including the MICCAI 2017 Liver Tumor Segmentation and the 3DIRCADb datasets, and outperforms other state-of-the-art methods. RA-UNet is also extended to perform brain tumor segmentation on the BraTS2018 and BraTS2017 datasets, and it achieves good performance.",1
"Locating region of interest for breast cancer masses in the mammographic image is a challenging problem in medical image processing. In this research work, the keen idea is to efficiently extract suspected mass region for further examination. In particular to this fact breast boundary segmentation on sliced rgb image using modified intensity based approach followed by quad tree based division to spot out suspicious area are proposed in the paper. To evaluate the performance DDSM standard dataset are experimented and achieved acceptable accuracy.",0
"The identification of the area of interest for detecting breast cancer masses in mammographic images is a difficult task in the field of medical image processing. This study aims to extract the region of suspected mass efficiently for further investigation. To achieve this, the research proposes a modified intensity-based approach for breast boundary segmentation on sliced RGB images, along with quad-tree-based division to identify suspicious areas. The paper evaluates the performance of the proposed method using the DDSM standard dataset and achieves satisfactory accuracy.",1
"We address the problem of segmenting 3D multi-modal medical images in scenarios where very few labeled examples are available for training. Leveraging the recent success of adversarial learning for semi-supervised segmentation, we propose a novel method based on Generative Adversarial Networks (GANs) to train a segmentation model with both labeled and unlabeled images. The proposed method prevents over-fitting by learning to discriminate between true and fake patches obtained by a generator network. Our work extends current adversarial learning approaches, which focus on 2D single-modality images, to the more challenging context of 3D volumes of multiple modalities. The proposed method is evaluated on the problem of segmenting brain MRI from the iSEG-2017 and MRBrainS 2013 datasets. Significant performance improvement is reported, compared to state-of-art segmentation networks trained in a fully-supervised manner. In addition, our work presents a comprehensive analysis of different GAN architectures for semi-supervised segmentation, showing recent techniques like feature matching to yield a higher performance than conventional adversarial training approaches. Our code is publicly available at https://github.com/arnab39/FewShot_GAN-Unet3D",0
"Our focus is to tackle the challenge of segmenting multi-modal medical images in situations where there are very few labeled examples available for training. To achieve this, we propose a new approach based on Generative Adversarial Networks (GANs) that leverages the success of adversarial learning for semi-supervised segmentation. Our method involves training a segmentation model with both labeled and unlabeled images, and it prevents over-fitting by teaching the model to differentiate between true and fake patches generated by a generator network. We extend current adversarial learning techniques that concentrate on 2D single-modality images to the more complex context of 3D volumes of multiple modalities. To evaluate our approach, we use brain MRI datasets from iSEG-2017 and MRBrainS 2013, where we report significant performance improvement compared to state-of-art segmentation networks trained under full supervision. Our work also includes an extensive analysis of different GAN architectures for semi-supervised segmentation, demonstrating that recent techniques, such as feature matching, outperform conventional adversarial training approaches. Our code is publicly available at https://github.com/arnab39/FewShot_GAN-Unet3D.",1
"Collecting training data from the physical world is usually time-consuming and even dangerous for fragile robots, and thus, recent advances in robot learning advocate the use of simulators as the training platform. Unfortunately, the reality gap between synthetic and real visual data prohibits direct migration of the models trained in virtual worlds to the real world. This paper proposes a modular architecture for tackling the virtual-to-real problem. The proposed architecture separates the learning model into a perception module and a control policy module, and uses semantic image segmentation as the meta representation for relating these two modules. The perception module translates the perceived RGB image to semantic image segmentation. The control policy module is implemented as a deep reinforcement learning agent, which performs actions based on the translated image segmentation. Our architecture is evaluated in an obstacle avoidance task and a target following task. Experimental results show that our architecture significantly outperforms all of the baseline methods in both virtual and real environments, and demonstrates a faster learning curve than them. We also present a detailed analysis for a variety of variant configurations, and validate the transferability of our modular architecture.",0
"Gathering training data in the real world can be a risky and time-consuming process for delicate robots, which is why recent developments in robot learning suggest using simulators as a training platform. However, the gap between synthetic and real visual data makes it difficult to apply the models learned in virtual worlds to the real world. This paper proposes a modular architecture that addresses this virtual-to-real problem. The architecture separates the learning model into two modules: a perception module and a control policy module, and utilizes semantic image segmentation as a meta representation to link the two. The perception module converts the perceived RGB image into semantic image segmentation, while the control policy module is implemented as a deep reinforcement learning agent that takes actions based on the translated image segmentation. The performance of our architecture is evaluated in an obstacle avoidance task and a target following task, and the results show that it outperforms all baseline methods in both virtual and real environments, and has a faster learning curve. We also present a detailed analysis of various configurations and confirm the transferability of our modular architecture.",1
"State of the art methods for semantic image segmentation are trained in a supervised fashion using a large corpus of fully labeled training images. However, gathering such a corpus is expensive, due to human annotation effort, in contrast to gathering unlabeled data. We propose an active learning-based strategy, called CEREALS, in which a human only has to hand-label a few, automatically selected, regions within an unlabeled image corpus. This minimizes human annotation effort while maximizing the performance of a semantic image segmentation method. The automatic selection procedure is achieved by: a) using a suitable information measure combined with an estimate about human annotation effort, which is inferred from a learned cost model, and b) exploiting the spatial coherency of an image. The performance of CEREALS is demonstrated on Cityscapes, where we are able to reduce the annotation effort to 17%, while keeping 95% of the mean Intersection over Union (mIoU) of a model that was trained with the fully annotated training set of Cityscapes.",0
"Supervised methods for semantic image segmentation use a large corpus of fully labeled training images, which is expensive to gather due to human annotation effort. In contrast, unlabeled data is easier to collect. To address this issue, we propose CEREALS, an active learning-based strategy that minimizes human annotation effort while maximizing performance. With CEREALS, a human only needs to hand-label a few regions within an unlabeled image corpus that are automatically selected using an information measure and a learned cost model. Additionally, the spatial coherency of the image is exploited. We demonstrate the effectiveness of CEREALS on Cityscapes, where we reduce annotation effort to 17% while retaining 95% of the mean Intersection over Union of a model trained with a fully annotated training set of Cityscapes.",1
"Objective: Deformable image registration is a fundamental problem in medical image analysis, with applications such as longitudinal studies, population modeling, and atlas based image segmentation. Registration is often phrased as an optimization problem, i.e., finding a deformation field that is optimal according to a given objective function. Discrete, combinatorial, optimization techniques have successfully been employed to solve the resulting optimization problem. Specifically, optimization based on $\alpha$-expansion with minimal graph cuts has been proposed as a powerful tool for image registration. The high computational cost of the graph-cut based optimization approach, however, limits the utility of this approach for registration of large volume images. Methods: Here, we propose to accelerate graph-cut based deformable registration by dividing the image into overlapping sub-regions and restricting the $\alpha$-expansion moves to a single sub-region at a time. Results: We demonstrate empirically that this approach can achieve a large reduction in computation time -- from days to minutes -- with only a small penalty in terms of solution quality. Conclusion: The reduction in computation time provided by the proposed method makes graph cut based deformable registration viable for large volume images. Significance: Graph cut based image registration has previously been shown to produce excellent results, but the high computational cost has hindered the adoption of the method for registration of large medical volume images. Our proposed method lifts this restriction, requiring only a small fraction of the computational cost to produce results of comparable quality.",0
"The aim of this study is to address the issue of deformable image registration in medical image analysis, which has various applications such as atlas-based image segmentation, population modeling, and longitudinal studies. This problem involves finding an optimal deformation field by optimizing an objective function. While combinatorial optimization techniques like $\alpha$-expansion with minimal graph cuts have been successful in solving this problem, the high computational cost of this method limits its effectiveness for larger images. To accelerate this process, we propose dividing the image into overlapping sub-regions and limiting $\alpha$-expansion moves to one sub-region at a time. Empirical evidence suggests that this approach significantly reduces computation time without sacrificing solution quality. Therefore, the proposed method enables graph cut based deformable registration for larger volume images, which was previously unfeasible due to computational constraints.",1
"Weakly-supervised image segmentation is an important task in computer vision. A key problem is how to obtain high quality objects location from image-level category. Classification activation mapping is a common method which can be used to generate high-precise object location cues. However these location cues are generally very sparse and small such that they can not provide effective information for image segmentation. In this paper, we propose a saliency guided image segmentation network to resolve this problem. We employ a self-attention saliency method to generate subtle saliency maps, and render the location cues grow as seeds by seeded region growing method to expand pixel-level labels extent. In the process of seeds growing, we use the saliency values to weight the similarity between pixels to control the growing. Therefore saliency information could help generate discriminative object regions, and the effects of wrong salient pixels can be suppressed efficiently. Experimental results on a common segmentation dataset PASCAL VOC2012 demonstrate the effectiveness of our method.",0
"The task of weakly-supervised image segmentation is crucial in computer vision, and a major challenge is obtaining accurate object locations from image-level categories. While classification activation mapping is a popular approach for generating object location cues, these cues are often too sparse and small to be useful for effective image segmentation. This paper proposes a saliency-guided image segmentation network to address this issue. The network utilizes a self-attention saliency method to create subtle saliency maps, which are then used as seeds for a seeded region growing method to expand pixel-level labels. The saliency values are employed to control the growing process and generate discriminative object regions while suppressing the effects of incorrect salient pixels. Experimental results using PASCAL VOC2012, a standard segmentation dataset, demonstrate the efficacy of this method.",1
"We propose a generalized focal loss function based on the Tversky index to address the issue of data imbalance in medical image segmentation. Compared to the commonly used Dice loss, our loss function achieves a better trade off between precision and recall when training on small structures such as lesions. To evaluate our loss function, we improve the attention U-Net model by incorporating an image pyramid to preserve contextual features. We experiment on the BUS 2017 dataset and ISIC 2018 dataset where lesions occupy 4.84% and 21.4% of the images area and improve segmentation accuracy when compared to the standard U-Net by 25.7% and 3.6%, respectively.",0
"Our proposed solution for addressing data imbalance in medical image segmentation involves a generalized focal loss function that is based on the Tversky index. This loss function provides a better balance between precision and recall compared to the commonly used Dice loss, especially when dealing with small structures such as lesions. In order to test the effectiveness of our approach, we enhanced the attention U-Net model by adding an image pyramid to preserve contextual features. Our experiments were conducted on two datasets, BUS 2017 and ISIC 2018, where lesions occupy 4.84% and 21.4% of the image area, respectively. Our results show a 25.7% improvement in segmentation accuracy on the BUS 2017 dataset and a 3.6% improvement on the ISIC 2018 dataset, compared to the standard U-Net model.",1
"In this paper, we focus on three problems in deep learning based medical image segmentation. Firstly, U-net, as a popular model for medical image segmentation, is difficult to train when convolutional layers increase even though a deeper network usually has a better generalization ability because of more learnable parameters. Secondly, the exponential ReLU (ELU), as an alternative of ReLU, is not much different from ReLU when the network of interest gets deep. Thirdly, the Dice loss, as one of the pervasive loss functions for medical image segmentation, is not effective when the prediction is close to ground truth and will cause oscillation during training. To address the aforementioned three problems, we propose and validate a deeper network that can fit medical image datasets that are usually small in the sample size. Meanwhile, we propose a new loss function to accelerate the learning process and a combination of different activation functions to improve the network performance. Our experimental results suggest that our network is comparable or superior to state-of-the-art methods.",0
"This research paper delves into the challenges surrounding medical image segmentation in deep learning and identifies three main issues. Firstly, while deeper networks may have better generalization capabilities, popular models like U-net become difficult to train with increased convolutional layers. Secondly, the alternative of ReLU, exponential ReLU (ELU), does not offer many advantages when the network is deep. Thirdly, the commonly used Dice loss function can cause oscillation during training when the prediction is close to ground truth. To tackle these problems, we propose a deeper network that is suitable for small medical image datasets, a new loss function to speed up learning, and a combination of different activation functions for improved network performance. Our experimental results demonstrate that our proposed network is comparable or superior to existing state-of-the-art methods.",1
"Magnetic resonance imaging (MRI) is the non-invasive modality of choice for body tissue composition analysis due to its excellent soft tissue contrast and lack of ionizing radiation. However, quantification of body composition requires an accurate segmentation of fat, muscle and other tissues from MR images, which remains a challenging goal due to the intensity overlap between them. In this study, we propose a fully automated, data-driven image segmentation platform that addresses multiple difficulties in segmenting MR images such as varying inhomogeneity, non-standardness, and noise, while producing high-quality definition of different tissues. In contrast to most approaches in the literature, we perform segmentation operation by combining three different MRI contrasts and a novel segmentation tool which takes into account variability in the data. The proposed system, based on a novel affinity definition within the fuzzy connectivity (FC) image segmentation family, prevents the need for user intervention and reparametrization of the segmentation algorithms. In order to make the whole system fully automated, we adapt an affinity propagation clustering algorithm to roughly identify tissue regions and image background. We perform a thorough evaluation of the proposed algorithm's individual steps as well as comparison with several approaches from the literature for the main application of muscle/fat separation. Furthermore, whole-body tissue composition and brain tissue delineation were conducted to show the generalization ability of the proposed system. This new automated platform outperforms other state-of-the-art segmentation approaches both in accuracy and efficiency.",0
"MRI is the preferred non-invasive method for analyzing body tissue composition due to its lack of ionizing radiation and exceptional soft tissue contrast. However, accurately segmenting fat, muscle, and other tissues from MR images remains challenging due to their intensity overlap, varying inhomogeneity, non-standardness, and noise. In this study, we introduce an automated image segmentation platform that utilizes three different MRI contrasts and a novel segmentation tool to address these difficulties and produce high-quality tissue definitions without user intervention or reparametrization. Our system is based on a novel affinity definition within the fuzzy connectivity (FC) image segmentation family and uses an affinity propagation clustering algorithm to identify tissue regions and image background. We thoroughly evaluate the proposed algorithm's individual steps and compare it with several approaches from the literature for muscle/fat separation and whole-body tissue composition and brain tissue delineation. Our new platform outperforms other state-of-the-art segmentation approaches in both accuracy and efficiency.",1
"We propose a segmentation framework that uses deep neural networks and introduce two innovations. First, we describe a biophysics-based domain adaptation method. Second, we propose an automatic method to segment white and gray matter, and cerebrospinal fluid, in addition to tumorous tissue. Regarding our first innovation, we use a domain adaptation framework that combines a novel multispecies biophysical tumor growth model with a generative adversarial model to create realistic looking synthetic multimodal MR images with known segmentation. Regarding our second innovation, we propose an automatic approach to enrich available segmentation data by computing the segmentation for healthy tissues. This segmentation, which is done using diffeomorphic image registration between the BraTS training data and a set of prelabeled atlases, provides more information for training and reduces the class imbalance problem. Our overall approach is not specific to any particular neural network and can be used in conjunction with existing solutions. We demonstrate the performance improvement using a 2D U-Net for the BraTS'18 segmentation challenge. Our biophysics based domain adaptation achieves better results, as compared to the existing state-of-the-art GAN model used to create synthetic data for training.",0
"Our proposed framework for segmentation employs deep neural networks and presents two novel approaches. Firstly, we introduce a biophysics-based domain adaptation technique. Secondly, we suggest an automatic method for segmenting white and gray matter, cerebrospinal fluid, and tumorous tissue. Our domain adaptation methodology combines a multispecies biophysical tumor growth model with a generative adversarial model to generate synthetic multimodal MR images that look realistic and come with known segmentation. Our second innovation involves computing the segmentation for healthy tissues via diffeomorphic image registration between the BraTS training data and a collection of prelabeled atlases. This approach enriches available segmentation data, provides more information for training, and reduces the class imbalance issue. Our overall method is not specific to any particular neural network and can be integrated with existing solutions. We demonstrate the effectiveness of our approach using a 2D U-Net for the BraTS'18 segmentation challenge. Our biophysics-based domain adaptation achieves better results than the state-of-the-art GAN model used for creating synthetic data.",1
"Image segmentation is the process of partitioning an image into meaningful segments. The meaning of the segments is subjective due to the definition of homogeneity is varied based on the users perspective hence the automation of the segmentation is challenging. Watershed is a popular segmentation technique which assumes topographic map in an image, with the brightness of each pixel representing its height, and finds the lines that run along the tops of ridges. The results from the algorithm typically suffer from over segmentation due to the lack of knowledge of the objects being classified. This paper presents an approach to reduce the over segmentation of watershed algorithm by assuming that the different adjacent segments of an object have similar color distribution. The approach demonstrates an improvement over conventional watershed algorithm.",0
"The act of dividing an image into meaningful sections is known as image segmentation. Due to the subjective definition of homogeneity, determining the significance of these segments can be challenging when automating the process. The commonly used Watershed segmentation technique locates lines running along the tops of ridges by assuming a topographic map in the image, where each pixel's brightness represents its height. However, the outcomes of this algorithm often result in excessive segmentation due to a lack of knowledge of the classified objects. This article proposes a solution to this problem by assuming that adjacent segments of an object share similar color distributions, thereby reducing the over-segmentation of the Watershed algorithm. The approach shows an improvement over the traditional Watershed algorithm.",1
"We consider an important task of effective and efficient semantic image segmentation. In particular, we adapt a powerful semantic segmentation architecture, called RefineNet, into the more compact one, suitable even for tasks requiring real-time performance on high-resolution inputs. To this end, we identify computationally expensive blocks in the original setup, and propose two modifications aimed to decrease the number of parameters and floating point operations. By doing that, we achieve more than twofold model reduction, while keeping the performance levels almost intact. Our fastest model undergoes a significant speed-up boost from 20 FPS to 55 FPS on a generic GPU card on 512x512 inputs with solid 81.1% mean iou performance on the test set of PASCAL VOC, while our slowest model with 32 FPS (from original 17 FPS) shows 82.7% mean iou on the same dataset. Alternatively, we showcase that our approach is easily mixable with light-weight classification networks: we attain 79.2% mean iou on PASCAL VOC using a model that contains only 3.3M parameters and performs only 9.3B floating point operations.",0
"Our goal is to achieve effective and efficient semantic image segmentation, which requires adapting the RefineNet architecture to be more compact and suitable for real-time performance on high-resolution inputs. We accomplished this by identifying computationally expensive blocks in the original setup and proposing two modifications to reduce the number of parameters and floating point operations. This resulted in a more than twofold model reduction while maintaining performance levels. Our fastest model achieved a significant speed-up boost from 20 FPS to 55 FPS on a generic GPU card with solid 81.1% mean iou performance on the PASCAL VOC test set. Our slowest model achieved 32 FPS (from the original 17 FPS) with 82.7% mean iou on the same dataset. Additionally, our approach is easily mixable with light-weight classification networks, which allowed us to achieve 79.2% mean iou on PASCAL VOC using a model containing only 3.3M parameters and performing only 9.3B floating point operations.",1
"In this paper, we propose a novel deep learning framework for anatomy segmentation and automatic landmark- ing. Specifically, we focus on the challenging problem of mandible segmentation from cone-beam computed tomography (CBCT) scans and identification of 9 anatomical landmarks of the mandible on the geodesic space. The overall approach employs three inter-related steps. In step 1, we propose a deep neu- ral network architecture with carefully designed regularization, and network hyper-parameters to perform image segmentation without the need for data augmentation and complex post- processing refinement. In step 2, we formulate the landmark localization problem directly on the geodesic space for sparsely- spaced anatomical landmarks. In step 3, we propose to use a long short-term memory (LSTM) network to identify closely- spaced landmarks, which is rather difficult to obtain using other standard detection networks. The proposed fully automated method showed superior efficacy compared to the state-of-the- art mandible segmentation and landmarking approaches in craniofacial anomalies and diseased states. We used a very challenging CBCT dataset of 50 patients with a high-degree of craniomaxillofacial (CMF) variability that is realistic in clinical practice. Complementary to the quantitative analysis, the qualitative visual inspection was conducted for distinct CBCT scans from 250 patients with high anatomical variability. We have also shown feasibility of the proposed work in an independent dataset from MICCAI Head-Neck Challenge (2015) achieving the state-of-the-art performance. Lastly, we present an in-depth analysis of the proposed deep networks with respect to the choice of hyper-parameters such as pooling and activation functions.",0
"In this study, we introduce a new deep learning framework that can automatically segment anatomy and identify landmarks. Specifically, we focus on the difficult task of segmenting the mandible from cone-beam computed tomography scans and locating nine anatomical landmarks in the geodesic space. Our approach consists of three related steps. First, we propose a neural network architecture with carefully chosen regularization and hyper-parameters to perform segmentation without requiring data augmentation or complex post-processing. Second, we formulate the landmark localization problem directly on the geodesic space for sparsely-spaced landmarks. Third, we use a long short-term memory network to identify closely-spaced landmarks, which is challenging with other standard detection networks. Our fully automated method outperforms current state-of-the-art approaches in craniofacial anomalies and diseased states, using a challenging dataset of 50 patients with high craniomaxillofacial variability. We also conduct qualitative visual inspection of distinct CBCT scans from 250 patients with high anatomical variability, and show feasibility in an independent dataset achieving state-of-the-art performance. Lastly, we provide an in-depth analysis of our deep networks with respect to hyper-parameters such as pooling and activation functions.",1
"This work addresses the problem of semantic image segmentation of nighttime scenes. Although considerable progress has been made in semantic image segmentation, it is mainly related to daytime scenarios. This paper proposes a novel method to progressive adapt the semantic models trained on daytime scenes, along with large-scale annotations therein, to nighttime scenes via the bridge of twilight time -- the time between dawn and sunrise, or between sunset and dusk. The goal of the method is to alleviate the cost of human annotation for nighttime images by transferring knowledge from standard daytime conditions. In addition to the method, a new dataset of road scenes is compiled; it consists of 35,000 images ranging from daytime to twilight time and to nighttime. Also, a subset of the nighttime images are densely annotated for method evaluation. Our experiments show that our method is effective for model adaptation from daytime scenes to nighttime scenes, without using extra human annotation.",0
"This research focuses on the challenge of semantic image segmentation in nighttime environments. Although significant advancements have been made in this area, most have been geared towards daytime scenarios. This study introduces a fresh approach that progressively adapts semantic models, which are trained on daytime data with large annotations, to nighttime scenes through the use of the twilight period. The objective of this technique is to reduce the cost of human annotation for nighttime images by leveraging the knowledge acquired from standard daytime conditions. Along with the approach, a new dataset of road scenes is created, containing 35,000 images spanning from daytime to twilight and nighttime. Furthermore, a subset of the nighttime photographs are densely annotated to assess the effectiveness of the method. The experiment provides evidence that our method can adapt models from daytime to nighttime without requiring extra human annotation.",1
"In this paper, we propose a novel fully convolutional two-stream fusion network (FCTSFN) for interactive image segmentation. The proposed network includes two sub-networks: a two-stream late fusion network (TSLFN) that predicts the foreground at a reduced resolution, and a multi-scale refining network (MSRN) that refines the foreground at full resolution. The TSLFN includes two distinct deep streams followed by a fusion network. The intuition is that, since user interactions are more direct information on foreground/background than the image itself, the two-stream structure of the TSLFN reduces the number of layers between the pure user interaction features and the network output, allowing the user interactions to have a more direct impact on the segmentation result. The MSRN fuses the features from different layers of TSLFN with different scales, in order to seek the local to global information on the foreground to refine the segmentation result at full resolution. We conduct comprehensive experiments on four benchmark datasets. The results show that the proposed network achieves competitive performance compared to current state-of-the-art interactive image segmentation methods",0
"Our paper introduces a new approach to interactive image segmentation, called the Fully Convolutional Two-Stream Fusion Network (FCTSFN). This network consists of two sub-networks: the Two-Stream Late Fusion Network (TSLFN), which predicts the foreground at a lower resolution, and the Multi-Scale Refining Network (MSRN), which refines the foreground at full resolution. The TSLFN has two separate streams followed by a fusion network that is designed to reduce the number of layers between user interactions and the network output. This allows user interactions to have a more direct impact on the segmentation result. The MSRN fuses information from different layers of the TSLFN with varying scales to refine the segmentation result at full resolution. We tested our method on four benchmark datasets and found that it performs competitively compared to current state-of-the-art interactive image segmentation methods.",1
"We propose a novel framework for structured prediction via adversarial learning. Existing adversarial learning methods involve two separate networks, i.e., the structured prediction models and the discriminative models, in the training. The information captured by discriminative models complements that in the structured prediction models, but few existing researches have studied on utilizing such information to improve structured prediction models at the inference stage. In this work, we propose to refine the predictions of structured prediction models by effectively integrating discriminative models into the prediction. Discriminative models are treated as energy-based models. Similar to the adversarial learning, discriminative models are trained to estimate scores which measure the quality of predicted outputs, while structured prediction models are trained to predict contrastive outputs with maximal energy scores. In this way, the gradient vanishing problem is ameliorated, and thus we are able to perform inference by following the ascent gradient directions of discriminative models to refine structured prediction models. The proposed method is able to handle a range of tasks, e.g., multi-label classification and image segmentation. Empirical results on these two tasks validate the effectiveness of our learning method.",0
"A new approach to structured prediction via adversarial learning is introduced in this study. Traditional adversarial learning involves two distinct networks: structured prediction models and discriminative models, for training purposes. The discriminative models capture information that complements the structured prediction models, yet few studies have explored leveraging this information to improve structured prediction models during the inference stage. To address this gap, this study proposes integrating discriminative models into predictions to refine structured prediction models. Discriminative models are treated as energy-based models and are trained to estimate scores that measure the quality of predicted outputs, while structured prediction models are trained to predict contrastive outputs with maximum energy scores. This approach ameliorates the gradient vanishing problem, allowing for inference by following the ascent gradient directions of discriminative models to refine structured prediction models. The proposed technique is applicable to various tasks, including multi-label classification and image segmentation, and empirical results support the effectiveness of this learning method.",1
"Histopathological prognostication of neoplasia including most tumor grading systems are based upon a number of criteria. Probably the most important is the number of mitotic figures which are most commonly determined as the mitotic count (MC), i.e. number of mitotic figures within 10 consecutive high power fields. Often the area with the highest mitotic activity is to be selected for the MC. However, since mitotic activity is not known in advance, an arbitrary choice of this region is considered one important cause for high variability in the prognostication and grading.   In this work, we present an algorithmic approach that first calculates a mitotic cell map based upon a deep convolutional network. This map is in a second step used to construct a mitotic activity estimate. Lastly, we select the image segment representing the size of ten high power fields with the overall highest mitotic activity as a region proposal for an expert MC determination. We evaluate the approach using a dataset of 32 completely annotated whole slide images, where 22 were used for training of the network and 10 for test. We find a correlation of r=0.936 in mitotic count estimate.",0
"The prediction of neoplasia prognosis through histopathology and tumor grading systems is based on various factors, with the number of mitotic figures being the most crucial. The mitotic count (MC) is determined by the number of mitotic figures within 10 consecutive high power fields, often chosen from the area with the highest mitotic activity. However, the arbitrary selection of this area leads to variability in grading and prognostication. To address this issue, we propose an algorithmic approach that utilizes a deep convolutional network to create a mitotic cell map. This map is then used to estimate mitotic activity, and the image segment with the highest activity in ten high power fields is selected for expert MC determination. The method was evaluated on 32 fully annotated whole slide images, with 22 used for training and 10 for testing. The results show a strong correlation (r=0.936) with the mitotic count estimate.",1
"Left atrium shape has been shown to be an independent predictor of recurrence after atrial fibrillation (AF) ablation. Shape-based representation is imperative to such an estimation process, where correspondence-based representation offers the most flexibility and ease-of-computation for population-level shape statistics. Nonetheless, population-level shape representations in the form of image segmentation and correspondence models derived from cardiac MRI require significant human resources with sufficient anatomy-specific expertise. In this paper, we propose a machine learning approach that uses deep networks to estimate AF recurrence by predicting shape descriptors directly from MRI images, with NO image pre-processing involved. We also propose a novel data augmentation scheme to effectively train a deep network in a limited training data setting. We compare this new method of estimating shape descriptors from images with the state-of-the-art correspondence-based shape modeling that requires image segmentation and correspondence optimization. Results show that the proposed method and the current state-of-the-art produce statistically similar outcomes on AF recurrence, eliminating the need for expensive pre-processing pipelines and associated human labor.",0
"The shape of the left atrium has been found to be a predictor of recurrence after atrial fibrillation (AF) ablation. To estimate this, a shape-based representation is necessary, with correspondence-based representation being the most flexible and computationally efficient for population-level shape statistics. However, creating population-level shape representations from cardiac MRI requires significant human resources and anatomy-specific expertise. In this study, we propose a machine learning approach using deep networks to directly predict shape descriptors from MRI images without the need for image pre-processing. We also introduce a novel data augmentation scheme to train the network effectively with limited data. We compare this method to the current state-of-the-art, which requires image segmentation and correspondence optimization, and find that they produce similar results in predicting AF recurrence. This eliminates the need for expensive pre-processing and associated human labor.",1
"Accurate and reliable image segmentation is an essential part of biomedical image analysis. In this paper, we consider the problem of biomedical image segmentation using deep convolutional neural networks. We propose a new end-to-end network architecture that effectively integrates local and global contextual patterns of histologic primitives to obtain a more reliable segmentation result. Specifically, we introduce a deep fully convolution residual network with a new skip connection strategy to control the contextual information passed forward. Moreover, our trained model is also computationally inexpensive due to its small number of network parameters. We evaluate our method on two public datasets for epithelium segmentation and tubule segmentation tasks. Our experimental results show that the proposed method provides a fast and effective way of producing a pixel-wise dense prediction of biomedical images.",0
"In biomedical image analysis, image segmentation that is both accurate and reliable is crucial. This study focuses on using deep convolutional neural networks to address the issue of biomedical image segmentation. A novel end-to-end network architecture is proposed, which effectively integrates both local and global contextual patterns of histologic primitives, resulting in a more dependable segmentation outcome. Specifically, a deep fully convolution residual network is introduced, featuring a new skip connection strategy that controls the contextual information being passed forward. Additionally, the model is computationally efficient due to its small number of network parameters. The proposed method is evaluated on two public datasets for epithelium segmentation and tubule segmentation tasks, with results indicating that it provides a speedy and efficient solution for producing pixel-wise dense prediction of biomedical images.",1
"Being able to effectively identify clouds and monitor their evolution is one important step toward more accurate quantitative precipitation estimation and forecast. In this study, a new gradient-based cloud-image segmentation technique is developed using tools from image processing techniques. This method integrates morphological image gradient magnitudes to separable cloud systems and patches boundaries. A varying scale-kernel is implemented to reduce the sensitivity of image segmentation to noise and capture objects with various finenesses of the edges in remote-sensing images. The proposed method is flexible and extendable from single- to multi-spectral imagery. Case studies were carried out to validate the algorithm by applying the proposed segmentation algorithm to synthetic radiances for channels of the Geostationary Operational Environmental Satellites (GOES-R) simulated by a high-resolution weather prediction model. The proposed method compares favorably with the existing cloud-patch-based segmentation technique implemented in the PERSIANN-CCS (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Network - Cloud Classification System) rainfall retrieval algorithm. Evaluation of event-based images indicates that the proposed algorithm has potential to improve rain detection and estimation skills with an average of more than 45% gain comparing to the segmentation technique used in PERSIANN-CCS and identifying cloud regions as objects with accuracy rates up to 98%.",0
"The ability to accurately identify and track cloud formations is a crucial step in achieving more precise quantitative precipitation estimates and forecasts. This study introduces a novel cloud-image segmentation technique that utilizes image processing tools and morphological image gradient magnitudes to isolate cloud systems and patch boundaries. To enhance the method's accuracy in remote-sensing imagery, a varying scale-kernel is employed to reduce sensitivity to noise and capture objects with different edge fineness. The approach can be applied to both single- and multi-spectral images. The proposed algorithm is validated through case studies with synthetic radiances for channels of the Geostationary Operational Environmental Satellites (GOES-R) simulated by a high-resolution weather prediction model. The results demonstrate that the proposed method outperforms the existing cloud-patch-based segmentation technique used in the PERSIANN-CCS rainfall retrieval algorithm, with a potential gain in rain detection and estimation skills of over 45%. Moreover, cloud regions are accurately identified as objects with an accuracy rate of up to 98% in event-based images.",1
"The U-Net was presented in 2015. With its straight-forward and successful architecture it quickly evolved to a commonly used benchmark in medical image segmentation. The adaptation of the U-Net to novel problems, however, comprises several degrees of freedom regarding the exact architecture, preprocessing, training and inference. These choices are not independent of each other and substantially impact the overall performance. The present paper introduces the nnU-Net ('no-new-Net'), which refers to a robust and self-adapting framework on the basis of 2D and 3D vanilla U-Nets. We argue the strong case for taking away superfluous bells and whistles of many proposed network designs and instead focus on the remaining aspects that make out the performance and generalizability of a method. We evaluate the nnU-Net in the context of the Medical Segmentation Decathlon challenge, which measures segmentation performance in ten disciplines comprising distinct entities, image modalities, image geometries and dataset sizes, with no manual adjustments between datasets allowed. At the time of manuscript submission, nnU-Net achieves the highest mean dice scores across all classes and seven phase 1 tasks (except class 1 in BrainTumour) in the online leaderboard of the challenge.",0
"Introduced in 2015, the U-Net quickly became a widely used benchmark in medical image segmentation due to its effective and straightforward architecture. However, adapting the U-Net to new problems involves various decision points, such as architecture, preprocessing, training, and inference, which are interdependent and significantly impact overall performance. To address this issue, the nnU-Net framework was developed, which is a robust and adaptable approach based on 2D and 3D vanilla U-Nets. The nnU-Net focuses on the essential aspects that contribute to performance and generalizability while eliminating unnecessary design features. In the Medical Segmentation Decathlon challenge, nnU-Net achieved the highest mean dice scores across all classes and seven phase 1 tasks (except class 1 in BrainTumour) on the online leaderboard.",1
"This paper proposes a deep learning architecture that attains statistically significant improvements over traditional algorithms in Poisson image denoising espically when the noise is strong. Poisson noise commonly occurs in low-light and photon- limited settings, where the noise can be most accurately modeled by the Poission distribution. Poisson noise traditionally prevails only in specific fields such as astronomical imaging. However, with the booming market of surveillance cameras, which commonly operate in low-light environments, or mobile phones, which produce noisy night scene pictures due to lower-grade sensors, the necessity for an advanced Poisson image denoising algorithm has increased. Deep learning has achieved amazing breakthroughs in other imaging problems, such image segmentation and recognition, and this paper proposes a deep learning denoising network that outperforms traditional algorithms in Poisson denoising especially when the noise is strong. The architecture incorporates a hybrid of convolutional and deconvolutional layers along with symmetric connections. The denoising network achieved statistically significant 0.38dB, 0.68dB, and 1.04dB average PSNR gains over benchmark traditional algorithms in experiments with image peak values 4, 2, and 1. The denoising network can also operate with shorter computational time while still outperforming the benchmark algorithm by tuning the reconstruction stride sizes.",0
"In this paper, a deep learning architecture is proposed to improve Poisson image denoising compared to traditional algorithms, especially in cases where the noise is strong. Poisson noise is commonly found in low-light and photon-limited settings and can be accurately modeled using the Poisson distribution. Although traditionally associated with specific fields such as astronomical imaging, there is a growing need for advanced Poisson image denoising algorithms due to the increasing use of surveillance cameras and mobile phones in low-light environments. The proposed deep learning denoising network combines convolutional and deconvolutional layers with symmetric connections and achieves significant improvements over benchmark traditional algorithms in experiments with image peak values of 4, 2, and 1. The network can also operate with shorter computational time while still outperforming the benchmark algorithm by adjusting the reconstruction stride sizes. The results demonstrate the potential of deep learning for improving Poisson image denoising.",1
"Over the past years, computer vision community has contributed to enormous progress in semantic image segmentation, a per-pixel classification task, crucial for dense scene understanding and rapidly becoming vital in lots of real-world applications, including driverless cars and medical imaging. Most recent models are now reaching previously unthinkable numbers (e.g., 89% mean iou on PASCAL VOC, 83% on CityScapes), and, while intersection-over-union and a range of other metrics provide the general picture of model performance, in this paper we aim to extend them into other meaningful and important for applications characteristics, answering such questions as 'how accurate the model segmentation is on small objects in the general scene?', or 'what are the sources of uncertainty that cause the model to make an erroneous prediction?'. Besides establishing a methodology that covers the performance of a single model from different perspectives, we also showcase several extensions that can be worth pursuing in order to further improve current results in semantic segmentation.",0
"The computer vision community has made significant strides in semantic image segmentation, a task involving the classification of each pixel in an image, which is crucial for understanding dense scenes and is increasingly important in real-world applications such as medical imaging and autonomous driving. Recent models have achieved unprecedented levels of accuracy, such as 89% mean intersection-over-union on PASCAL VOC and 83% on CityScapes. While these metrics provide a general idea of model performance, this paper aims to explore other important characteristics, such as how well the model performs on small objects in the scene and what causes errors in predictions. By developing a methodology that evaluates model performance from multiple perspectives and proposing potential extensions to improve semantic segmentation, this research contributes to advancing this field.",1
"Encoding images as a series of high-level constructs, such as brush strokes or discrete shapes, can often be key to both human and machine understanding. In many cases, however, data is only available in pixel form. We present a method for generating images directly in a high-level domain (e.g. brush strokes), without the need for real pairwise data. Specifically, we train a ""canvas"" network to imitate the mapping of high-level constructs to pixels, followed by a high-level ""drawing"" network which is optimized through this mapping towards solving a desired image recreation or translation task. We successfully discover sequential vector representations of symbols, large sketches, and 3D objects, utilizing only pixel data. We display applications of our method in image segmentation, and present several ablation studies comparing various configurations.",0
"Converting images into high-level constructs like brush strokes or shapes is essential for both human and machine comprehension. However, in some cases, only pixel data is available. To solve this problem, we have introduced a technique to produce images directly in a high-level domain, such as brush strokes, without real pairwise data. Our approach involves training a ""canvas"" network to replicate the conversion of high-level constructs to pixels, followed by a high-level ""drawing"" network that is optimized through this process to achieve the desired image recreation or translation task. We have successfully generated sequential vector representations of symbols, large sketches, and 3D objects using only pixel data. Furthermore, we have demonstrated the potential of our method in image segmentation and conducted various ablation studies to compare different configurations.",1
"With the introduction of fully convolutional neural networks, deep learning has raised the benchmark for medical image segmentation on both speed and accuracy, and different networks have been proposed for 2D and 3D segmentation with promising results. Nevertheless, most networks only handle relatively small numbers of labels (<10), and there are very limited works on handling highly unbalanced object sizes especially in 3D segmentation. In this paper, we propose a network architecture and the corresponding loss function which improve segmentation of very small structures. By combining skip connections and deep supervision with respect to the computational feasibility of 3D segmentation, we propose a fast converging and computationally efficient network architecture for accurate segmentation. Furthermore, inspired by the concept of focal loss, we propose an exponential logarithmic loss which balances the labels not only by their relative sizes but also by their segmentation difficulties. We achieve an average Dice coefficient of 82% on brain segmentation with 20 labels, with the ratio of the smallest to largest object sizes as 0.14%. Less than 100 epochs are required to reach such accuracy, and segmenting a 128x128x128 volume only takes around 0.4 s.",0
"Medical image segmentation has seen significant improvement in terms of speed and accuracy with the advent of fully convolutional neural networks in deep learning. Various networks have been proposed for 2D and 3D segmentation, yielding promising results. However, these networks are limited in their ability to handle highly unbalanced object sizes and typically only tackle relatively small numbers of labels (<10). Our study addresses these limitations by proposing a network architecture and corresponding loss function that enhance segmentation of very small structures. Our approach combines skip connections and deep supervision to achieve computational feasibility in 3D segmentation, resulting in a fast converging and computationally efficient network architecture. Additionally, we introduce an exponential logarithmic loss inspired by the concept of focal loss, which balances labels based on their relative sizes and segmentation difficulties. Our proposed methodology achieved an average Dice coefficient of 82% on brain segmentation with 20 labels, with the smallest to largest object size ratio at 0.14%. Furthermore, we only require less than 100 epochs to achieve such accuracy, and segmenting a 128x128x128 volume takes around 0.4 s.",1
"Recently, diagnosis, therapy and monitoring of human diseases involve a variety of imaging modalities, such as magnetic resonance imaging(MRI), computed tomography(CT), Ultrasound(US) and Positron-emission tomography(PET) as well as a variety of modern optical techniques. Over the past two decade, it has been recognized that advanced image processing techniques provide valuable information to physicians for diagnosis, image guided therapy and surgery, and monitoring of the treated organ to the therapy. Many researchers and companies have invested significant efforts in the developments of advanced medical image analysis methods; especially in the two core studies of medical image segmentation and registration, segmentations of organs and lesions are used to quantify volumes and shapes used in diagnosis and monitoring treatment; registration of multimodality images of organs improves detection, diagnosis and staging of diseases as well as image-guided surgery and therapy, registration of images obtained from the same modality are used to monitor progression of therapy. These challenging clinical-motivated applications introduce novel and sophisticated mathematical problems which stimulate developments of advanced optimization and computing methods, especially convex optimization attaining optimum in a global sense, hence, bring an enormous spread of research topics for recent computational medical image analysis. Particularly, distinct from the usual image processing, most medical images have a big volume of acquired data, often in 3D or 4D (3D + t) along with great noises or incomplete image information, and form the challenging large-scale optimization problems; how to process such poor 'big data' of medical images efficiently and solve the corresponding optimization problems robustly are the key factors of modern medical image analysis.",0
"In modern medicine, various imaging techniques, including MRI, CT, US, and PET, are used for diagnosing, treating, and monitoring human diseases. Advanced image processing methods have been recognized as valuable tools for providing physicians with useful information regarding diagnosis, image-guided therapy and surgery, and monitoring of treated organs. Researchers and companies have invested significant efforts in developing advanced medical image analysis methods, particularly in medical image segmentation and registration. Segmentation is used to quantify volumes and shapes of organs and lesions, while registration of multimodality images improves detection, diagnosis, and staging of diseases, as well as image-guided surgery and therapy. These clinical-motivated applications introduce novel and sophisticated mathematical problems that stimulate the development of advanced optimization and computing methods, especially convex optimization for attaining global optima. Medical images typically contain large volumes of data, often in 3D or 4D, accompanied by significant noise or incomplete information, posing challenging large-scale optimization problems. The key factors of modern medical image analysis are efficiently processing such large and complex medical images and solving corresponding optimization problems robustly.",1
"Semantic image segmentation, which becomes one of the key applications in image processing and computer vision domain, has been used in multiple domains such as medical area and intelligent transportation. Lots of benchmark datasets are released for researchers to verify their algorithms. Semantic segmentation has been studied for many years. Since the emergence of Deep Neural Network (DNN), segmentation has made a tremendous progress. In this paper, we divide semantic image segmentation methods into two categories: traditional and recent DNN method. Firstly, we briefly summarize the traditional method as well as datasets released for segmentation, then we comprehensively investigate recent methods based on DNN which are described in the eight aspects: fully convolutional network, upsample ways, FCN joint with CRF methods, dilated convolution approaches, progresses in backbone network, pyramid methods, Multi-level feature and multi-stage method, supervised, weakly-supervised and unsupervised methods. Finally, a conclusion in this area is drawn.",0
"Semantic image segmentation, an essential application in the field of image processing and computer vision, has been implemented in various domains such as medical and intelligent transportation. Several benchmark datasets have been made available to researchers for the validation of their algorithms. The study of semantic segmentation has been ongoing for several years, and since the inception of Deep Neural Network (DNN), segmentation has witnessed significant progress. This paper categorizes semantic image segmentation methods into two groups: traditional and recent DNN-based approaches. The traditional methods and segmentation datasets are briefly summarized, followed by a comprehensive examination of recent DNN-based methods across eight aspects, including fully convolutional network, upsample methods, FCN joint with CRF methods, dilated convolution approaches, progress in backbone network, pyramid methods, Multi-level feature and multi-stage method, supervised, weakly-supervised and unsupervised methods. Finally, a conclusion is drawn in this field.",1
"Deep convolutional neural network (DCNN) is the state-of-the-art method for image segmentation, which is one of key challenging computer vision tasks. However, DCNN requires a lot of training images with corresponding image masks to get a good segmentation result. Image annotation software which is easy to use and allows fast image mask generation is in great demand. To the best of our knowledge, all existing image annotation software support only drawing bounding polygons, bounding boxes, or bounding ellipses to mark target objects. These existing software are inefficient when targeting objects that have irregular shapes (e.g., defects in fabric images or tire images). In this paper we design an easy-to-use image annotation software called Mask Editor for image mask generation. Mask Editor allows drawing any bounding curve to mark objects and improves efficiency to mark objects with irregular shapes. Mask Editor also supports drawing bounding polygons, drawing bounding boxes, drawing bounding ellipses, painting, erasing, super-pixel-marking, image cropping, multi-class masks, mask loading, and mask modifying.",0
"The current leading approach for image segmentation, a challenging computer vision task, is the deep convolutional neural network (DCNN). However, a good segmentation result requires a large number of training images with corresponding image masks. Therefore, there is a high demand for user-friendly image annotation software that can quickly generate image masks. Existing software only supports drawing bounding polygons, boxes, or ellipses to mark target objects, making them inefficient for objects with irregular shapes such as defects in fabric or tire images. To address this issue, we introduce Mask Editor, an easy-to-use image annotation software that enables users to draw any bounding curve to mark objects and improve efficiency for irregular shapes. Additionally, Mask Editor offers various features such as painting, erasing, super-pixel-marking, image cropping, multi-class masks, mask loading, and mask modifying.",1
"Tumor detection in biomedical imaging is a time-consuming process for medical professionals and is not without errors. Thus in recent decades, researchers have developed algorithmic techniques for image processing using a wide variety of mathematical methods, such as statistical modeling, variational techniques, and machine learning. In this paper, we propose a semi-automatic method for liver segmentation of 2D CT scans into three labels denoting healthy, vessel, or tumor tissue based on graph cuts. First, we create a feature vector for each pixel in a novel way that consists of the 59 intensity values in the time series data and propose a simplified perimeter cost term in the energy functional. We normalize the data and perimeter terms in the functional to expedite the graph cut without having to optimize the scaling parameter $\lambda$. In place of a training process, predetermined tissue means are computed based on sample regions identified by expert radiologists. The proposed method also has the advantage of being relatively simple to implement computationally. It was evaluated against the ground truth on a clinical CT dataset of 10 tumors and yielded segmentations with a mean Dice similarity coefficient (DSC) of .77 and mean volume overlap error (VOE) of 36.7%. The average processing time was 1.25 minutes per slice.",0
"Medical professionals face challenges in detecting tumors in biomedical imaging due to the time-intensive process and potential for errors. To address these issues, researchers have developed algorithmic techniques using mathematical methods like statistical modeling, variational techniques, and machine learning. In this paper, a semi-automatic method for liver segmentation of 2D CT scans is proposed, using graph cuts to identify healthy, vessel, or tumor tissue. The method involves creating a feature vector for each pixel, normalizing data and perimeter terms, and computing predetermined tissue means based on expert radiologists' sample regions. The proposed method is relatively simple to implement computationally and was evaluated against the ground truth on a clinical CT dataset, producing segmentations with a mean Dice similarity coefficient (DSC) of .77 and mean volume overlap error (VOE) of 36.7%. The average processing time was 1.25 minutes per slice.",1
"The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7\% on Cityscapes (street scene parsing), 71.3\% on PASCAL-Person-Part (person-part segmentation), and 87.9\% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.",0
"Developing neural network architectures is crucial for achieving top-notch performance in machine learning systems for various tasks. Researchers have attempted to design and construct architectures automatically using a search space and simple learning algorithms. Recent advancements have shown that these meta-learning techniques can outperform human-designed architectures in image classification tasks. However, the extent to which such techniques can be applied to new domains remains unclear. This study focuses on dense image prediction tasks, such as scene parsing, person-part segmentation, and semantic image segmentation, which pose challenges due to multi-scale visual representation and high-resolution imagery. By constructing a recursive search space and conducting efficient random search, the study identifies architectures that outperform human-designed ones and achieve state-of-the-art performance in three dense prediction tasks. The resulting architecture is also more computationally efficient, requiring half the parameters and computational cost of previous state-of-the-art systems.",1
"Robot perception systems need to perform reliable image segmentation in real-time on noisy, raw perception data. State-of-the-art segmentation approaches use large CNN models and carefully constructed datasets; however, these models focus on accuracy at the cost of real-time inference. Furthermore, the standard semantic segmentation datasets are not large enough for training CNNs without augmentation and are not representative of noisy, uncurated robot perception data. We propose improving the performance of real-time segmentation frameworks on robot perception data by transferring features learned from synthetic segmentation data. We show that pretraining real-time segmentation architectures with synthetic segmentation data instead of ImageNet improves fine-tuning performance by reducing the bias learned in pretraining and closing the \textit{transfer gap} as a result. Our experiments show that our real-time robot perception models pretrained on synthetic data outperform those pretrained on ImageNet for every scale of fine-tuning data examined. Moreover, the degree to which synthetic pretraining outperforms ImageNet pretraining increases as the availability of robot data decreases, making our approach attractive for robotics domains where dataset collection is hard and/or expensive.",0
"To achieve reliable image segmentation in real-time on noisy, raw perception data, robot perception systems require advanced techniques. State-of-the-art segmentation approaches rely on large CNN models and well-curated datasets, which prioritize accuracy over real-time inference. However, these datasets are inadequate for training CNNs without augmentation and do not represent the uncurated and noisy robot perception data. Our proposal to enhance the performance of real-time segmentation frameworks on robot perception data involves transferring features learned from synthetic segmentation data. Our experiments demonstrate that pretraining real-time segmentation architectures with synthetic segmentation data rather than ImageNet enhances the fine-tuning performance by minimizing the bias learned in the pretraining phase and narrowing the transfer gap. Our results indicate that our real-time robot perception models pretrained on synthetic data outperform those pretrained on ImageNet, particularly in robotics domains where dataset collection is challenging or expensive.",1
"Binary image segmentation plays an important role in computer vision and has been widely used in many applications such as image and video editing, object extraction, and photo composition. In this paper, we propose a novel interactive binary image segmentation method based on the Markov Random Field (MRF) framework and the fast bilateral solver (FBS) technique. Specifically, we employ the geodesic distance component to build the unary term. To ensure both computation efficiency and effective responsiveness for interactive segmentation, superpixels are used in computing geodesic distances instead of pixels. Furthermore, we take a bilateral affinity approach for the pairwise term in order to preserve edge information and denoise. Through the alternating direction strategy, the MRF energy minimization problem is divided into two subproblems, which then can be easily solved by steepest gradient descent (SGD) and FBS respectively. Experimental results on the VGG interactive image segmentation dataset show that the proposed algorithm outperforms several state-of-the-art ones, and in particular, it can achieve satisfactory edge-smooth segmentation results even when the foreground and background color appearances are quite indistinctive.",0
"The use of binary image segmentation has become widespread in various computer vision applications, including object extraction, image and video editing, and photo composition. This paper introduces a new method for interactive binary image segmentation that utilizes the Markov Random Field (MRF) framework and the fast bilateral solver (FBS) technique. The unary term is built using the geodesic distance component, with superpixels instead of pixels used for computation efficiency and effective responsiveness. Additionally, the pairwise term employs a bilateral affinity approach to preserve edge information and denoise. The MRF energy minimization problem is divided into two subproblems using an alternating direction strategy, and solved with steepest gradient descent (SGD) and FBS. Experimental results on the VGG interactive image segmentation dataset show that this method outperforms several state-of-the-art algorithms, particularly in cases where foreground and background color appearances are indistinctive, achieving satisfactory edge-smooth segmentation results.",1
"Autosomal Dominant Polycystic Kidney Disease (ADPKD) characterized by progressive growth of renal cysts is the most prevalent and potentially lethal monogenic renal disease, affecting one in every 500-100 people. Total Kidney Volume (TKV) and its growth computed from Computed Tomography images has been accepted as an essential prognostic marker for renal function loss. Due to large variation in shape and size of kidney in ADPKD, existing methods to compute TKV (i.e. to segment ADKP) including those based on 2D convolutional neural networks are not accurate enough to be directly useful in clinical practice. In this work, we propose multi-task 3D Convolutional Neural Networks to segment ADPK and achieve a mean DICE score of 0.95 and mean absolute percentage TKV error of 3.86. Additionally, to solve the challenge of class imbalance, we propose to simply bootstrap cross entropy loss and compare results with recently prevalent dice loss in medical image segmentation community.",0
"ADPKD is a monogenic renal disease that affects approximately one out of every 500-100 people and is characterized by the progressive growth of renal cysts. The Total Kidney Volume (TKV) and its growth have been recognized as a crucial prognostic marker for renal function loss, which is computed from Computed Tomography images. However, existing methods to compute TKV and segment ADPKD, including those based on 2D convolutional neural networks, are not accurate enough to be directly applicable in clinical practice due to the large variation in the shape and size of the kidney in ADPKD. To address this issue, we propose the use of multi-task 3D Convolutional Neural Networks, which achieved a mean DICE score of 0.95 and mean absolute percentage TKV error of 3.86. Additionally, we propose to bootstrap cross entropy loss to solve the challenge of class imbalance and compare the results with the recently prevalent dice loss in the medical image segmentation community.",1
"Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatialtemporal features for video segmentation is largely limited by the scale of available video segmentation datasets, i.e., even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 4,453 YouTube video clips and 94 object categories. This is by far the largest video object segmentation dataset to our knowledge and has been released at http://youtube-vos.org. We further evaluate several existing state-of-the-art video object segmentation algorithms on this dataset which aims to establish baselines for the development of new algorithms in the future.",0
"For various video analysis tasks, it is crucial to learn long-term spatial-temporal features. However, current video segmentation techniques mostly depend on static image segmentation methods, and approaches that consider temporal dependency for segmentation require pretrained optical flow models, leading to suboptimal outcomes. The capability of end-to-end sequential learning to explore spatial-temporal features for video segmentation is restricted by the size of available video segmentation datasets. Even the most extensive video segmentation dataset only includes 90 short video clips. To address this issue, we have created a new large-scale video object segmentation dataset named YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset comprises 4,453 YouTube video clips and 94 object categories, making it the most extensive video object segmentation dataset known to date. It is now available at http://youtube-vos.org. We have also assessed various existing state-of-the-art video object segmentation algorithms on our dataset, aiming to establish a baseline for the development of new algorithms in the future.",1
"Contemporary deep learning based medical image segmentation algorithms require hours of annotation labor by domain experts. These data hungry deep models perform sub-optimally in the presence of limited amount of labeled data. In this paper, we present a data efficient learning framework using the recent concept of Generative Adversarial Networks; this allows a deep neural network to perform significantly better than its fully supervised counterpart in low annotation regime. The proposed method is an extension of our previous work with the addition of a new unsupervised adversarial loss and a structured prediction based architecture. To the best of our knowledge, this work is the first demonstration of an adversarial framework based structured prediction model for medical image segmentation. Though generic, we apply our method for segmentation of blood vessels in retinal fundus images. We experiment with extreme low annotation budget (0.8 - 1.6% of contemporary annotation size). On DRIVE and STARE datasets, the proposed method outperforms our previous method and other fully supervised benchmark models by significant margins especially with very low number of annotated examples. In addition, our systematic ablation studies suggest some key recipes for successfully training GAN based semi-supervised algorithms with an encoder-decoder style network architecture.",0
"Medical image segmentation algorithms based on deep learning techniques in contemporary times require a considerable amount of time and effort to annotate by domain experts. The deep models that consume a lot of data do not perform as expected when the labeled data is limited. This study proposes a data-efficient learning approach using the concept of Generative Adversarial Networks (GANs) to enhance the performance of deep neural networks in low-annotation situations. The new unsupervised adversarial loss and structured prediction-based architecture in the proposed method is an extension of the previous work of the researchers. This study marks the first demonstration of an adversarial-based structured prediction model for medical image segmentation. The researchers use this method for blood vessel segmentation in retinal fundus images and experiment with an extremely low annotation budget of 0.8-1.6% of the contemporary annotation size. The proposed method outperforms the previous method and other fully supervised benchmark models by significant margins, particularly with minimal annotated examples. Furthermore, the systematic ablation studies conducted by the researchers provide some key recipes for successfully training GAN-based semi-supervised algorithms with an encoder-decoder style network architecture.",1
"This chapter provides insight on how iris recognition, one of the leading biometric identification technologies in the world, can be impacted by pathologies and illnesses present in the eye, what are the possible repercussions of this influence, and what are the possible means for taking such effects into account when matching iris samples.   To make this study possible, a special database of iris images has been used, representing more than 20 different medical conditions of the ocular region (including cataract, glaucoma, rubeosis iridis, synechiae, iris defects, corneal pathologies and other) and containing almost 3000 samples collected from 230 distinct irises. Then, with the use of four different iris recognition methods, a series of experiments has been conducted, concluding in several important observations.   One of the most popular ocular disorders worldwide - the cataract - is shown to worsen genuine comparison scores when results obtained from cataract-affected eyes are compared to those coming from healthy irises. An analysis devoted to different types of impact on eye structures caused by diseases is also carried out with significant results. The enrollment process is highly sensitive to those eye conditions that make the iris obstructed or introduce geometrical distortions. Disorders affecting iris geometry, or producing obstructions are exceptionally capable of degrading the genuine comparison scores, so that the performance of the entire biometric system can be influenced. Experiments also reveal that imperfect execution of the image segmentation stage is the most prominent contributor to recognition errors.",0
"In this chapter, the impact of various ocular pathologies and illnesses on iris recognition technology is explored. A special database of iris images featuring over 20 medical conditions, including cataract, glaucoma, and iris defects, was used to conduct experiments using four different iris recognition methods. The results showed that cataracts in particular worsen comparison scores, while conditions affecting iris geometry and causing obstructions can significantly degrade system performance. Imperfect image segmentation was found to be the main cause of recognition errors. This study highlights the importance of accounting for ocular conditions when using iris recognition technology.",1
"In this paper, we propose an efficient pseudo-marginal Markov chain Monte Carlo (MCMC) sampling approach to draw samples from posterior shape distributions for image segmentation. The computation time of the proposed approach is independent from the size of the training set used to learn the shape prior distribution nonparametrically. Therefore, it scales well for very large data sets. Our approach is able to characterize the posterior probability density in the space of shapes through its samples, and to return multiple solutions, potentially from different modes of a multimodal probability density, which would be encountered, e.g., in segmenting objects from multiple shape classes. Experimental results demonstrate the potential of the proposed approach.",0
"An efficient pseudo-marginal Markov chain Monte Carlo (MCMC) sampling method is presented in this paper for drawing samples from posterior shape distributions for image segmentation. The proposed approach's computation time is not influenced by the size of the training set used to nonparametrically learn the shape prior distribution, making it ideal for very large data sets. By utilizing its samples, our approach is capable of characterizing the posterior probability density in the shape space and returning multiple solutions, even from different modes of a multimodal probability density. This feature is particularly useful in segmenting objects from multiple shape classes. Experimental results demonstrate the potential of the proposed approach.",1
"Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatial-temporal features for video segmentation is largely limited by the scale of available video segmentation datasets, i.e., even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 3,252 YouTube video clips and 78 categories including common objects and human activities. This is by far the largest video object segmentation dataset to our knowledge and we have released it at https://youtube-vos.org. Based on this dataset, we propose a novel sequence-to-sequence network to fully exploit long-term spatial-temporal information in videos for segmentation. We demonstrate that our method is able to achieve the best results on our YouTube-VOS test set and comparable results on DAVIS 2016 compared to the current state-of-the-art methods. Experiments show that the large scale dataset is indeed a key factor to the success of our model.",0
"For various video analysis tasks, it is crucial to learn long-term spatial-temporal features. However, current video segmentation methods mainly rely on static image segmentation techniques, and techniques that capture temporal dependency for segmentation must depend on pretrained optical flow models, resulting in suboptimal solutions. Moreover, the scale of available video segmentation datasets largely limits end-to-end sequential learning to explore spatial-temporal features for video segmentation, as even the largest video segmentation dataset contains only 90 short video clips. To address this issue, we have created a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS), which includes 3,252 YouTube video clips and 78 categories of common objects and human activities. This dataset is the most extensive video object segmentation dataset to our knowledge, and it is available at https://youtube-vos.org. Using this dataset, we have proposed a novel sequence-to-sequence network to fully exploit long-term spatial-temporal information in videos for segmentation. Our method achieved the best results on our YouTube-VOS test set and comparable results on DAVIS 2016 compared to current state-of-the-art methods. Our experiments suggest that the large scale dataset is a crucial factor in the success of our model.",1
"This paper delivers a new database of iris images collected in visible light using a mobile phone's camera and presents results of experiments involving existing commercial and open-source iris recognition methods, namely: IriCore, VeriEye, MIRLIN and OSIRIS. Several important observations are made.   First, we manage to show that after simple preprocessing, such images offer good visibility of iris texture even in heavily-pigmented irides. Second, for all four methods, the enrollment stage is not much affected by the fact that different type of data is used as input. This translates to zero or close-to-zero Failure To Enroll, i.e., cases when templates could not be extracted from the samples. Third, we achieved good matching accuracy, with correct genuine match rate exceeding 94.5% for all four methods, while simultaneously being able to maintain zero false match rate in every case. Correct genuine match rate of over 99.5% was achieved using one of the commercial methods, showing that such images can be used with the existing biometric solutions with minimum additional effort required. Finally, the experiments revealed that incorrect image segmentation is the most prevalent cause of recognition accuracy decrease.   To our best knowledge, this is the first database of iris images captured using a mobile device, in which image quality exceeds this of a near-infrared illuminated iris images, as defined in ISO/IEC 19794-6 and 29794-6 documents. This database will be publicly available to all researchers.",0
"A new database of iris images captured in visible light using a mobile phone's camera is presented in this paper. The study includes experiments with existing commercial and open-source iris recognition methods, namely IriCore, VeriEye, MIRLIN, and OSIRIS, and several significant observations are made. The images offer good visibility of iris texture, even in heavily-pigmented irides, after simple preprocessing. The enrollment stage is not much affected by the use of different types of data as input, resulting in zero or close-to-zero Failure To Enroll. Good matching accuracy is achieved, with a correct genuine match rate exceeding 94.5% for all four methods, and a zero false match rate in every case. The database is publicly available to researchers, and it is the first of its kind to exceed the image quality of near-infrared illuminated iris images according to ISO/IEC 19794-6 and 29794-6 documents. Finally, the experiments show that incorrect image segmentation is the most prevalent cause of recognition accuracy decrease. The study demonstrates the potential of using mobile devices for iris recognition with minimum additional effort required for existing biometric solutions.",1
"This paper presents the experimental study revealing weaker performance of the automatic iris recognition methods for cataract-affected eyes when compared to healthy eyes. There is little research on the topic, mostly incorporating scarce databases that are often deficient in images representing more than one illness. We built our own database, acquiring 1288 eye images of 37 patients of the Medical University of Warsaw. Those images represent several common ocular diseases, such as cataract, along with less ordinary conditions, such as iris pattern alterations derived from illness or eye trauma. Images were captured in near-infrared light (used in biometrics) and for selected cases also in visible light (used in ophthalmological diagnosis). Since cataract is a disorder that is most populated by samples in the database, in this paper we focus solely on this illness. To assess the extent of the performance deterioration we use three iris recognition methodologies (commercial and academic solutions) to calculate genuine match scores for healthy eyes and those influenced by cataract. Results show a significant degradation in iris recognition reliability manifesting by worsening the genuine scores in all three matchers used in this study (12% of genuine score increase for an academic matcher, up to 175% of genuine score increase obtained for an example commercial matcher). This increase in genuine scores affected the final false non-match rate in two matchers. To our best knowledge this is the only study of such kind that employs more than one iris matcher, and analyzes the iris image segmentation as a potential source of decreased reliability.",0
"In this research, we conducted an experiment to investigate the performance of automatic iris recognition methods on cataract-affected eyes in comparison to healthy eyes. The topic has been sparsely researched, with most studies using databases with limited images representing only a single illness. To address this, we created our own database of 1288 eye images from 37 patients with various ocular diseases, including cataract and iris pattern alterations resulting from illness or trauma. The images were captured in both near-infrared and visible light. Since cataract was the most prevalent disorder in our database, we focused solely on this illness in our study. To evaluate the extent of performance deterioration, we used three iris recognition methodologies, including both commercial and academic solutions, to calculate genuine match scores for healthy and cataract-affected eyes. Our results indicated a significant degradation in iris recognition reliability for cataract-affected eyes, as evidenced by a decrease in genuine scores across all three matchers used in the study. This decrease in genuine scores also affected the final false non-match rate in two matchers. To our knowledge, this is the only study that employs multiple iris matchers and analyzes the iris image segmentation as a potential source of decreased reliability.",1
"Leaping into the rapidly developing world of deep learning is an exciting and sometimes confusing adventure. All of the advice and tutorials available can be hard to organize and work through, especially when training specific models on specific datasets, different from those originally used to train the network. In this short guide, we aim to walk the reader through the techniques that we have used to successfully train two deep neural networks for pixel-wise classification, including some data management and augmentation approaches for working with image data that may be insufficiently annotated or relatively homogenous.",0
"Embarking on the swiftly evolving realm of deep learning can be an enthralling but bewildering journey. The abundance of guidance and tutorials can be challenging to arrange and navigate, particularly when training particular models on data sets that are distinct from those initially utilized to train the network. The purpose of this concise handbook is to lead the reader through the methods that we have effectively employed to train two deep neural networks for pixel-wise classification, incorporating various data management and augmentation strategies for dealing with image data that may have inadequate annotations or uniformity.",1
"The tracing of neural pathways through large volumes of image data is an incredibly tedious and time-consuming process that significantly encumbers progress in neuroscience. We are exploring deep learning's potential to automate segmentation of high-resolution scanning electron microscope (SEM) image data to remove that barrier. We have started with neural pathway tracing through 5.1GB of whole-brain serial-section slices from larval zebrafish collected by the Center for Brain Science at Harvard University. This kind of manual image segmentation requires years of careful work to properly trace the neural pathways in an organism as small as a zebrafish larva (approximately 5mm in total body length). In automating this process, we would vastly improve productivity, leading to faster data analysis and breakthroughs in understanding the complexity of the brain. We will build upon prior attempts to employ deep learning for automatic image segmentation extending methods for unconventional deep learning data.",0
"The process of tracing neural pathways in image data is laborious and time-consuming, hindering progress in neuroscience. In order to eliminate this barrier, we are investigating the potential of deep learning to automate the segmentation of high-resolution SEM image data. To start, we are focusing on neural pathway tracing in a large dataset of whole-brain serial-section slices from larval zebrafish, which requires years of manual image segmentation. Automating this process would greatly increase productivity and accelerate breakthroughs in understanding the complexities of the brain. We will expand upon previous attempts to use deep learning for automatic image segmentation, including unconventional data.",1
"In this work, a region-based Deep Convolutional Neural Network framework is proposed for document structure learning. The contribution of this work involves efficient training of region based classifiers and effective ensembling for document image classification. A primary level of `inter-domain' transfer learning is used by exporting weights from a pre-trained VGG16 architecture on the ImageNet dataset to train a document classifier on whole document images. Exploiting the nature of region based influence modelling, a secondary level of `intra-domain' transfer learning is used for rapid training of deep learning models for image segments. Finally, stacked generalization based ensembling is utilized for combining the predictions of the base deep neural network models. The proposed method achieves state-of-the-art accuracy of 92.2% on the popular RVL-CDIP document image dataset, exceeding benchmarks set by existing algorithms.",0
"This study proposes a framework for document structure learning using a region-based Deep Convolutional Neural Network. The study's contribution includes efficient training of classifiers based on regions and effective ensembling for image classification. The researchers employed two levels of transfer learning: primary level inter-domain transfer learning using pre-trained VGG16 architecture from ImageNet to train a document classifier on whole document images, and secondary level intra-domain transfer learning for quick training of deep learning models for image segments. Finally, they used stacked generalization based ensembling to combine base deep neural network models' predictions, resulting in a state-of-the-art accuracy of 92.2% on the RVL-CDIP document image dataset, surpassing existing algorithms.",1
"For the task of subdecimeter aerial imagery segmentation, fine-grained semantic segmentation results are usually difficult to obtain because of complex remote sensing content and optical conditions. Recently, convolutional neural networks (CNNs) have shown outstanding performance on this task. Although many deep neural network structures and techniques have been applied to improve the accuracy, few have paid attention to better differentiating the easily confused classes. In this paper, we propose TreeSegNet which adopts an adaptive network to increase the classification rate at the pixelwise level. Specifically, based on the infrastructure of DeepUNet, a Tree-CNN block in which each node represents a ResNeXt unit is constructed adaptively according to the confusion matrix and the proposed TreeCutting algorithm. By transporting feature maps through concatenating connections, the Tree-CNN block fuses multiscale features and learns best weights for the model. In experiments on the ISPRS 2D semantic labeling Potsdam dataset, the results obtained by TreeSegNet are better than those of other published state-of-the-art methods. Detailed comparison and analysis show that the improvement brought by the adaptive Tree-CNN block is significant.",0
"Obtaining fine-grained semantic segmentation results for the subdecimeter aerial imagery segmentation task is challenging due to complex remote sensing content and optical conditions. However, convolutional neural networks (CNNs) have demonstrated exceptional performance in this area. Despite the application of various deep neural network structures and techniques to enhance accuracy, few have focused on better distinguishing easily confused classes. In this study, we introduce TreeSegNet, which employs an adaptive network to enhance pixelwise classification. TreeSegNet constructs a Tree-CNN block, with each node representing a ResNeXt unit, based on the confusion matrix and the proposed TreeCutting algorithm, using the DeepUNet infrastructure. By concatenating connections to transport feature maps, the Tree-CNN block merges multiscale features and learns optimal weights for the model. Our experiments on the ISPRS 2D semantic labeling Potsdam dataset demonstrate that TreeSegNet outperforms other published state-of-the-art methods. Furthermore, detailed comparisons and analyses indicate that the adaptive Tree-CNN block's improvement is significant.",1
"Every year millions of people die due to disease of Cancer. Due to its invasive nature it is very complex to cure even in primary stages. Hence, only method to survive this disease completely is via forecasting by analyzing the early mutation in cells of the patient biopsy. Cell Segmentation can be used to find cell which have left their nuclei. This enables faster cure and high rate of survival. Cell counting is a hard, yet tedious task that would greatly benefit from automation. To accomplish this task, segmentation of cells need to be accurate. In this paper, we have improved the learning of training data by our network. It can annotate precise masks on test data. we examine the strength of activation functions in medical image segmentation task by improving learning rates by our proposed Carving Technique. Identifying the cells nuclei is the starting point for most analyses, identifying nuclei allows researchers to identify each individual cell in a sample, and by measuring how cells react to various treatments, the researcher can understand the underlying biological processes at work. Experimental results shows the efficiency of the proposed work.",0
"Every year, millions of people succumb to cancer, which is a highly complex disease that is challenging to cure, especially in its early stages. To increase the chances of survival, it is essential to forecast and analyze early cell mutations in patient biopsies. Cell Segmentation is a valuable tool that can accurately identify cells that have left their nuclei, resulting in quicker treatment and higher survival rates. Although cell counting is a difficult and tedious task, automating it would be beneficial. In this study, we improved the training data learning process by enhancing the accuracy of cell segmentation through our proposed Carving Technique. By identifying the nuclei of cells, researchers can analyze individual cells and understand the biological processes at work. The experimental results of our proposed work demonstrate its effectiveness.",1
"Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\% and 82.1\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \url{https://github.com/tensorflow/models/tree/master/research/deeplab}.",0
"To enhance the accuracy of semantic segmentation in deep neural networks, two techniques are commonly used: spatial pyramid pooling module and encode-decoder structure. Spatial pyramid pooling module utilizes filters and pooling operations at various rates and fields-of-view to encode multi-scale contextual information, while encode-decoder structure gradually recovers spatial information to capture sharper object boundaries. This paper proposes a combination of both techniques in a new model called DeepLabv3+. The model adds a decoder module to refine segmentation results, particularly along object boundaries, and employs the Xception model and depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a stronger and faster encoder-decoder network. The proposed model is tested on PASCAL VOC 2012 and Cityscapes datasets, achieving impressive results without post-processing. The paper also provides a publicly available reference implementation of the proposed models in Tensorflow, accessible at \url{https://github.com/tensorflow/models/tree/master/research/deeplab}.",1
"For many biological image segmentation tasks, including topological knowledge, such as the nesting of classes, can greatly improve results. However, most `out-of-the-box' CNN models are still blind to such prior information. In this paper, we propose a novel approach to encode this information, through a multi-level activation layer and three compatible losses. We benchmark all of them on nuclei segmentation in bright-field microscopy cell images from the 2018 Data Science Bowl challenge, offering an exemplary segmentation task with cells and nested subcellular structures. Our scheme greatly speeds up learning, and outperforms standard multi-class classification with soft-max activation and a previously proposed method stemming from it, improving the Dice score significantly (p-values<0.007). Our approach is conceptually simple, easy to implement and can be integrated in any CNN architecture. It can be generalized to a higher number of classes, with or without further relations of containment.",0
"Including topological knowledge, such as class nesting, can significantly enhance results for various biological image segmentation tasks. However, conventional CNN models lack such prior information. This paper introduces a new method to incorporate such knowledge by using a multi-level activation layer and three compatible losses. The approach is evaluated on nuclei segmentation in bright-field microscopy cell images from the 2018 Data Science Bowl challenge, which is an exemplary segmentation task involving cells and nested subcellular structures. The proposed method accelerates learning and outperforms standard multi-class classification with soft-max activation and a previously proposed method derived from it, with a significant improvement in the Dice score (p-values<0.007). The approach is straightforward, easy to implement, and can be integrated into any CNN architecture. Moreover, it can be extended to a higher number of classes, with or without further containment relationships.",1
"Confocal Laser Endomicroscope (CLE) is a novel handheld fluorescence imaging device that has shown promise for rapid intraoperative diagnosis of brain tumor tissue. Currently CLE is capable of image display only and lacks an automatic system to aid the surgeon in analyzing the images. The goal of this project was to develop a computer-aided diagnostic approach for CLE imaging of human glioma with feature localization function. Despite the tremendous progress in object detection and image segmentation methods in recent years, most of such methods require large annotated datasets for training. However, manual annotation of thousands of histopathological images by physicians is costly and time consuming. To overcome this problem, we propose a Weakly-Supervised Learning (WSL)-based model for feature localization that trains on image-level annotations, and then localizes incidences of a class-of-interest in the test image. We developed a novel convolutional neural network for diagnostic features localization from CLE images by employing a novel multiscale activation map that is laterally inhibited and collaterally integrated. To validate our method, we compared proposed model's output to the manual annotation performed by four neurosurgeons on test images. Proposed model achieved 88% mean accuracy and 86% mean intersection over union on intermediate features and 87% mean accuracy and 88% mean intersection over union on restrictive fine features, while outperforming other state of the art methods tested. This system can improve accuracy and efficiency in characterization of CLE images of glioma tissue during surgery, augment intraoperative decision-making process regarding tumor margin and affect resection rates.",0
"The Confocal Laser Endomicroscope (CLE) is a new portable fluorescence imaging device that has shown potential for quick intraoperative diagnosis of brain tumor tissue. However, it currently lacks an automatic system to help surgeons analyze the images it produces. This project aimed to create a computer-aided diagnostic approach for CLE imaging of human glioma, with a feature localization function. While recent progress has been made in object detection and image segmentation methods, these techniques often require costly and time-consuming manual annotation of large datasets. To address this issue, we propose a Weakly-Supervised Learning (WSL)-based model that can train on image-level annotations and then localize incidences of a class-of-interest in the test image. We developed a new convolutional neural network with a novel multiscale activation map that is laterally inhibited and collaterally integrated for diagnostic features localization from CLE images. To validate our method, we compared the model's output to manual annotation performed by four neurosurgeons on test images. Our proposed model achieved 88% mean accuracy and 86% mean intersection over union on intermediate features and 87% mean accuracy and 88% mean intersection over union on restrictive fine features, outperforming other state-of-the-art methods tested. This system has the potential to improve accuracy and efficiency in characterizing CLE images of glioma tissue during surgery, augment the intraoperative decision-making process regarding tumor margin, and affect resection rates.",1
"Existing works on semantic segmentation typically consider a small number of labels, ranging from tens to a few hundreds. With a large number of labels, training and evaluation of such task become extremely challenging due to correlation between labels and lack of datasets with complete annotations. We formulate semantic segmentation as a problem of image segmentation given a semantic concept, and propose a novel system which can potentially handle an unlimited number of concepts, including objects, parts, stuff, and attributes. We achieve this using a weakly and semi-supervised framework leveraging multiple datasets with different levels of supervision. We first train a deep neural network on a 6M stock image dataset with only image-level labels to learn visual-semantic embedding on 18K concepts. Then, we refine and extend the embedding network to predict an attention map, using a curated dataset with bounding box annotations on 750 concepts. Finally, we train an attention-driven class agnostic segmentation network using an 80-category fully annotated dataset. We perform extensive experiments to validate that the proposed system performs competitively to the state of the art on fully supervised concepts, and is capable of producing accurate segmentations for weakly learned and unseen concepts.",0
"Current research on semantic segmentation typically deals with a limited number of labels, ranging from a few tens to a few hundred. Since there are interdependencies between labels and a lack of datasets with complete annotations, training and evaluating such tasks become very difficult when there are a large number of labels. We propose a new system that can potentially handle an infinite number of concepts, including objects, parts, stuff, and attributes, by formulating semantic segmentation as an image segmentation problem based on a semantic concept. We accomplish this by using a weakly and semi-supervised framework that exploits various datasets with different levels of supervision. We first train a deep neural network on a 6M stock image dataset with only image-level labels to learn visual-semantic embedding on 18K concepts. We then refine and extend the embedding network to predict an attention map, using a curated dataset with bounding box annotations on 750 concepts. Finally, we train an attention-driven class agnostic segmentation network using an 80-category fully annotated dataset. We conduct extensive experiments to demonstrate that the proposed system performs competitively with the state of the art on fully supervised concepts and can produce accurate segmentations for weakly learned and unseen concepts.",1
"Within this thesis we propose a platform for combining Augmented Reality (AR) hardware with machine learning in a user-oriented pipeline, offering to the medical staff an intuitive 3D visualization of volumetric Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) medical image segmentations inside the AR headset, that does not need human intervention for loading, processing and segmentation of medical images. The AR visualization, based on Microsoft HoloLens, employs a modular and thus scalable frontend-backend architecture for real-time visualizations on multiple AR headsets. As Convolutional Neural Networks (CNNs) have lastly demonstrated superior performance for the machine learning task of image semantic segmentation, the pipeline also includes a fully automated CNN algorithm for the segmentation of the liver from CT scans. The model is based on the Deep Retinal Image Understanding (DRIU) model which is a Fully Convolutional Network with side outputs from feature maps with different resolution, extracted at different stages of the network. The algorithm is 2.5D which means that the input is a set of consecutive scan slices. The experiments have been performed on the Liver Tumor Segmentation Challenge (LiTS) dataset for liver segmentation and demonstrated good results and flexibility. While multiple approaches exist in the domain, only few of them have focused on overcoming the practical aspects which still largely hold this technology away from the operating rooms. In line with this, we also are next planning an evaluation from medical doctors and radiologists in a real-world environment.",0
"In this thesis, we suggest a user-friendly pipeline that combines Augmented Reality (AR) hardware with machine learning to provide medical staff with a simple 3D view of volumetric Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) medical image segmentations within the AR headset. The AR visualization is based on Microsoft HoloLens and uses a scalable frontend-backend architecture for real-time visualizations on multiple AR headsets. The pipeline includes a fully automated CNN algorithm for liver segmentation from CT scans. The model is based on the Deep Retinal Image Understanding (DRIU) model, which is a Fully Convolutional Network with different feature maps. The algorithm uses consecutive scan slices (2.5D). The experiments showed good results and flexibility using the Liver Tumor Segmentation Challenge (LiTS) dataset. While there are multiple approaches in this field, few of them focus on overcoming the practical challenges that prevent this technology from being used in operating rooms. Therefore, we plan to evaluate the system in a real-world environment with medical doctors and radiologists.",1
"Unsupervised learning poses one of the most difficult challenges in computer vision today. The task has an immense practical value with many applications in artificial intelligence and emerging technologies, as large quantities of unlabeled videos can be collected at relatively low cost. In this paper, we address the unsupervised learning problem in the context of detecting the main foreground objects in single images. We train a student deep network to predict the output of a teacher pathway that performs unsupervised object discovery in videos or large image collections. Our approach is different from published methods on unsupervised object discovery. We move the unsupervised learning phase during training time, then at test time we apply the standard feed-forward processing along the student pathway. This strategy has the benefit of allowing increased generalization possibilities during training, while remaining fast at testing. Our unsupervised learning algorithm can run over several generations of student-teacher training. Thus, a group of student networks trained in the first generation collectively create the teacher at the next generation. In experiments our method achieves top results on three current datasets for object discovery in video, unsupervised image segmentation and saliency detection. At test time the proposed system is fast, being one to two orders of magnitude faster than published unsupervised methods.",0
"Computer vision faces a significant challenge in unsupervised learning, which holds immense practical value for artificial intelligence and emerging technologies, as large amounts of unlabeled videos can be obtained inexpensively. This paper focuses on solving the unsupervised learning problem by detecting the main foreground objects in single images. Our approach trains a deep network to predict the output of a teacher pathway that performs unsupervised object discovery in videos or large image collections, which differs from published methods. By moving the unsupervised learning phase during training time, we increase generalization possibilities and maintain speed during testing. Our algorithm can run over several generations of student-teacher training, where a group of student networks trained in the first generation collectively create the teacher at the next generation. Our method achieves top results on three current datasets for object discovery in video, unsupervised image segmentation, and saliency detection. At test time, the proposed system is much faster than published unsupervised methods, being one to two orders of magnitude faster.",1
"The interactive image segmentation algorithm can provide an intelligent ways to understand the intention of user input. Many interactive methods have the problem of that ask for large number of user input. To efficient produce intuitive segmentation under limited user input is important for industrial application. In this paper, we reveal a positive feedback system on image segmentation to show the pixels of self-learning. Two approaches, iterative random walks and boundary random walks, are proposed for segmentation potential, which is the key step in feedback system. Experiment results on image segmentation indicates that proposed algorithms can obtain more efficient input to random walks. And higher segmentation performance can be obtained by applying the iterative boundary random walks algorithm.",0
"The image segmentation algorithm that is interactive can provide a smart approach to understanding the user's input intentions. One issue with many interactive methods is that they require a large amount of user input. It is important to efficiently create an intuitive segmentation with limited user input for industrial applications. This paper discusses a feedback system on image segmentation that reveals the pixels of self-learning. The feedback system has two approaches: iterative random walks and boundary random walks, which are proposed for potential segmentation, a crucial step in the feedback system. The experimental results on image segmentation indicate that the proposed algorithms can achieve more efficient input to random walks, and the iterative boundary random walks algorithm leads to higher segmentation performance.",1
"Segmentation of the left atrial chamber and assessing its morphology, are essential for improving our understanding of atrial fibrillation, the most common type of cardiac arrhythmia. Automation of this process in 3D gadolinium enhanced-MRI (GE-MRI) data is desirable, as manual delineation is time-consuming, challenging and observer-dependent. Recently, deep convolutional neural networks (CNNs) have gained tremendous traction and achieved state-of-the-art results in medical image segmentation. However, it is difficult to incorporate local and global information without using contracting (pooling) layers, which in turn reduces segmentation accuracy for smaller structures. In this paper, we propose a 3D CNN for volumetric segmentation of the left atrial chamber in LGE-MRI. Our network is based on the well known U-Net architecture. We employ a 3D fully convolutional network, with dilated convolutions in the lowest level of the network, and residual connections between encoder blocks to incorporate local and global knowledge. The results show that including global context through the use of dilated convolutions, helps in domain adaptation, and the overall segmentation accuracy is improved in comparison to a 3D U-Net.",0
"Improving our understanding of atrial fibrillation, the most common type of cardiac arrhythmia, requires segmenting the left atrial chamber and assessing its morphology. However, manual delineation of the chamber in 3D gadolinium enhanced-MRI (GE-MRI) data is time-consuming, challenging, and observer-dependent. To address this, deep convolutional neural networks (CNNs) have been used for medical image segmentation with state-of-the-art results. However, pooling layers used to incorporate local and global information can reduce segmentation accuracy for smaller structures. In this paper, we propose a 3D CNN based on the U-Net architecture for volumetric segmentation of the left atrial chamber in LGE-MRI. Our network incorporates dilated convolutions and residual connections between encoder blocks to incorporate both local and global knowledge. Results show improved segmentation accuracy compared to a 3D U-Net, demonstrating the benefits of including global context through dilated convolutions for domain adaptation.",1
"In this paper, we adopt 3D Convolutional Neural Networks to segment volumetric medical images. Although deep neural networks have been proven to be very effective on many 2D vision tasks, it is still challenging to apply them to 3D tasks due to the limited amount of annotated 3D data and limited computational resources. We propose a novel 3D-based coarse-to-fine framework to effectively and efficiently tackle these challenges. The proposed 3D-based framework outperforms the 2D counterpart to a large margin since it can leverage the rich spatial infor- mation along all three axes. We conduct experiments on two datasets which include healthy and pathological pancreases respectively, and achieve the current state-of-the-art in terms of Dice-S{\o}rensen Coefficient (DSC). On the NIH pancreas segmentation dataset, we outperform the previous best by an average of over 2%, and the worst case is improved by 7% to reach almost 70%, which indicates the reliability of our framework in clinical applications.",0
"This paper utilizes 3D Convolutional Neural Networks to segment medical images that are volumetric in nature. While neural networks have been successful in 2D vision tasks, applying them to 3D tasks is challenging due to limited annotated 3D data and computational resources. To address this, we propose a novel 3D-based coarse-to-fine framework that effectively and efficiently tackles these challenges. This framework outperforms the 2D counterpart significantly by leveraging rich spatial information along all three axes. We conducted experiments on two datasets, healthy and pathological pancreases, and achieved state-of-the-art results in terms of Dice-S{\o}rensen Coefficient (DSC). On the NIH pancreas segmentation dataset, we outperformed the previous best by over 2% on average and improved the worst case by 7% to reach almost 70%, demonstrating the reliability of our framework in clinical applications.",1
"Multi-atlas segmentation approach is one of the most widely-used image segmentation techniques in biomedical applications. There are two major challenges in this category of methods, i.e., atlas selection and label fusion. In this paper, we propose a novel multi-atlas segmentation method that formulates multi-atlas segmentation in a deep learning framework for better solving these challenges. The proposed method, dubbed deep fusion net (DFN), is a deep architecture that integrates a feature extraction subnet and a non-local patch-based label fusion (NL-PLF) subnet in a single network. The network parameters are learned by end-to-end training for automatically learning deep features that enable optimal performance in a NL-PLF framework. The learned deep features are further utilized in defining a similarity measure for atlas selection. By evaluating on two public cardiac MR datasets of SATA-13 and LV-09 for left ventricle segmentation, our approach achieved 0.833 in averaged Dice metric (ADM) on SATA-13 dataset and 0.95 in ADM for epicardium segmentation on LV-09 dataset, comparing favorably with the other automatic left ventricle segmentation methods. We also tested our approach on Cardiac Atlas Project (CAP) testing set of MICCAI 2013 SATA Segmentation Challenge, and our method achieved 0.815 in ADM, ranking highest at the time of writing.",0
"The multi-atlas segmentation technique is commonly used in biomedical applications, but it faces challenges in atlas selection and label fusion. To address these issues, we propose a new approach called deep fusion net (DFN) that incorporates a feature extraction subnet and a non-local patch-based label fusion (NL-PLF) subnet into a single network. By training the network end-to-end, we can automatically learn deep features that optimize performance in an NL-PLF framework and use them to define a similarity measure for atlas selection. Our approach achieved impressive results in left ventricle segmentation for two public cardiac MR datasets, with an averaged Dice metric of 0.833 on SATA-13 and 0.95 for epicardium segmentation on LV-09. We also tested our approach on the Cardiac Atlas Project (CAP) testing set of MICCAI 2013 SATA Segmentation Challenge, where it achieved the highest ranking at the time of writing, with an averaged Dice metric of 0.815.",1
"Confusing classes that are ubiquitous in real world often degrade performance for many vision related applications like object detection, classification, and segmentation. The confusion errors are not only caused by similar visual patterns but also amplified by various factors during the training of our designed models, such as reduced feature resolution in the encoding process or imbalanced data distributions. A large amount of deep learning based network structures has been proposed in recent years to deal with these individual factors and improve network performance. However, to our knowledge, no existing work in semantic image segmentation is designed to tackle confusion errors explicitly. In this paper, we present a novel and general network structure that reduces confusion errors in more direct manner and apply the network for semantic segmentation. There are two major contributions in our network structure: 1) We ensemble subnets with heterogeneous output spaces based on the discriminative confusing groups. The training for each subnet can distinguish confusing classes within the group without affecting unrelated classes outside the group. 2) We propose an improved cross-entropy loss function that maximizes the probability assigned to the correct class and penalizes the probabilities assigned to the confusing classes at the same time. Our network structure is a general structure and can be easily adapted to any other networks to further reduce confusion errors. Without any changes in the feature encoder and post-processing steps, our experiments demonstrate consistent and significant improvements on different baseline models on Cityscapes and PASCAL VOC datasets (e.g., 3.05% over ResNet-101 and 1.30% over ResNet-38).",0
"Vision related applications such as object detection, classification, and segmentation are often hindered by confusing classes that are commonly found in the real world. These errors are not only caused by similar visual patterns, but are also amplified by various factors during model training, such as imbalanced data distributions or reduced feature resolution. While many deep learning based network structures have been proposed to address these individual factors and improve network performance, none have explicitly tackled confusion errors in semantic image segmentation. This paper presents a novel and general network structure that directly reduces confusion errors and applies it to semantic segmentation. The network structure has two major contributions: 1) ensemble subnets with different output spaces based on discriminative confusing groups, and 2) an improved cross-entropy loss function that maximizes the probability of the correct class while penalizing confusing classes. The proposed network structure can be easily adapted to other networks and shows consistent and significant improvements on different baseline models on Cityscapes and PASCAL VOC datasets. Specifically, the proposed model achieves a 3.05% improvement over ResNet-101 and a 1.30% improvement over ResNet-38 without any changes to the feature encoder or post-processing steps.",1
"Semantic Segmentation using deep convolutional neural network pose more complex challenge for any GPU intensive task. As it has to compute million of parameters, it results to huge memory consumption. Moreover, extracting finer features and conducting supervised training tends to increase the complexity. With the introduction of Fully Convolutional Neural Network, which uses finer strides and utilizes deconvolutional layers for upsampling, it has been a go to for any image segmentation task. In this paper, we propose two segmentation architecture which not only needs one-third the parameters to compute but also gives better accuracy than the similar architectures. The model weights were transferred from the popular neural net like VGG19 and VGG16 which were trained on Imagenet classification data-set. Then we transform all the fully connected layers to convolutional layers and use dilated convolution for decreasing the parameters. Lastly, we add finer strides and attach four skip architectures which are element-wise summed with the deconvolutional layers in steps. We train and test on different sparse and fine data-sets like Pascal VOC2012, Pascal-Context and NYUDv2 and show how better our model performs in this tasks. On the other hand our model has a faster inference time and consumes less memory for training and testing on NVIDIA Pascal GPUs, making it more efficient and less memory consuming architecture for pixel-wise segmentation.",0
"Deep convolutional neural networks pose a complex challenge for GPU-intensive tasks in semantic segmentation. This is due to the need to compute millions of parameters, resulting in extensive memory consumption. Furthermore, the complexity increases as finer features are extracted and supervised training is conducted. However, the Fully Convolutional Neural Network addresses these issues by utilizing deconvolutional layers for upsampling and finer strides. In this paper, we propose two segmentation architectures that require one-third of the parameters to compute and offer better accuracy than similar architectures. We achieve this by transferring model weights from popular neural nets like VGG19 and VGG16, transforming fully connected layers into convolutional layers, and using dilated convolution to decrease parameters. Additionally, we add finer strides and attach four skip architectures, which are element-wise summed with the deconvolutional layers in steps. We train and test on sparse and fine datasets like Pascal VOC2012, Pascal-Context, and NYUDv2, demonstrating the superior performance of our model. Moreover, our model has a faster inference time and consumes less memory, making it an efficient and less memory-consuming architecture for pixel-wise segmentation on NVIDIA Pascal GPUs.",1
"Thalamic alterations are relevant to many neurological disorders including Alzheimer's disease, Parkinson's disease and multiple sclerosis. Routine interventions to improve symptom severity in movement disorders, for example, often consist of surgery or deep brain stimulation to diencephalic nuclei. Therefore, accurate delineation of grey matter thalamic subregions is of the upmost clinical importance. MRI is highly appropriate for structural segmentation as it provides different views of the anatomy from a single scanning session. Though with several contrasts potentially available, it is also of increasing importance to develop new image segmentation techniques that can operate multi-spectrally. We hereby propose a new segmentation method for use with multi-modality data, which we evaluated for automated segmentation of major thalamic subnuclear groups using T1-, T2*-weighted and quantitative susceptibility mapping (QSM) information. The proposed method consists of four steps: highly iterative image co-registration, manual segmentation on the average training-data template, supervised learning for pattern recognition, and a final convex optimisation step imposing further spatial constraints to refine the solution. This led to solutions in greater agreement with manual segmentation than the standard Morel atlas based approach. Furthermore, we show that the multi-contrast approach boosts segmentation performances. We then investigated whether prior knowledge using the training-template contours could further improve convex segmentation accuracy and robustness, which led to highly precise multi-contrast segmentations in single subjects. This approach can be extended to most 3D imaging data types and any region of interest discernible in single scans or multi-subject templates.",0
"Many neurological disorders, such as Alzheimer's disease, Parkinson's disease, and multiple sclerosis, are associated with changes in the thalamus. In order to alleviate symptoms, surgical interventions or deep brain stimulation of diencephalic nuclei are often employed for movement disorders. Therefore, it is crucial to accurately identify the different grey matter thalamic subregions. MRI is a suitable tool for structural segmentation as it provides multiple views of the anatomy from a single scanning session. However, to improve accuracy, new image segmentation techniques that can operate multi-spectrally are needed. We propose a new segmentation method for automated segmentation of major thalamic subnuclear groups using T1-, T2*-weighted, and quantitative susceptibility mapping (QSM) information. This method involves highly iterative image co-registration, manual segmentation on the average training-data template, supervised learning for pattern recognition, and a final convex optimisation step. Our approach leads to improved segmentation accuracy compared to the standard Morel atlas based approach and the multi-contrast approach further boosts segmentation performance. We also show that prior knowledge using the training-template contours can enhance segmentation accuracy and robustness. This method can be applied to most 3D imaging data types to identify any region of interest in single scans or multi-subject templates.",1
"Mesh labeling is the key problem of classifying the facets of a 3D mesh with a label among a set of possible ones. State-of-the-art methods model mesh labeling as a Markov Random Field over the facets. These algorithms map image segmentations to the mesh by minimizing an energy function that comprises a data term, a smoothness terms, and class-specific priors. The latter favor a labeling with respect to another depending on the orientation of the facet normals. In this paper we propose a novel energy term that acts as a prior, but does not require any prior knowledge about the scene nor scene-specific relationship among classes. It bootstraps from a coarse mapping of the 2D segmentations on the mesh, and it favors the facets to be labeled according to the statistics of the mesh normals in their neighborhood. We tested our approach against five different datasets and, even if we do not inject prior knowledge, our method adapts to the data and overcomes the state-of-the-art.",0
"The primary challenge in 3D mesh classification is assigning a label to each facet from a set of possible options, which is known as mesh labeling. Existing methods treat mesh labeling as a Markov Random Field and use algorithms to minimize an energy function that includes a data term, a smoothness term, and class-specific priors. These priors encourage labeling based on facet normals. Our paper introduces a new energy term that acts as a prior, but does not require prior knowledge about the scene or class relationships. Instead, it utilizes a coarse mapping of 2D segmentations on the mesh and favors labeling based on the mesh normals in the facet's neighborhood. We evaluated our approach on five datasets and found that, even without prior knowledge, it outperforms state-of-the-art methods due to its ability to adapt to the data.",1
"In recent years, Fully Convolutional Networks (FCN) has been widely used in various semantic segmentation tasks, including multi-modal remote sensing imagery. How to fuse multi-modal data to improve the segmentation performance has always been a research hotspot. In this paper, a novel end-toend fully convolutional neural network is proposed for semantic segmentation of natural color, infrared imagery and Digital Surface Models (DSM). It is based on a modified DeepUNet and perform the segmentation in a multi-task way. The channels are clustered into groups and processed on different task pipelines. After a series of segmentation and fusion, their shared features and private features are successfully merged together. Experiment results show that the feature fusion network is efficient. And our approach achieves good performance in ISPRS Semantic Labeling Contest (2D).",0
"FCNs have gained widespread use for semantic segmentation tasks, including multi-modal remote sensing imagery, in recent years. Improving segmentation performance through the fusion of multi-modal data has been a research focus. This paper proposes a new end-to-end fully convolutional neural network for natural color, infrared imagery, and DSM semantic segmentation. It modifies DeepUNet and performs segmentation through a multi-task approach. The channels are grouped and processed on various task pipelines, with shared and private features successfully merged after segmentation and fusion. Experimental results show the effectiveness of the feature fusion network, and our approach performs well in ISPRS Semantic Labeling Contest (2D).",1
"As melanoma diagnoses increase across the US, automated efforts to identify malignant lesions become increasingly of interest to the research community. Segmentation of dermoscopic images is the first step in this process, thus accuracy is crucial. Although techniques utilizing convolutional neural networks have been used in the past for lesion segmentation, we present a solution employing the recently published DeepLab 3, an atrous convolution method for image segmentation. Although the results produced by this run are not ideal, with a mean Jaccard index of 0.498, we believe that with further adjustments and modifications to the compatibility with the DeepLab code and with training on more powerful processing units, this method may achieve better results in future trials.",0
"The increasing number of melanoma cases in the US has led the research community to focus on automated methods for identifying cancerous skin lesions. The first step in this process is segmenting dermoscopic images with high accuracy. While convolutional neural networks have been used in the past, we propose a solution using the recently published DeepLab 3, which employs atrous convolution for image segmentation. Although the initial results are not ideal, with a mean Jaccard index of 0.498, we believe that with further adjustments and modifications, along with training on more powerful processing units, this method has the potential to produce better results in future trials.",1
"One of the problems on the way to successful implementation of neural networks is the quality of annotation. For instance, different annotators can annotate images in a different way and very often their decisions do not match exactly and in extreme cases are even mutually exclusive which results in noisy annotations and, consequently, inaccurate predictions.   To avoid that problem in the task of computed tomography (CT) imaging segmentation we propose a clearing algorithm for annotations. It consists of 3 stages:   - annotators scoring, which assigns a higher confidence level to better annotators;   - nodules scoring, which assigns a higher confidence level to nodules confirmed by good annotators;   - nodules merging, which aggregates annotations according to nodules confidence.   In general, the algorithm can be applied to many different tasks (namely, binary and multi-class semantic segmentation, and also with trivial adjustments to classification and regression) where there are several annotators labeling each image.",0
"The success of neural network implementation can be hindered by the issue of annotation quality. This is because different annotators can interpret images differently, leading to discrepancies in their decisions and ultimately producing inaccurate predictions. To address this issue in the context of computed tomography (CT) imaging segmentation, we propose a clearing algorithm for annotations. This algorithm involves three stages: annotator scoring to assign higher confidence to more reliable annotators, nodule scoring to assign higher confidence to nodules confirmed by trustworthy annotators, and nodule merging to aggregate annotations based on nodule confidence. This algorithm can be adapted for various tasks, including binary and multi-class semantic segmentation, classification, and regression, where multiple annotators are involved in image labeling.",1
"White matter hyperintensity (WMH) is commonly found in elder individuals and appears to be associated with brain diseases. U-net is a convolutional network that has been widely used for biomedical image segmentation. Recently, U-net has been successfully applied to WMH segmentation. Random initialization is usally used to initialize the model weights in the U-net. However, the model may coverage to different local optima with different randomly initialized weights. We find a combination of thresholding and averaging the outputs of U-nets with different random initializations can largely improve the WMH segmentation accuracy. Based on this observation, we propose a post-processing technique concerning the way how averaging and thresholding are conducted. Specifically, we first transfer the score maps from three U-nets to binary masks via thresholding and then average those binary masks to obtain the final WMH segmentation. Both quantitative analysis (via the Dice similarity coefficient) and qualitative analysis (via visual examinations) reveal the superior performance of the proposed method. This post-processing technique is independent of the model used. As such, it can also be applied to situations where other deep learning models are employed, especially when random initialization is adopted and pre-training is unavailable.",0
"WMH is often observed in older individuals and has been linked to brain disorders. U-net is a commonly used convolutional network for biomedical image segmentation and has been successful in WMH segmentation. Typically, U-net model weights are randomly initialized, but this can lead to different local optima. To improve accuracy, we propose a post-processing technique that combines thresholding and averaging of the outputs from U-nets with different random initializations. We transfer the score maps from three U-nets to binary masks using thresholding and then average those masks to obtain the final WMH segmentation. This technique is independent of the model used and can be applied when random initialization is used and pre-training is unavailable. Our proposed method outperforms other methods according to both quantitative and qualitative analyses.",1
"Convolutional neural networks are powerful tools for image segmentation and classification. Here, we use this method to identify and mark the heart region of Drosophila at different developmental stages in the cross-sectional images acquired by a custom optical coherence microscopy (OCM) system. With our well-trained convolutional neural network model, the heart regions through multiple heartbeat cycles can be marked with an intersection over union (IOU) of ~86%. Various morphological and dynamical cardiac parameters can be quantified accurately with automatically segmented heart regions. This study demonstrates an efficient heart segmentation method to analyze OCM images of the beating heart in Drosophila.",0
"The utilization of convolutional neural networks is a potent technique for image classification and segmentation. In this study, we apply this approach to identify and indicate the heart area of Drosophila in diverse developmental stages via cross-sectional images obtained from a customized optical coherence microscopy (OCM) system. By employing a well-trained convolutional neural network model, we can precisely mark the heart regions across numerous heartbeat cycles, achieving an intersection over union (IOU) of approximately 86%. This automated heart segmentation method allows for the precise quantification of different morphological and dynamical cardiac parameters. The findings of this study demonstrate an effective technique for heart segmentation that can be utilized to analyze OCM images of the beating heart in Drosophila.",1
"Deep neural network architectures have traditionally been designed and explored with human expertise in a long-lasting trial-and-error process. This process requires huge amount of time, expertise, and resources. To address this tedious problem, we propose a novel algorithm to optimally find hyperparameters of a deep network architecture automatically. We specifically focus on designing neural architectures for medical image segmentation task. Our proposed method is based on a policy gradient reinforcement learning for which the reward function is assigned a segmentation evaluation utility (i.e., dice index). We show the efficacy of the proposed method with its low computational cost in comparison with the state-of-the-art medical image segmentation networks. We also present a new architecture design, a densely connected encoder-decoder CNN, as a strong baseline architecture to apply the proposed hyperparameter search algorithm. We apply the proposed algorithm to each layer of the baseline architectures. As an application, we train the proposed system on cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017. Starting from a baseline segmentation architecture, the resulting network architecture obtains the state-of-the-art results in accuracy without performing any trial-and-error based architecture design approaches or close supervision of the hyperparameters changes.",0
"In the past, deep neural network architectures were developed through a time-consuming trial-and-error process, which required significant expertise and resources. To tackle this problem, we introduce an innovative algorithm that can automatically find the optimal hyperparameters for a deep network architecture. Our focus is on designing neural architectures for medical image segmentation tasks, and our approach involves using policy gradient reinforcement learning with a segmentation evaluation utility as the reward function. Our method is computationally efficient and outperforms the existing state-of-the-art medical image segmentation networks. We also propose a new architecture design, a densely connected encoder-decoder CNN, which serves as a strong baseline architecture for our hyperparameter search algorithm. By applying our algorithm to each layer of the baseline architectures, we obtained state-of-the-art accuracy on cine cardiac MR images from the Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017 dataset. Our approach does not require trial-and-error based architecture design approaches or close supervision of the hyperparameters changes.",1
"The Conditional Random Field as a Recurrent Neural Network layer is a recently proposed algorithm meant to be placed on top of an existing Fully-Convolutional Neural Network to improve the quality of semantic segmentation. In this paper, we test whether this algorithm, which was shown to improve semantic segmentation for 2D RGB images, is able to improve segmentation quality for 3D multi-modal medical images. We developed an implementation of the algorithm which works for any number of spatial dimensions, input/output image channels, and reference image channels. As far as we know this is the first publicly available implementation of this sort. We tested the algorithm with two distinct 3D medical imaging datasets, we concluded that the performance differences observed were not statistically significant. Finally, in the discussion section of the paper, we go into the reasons as to why this technique transfers poorly from natural images to medical images.",0
"Recently proposed, the Conditional Random Field as a Recurrent Neural Network layer is utilized to enhance the quality of semantic segmentation on an existing Fully-Convolutional Neural Network. This study examines the algorithm's effectiveness on 3D multi-modal medical images, following its proven success with 2D RGB images. Our implementation of the algorithm is adaptable to various spatial dimensions, input/output image channels, and reference image channels, which we believe to be the first publicly available. We conducted tests on two distinct 3D medical imaging datasets and found no statistically significant differences in performance. We discuss why this technique does not transfer well from natural to medical images in the paper's discussion section.",1
"In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.",0
"This paper introduces UNet++, a more robust architecture for medical image segmentation. Our architecture adopts a deeply-supervised encoder-decoder network that leverages nested, dense skip pathways to bridge the gap between the feature maps of the encoder and decoder sub-networks. Our redesigned skip pathways aim to reduce the semantic gap to facilitate easier learning for the optimizer. We evaluated UNet++ against U-Net and wide U-Net architectures across multiple medical image segmentation tasks, including nodule segmentation, nuclei segmentation, liver segmentation, and polyp segmentation. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.",1
"We propose a novel method for learning convolutional neural image representations without manual supervision. We use motion cues in the form of optical flow, to supervise representations of static images. The obvious approach of training a network to predict flow from a single image can be needlessly difficult due to intrinsic ambiguities in this prediction task. We instead propose a much simpler learning goal: embed pixels such that the similarity between their embeddings matches that between their optical flow vectors. At test time, the learned deep network can be used without access to video or flow information and transferred to tasks such as image classification, detection, and segmentation. Our method, which significantly simplifies previous attempts at using motion for self-supervision, achieves state-of-the-art results in self-supervision using motion cues, competitive results for self-supervision in general, and is overall state of the art in self-supervised pretraining for semantic image segmentation, as demonstrated on standard benchmarks.",0
"Our innovative approach for acquiring convolutional neural image representations does not require manual supervision. Instead, we utilize motion cues in the form of optical flow to supervise static image representations. Although training a network to predict flow from a single image is challenging due to inherent prediction task ambiguities, we offer a more straightforward learning objective. Our goal is to embed pixels, so their embeddings' similarity corresponds to their optical flow vectors. The deep network we develop can be applied during testing without access to video or flow information, allowing for image classification, detection, and segmentation tasks. Our process simplifies previous motion-based self-supervision efforts, achieves self-supervision results comparable to prior work, and is the leading self-supervised pretraining method for semantic image segmentation on standard benchmarks.",1
"Over the past decades, state-of-the-art medical image segmentation has heavily rested on signal processing paradigms, most notably registration-based label propagation and pair-wise patch comparison, which are generally slow despite a high segmentation accuracy. In recent years, deep learning has revolutionalized computer vision with many practices outperforming prior art, in particular the convolutional neural network (CNN) studies on image classification. Deep CNN has also started being applied to medical image segmentation lately, but generally involves long training and demanding memory requirements, achieving limited success. We propose a patch-based deep learning framework based on a revisit to the classic neural network model with substantial modernization, including the use of Rectified Linear Unit (ReLU) activation, dropout layers, 2.5D tri-planar patch multi-pathway settings. In a test application to hippocampus segmentation using 100 brain MR images from the ADNI database, our approach significantly outperformed prior art in terms of both segmentation accuracy and speed: scoring a median Dice score up to 90.98% on a near real-time performance (<1s).",0
"Medical image segmentation has relied on signal processing methods like registration-based label propagation and pair-wise patch comparison for many years, but these methods are often slow despite their high accuracy. While deep learning, especially the convolutional neural network (CNN), has revolutionized computer vision and surpassed previous techniques in image classification, its application to medical image segmentation has been limited by long training and high memory requirements. However, we propose a new patch-based deep learning framework that updates the classic neural network model with modern features like ReLU activation, dropout layers, and 2.5D tri-planar patch multi-pathway settings. Our approach was tested on hippocampus segmentation using 100 brain MR images from the ADNI database and achieved significantly better segmentation accuracy and speed than previous methods, with a median Dice score of up to 90.98% in near real-time (<1s).",1
Semantic segmentation of medical images is a crucial step for the quantification of healthy anatomy and diseases alike. The majority of the current state-of-the-art segmentation algorithms are based on deep neural networks and rely on large datasets with full pixel-wise annotations. Producing such annotations can often only be done by medical professionals and requires large amounts of valuable time. Training a medical image segmentation network with weak annotations remains a relatively unexplored topic. In this work we investigate training strategies to learn the parameters of a pixel-wise segmentation network from scribble annotations alone. We evaluate the techniques on public cardiac (ACDC) and prostate (NCI-ISBI) segmentation datasets. We find that the networks trained on scribbles suffer from a remarkably small degradation in Dice of only 2.9% (cardiac) and 4.5% (prostate) with respect to a network trained on full annotations.,0
"The quantification of healthy anatomy and diseases through semantic segmentation of medical images is essential. Most advanced segmentation algorithms are based on deep neural networks and require large datasets with full pixel-wise annotations, which can only be produced by medical professionals and take up valuable time. However, training a medical image segmentation network with weak annotations is an under-researched topic. This study explores the use of scribble annotations to train a pixel-wise segmentation network and evaluates the techniques on public cardiac (ACDC) and prostate (NCI-ISBI) segmentation datasets. The results show that the networks trained on scribbles only have a small degradation in Dice compared to those trained on full annotations, with only 2.9% (cardiac) and 4.5% (prostate) reduction.",1
"Unpaired image-to-image translation is the problem of mapping an image in the source domain to one in the target domain, without requiring corresponding image pairs. To ensure the translated images are realistically plausible, recent works, such as Cycle-GAN, demands this mapping to be invertible. While, this requirement demonstrates promising results when the domains are unimodal, its performance is unpredictable in a multi-modal scenario such as in an image segmentation task. This is because, invertibility does not necessarily enforce semantic correctness. To this end, we present a semantically-consistent GAN framework, dubbed Sem-GAN, in which the semantics are defined by the class identities of image segments in the source domain as produced by a semantic segmentation algorithm. Our proposed framework includes consistency constraints on the translation task that, together with the GAN loss and the cycle-constraints, enforces that the images when translated will inherit the appearances of the target domain, while (approximately) maintaining their identities from the source domain. We present experiments on several image-to-image translation tasks and demonstrate that Sem-GAN improves the quality of the translated images significantly, sometimes by more than 20% on the FCN score. Further, we show that semantic segmentation models, trained with synthetic images translated via Sem-GAN, leads to significantly better segmentation results than other variants.",0
"The problem of unpaired image-to-image translation involves mapping an image from the source domain to the target domain without requiring corresponding pairs. To ensure that the images produced are realistically plausible, recent works like Cycle-GAN require the mapping to be invertible. However, this requirement may not guarantee semantic correctness, especially in multi-modal scenarios like image segmentation tasks. Therefore, we propose a semantically-consistent GAN framework called Sem-GAN, which defines semantics based on the class identities of image segments in the source domain produced by a semantic segmentation algorithm. Our framework includes consistency constraints, GAN loss, and cycle-constraints, which enforce that the translated images inherit the target domain's appearances while maintaining their identities from the source domain. We conducted experiments on various image-to-image translation tasks and observed a significant improvement in the quality of the translated images, sometimes by more than 20% on the FCN score. Additionally, we demonstrate that Sem-GAN produces better segmentation results when used to train semantic segmentation models with synthetic images.",1
"Herein, we present a system for hyperspectral image segmentation that utilizes multiple class--based denoising autoencoders which are efficiently trained. Moreover, we present a novel hyperspectral data augmentation method for labelled HSI data using linear mixtures of pixels from each class, which helps the system with edge pixels which are almost always mixed pixels. Finally, we utilize a deep neural network and morphological hole-filling to provide robust image classification. Results run on the Salinas dataset verify the high performance of the proposed algorithm.",0
"In this article, we introduce a hyperspectral image segmentation system that utilizes multiple class-based denoising autoencoders. These autoencoders are trained efficiently. Additionally, we present a new method of hyperspectral data augmentation for labeled HSI data. This method involves linear mixtures of pixels from each class and is useful for edge pixels that are typically mixed pixels. Lastly, we utilize a deep neural network and morphological hole-filling to provide reliable image classification. The results obtained from the Salinas dataset confirm the excellent performance of our algorithm.",1
"This paper presents a method for segmenting iris images obtained from the deceased subjects, by training a deep convolutional neural network (DCNN) designed for the purpose of semantic segmentation. Post-mortem iris recognition has recently emerged as an alternative, or additional, method useful in forensic analysis. At the same time it poses many new challenges from the technological standpoint, one of them being the image segmentation stage, which has proven difficult to be reliably executed by conventional iris recognition methods. Our approach is based on the SegNet architecture, fine-tuned with 1,300 manually segmented post-mortem iris images taken from the Warsaw-BioBase-Post-Mortem-Iris v1.0 database. The experiments presented in this paper show that this data-driven solution is able to learn specific deformations present in post-mortem samples, which are missing from alive irises, and offers a considerable improvement over the state-of-the-art, conventional segmentation algorithm (OSIRIS): the Intersection over Union (IoU) metric was improved from 73.6% (for OSIRIS) to 83% (for DCNN-based presented in this paper) averaged over subject-disjoint, multiple splits of the data into train and test subsets. This paper offers the first known to us method of automatic processing of post-mortem iris images. We offer source codes with the trained DCNN that perform end-to-end segmentation of post-mortem iris images, as described in this paper. Also, we offer binary masks corresponding to manual segmentation of samples from Warsaw-BioBase-Post-Mortem-Iris v1.0 database to facilitate development of alternative methods for post-mortem iris segmentation.",0
"The aim of this study is to segment iris images obtained from deceased individuals. The authors propose a data-driven solution using a deep convolutional neural network (DCNN) based on the SegNet architecture, which is designed for semantic segmentation. This approach addresses the challenges faced with conventional iris recognition methods, particularly in the image segmentation stage. The proposed method is fine-tuned using 1,300 manually segmented post-mortem iris images from the Warsaw-BioBase-Post-Mortem-Iris v1.0 database. The results demonstrate that the DCNN-based method offers a significant improvement over the state-of-the-art conventional segmentation algorithm (OSIRIS), with an Intersection over Union (IoU) metric of 83% compared to 73.6% for OSIRIS. This study presents the first known method for automatic processing of post-mortem iris images and provides source codes and binary masks for alternative methods development.",1
"Yield and its prediction is one of the most important tasks in grapevine breeding purposes and vineyard management. Commonly, this trait is estimated manually right before harvest by extrapolation, which mostly is labor-intensive, destructive and inaccurate. In the present study an automated image-based workflow was developed quantifying inflorescences and single flowers in unprepared field images of grapevines, i.e. no artificial background or light was applied. It is a novel approach for non-invasive, inexpensive and objective phenotyping with high-throughput.   First, image regions depicting inflorescences were identified and localized. This was done by segmenting the images into the classes ""inflorescence"" and ""non-inflorescence"" using a Fully Convolutional Network (FCN). Efficient image segmentation hereby is the most challenging step regarding the small geometry and dense distribution of flowers (several hundred flowers per inflorescence), similar color of all plant organs in the fore- and background as well as the circumstance that only approximately 5% of an image show inflorescences. The trained FCN achieved a mean Intersection Over Union (IOU) of 87.6% on the test data set. Finally, individual flowers were extracted from the ""inflorescence""-areas using Circular Hough Transform. The flower extraction achieved a recall of 80.3% and a precision of 70.7% using the segmentation derived by the trained FCN model.   Summarized, the presented approach is a promising strategy in order to predict yield potential automatically in the earliest stage of grapevine development which is applicable for objective monitoring and evaluations of breeding material, genetic repositories or commercial vineyards.",0
"In grapevine breeding and vineyard management, predicting yield is very important. Manual estimation of this trait is commonly used but it is labor-intensive, destructive and inaccurate. In this study, a new approach was developed using automated image-based workflow to quantify inflorescences and single flowers in unprepared field images of grapevines. This approach is non-invasive, inexpensive and objective. A Fully Convolutional Network (FCN) was used to segment images into ""inflorescence"" and ""non-inflorescence"" classes. The trained FCN achieved a mean Intersection Over Union (IOU) of 87.6% on the test data set. Individual flowers were then extracted using Circular Hough Transform with a recall of 80.3% and a precision of 70.7% using the segmentation derived by the trained FCN model. This approach is a promising strategy in predicting yield potential automatically and can be applied for objective monitoring and evaluations of breeding material, genetic repositories or commercial vineyards.",1
"Training deep neural networks on large and sparse datasets is still challenging and can require large amounts of computation and memory. In this work, we address the task of performing semantic segmentation on large volumetric data sets, such as CT scans. Our contribution is threefold: 1) We propose a boosted sampling scheme that uses a-posterior error maps, generated throughout training, to focus sampling on difficult regions, resulting in a more informative loss. This results in a significant training speed up and improves learning performance for image segmentation. 2) We propose a novel algorithm for boosting the SGD learning rate schedule by adaptively increasing and lowering the learning rate, avoiding the need for extensive hyperparameter tuning. 3) We show that our method is able to attain new state-of-the-art results on the VISCERAL Anatomy benchmark.",0
"Performing semantic segmentation on large volumetric data sets, such as CT scans, remains a challenge due to their sparsity and size, which can necessitate substantial computational and memory resources. This study proposes three contributions to tackle this issue. Firstly, a boosted sampling scheme is introduced that utilizes a-posterior error maps generated during training to pinpoint difficult regions, resulting in a more informative loss. This approach significantly improves learning performance and expedites training. Secondly, a novel algorithm is presented for boosting the SGD learning rate schedule by adaptively increasing and decreasing the learning rate, negating the need for extensive hyperparameter tuning. Finally, the study demonstrates that the proposed method achieves new state-of-the-art results on the VISCERAL Anatomy benchmark.",1
"We develop three efficient approaches for generating visual explanations from 3D convolutional neural networks (3D-CNNs) for Alzheimer's disease classification. One approach conducts sensitivity analysis on hierarchical 3D image segmentation, and the other two visualize network activations on a spatial map. Visual checks and a quantitative localization benchmark indicate that all approaches identify important brain parts for Alzheimer's disease diagnosis. Comparative analysis show that the sensitivity analysis based approach has difficulty handling loosely distributed cerebral cortex, and approaches based on visualization of activations are constrained by the resolution of the convolutional layer. The complementarity of these methods improves the understanding of 3D-CNNs in Alzheimer's disease classification from different perspectives.",0
"We have devised three efficient methods for generating visual explanations from 3D convolutional neural networks (3D-CNNs) that aid in the classification of Alzheimer's disease. The first method involves conducting sensitivity analysis on a hierarchical 3D image segmentation, while the other two methods visualize network activations on a spatial map. Our visual checks and quantitative localization benchmark prove that all methods successfully identify critical brain regions for Alzheimer's disease diagnosis. However, our comparative analysis reveals that the sensitivity analysis-based approach struggles with loosely distributed cerebral cortex, and the activation visualization methods have limitations due to the convolutional layer's resolution. The complementary nature of these methods enhances our understanding of 3D-CNNs in Alzheimer's disease classification from various perspectives.",1
"Liver cancer is one of the leading causes of cancer death. To assist doctors in hepatocellular carcinoma diagnosis and treatment planning, an accurate and automatic liver and tumor segmentation method is highly demanded in the clinical practice. Recently, fully convolutional neural networks (FCNs), including 2D and 3D FCNs, serve as the back-bone in many volumetric image segmentation. However, 2D convolutions can not fully leverage the spatial information along the third dimension while 3D convolutions suffer from high computational cost and GPU memory consumption. To address these issues, we propose a novel hybrid densely connected UNet (H-DenseUNet), which consists of a 2D DenseUNet for efficiently extracting intra-slice features and a 3D counterpart for hierarchically aggregating volumetric contexts under the spirit of the auto-context algorithm for liver and tumor segmentation. We formulate the learning process of H-DenseUNet in an end-to-end manner, where the intra-slice representations and inter-slice features can be jointly optimized through a hybrid feature fusion (HFF) layer. We extensively evaluated our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge and 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on the segmentation results of tumors and achieved very competitive performance for liver segmentation even with a single model.",0
"Liver cancer is a leading cause of cancer-related deaths, and there is a high demand for an accurate and automatic liver and tumor segmentation method to aid in diagnosis and treatment planning for hepatocellular carcinoma. Fully convolutional neural networks (FCNs) have become popular for volumetric image segmentation, but 2D convolutions fail to capture spatial information along the third dimension, and 3D convolutions are computationally expensive and consume a lot of GPU memory. To overcome these limitations, a novel hybrid densely connected UNet (H-DenseUNet) has been proposed. The H-DenseUNet comprises a 2D DenseUNet for efficient intra-slice feature extraction and a 3D counterpart for hierarchical aggregation of volumetric contexts. The learning process is formulated in an end-to-end manner, with joint optimization of intra-slice representations and inter-slice features using a hybrid feature fusion (HFF) layer. The proposed method was evaluated on the MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge and 3DIRCADb Dataset, outperforming other state-of-the-art methods on tumor segmentation results and achieving competitive performance for liver segmentation with a single model.",1
"Cross modal image syntheses is gaining significant interests for its ability to estimate target images of a different modality from a given set of source images,like estimating MR to MR, MR to CT, CT to PET etc, without the need for an actual acquisition.Though they show potential for applications in radiation therapy planning,image super resolution, atlas construction, image segmentation etc.The synthesis results are not as accurate as the actual acquisition.In this paper,we address the problem of multi modal image synthesis by proposing a fully convolutional deep learning architecture called the SynNet.We extend the proposed architecture for various input output configurations. And finally, we propose a structure preserving custom loss function for cross-modal image synthesis.We validate the proposed SynNet and its extended framework on BRATS dataset with comparisons against three state-of-the art methods.And the results of the proposed custom loss function is validated against the traditional loss function used by the state-of-the-art methods for cross modal image synthesis.",0
"There is growing interest in cross modal image syntheses, which involves estimating target images of a different modality from a given set of source images (such as MR to MR, MR to CT, CT to PET, etc.) without the need for actual acquisition. While this technique shows potential for applications in radiation therapy planning, image super resolution, atlas construction, image segmentation, etc., the synthesized results are not as accurate as actual acquisition. The present study proposes a fully convolutional deep learning architecture called SynNet to address the problem of multi-modal image synthesis. The proposed architecture is extended for various input-output configurations, and a structure-preserving custom loss function is proposed for cross-modal image synthesis. The proposed SynNet and its extended framework are validated on the BRATS dataset with comparisons against three state-of-the-art methods. Furthermore, the proposed custom loss function is compared against the traditional loss function used by the state-of-the-art methods for cross modal image synthesis.",1
"Deep neural networks (DNNs) have become increasingly important due to their excellent empirical performance on a wide range of problems. However, regularization is generally achieved by indirect means, largely due to the complex set of functions defined by a network and the difficulty in measuring function complexity. There exists no method in the literature for additive regularization based on a norm of the function, as is classically considered in statistical learning theory. In this work, we propose sampling-based approximations to weighted function norms as regularizers for deep neural networks. We provide, to the best of our knowledge, the first proof in the literature of the NP-hardness of computing function norms of DNNs, motivating the necessity of an approximate approach. We then derive a generalization bound for functions trained with weighted norms and prove that a natural stochastic optimization strategy minimizes the bound. Finally, we empirically validate the improved performance of the proposed regularization strategies for both convex function sets as well as DNNs on real-world classification and image segmentation tasks demonstrating improved performance over weight decay, dropout, and batch normalization. Source code will be released at the time of publication.",0
"The excellent empirical performance of deep neural networks (DNNs) has made them increasingly important in solving a wide range of problems. However, regularization is often achieved indirectly due to the complex functions defined by a network and the difficulty in measuring function complexity. Unlike in statistical learning theory, there is no method for additive regularization based on a norm of the function. In this study, we introduce sampling-based approximations to weighted function norms as regularizers for DNNs. Our research provides the first proof of the NP-hardness of computing function norms of DNNs, highlighting the need for an approximate method. We also derive a generalization bound for functions trained with weighted norms and prove that a natural stochastic optimization strategy minimizes the bound. Finally, we demonstrate through empirical validation that our regularization strategies improve the performance of both convex function sets and DNNs on real-world classification and image segmentation tasks, outperforming weight decay, dropout, and batch normalization. We will release the source code at the time of publication.",1
"One of the time-consuming routine work for a radiologist is to discern anatomical structures from tomographic images. For assisting radiologists, this paper develops an automatic segmentation method for pelvic magnetic resonance (MR) images. The task has three major challenges 1) A pelvic organ can have various sizes and shapes depending on the axial image, which requires local contexts to segment correctly. 2) Different organs often have quite similar appearance in MR images, which requires global context to segment. 3) The number of available annotated images are very small to use the latest segmentation algorithms. To address the challenges, we propose a novel convolutional neural network called Attention-Pyramid network (APNet) that effectively exploits both local and global contexts, in addition to a data-augmentation technique that is particularly effective for MR images. In order to evaluate our method, we construct fine-grained (50 pelvic organs) MR image segmentation dataset, and experimentally confirm the superior performance of our techniques over the state-of-the-art image segmentation methods.",0
"One of the most time-consuming tasks for radiologists is distinguishing anatomical structures from tomographic images. To aid in this process, a new method for automatically segmenting pelvic magnetic resonance (MR) images has been developed. This task presents three significant challenges: 1) pelvic organs can vary in size and shape depending on the axial image, requiring local contexts for proper segmentation, 2) different organs can appear similar in MR images, necessitating global context for segmentation, and 3) the number of annotated images available is too small to use the latest segmentation algorithms. To overcome these challenges, a novel convolutional neural network, called Attention-Pyramid network (APNet), has been proposed. APNet effectively utilizes both local and global contexts, along with a data-augmentation technique that is particularly effective for MR images. To evaluate the method, a fine-grained MR image segmentation dataset (with 50 pelvic organs) was constructed, and the superior performance of APNet over state-of-the-art image segmentation methods was experimentally confirmed.",1
"We present a simple solution for segmenting grayscale images using existing Connected Component Labeling (CCL) algorithms (which are generally applied to binary images), which was efficient enough to be implemented in a constrained (embedded automotive) architecture. Our solution customizes the region growing and merging approach, and is primarily targeted for stereoscopic disparity images where nearer objects carry more relevance. We provide results from a standard OpenCV implementation for some basic cases and an image from the Tsukuba stereo-pair dataset.",0
"Our solution offers an uncomplicated method for segmenting grayscale images through the utilization of pre-existing Connected Component Labeling (CCL) algorithms, which are typically utilized for binary images. The implementation of our solution is efficient enough for use in a limited (embedded automotive) architecture. The approach we have developed for region growing and merging is tailored for stereoscopic disparity images, with a focus on objects that are closer having greater significance. We have included outcomes from a standard OpenCV implementation for a few fundamental cases, as well as an image from the Tsukuba stereo-pair dataset.",1
"We introduce an approach for image segmentation based on sparse correspondences between keypoints in testing and training images. Keypoints represent automatically identified distinctive image locations, where each keypoint correspondence suggests a transformation between images. We use these correspondences to transfer label maps of entire organs from the training images to the test image. The keypoint transfer algorithm includes three steps: (i) keypoint matching, (ii) voting-based keypoint labeling, and (iii) keypoint-based probabilistic transfer of organ segmentations. We report segmentation results for abdominal organs in whole-body CT and MRI, as well as in contrast-enhanced CT and MRI. Our method offers a speed-up of about three orders of magnitude in comparison to common multi-atlas segmentation, while achieving an accuracy that compares favorably. Moreover, keypoint transfer does not require the registration to an atlas or a training phase. Finally, the method allows for the segmentation of scans with highly variable field-of-view.",0
"Our proposed image segmentation approach relies on sparse correspondences between keypoints in the testing and training images. These keypoints represent distinctive locations in the image and their correspondence suggests a transformation between images. We utilize these correspondences to transfer label maps of entire organs from the training images to the test image. This keypoint transfer algorithm consists of three steps: keypoint matching, voting-based keypoint labeling, and keypoint-based probabilistic transfer of organ segmentations. We have demonstrated the effectiveness of this approach in segmenting abdominal organs in whole-body CT and MRI, as well as in contrast-enhanced CT and MRI. Our method offers a significant speed-up compared to common multi-atlas segmentation, while maintaining high accuracy. Additionally, it does not require registration to an atlas or a training phase, and is capable of segmenting scans with varying field-of-view.",1
"Automated medical image segmentation, specifically using deep learning, has shown outstanding performance in semantic segmentation tasks. However, these methods rarely quantify their uncertainty, which may lead to errors in downstream analysis. In this work we propose to use Bayesian neural networks to quantify uncertainty within the domain of semantic segmentation. We also propose a method to convert voxel-wise segmentation uncertainty into volumetric uncertainty, and calibrate the accuracy and reliability of confidence intervals of derived measurements. When applied to a tumour volume estimation application, we demonstrate that by using such modelling of uncertainty, deep learning systems can be made to report volume estimates with well-calibrated error-bars, making them safer for clinical use. We also show that the uncertainty estimates extrapolate to unseen data, and that the confidence intervals are robust in the presence of artificial noise. This could be used to provide a form of quality control and quality assurance, and may permit further adoption of deep learning tools in the clinic.",0
"The use of deep learning in automated medical image segmentation has been highly effective in semantic segmentation tasks. However, these methods often lack the ability to measure uncertainty, which can result in errors during downstream analysis. To address this issue, we suggest implementing Bayesian neural networks to quantify uncertainty in semantic segmentation. We also propose a method to convert voxel-wise segmentation uncertainty into volumetric uncertainty and ensure the accuracy and reliability of confidence intervals of derived measurements. By applying this approach to a tumor volume estimation application, we demonstrate that deep learning systems can report volume estimates with well-calibrated error-bars, making them safer for clinical use. Additionally, we prove that the uncertainty estimates can be extrapolated to unseen data and that the confidence intervals remain robust in the presence of artificial noise. This technique offers a means of quality control and quality assurance that could encourage further use of deep learning tools in clinical settings.",1
"Mucous glands lesions analysis and assessing of malignant potential of colon polyps are very important tasks of surgical pathology. However, differential diagnosis of colon polyps often seems impossible by classical methods and it is necessary to involve computer methods capable of assessing minimal differences to extend the capabilities of the classical pathology examination. Accurate segmentation of mucous glands from histology images is a crucial step to obtain reliable morphometric criteria for quantitative diagnostic methods. We review major trends in histological images segmentation and design a new convolutional neural network for mucous gland segmentation.",0
"The examination of lesions in mucous glands and evaluating the possibility of colon polyps becoming malignant are crucial responsibilities for surgical pathology. Despite the difficulty in distinguishing between different types of colon polyps through traditional methods, it is essential to employ computer techniques that can detect even the slightest variances to enhance the capabilities of pathology examinations. It is imperative to accurately isolate mucous glands from histology images to obtain dependable morphometric parameters for quantitative diagnostic procedures. We examine the primary trends in histological image segmentation and introduce a novel convolutional neural network for the segmentation of mucous glands.",1
"Myocardial contrast echocardiography (MCE) is an imaging technique that assesses left ventricle function and myocardial perfusion for the detection of coronary artery diseases. Automatic MCE perfusion quantification is challenging and requires accurate segmentation of the myocardium from noisy and time-varying images. Random forests (RF) have been successfully applied to many medical image segmentation tasks. However, the pixel-wise RF classifier ignores contextual relationships between label outputs of individual pixels. RF which only utilizes local appearance features is also susceptible to data suffering from large intensity variations. In this paper, we demonstrate how to overcome the above limitations of classic RF by presenting a fully automatic segmentation pipeline for myocardial segmentation in full-cycle 2D MCE data. Specifically, a statistical shape model is used to provide shape prior information that guide the RF segmentation in two ways. First, a novel shape model (SM) feature is incorporated into the RF framework to generate a more accurate RF probability map. Second, the shape model is fitted to the RF probability map to refine and constrain the final segmentation to plausible myocardial shapes. We further improve the performance by introducing a bounding box detection algorithm as a preprocessing step in the segmentation pipeline. Our approach on 2D image is further extended to 2D+t sequence which ensures temporal consistency in the resultant sequence segmentations. When evaluated on clinical MCE data, our proposed method achieves notable improvement in segmentation accuracy and outperforms other state-of-the-art methods including the classic RF and its variants, active shape model and image registration.",0
"Myocardial contrast echocardiography (MCE) is a type of imaging that examines left ventricle function and myocardial perfusion to identify coronary artery diseases. Automatic MCE perfusion quantification is difficult because of the need for precise segmentation of the myocardium from noisy and time-varying images. Although random forests (RF) have been successful in many medical image segmentation tasks, their pixel-wise classifier does not consider contextual relationships between label outputs of individual pixels. Additionally, the RF is vulnerable to data with significant intensity variations. This paper presents an automatic segmentation pipeline for myocardial segmentation in full-cycle 2D MCE data that overcomes the limitations of classic RF. The pipeline employs a statistical shape model to provide shape prior information that guides the RF segmentation in two ways. First, a novel shape model (SM) feature is integrated into the RF framework to generate a more precise RF probability map. Second, the shape model is fitted to the RF probability map to refine and constrain the final segmentation to plausible myocardial shapes. To further enhance performance, a bounding box detection algorithm is introduced as a preprocessing step in the segmentation pipeline. This approach is extended to 2D+t sequence to ensure temporal consistency in the resultant sequence segmentations. Our proposed method achieves notable improvement in segmentation accuracy when evaluated on clinical MCE data and outperforms other state-of-the-art methods such as classic RF, active shape model, and image registration.",1
"Convolutional networks (ConvNets) have achieved great successes in various challenging vision tasks. However, the performance of ConvNets would degrade when encountering the domain shift. The domain adaptation is more significant while challenging in the field of biomedical image analysis, where cross-modality data have largely different distributions. Given that annotating the medical data is especially expensive, the supervised transfer learning approaches are not quite optimal. In this paper, we propose an unsupervised domain adaptation framework with adversarial learning for cross-modality biomedical image segmentations. Specifically, our model is based on a dilated fully convolutional network for pixel-wise prediction. Moreover, we build a plug-and-play domain adaptation module (DAM) to map the target input to features which are aligned with source domain feature space. A domain critic module (DCM) is set up for discriminating the feature space of both domains. We optimize the DAM and DCM via an adversarial loss without using any target domain label. Our proposed method is validated by adapting a ConvNet trained with MRI images to unpaired CT data for cardiac structures segmentations, and achieved very promising results.",0
"Convolutional networks (ConvNets) have been highly successful in many challenging vision tasks. However, their performance deteriorates when faced with domain shifts, which is particularly problematic in biomedical image analysis due to the significant differences in cross-modality data distributions. Because medical data annotation is costly, supervised transfer learning approaches are not ideal. To address this issue, we present an unsupervised domain adaptation framework that utilizes adversarial learning to segment cross-modality biomedical images. Our approach employs a dilated fully convolutional network for pixel-wise prediction and includes a plug-and-play domain adaptation module (DAM) that maps target input to features aligned with the source domain feature space. We also incorporate a domain critic module (DCM) to distinguish between both domains' feature spaces. We optimize the DAM and DCM using an adversarial loss, without any target domain labels. Our proposed method was tested by adapting an MRI-trained ConvNet to unpaired CT data for cardiac structure segmentation, achieving very promising results.",1
"Learning inter-domain mappings from unpaired data can improve performance in structured prediction tasks, such as image segmentation, by reducing the need for paired data. CycleGAN was recently proposed for this problem, but critically assumes the underlying inter-domain mapping is approximately deterministic and one-to-one. This assumption renders the model ineffective for tasks requiring flexible, many-to-many mappings. We propose a new model, called Augmented CycleGAN, which learns many-to-many mappings between domains. We examine Augmented CycleGAN qualitatively and quantitatively on several image datasets.",0
"Improving performance in structured prediction tasks, such as image segmentation, can be achieved by learning inter-domain mappings from unpaired data, which reduces the need for paired data. While CycleGAN was developed to address this issue, it assumes an approximately deterministic and one-to-one underlying inter-domain mapping. Therefore, it is not effective for tasks that require flexible, many-to-many mappings. To overcome this limitation, we introduce a new model named Augmented CycleGAN, which can learn many-to-many mappings between domains. We evaluate Augmented CycleGAN both qualitatively and quantitatively on various image datasets.",1
"Semantic image segmentation plays an important role in modeling patient-specific anatomy. We propose a convolution neural network, called Kid-Net, along with a training schema to segment kidney vessels: artery, vein and collecting system. Such segmentation is vital during the surgical planning phase in which medical decisions are made before surgical incision. Our main contribution is developing a training schema that handles unbalanced data, reduces false positives and enables high-resolution segmentation with a limited memory budget. These objectives are attained using dynamic weighting, random sampling and 3D patch segmentation. Manual medical image annotation is both time-consuming and expensive. Kid-Net reduces kidney vessels segmentation time from matter of hours to minutes. It is trained end-to-end using 3D patches from volumetric CT-images. A complete segmentation for a 512x512x512 CT-volume is obtained within a few minutes (1-2 mins) by stitching the output 3D patches together. Feature down-sampling and up-sampling are utilized to achieve higher classification and localization accuracies. Quantitative and qualitative evaluation results on a challenging testing dataset show Kid-Net competence.",0
"The segmentation of kidney vessels, including the artery, vein and collecting system, is a crucial aspect of surgical planning for patient-specific anatomy. To achieve this, we have developed a convolution neural network named Kid-Net, which utilizes a novel training schema. Our approach addresses the challenges of unbalanced data, false positives and limited memory budget, by employing dynamic weighting, random sampling and 3D patch segmentation. The time-consuming and costly process of manual medical image annotation is reduced significantly by Kid-Net, which can segment kidney vessels within minutes. The network is trained end-to-end using volumetric CT-images and achieves higher classification and localization accuracies with feature down-sampling and up-sampling. Our evaluation results demonstrate the effectiveness of Kid-Net in accurately segmenting kidney vessels.",1
"Brain extraction is a fundamental step for most brain imaging studies. In this paper, we investigate the problem of skull stripping and propose complementary segmentation networks (CompNets) to accurately extract the brain from T1-weighted MRI scans, for both normal and pathological brain images. The proposed networks are designed in the framework of encoder-decoder networks and have two pathways to learn features from both the brain tissue and its complementary part located outside of the brain. The complementary pathway extracts the features in the non-brain region and leads to a robust solution to brain extraction from MRIs with pathologies, which do not exist in our training dataset. We demonstrate the effectiveness of our networks by evaluating them on the OASIS dataset, resulting in the state of the art performance under the two-fold cross-validation setting. Moreover, the robustness of our networks is verified by testing on images with introduced pathologies and by showing its invariance to unseen brain pathologies. In addition, our complementary network design is general and can be extended to address other image segmentation problems with better generalization.",0
"Most brain imaging studies require the extraction of the brain as a crucial step. This study focuses on skull stripping and introduces Complementary Segmentation Networks (CompNets) that accurately extract the brain from T1-weighted MRI scans of both normal and pathological brains. The CompNets are designed based on the encoder-decoder framework, featuring two pathways to learn features from both the brain tissue and the non-brain region outside of it. The complementary pathway extracts features from the non-brain region and provides a robust solution to extract brains from MRIs with pathologies that were not included in the training dataset. The effectiveness of the proposed networks is demonstrated by evaluating them on the OASIS dataset, where they achieve state-of-the-art performance under a two-fold cross-validation setting. The networks' robustness is also confirmed by testing them on images with introduced pathologies and showing their invariance to unseen brain pathologies. Furthermore, the complementary network design is general and can be applied to address other image segmentation problems with better generalizability.",1
"Recent advances in deep learning based image segmentation methods have enabled real-time performance with human-level accuracy. However, occasionally even the best method fails due to low image quality, artifacts or unexpected behaviour of black box algorithms. Being able to predict segmentation quality in the absence of ground truth is of paramount importance in clinical practice, but also in large-scale studies to avoid the inclusion of invalid data in subsequent analysis.   In this work, we propose two approaches of real-time automated quality control for cardiovascular MR segmentations using deep learning. First, we train a neural network on 12,880 samples to predict Dice Similarity Coefficients (DSC) on a per-case basis. We report a mean average error (MAE) of 0.03 on 1,610 test samples and 97% binary classification accuracy for separating low and high quality segmentations. Secondly, in the scenario where no manually annotated data is available, we train a network to predict DSC scores from estimated quality obtained via a reverse testing strategy. We report an MAE=0.14 and 91% binary classification accuracy for this case. Predictions are obtained in real-time which, when combined with real-time segmentation methods, enables instant feedback on whether an acquired scan is analysable while the patient is still in the scanner. This further enables new applications of optimising image acquisition towards best possible analysis results.",0
"Advancements in deep learning techniques for image segmentation have resulted in real-time performance with human-level accuracy. However, there are instances where even the most reliable methods fail due to factors such as low image quality, artifacts, or unpredictable behavior of black box algorithms. Predicting segmentation quality without ground truth is crucial in clinical practice and large-scale studies to avoid invalid data in subsequent analysis. This study proposes two real-time automated quality control approaches for cardiovascular MR segmentations using deep learning. The first approach involves training a neural network on 12,880 samples to predict Dice Similarity Coefficients (DSC) on a per-case basis, resulting in 97% binary classification accuracy for separating low and high quality segmentations. The second approach involves training a network to predict DSC scores from estimated quality obtained via a reverse testing strategy, resulting in 91% binary classification accuracy. These predictions are obtained in real-time, allowing for instant feedback on whether an acquired scan is analyzable while the patient is still in the scanner, leading to new applications of optimizing image acquisition for better analysis results.",1
"Automatic parsing of anatomical objects in X-ray images is critical to many clinical applications in particular towards image-guided invention and workflow automation. Existing deep network models require a large amount of labeled data. However, obtaining accurate pixel-wise labeling in X-ray images relies heavily on skilled clinicians due to the large overlaps of anatomy and the complex texture patterns. On the other hand, organs in 3D CT scans preserve clearer structures as well as sharper boundaries and thus can be easily delineated. In this paper, we propose a novel model framework for learning automatic X-ray image parsing from labeled CT scans. Specifically, a Dense Image-to-Image network (DI2I) for multi-organ segmentation is first trained on X-ray like Digitally Reconstructed Radiographs (DRRs) rendered from 3D CT volumes. Then we introduce a Task Driven Generative Adversarial Network (TD-GAN) architecture to achieve simultaneous style transfer and parsing for unseen real X-ray images. TD-GAN consists of a modified cycle-GAN substructure for pixel-to-pixel translation between DRRs and X-ray images and an added module leveraging the pre-trained DI2I to enforce segmentation consistency. The TD-GAN framework is general and can be easily adapted to other learning tasks. In the numerical experiments, we validate the proposed model on 815 DRRs and 153 topograms. While the vanilla DI2I without any adaptation fails completely on segmenting the topograms, the proposed model does not require any topogram labels and is able to provide a promising average dice of 85% which achieves the same level accuracy of supervised training (88%).",0
"Parsing anatomical objects in X-ray images automatically is crucial for various clinical applications, especially for image-guided intervention and workflow automation. However, deep network models currently in use require a significant amount of labeled data. In X-ray images, accurate pixel-wise labeling is usually reliant on skilled clinicians due to the complex texture patterns and overlapping anatomy. In contrast, 3D CT scans provide clearer structures and sharper boundaries for organ delineation. In this paper, we propose a novel model framework that utilizes labeled CT scans to learn automatic X-ray image parsing. We train a Dense Image-to-Image network (DI2I) for multi-organ segmentation on X-ray-like Digitally Reconstructed Radiographs (DRRs) rendered from 3D CT volumes. Then, we introduce a Task Driven Generative Adversarial Network (TD-GAN) architecture for simultaneous style transfer and parsing for unseen real X-ray images. The TD-GAN framework is versatile and can be adapted to other learning tasks. We validate our proposed model on 815 DRRs and 153 topograms through numerical experiments. While the vanilla DI2I fails completely on segmenting topograms, our proposed model achieves an average dice of 85% without any topogram labels, which is comparable to supervised training (88%).",1
"NeuroNet is a deep convolutional neural network mimicking multiple popular and state-of-the-art brain segmentation tools including FSL, SPM, and MALPEM. The network is trained on 5,000 T1-weighted brain MRI scans from the UK Biobank Imaging Study that have been automatically segmented into brain tissue and cortical and sub-cortical structures using the standard neuroimaging pipelines. Training a single model from these complementary and partially overlapping label maps yields a new powerful ""all-in-one"", multi-output segmentation tool. The processing time for a single subject is reduced by an order of magnitude compared to running each individual software package. We demonstrate very good reproducibility of the original outputs while increasing robustness to variations in the input data. We believe NeuroNet could be an important tool in large-scale population imaging studies and serve as a new standard in neuroscience by reducing the risk of introducing bias when choosing a specific software package.",0
"NeuroNet is a deep convolutional neural network that imitates several prominent brain segmentation tools such as FSL, SPM, and MALPEM. The network is trained on 5,000 T1-weighted brain MRI scans from the UK Biobank Imaging Study, which have been automatically segmented into brain tissue and cortical and sub-cortical structures using standard neuroimaging pipelines. By training a single model using these complementary and partially overlapping label maps, NeuroNet creates a powerful multi-output segmentation tool that substantially reduces processing time compared to running each individual software package. Our tests demonstrate excellent reproducibility of the original outputs while improving robustness to variations in the input data. NeuroNet could be a valuable tool in large-scale population imaging studies and may become the new standard in neuroscience, reducing the risk of introducing bias when selecting a specific software package.",1
"This paper proposed a retinal image segmentation method based on conditional Generative Adversarial Network (cGAN) to segment optic disc. The proposed model consists of two successive networks: generator and discriminator. The generator learns to map information from the observing input (i.e., retinal fundus color image), to the output (i.e., binary mask). Then, the discriminator learns as a loss function to train this mapping by comparing the ground-truth and the predicted output with observing the input image as a condition.Experiments were performed on two publicly available dataset; DRISHTI GS1 and RIM-ONE. The proposed model outperformed state-of-the-art-methods by achieving around 0.96% and 0.98% of Jaccard and Dice coefficients, respectively. Moreover, an image segmentation is performed in less than a second on recent GPU.",0
"A new approach to segmenting the optic disc in retinal images is presented in this paper, using a conditional Generative Adversarial Network (cGAN). The method involves two networks: a generator that maps information from the input image to a binary mask, and a discriminator that learns to compare the predicted output with the ground truth. The proposed model was tested on two publicly available datasets, achieving better results than existing methods with Jaccard and Dice coefficients of approximately 0.96% and 0.98%, respectively. Additionally, the segmentation process takes less than a second on a recent GPU.",1
"We investigate the problem of Language-Based Image Editing (LBIE). Given a source image and a natural language description, we want to generate a target image by editing the source image based on the description. We propose a generic modeling framework for two sub-tasks of LBIE: language-based image segmentation and image colorization. The framework uses recurrent attentive models to fuse image and language features. Instead of using a fixed step size, we introduce for each region of the image a termination gate to dynamically determine after each inference step whether to continue extrapolating additional information from the textual description. The effectiveness of the framework is validated on three datasets. First, we introduce a synthetic dataset, called CoSaL, to evaluate the end-to-end performance of our LBIE system. Second, we show that the framework leads to state-of-the-art performance on image segmentation on the ReferIt dataset. Third, we present the first language-based colorization result on the Oxford-102 Flowers dataset.",0
"Our focus is on Language-Based Image Editing (LBIE) and how to generate a target image by editing a source image based on a natural language description. We have proposed a modeling framework that encompasses two sub-tasks of LBIE: image colorization and language-based image segmentation. Our framework uses recurrent attentive models to combine the features of the image and the language. Rather than using a fixed step size, our framework includes a termination gate for each region of the image, which dynamically determines whether to continue extrapolating additional information from the textual description. We have validated our framework's effectiveness on three datasets. The first dataset, CoSaL, is a synthetic dataset we created to assess the end-to-end performance of our LBIE system. The second dataset, ReferIt, demonstrates that our framework delivers state-of-the-art results on image segmentation. Finally, we have presented the first language-based colorization outcome on the Oxford-102 Flowers dataset.",1
"Fully convolutional neural networks (F-CNNs) have set the state-of-the-art in image segmentation for a plethora of applications. Architectural innovations within F-CNNs have mainly focused on improving spatial encoding or network connectivity to aid gradient flow. In this paper, we explore an alternate direction of recalibrating the feature maps adaptively, to boost meaningful features, while suppressing weak ones. We draw inspiration from the recently proposed squeeze & excitation (SE) module for channel recalibration of feature maps for image classification. Towards this end, we introduce three variants of SE modules for image segmentation, (i) squeezing spatially and exciting channel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE) and (iii) concurrent spatial and channel squeeze & excitation (scSE). We effectively incorporate these SE modules within three different state-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistent improvement of performance across all architectures, while minimally effecting model complexity. Evaluations are performed on two challenging applications: whole brain segmentation on MRI scans (Multi-Atlas Labelling Challenge Dataset) and organ segmentation on whole body contrast enhanced CT scans (Visceral Dataset).",0
"Image segmentation for a variety of applications has been revolutionized by fully convolutional neural networks (F-CNNs), which have established themselves as the state-of-the-art. To enhance gradient flow, architectural advancements in F-CNNs have focused on improving spatial encoding or network connectivity. However, in this paper, we explore a different approach of adaptively recalibrating feature maps to enhance strong features and suppress weak ones. Our inspiration comes from the squeeze & excitation (SE) module, which is proposed recently for channel recalibration for image classification. We introduce three variants of SE modules for image segmentation, namely, cSE, sSE, and scSE, which squeeze spatially and excite channel-wise, squeeze channel-wise and excite spatially, and concurrently squeeze spatially and channel-wise, respectively. These SE modules are effectively incorporated into three different state-of-the-art F-CNNs (DenseNet, SD-Net, U-Net), resulting in consistent performance improvement across all architectures with minimal impact on model complexity. We evaluate the proposed method on two challenging applications: whole brain segmentation on MRI scans (Multi-Atlas Labelling Challenge Dataset) and organ segmentation on whole body contrast enhanced CT scans (Visceral Dataset).",1
"Hourglass networks such as the U-Net and V-Net are popular neural architectures for medical image segmentation and counting problems. Typical instances of hourglass networks contain shortcut connections between mirroring layers. These shortcut connections improve the performance and it is hypothesized that this is due to mitigating effects on the vanishing gradient problem and the ability of the model to combine feature maps from earlier and later layers. We propose a method for not only combining feature maps of mirroring layers but also feature maps of layers with different spatial dimensions. For instance, the method enables the integration of the bottleneck feature map with those of the reconstruction layers. The proposed approach is applicable to any hourglass architecture. We evaluated the contextual hourglass networks on image segmentation and object counting problems in the medical domain. We achieve competitive results outperforming popular hourglass networks by up to 17 percentage points.",0
"Hourglass networks, including the U-Net and V-Net, are widely used in medical image segmentation and counting tasks. These networks typically incorporate shortcut connections between corresponding layers to enhance performance. These connections are thought to improve the model's ability to overcome the vanishing gradient problem and combine feature maps from earlier and later layers. Our proposed method goes beyond combining feature maps of corresponding layers and also integrates maps from layers with different spatial dimensions, like the bottleneck feature map and reconstruction layers. This approach is applicable to any hourglass architecture. We evaluated our contextual hourglass networks on medical image segmentation and object counting tasks, achieving results that surpassed those of popular hourglass networks by up to 17 percentage points.",1
"In this work, we evaluate the use of superpixel pooling layers in deep network architectures for semantic segmentation. Superpixel pooling is a flexible and efficient replacement for other pooling strategies that incorporates spatial prior information. We propose a simple and efficient GPU-implementation of the layer and explore several designs for the integration of the layer into existing network architectures. We provide experimental results on the IBSR and Cityscapes dataset, demonstrating that superpixel pooling can be leveraged to consistently increase network accuracy with minimal computational overhead. Source code is available at https://github.com/bermanmaxim/superpixPool",0
This study assesses the effectiveness of incorporating superpixel pooling layers in deep network structures for semantic segmentation. Superpixel pooling is a more adaptable and effective alternative to other pooling techniques that utilizes spatial prior knowledge. We suggest a straightforward and efficient GPU-implementation of the layer and investigate various approaches to integrate it into current network designs. Our experimental findings on the IBSR and Cityscapes datasets reveal that superpixel pooling can be utilized to enhance network accuracy without adding significant computational burden. The source code for this research is accessible at https://github.com/bermanmaxim/superpixPool.,1
"Uncertainty estimation methods are expected to improve the understanding and quality of computer-assisted methods used in medical applications (e.g., neurosurgical interventions, radiotherapy planning), where automated medical image segmentation is crucial. In supervised machine learning, a common practice to generate ground truth label data is to merge observer annotations. However, as many medical image tasks show a high inter-observer variability resulting from factors such as image quality, different levels of user expertise and domain knowledge, little is known as to how inter-observer variability and commonly used fusion methods affect the estimation of uncertainty of automated image segmentation. In this paper we analyze the effect of common image label fusion techniques on uncertainty estimation, and propose to learn the uncertainty among observers. The results highlight the negative effect of fusion methods applied in deep learning, to obtain reliable estimates of segmentation uncertainty. Additionally, we show that the learned observers' uncertainty can be combined with current standard Monte Carlo dropout Bayesian neural networks to characterize uncertainty of model's parameters.",0
"The improvement of computer-assisted methods in medical applications such as neurosurgical interventions and radiotherapy planning relies on the development of uncertainty estimation methods. Automated medical image segmentation is a crucial component in these methods. Supervised machine learning techniques commonly merge observer annotations to generate ground truth label data; however, factors such as image quality and varying levels of user expertise and domain knowledge can cause high inter-observer variability. The impact of this variability and common label fusion techniques on the estimation of automated image segmentation uncertainty is poorly understood. This paper examines the effect of label fusion techniques on uncertainty estimation and proposes learning observer uncertainty. The findings reveal that standard deep learning fusion methods negatively affect segmentation uncertainty estimation and that the learned observer uncertainty can enhance the characterization of model parameter uncertainty in Monte Carlo dropout Bayesian neural networks.",1
"Semantic image segmentation is one the most demanding task, especially for analysis of traffic conditions for self-driving cars. Here the results of application of several deep learning architectures (PSPNet and ICNet) for semantic image segmentation of traffic stereo-pair images are presented. The images from Cityscapes dataset and custom urban images were analyzed as to the segmentation accuracy and image inference time. For the models pre-trained on Cityscapes dataset, the inference time was equal in the limits of standard deviation, but the segmentation accuracy was different for various cities and stereo channels even. The distributions of accuracy (mean intersection over union - mIoU) values for each city and channel are asymmetric, long-tailed, and have many extreme outliers, especially for PSPNet network in comparison to ICNet network. Some statistical properties of these distributions (skewness, kurtosis) allow us to distinguish these two networks and open the question about relations between architecture of deep learning networks and statistical distribution of the predicted results (mIoU here). The results obtained demonstrated the different sensitivity of these networks to: (1) the local street view peculiarities in different cities that should be taken into account during the targeted fine tuning the models before their practical applications, (2) the right and left data channels in stereo-pairs. For both networks, the difference in the predicted results (mIoU here) for the right and left data channels in stereo-pairs is out of the limits of statistical error in relation to mIoU values. It means that the traffic stereo pairs can be effectively used not only for depth calculations (as it is usually used), but also as an additional data channel that can provide much more information about scene objects than simple duplication of the same street view images.",0
"The task of semantic image segmentation is highly demanding, particularly when analyzing traffic conditions for self-driving cars. This study presents the results of applying two deep learning architectures, PSPNet and ICNet, to semantic image segmentation of traffic stereo-pair images from the Cityscapes dataset and custom urban images. The accuracy of segmentation and image inference time were analyzed. For models pretrained on the Cityscapes dataset, inference time was similar within the standard deviation, but segmentation accuracy varied across different cities and stereo channels. The distribution of accuracy values for each city and channel was asymmetric, long-tailed, and had many extreme outliers, particularly for PSPNet network in comparison to ICNet network. Statistical properties such as skewness and kurtosis distinguished the two networks, raising questions about the relationship between deep learning network architecture and the statistical distribution of predicted results. Results showed that the networks had different sensitivity to local street view peculiarities and right and left data channels in stereo-pairs. The difference in predicted results for the right and left data channels was beyond the limits of statistical error, indicating that traffic stereo pairs can provide more information about scene objects than simple duplication of street view images and should be considered for targeted fine-tuning of models before practical applications.",1
"Recent neural-network-based architectures for image segmentation make extensive usage of feature forwarding mechanisms to integrate information from multiple scales. Although yielding good results, even deeper architectures and alternative methods for feature fusion at different resolutions have been scarcely investigated for medical applications. In this work we propose to implement segmentation via an encoder-decoder architecture which differs from any other previously published method since (i) it employs a very deep architecture based on residual learning and (ii) combines features via a convolutional Long Short Term Memory (LSTM), instead of concatenation or summation. The intuition is that the memory mechanism implemented by LSTMs can better integrate features from different scales through a coarse-to-fine strategy; hence the name Coarse-to-Fine Context Memory (CFCM). We demonstrate the remarkable advantages of this approach on two datasets: the Montgomery county lung segmentation dataset, and the EndoVis 2015 challenge dataset for surgical instrument segmentation.",0
"Current approaches to image segmentation using neural networks rely heavily on feature forwarding mechanisms to incorporate information from various scales. Although effective, there has been limited exploration into deeper architectures and alternative methods for fusing features at different resolutions, particularly in medical applications. Our proposed solution for segmentation involves an encoder-decoder architecture that stands apart from other methods due to its usage of residual learning in a very deep architecture, as well as a convolutional Long Short Term Memory (LSTM) for feature fusion instead of concatenation or summation. We call this approach Coarse-to-Fine Context Memory (CFCM) because we believe that the memory mechanism of LSTMs can better integrate features from different scales in a coarse-to-fine strategy. We have tested our approach on two datasets, the Montgomery county lung segmentation dataset and the EndoVis 2015 challenge dataset for surgical instrument segmentation, and have observed significant advantages.",1
"In recent years, deep learning (DL) methods have become powerful tools for biomedical image segmentation. However, high annotation efforts and costs are commonly needed to acquire sufficient biomedical training data for DL models. To alleviate the burden of manual annotation, in this paper, we propose a new weakly supervised DL approach for biomedical image segmentation using boxes only annotation. First, we develop a method to combine graph search (GS) and DL to generate fine object masks from box annotation, in which DL uses box annotation to compute a rough segmentation for GS and then GS is applied to locate the optimal object boundaries. During the mask generation process, we carefully utilize information from box annotation to filter out potential errors, and then use the generated masks to train an accurate DL segmentation network. Extensive experiments on gland segmentation in histology images, lymph node segmentation in ultrasound images, and fungus segmentation in electron microscopy images show that our approach attains superior performance over the best known state-of-the-art weakly supervised DL method and is able to achieve (1) nearly the same accuracy compared to fully supervised DL methods with far less annotation effort, (2) significantly better results with similar annotation time, and (3) robust performance in various applications.",0
"Biomedical image segmentation has benefited from the effectiveness of deep learning (DL) methods. However, acquiring enough biomedical training data for DL models requires significant annotation efforts and costs. To address this issue, a weakly supervised DL approach that uses only boxes for annotation is introduced in this paper. The proposed method combines DL and graph search (GS) to generate fine object masks from box annotation. DL computes a rough segmentation of the box annotation, which GS uses to locate the optimal object boundaries. Information from the box annotation is carefully utilized to filter potential errors during the mask generation process. The generated masks are then used to train an accurate DL segmentation network. Experimental results on gland segmentation in histology images, lymph node segmentation in ultrasound images, and fungus segmentation in electron microscopy images show that the proposed approach outperforms the best known state-of-the-art weakly supervised DL method. The approach achieves nearly the same accuracy as fully supervised DL methods with less annotation effort, significantly better results with similar annotation time, and robust performance in various applications.",1
"Deep learning has been widely accepted as a promising solution for medical image segmentation, given a sufficiently large representative dataset of images with corresponding annotations. With ever increasing amounts of annotated medical datasets, it is infeasible to train a learning method always with all data from scratch. This is also doomed to hit computational limits, e.g., memory or runtime feasible for training. Incremental learning can be a potential solution, where new information (images or anatomy) is introduced iteratively. Nevertheless, for the preservation of the collective information, it is essential to keep some ""important"" (i.e. representative) images and annotations from the past, while adding new information. In this paper, we introduce a framework for applying incremental learning for segmentation and propose novel methods for selecting representative data therein. We comparatively evaluate our methods in different scenarios using MR images and validate the increased learning capacity with using our methods.",0
"Medical image segmentation has found deep learning to be a promising solution, but only if there is a sufficiently large dataset of images that have corresponding annotations. However, as the amount of annotated medical datasets continues to increase, it becomes impractical to train a learning method with all the data from scratch. Additionally, this approach is limited by computational constraints like memory or runtime feasible for training. To address this issue, incremental learning can be used, allowing for the iterative introduction of new information. However, to preserve the collective information, it's important to retain some ""important"" or representative images and annotations from the past while adding new information. In this study, we present a framework for using incremental learning for segmentation and propose innovative methods for selecting representative data. We evaluate our methods in different scenarios using MR images and demonstrate increased learning capacity using our techniques.",1
"Deep learning (DL) based semantic segmentation methods have been providing state-of-the-art performance in the last few years. More specifically, these techniques have been successfully applied to medical image classification, segmentation, and detection tasks. One deep learning technique, U-Net, has become one of the most popular for these applications. In this paper, we propose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as well as a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Net models, which are named RU-Net and R2U-Net respectively. The proposed models utilize the power of U-Net, Residual Network, as well as RCNN. There are several advantages of these proposed architectures for segmentation tasks. First, a residual unit helps when training deep architecture. Second, feature accumulation with recurrent residual convolutional layers ensures better feature representation for segmentation tasks. Third, it allows us to design better U-Net architecture with same number of network parameters with better performance for medical image segmentation. The proposed models are tested on three benchmark datasets such as blood vessel segmentation in retina images, skin cancer segmentation, and lung lesion segmentation. The experimental results show superior performance on segmentation tasks compared to equivalent models including U-Net and residual U-Net (ResU-Net).",0
"Over the past few years, DL-based semantic segmentation methods have demonstrated exceptional performance, particularly in medical image classification, segmentation, and detection tasks. The U-Net technique has emerged as a popular approach for these applications. In this study, we introduce two new models based on U-Net: RU-Net, a Recurrent Convolutional Neural Network, and R2U-Net, a Recurrent Residual Convolutional Neural Network. These models combine the strengths of U-Net, Residual Network, and RCNN to enhance segmentation tasks. The proposed models offer several benefits, such as the use of residual units to aid in training deep architectures, feature accumulation using recurrent residual convolutional layers for better feature representation, and the ability to create superior U-Net architectures with the same number of network parameters for medical image segmentation. We assessed the performance of these models on three benchmark datasets, including blood vessel segmentation in retina images, skin cancer segmentation, and lung lesion segmentation. Our experimental results demonstrate that the proposed models outperform equivalent models, such as U-Net and Residual U-Net (ResU-Net), in segmentation tasks.",1
"This paper addresses the search for a fast and meaningful image segmentation in the context of $k$-means clustering. The proposed method builds on a widely-used local version of Lloyd's algorithm, called Simple Linear Iterative Clustering (SLIC). We propose an algorithm which extends SLIC to dynamically adjust the local search, adopting superpixel resolution dynamically to structure existent in the image, and thus provides for more meaningful superpixels in the same linear runtime as standard SLIC. The proposed method is evaluated against state-of-the-art techniques and improved boundary adherence and undersegmentation error are observed, whilst still remaining among the fastest algorithms which are tested.",0
"In this paper, the focus is on finding a speedy and significant approach to image segmentation using $k$-means clustering. The suggested technique enhances the commonly used Simple Linear Iterative Clustering (SLIC), which is a local version of Lloyd's algorithm. The proposed algorithm adapts the local search dynamically by adjusting the superpixel resolution to match the image's existing structure. This results in more meaningful superpixels without increasing the linear runtime of SLIC. The method is compared to modern techniques and shows better boundary adherence and under-segmentation error while still being one of the fastest algorithms tested.",1
"We desgin a novel fully convolutional network architecture for shapes, denoted by Shape Fully Convolutional Networks (SFCN). 3D shapes are represented as graph structures in the SFCN architecture, based on novel graph convolution and pooling operations, which are similar to convolution and pooling operations used on images. Meanwhile, to build our SFCN architecture in the original image segmentation fully convolutional network (FCN) architecture, we also design and implement a generating operation} with bridging function. This ensures that the convolution and pooling operation we have designed can be successfully applied in the original FCN architecture. In this paper, we also present a new shape segmentation approach based on SFCN. Furthermore, we allow more general and challenging input, such as mixed datasets of different categories of shapes} which can prove the ability of our generalisation. In our approach, SFCNs are trained triangles-to-triangles by using three low-level geometric features as input. Finally, the feature voting-based multi-label graph cuts is adopted to optimise the segmentation results obtained by SFCN prediction. The experiment results show that our method can effectively learn and predict mixed shape datasets of either similar or different characteristics, and achieve excellent segmentation results.",0
"Our new approach, Shape Fully Convolutional Networks (SFCN), is a unique architecture designed for shapes. This architecture represents 3D shapes as graph structures using novel graph convolution and pooling operations, similar to those used for images. To integrate SFCN into the original image segmentation fully convolutional network (FCN) architecture, we have developed a generating operation with a bridging function. This ensures the successful application of our convolution and pooling operations in the original FCN architecture. Our approach allows for more general and challenging inputs, such as mixed datasets of different categories of shapes, which demonstrates our generalization ability. SFCNs are trained triangles-to-triangles using three low-level geometric features as input. Finally, we optimize the segmentation results obtained by SFCN prediction using feature voting-based multi-label graph cuts. Our experiment results demonstrate that our method effectively learns and predicts mixed shape datasets of either similar or different characteristics, achieving excellent segmentation results.",1
"In this paper, we propose an efficient architecture for semantic image segmentation using the depth-to-space (D2S) operation. Our D2S model is comprised of a standard CNN encoder followed by a depth-to-space reordering of the final convolutional feature maps. Our approach eliminates the decoder portion of traditional encoder-decoder segmentation models and reduces the amount of computation almost by half. As a participant of the DeepGlobe Road Extraction competition, we evaluate our models on the corresponding road segmentation dataset. Our highly efficient D2S models exhibit comparable performance to standard segmentation models with much lower computational cost.",0
"Our paper presents a novel semantic image segmentation architecture that utilizes the depth-to-space (D2S) operation to achieve high efficiency. The D2S model consists of a typical CNN encoder, which is followed by the depth-to-space reordering of the final convolutional feature maps. This unique approach eliminates the decoder part in conventional encoder-decoder segmentation models and reduces computation almost by half. To evaluate our approach, we participated in the DeepGlobe Road Extraction competition and tested our models on the corresponding road segmentation dataset. The results show that our D2S models are highly efficient and exhibit comparable performance to standard segmentation models, while requiring significantly less computational cost.",1
"Many deep learning architectures for semantic segmentation involve a Fully Convolutional Neural Network (FCN) followed by a Conditional Random Field (CRF) to carry out inference over an image. These models typically involve unary potentials based on local appearance features computed by FCNs, and binary potentials based on the displacement between pixels. We show that while current methods succeed in segmenting whole objects, they perform poorly in situations involving a large number of object parts. We therefore suggest incorporating into the inference algorithm additional higher-order potentials inspired by the way humans identify and localize parts. We incorporate two relations that were shown to be useful to human object identification - containment and attachment - into the energy term of the CRF and evaluate their performance on the Pascal VOC Parts dataset. Our experimental results show that the segmentation of fine parts is positively affected by the addition of these two relations, and that the segmentation of fine parts can be further influenced by complex structural features.",0
"Semantic segmentation deep learning models often utilize a FCN and CRF for image inference. These models use unary potentials based on local appearance features and binary potentials based on pixel displacement. While these methods are effective for segmenting whole objects, they struggle with segmenting objects with numerous parts. To address this, we propose integrating higher-order potentials inspired by human part identification into the inference algorithm. We include two relations - containment and attachment - in the CRF energy term and test their impact on the Pascal VOC Parts dataset. Our results show that the addition of these relations improves the segmentation of fine parts and that complex structural features can further enhance the segmentation.",1
We propose a geometric convexity shape prior preservation method for variational level set based image segmentation methods. Our method is built upon the fact that the level set of a convex signed distanced function must be convex. This property enables us to transfer a complicated geometrical convexity prior into a simple inequality constraint on the function. An active set based Gauss-Seidel iteration is used to handle this constrained minimization problem to get an efficient algorithm. We apply our method to region and edge based level set segmentation models including Chan-Vese (CV) model with guarantee that the segmented region will be convex. Experimental results show the effectiveness and quality of the proposed model and algorithm.,0
"Our proposed method for variational level set based image segmentation involves preserving a geometric convexity shape prior. By utilizing the fact that the level set of a convex signed distanced function must also be convex, we are able to transform a complex geometrical convexity prior into a straightforward inequality constraint on the function. To efficiently handle this constrained minimization problem, we use an active set based Gauss-Seidel iteration. Our approach can be applied to both region and edge based level set segmentation models, such as the Chan-Vese (CV) model, and guarantees that the segmented region will maintain convexity. Our experimental results demonstrate the effectiveness and quality of the proposed algorithm and model.",1
"A variety of deep neural networks have been applied in medical image segmentation and achieve good performance. Unlike natural images, medical images of the same imaging modality are characterized by the same pattern, which indicates that same normal organs or tissues locate at similar positions in the images. Thus, in this paper we try to incorporate the prior knowledge of medical images into the structure of neural networks such that the prior knowledge can be utilized for accurate segmentation. Based on this idea, we propose a novel deep network called knowledge-based fully convolutional network (KFCN) for medical image segmentation. The segmentation function and corresponding error is analyzed. We show the existence of an asymptotically stable region for KFCN which traditional FCN doesn't possess. Experiments validate our knowledge assumption about the incorporation of prior knowledge into the convolution kernels of KFCN and show that KFCN can achieve a reasonable segmentation and a satisfactory accuracy.",0
"Medical image segmentation has seen success with the application of various deep neural networks. Unlike natural images, medical images of the same imaging modality exhibit a consistent pattern, indicating that similar normal tissues or organs can be found in identical positions within the images. To improve segmentation accuracy, this paper proposes incorporating prior knowledge of medical images into the neural network structure. A new deep network, known as the knowledge-based fully convolutional network (KFCN), is introduced based on this idea. The segmentation function and corresponding error are analyzed, revealing an asymptotically stable region specific to KFCN. Experiments support the assumption that prior knowledge can be integrated through convolution kernels in KFCN, resulting in reasonable segmentation and high accuracy.",1
"We provide initial seedings to the Quick Shift clustering algorithm, which approximate the locally high-density regions of the data. Such seedings act as more stable and expressive cluster-cores than the singleton modes found by Quick Shift. We establish statistical consistency guarantees for this modification. We then show strong clustering performance on real datasets as well as promising applications to image segmentation.",0
"The Quick Shift clustering algorithm is aided with initial seedings that approximate the data's regions with high density. These seedings serve as cluster-cores that are more reliable and descriptive than the singleton modes identified by Quick Shift. Our modification is supported by statistical consistency guarantees, and we demonstrate its impressive performance on actual datasets. Additionally, we highlight its potential for image segmentation applications.",1
"Medical image segmentation requires consensus ground truth segmentations to be derived from multiple expert annotations. A novel approach is proposed that obtains consensus segmentations from experts using graph cuts (GC) and semi supervised learning (SSL). Popular approaches use iterative Expectation Maximization (EM) to estimate the final annotation and quantify annotator's performance. Such techniques pose the risk of getting trapped in local minima. We propose a self consistency (SC) score to quantify annotator consistency using low level image features. SSL is used to predict missing annotations by considering global features and local image consistency. The SC score also serves as the penalty cost in a second order Markov random field (MRF) cost function optimized using graph cuts to derive the final consensus label. Graph cut obtains a global maximum without an iterative procedure. Experimental results on synthetic images, real data of Crohn's disease patients and retinal images show our final segmentation to be accurate and more consistent than competing methods.",0
"Consensus ground truth segmentations are essential in medical image segmentation, which are typically derived from multiple expert annotations. A new approach has been proposed that utilizes graph cuts (GC) and semi-supervised learning (SSL) to obtain consensus segmentations from experts. Traditional methods use iterative Expectation Maximization (EM) to estimate the final annotation and measure the performance of the annotator, but this is risky as it can get trapped in local minima. To overcome this, a self-consistency (SC) score is proposed to measure annotator consistency using low-level image features. SSL is used to predict missing annotations, taking into account global features and local image consistency. The SC score is also used as a penalty cost in a second-order Markov random field (MRF) cost function, which is optimized using graph cuts to derive the final consensus label. Graph cuts obtain a global maximum without an iterative procedure. Experimental results on synthetic images, real data of Crohn's disease patients, and retinal images show that the final segmentation is accurate and more consistent than competing methods.",1
We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.,0
"Our team has created a unique attention gate (AG) model that can automatically focus on structures of various sizes and shapes in medical imaging. By utilizing AGs within our trained models, we can eliminate the need for external tissue/organ localization modules that are commonly used in cascaded convolutional neural networks (CNNs). AGs are easily integrated into standard CNN architectures, such as the U-Net model, with minimal computational overhead, while increasing prediction accuracy and sensitivity. We have evaluated our proposed Attention U-Net architecture on two large CT abdominal datasets for multi-class image segmentation and have found that AGs consistently enhance the prediction performance of U-Net across different datasets and training sizes, while maintaining computational efficiency. Our code is publicly available for implementation.",1
"We introduce a new multi-dimensional nonlinear embedding -- Piecewise Flat Embedding (PFE) -- for image segmentation. Based on the theory of sparse signal recovery, piecewise flat embedding with diverse channels attempts to recover a piecewise constant image representation with sparse region boundaries and sparse cluster value scattering. The resultant piecewise flat embedding exhibits interesting properties such as suppressing slowly varying signals, and offers an image representation with higher region identifiability which is desirable for image segmentation or high-level semantic analysis tasks. We formulate our embedding as a variant of the Laplacian Eigenmap embedding with an $L_{1,p} (0<p\leq1)$ regularization term to promote sparse solutions. First, we devise a two-stage numerical algorithm based on Bregman iterations to compute $L_{1,1}$-regularized piecewise flat embeddings. We further generalize this algorithm through iterative reweighting to solve the general $L_{1,p}$-regularized problem. To demonstrate its efficacy, we integrate PFE into two existing image segmentation frameworks, segmentation based on clustering and hierarchical segmentation based on contour detection. Experiments on four major benchmark datasets, BSDS500, MSRC, Stanford Background Dataset, and PASCAL Context, show that segmentation algorithms incorporating our embedding achieve significantly improved results.",0
"Our new approach for image segmentation, called Piecewise Flat Embedding (PFE), is a multi-dimensional nonlinear embedding that uses sparse signal recovery theory. PFE aims to recover a piecewise constant image representation with sparse region boundaries and sparse cluster value scattering. This leads to a piecewise flat embedding that has unique properties such as the ability to suppress slowly varying signals and provide a higher region identifiability. To achieve this, we use a variant of the Laplacian Eigenmap embedding with an $L_{1,p} (0<p\leq1)$ regularization term to promote sparse solutions. Our numerical algorithm is based on Bregman iterations and is capable of computing $L_{1,1}$-regularized piecewise flat embeddings. Moreover, we extend this algorithm through iterative reweighting to solve the general $L_{1,p}$-regularized problem. We integrate PFE into two existing image segmentation frameworks, segmentation based on clustering and hierarchical segmentation based on contour detection, and demonstrate its efficacy by conducting experiments on four major benchmark datasets, BSDS500, MSRC, Stanford Background Dataset, and PASCAL Context. The results show that segmentation algorithms incorporating our embedding achieve significantly improved results.",1
"For the challenging semantic image segmentation task the most efficient models have traditionally combined the structured modelling capabilities of Conditional Random Fields (CRFs) with the feature extraction power of CNNs. In more recent works however, CRF post-processing has fallen out of favour. We argue that this is mainly due to the slow training and inference speeds of CRFs, as well as the difficulty of learning the internal CRF parameters. To overcome both issues we propose to add the assumption of conditional independence to the framework of fully-connected CRFs. This allows us to reformulate the inference in terms of convolutions, which can be implemented highly efficiently on GPUs. Doing so speeds up inference and training by a factor of more then 100. All parameters of the convolutional CRFs can easily be optimized using backpropagation. To facilitating further CRF research we make our implementation publicly available. Please visit: https://github.com/MarvinTeichmann/ConvCRF",0
"Traditionally, the most effective models for the challenging semantic image segmentation task have combined Conditional Random Fields (CRFs) with Convolutional Neural Networks (CNNs) to leverage their structured modelling capabilities and feature extraction power, respectively. However, recent works have steered away from CRF post-processing due to the slow training and inference speeds of CRFs, as well as the difficulty of learning their internal parameters. To address these issues, we propose a solution that involves introducing the assumption of conditional independence to the framework of fully-connected CRFs. This reformulation enables us to perform inference using convolutions, which can be implemented efficiently on GPUs, resulting in a more than 100-fold increase in inference and training speed. Additionally, all parameters of the convolutional CRFs can be optimized using backpropagation. To support further CRF research, we have made our implementation publicly accessible at https://github.com/MarvinTeichmann/ConvCRF.",1
"Urban facade segmentation from automatically acquired imagery, in contrast to traditional image segmentation, poses several unique challenges. 360-degree photospheres captured from vehicles are an effective way to capture a large number of images, but this data presents difficult-to-model warping and stitching artifacts. In addition, each pixel can belong to multiple facade elements, and different facade elements (e.g., window, balcony, sill, etc.) are correlated and vary wildly in their characteristics. In this paper, we propose three network architectures of varying complexity to achieve multilabel semantic segmentation of facade images while exploiting their unique characteristics. Specifically, we propose a MULTIFACSEGNET architecture to assign multiple labels to each pixel, a SEPARABLE architecture as a low-rank formulation that encourages extraction of rectangular elements, and a COMPATIBILITY network that simultaneously seeks segmentation across facade element types allowing the network to 'see' intermediate output probabilities of the various facade element classes. Our results on benchmark datasets show significant improvements over existing facade segmentation approaches for the typical facade elements. For example, on one commonly used dataset, the accuracy scores for window(the most important architectural element) increases from 0.91 to 0.97 percent compared to the best competing method, and comparable improvements on other element types.",0
"Segmenting urban facades from automatically acquired imagery presents unique challenges compared to traditional image segmentation. The use of 360-degree photospheres captured from vehicles is an efficient method of obtaining a large number of images, but this data is prone to warping and stitching artifacts that are difficult to model. Additionally, pixels can belong to multiple facade elements, and different elements (such as windows, balconies, and sills) vary greatly in their characteristics. In this study, we propose three network architectures of varying complexity to achieve multilabel semantic segmentation of facade images, while taking advantage of their unique features. Our proposed MULTIFACSEGNET architecture assigns multiple labels to each pixel, the SEPARABLE architecture extracts rectangular elements through a low-rank formulation, and the COMPATIBILITY network simultaneously segments across facade element types, allowing the network to consider intermediate output probabilities of different facade element classes. Our results on benchmark datasets show significant improvements over existing facade segmentation methods for typical facade elements. For instance, on one commonly used dataset, the accuracy scores for windows (the most crucial architectural element) increase from 0.91 to 0.97 percent compared to the best competing method, with similar improvements for other element types.",1
"Due to low tissue contrast, irregular object appearance, and unpredictable location variation, segmenting the objects from different medical imaging modalities (e.g., CT, MR) is considered as an important yet challenging task. In this paper, we present a novel method for interactive medical image segmentation with the following merits. (1) Our design is fundamentally different from previous pure patch-based and image-based segmentation methods. We observe that during delineation, the physician repeatedly check the inside-outside intensity changing to determine the boundary, which indicates that comparison in an inside-outside manner is extremely important. Thus, we innovatively model our segmentation task as learning the representation of the bi-directional sequential patches, starting from (or ending in) the given central point of the object. This can be realized by our proposed ConvRNN network embedded with a gated memory propagation unit. (2) Unlike previous interactive methods (requiring bounding box or seed points), we only ask the physician to merely click on the rough central point of the object before segmentation, which could simultaneously enhance the performance and reduce the segmentation time. (3) We utilize our method in a multi-level framework for better performance. We systematically evaluate our method in three different segmentation tasks including CT kidney tumor, MR prostate, and PROMISE12 challenge, showing promising results compared with state-of-the-art methods. The code is available here: \href{https://github.com/sunalbert/Sequential-patch-based-segmentation}{Sequential-patch-based-segmentation}.",0
"Segmenting objects from medical imaging modalities such as CT and MR is a challenging task due to low tissue contrast, irregular object appearance, and unpredictable location variation. In this paper, we introduce a novel method for interactive medical image segmentation that offers several advantages. First, our approach differs from previous patch-based and image-based segmentation methods, as we recognize the importance of comparison in an inside-outside manner during delineation. We model the segmentation task as learning the representation of bi-directional sequential patches using our proposed ConvRNN network with a gated memory propagation unit. Second, unlike previous interactive methods that require bounding box or seed points, we only ask the physician to click on the rough central point of the object before segmentation, which enhances performance and reduces segmentation time. Third, we use our method in a multi-level framework for better performance. We evaluate our method in three different segmentation tasks, including CT kidney tumor, MR prostate, and PROMISE12 challenge, and demonstrate promising results compared to state-of-the-art methods. Our code is available at \href{https://github.com/sunalbert/Sequential-patch-based-segmentation}{Sequential-patch-based-segmentation}.",1
"State-of-the-art semantic segmentation approaches increase the receptive field of their models by using either a downsampling path composed of poolings/strided convolutions or successive dilated convolutions. However, it is not clear which operation leads to best results. In this paper, we systematically study the differences introduced by distinct receptive field enlargement methods and their impact on the performance of a novel architecture, called Fully Convolutional DenseResNet (FC-DRN). FC-DRN has a densely connected backbone composed of residual networks. Following standard image segmentation architectures, receptive field enlargement operations that change the representation level are interleaved among residual networks. This allows the model to exploit the benefits of both residual and dense connectivity patterns, namely: gradient flow, iterative refinement of representations, multi-scale feature combination and deep supervision. In order to highlight the potential of our model, we test it on the challenging CamVid urban scene understanding benchmark and make the following observations: 1) downsampling operations outperform dilations when the model is trained from scratch, 2) dilations are useful during the finetuning step of the model, 3) coarser representations require less refinement steps, and 4) ResNets (by model construction) are good regularizers, since they can reduce the model capacity when needed. Finally, we compare our architecture to alternative methods and report state-of-the-art result on the Camvid dataset, with at least twice fewer parameters.",0
"Cutting-edge semantic segmentation techniques enhance their models' receptive field by utilizing either a downsampling path consisting of poolings/strided convolutions or consecutive dilated convolutions. Nevertheless, it remains unclear which operation produces the best outcomes. This article systematically investigates the variances introduced by different receptive field enlargement methods and their effects on a newly developed architecture called Fully Convolutional DenseResNet (FC-DRN). FC-DRN comprises a densely connected backbone consisting of residual networks, with receptive field enlargement operations that alter the representation level interspersed among residual networks, following conventional image segmentation architectural patterns. This design enables the model to exploit the advantages of both residual and dense connectivity patterns, such as gradient flow, iterative refinement of representations, multi-scale feature combination, and deep supervision. To demonstrate the potential of our model, we subjected it to the challenging CamVid urban scene understanding benchmark and made the following observations: 1) downsampling operations outperform dilations when the model is trained from scratch, 2) dilations are beneficial during the finetuning stage of the model, 3) coarser representations require fewer refinement steps, and 4) ResNets are effective regularizers because they can reduce the model capacity when necessary. Ultimately, we compared our architecture to alternative methods and achieved a state-of-the-art result on the Camvid dataset with at least twice fewer parameters.",1
"Machine learning models produce state-of-the-art results in many MRI images segmentation. However, most of these models are trained on very large datasets which come from experts manual labeling. This labeling process is very time consuming and costs experts work. Therefore finding a way to reduce this cost is on high demand. In this paper, we propose a segmentation method which exploits MRI images sequential structure to nearly drop out this labeling task. Only the first slice needs to be manually labeled to train the model which then infers the next slice's segmentation. Inference result is another datum used to train the model again. The updated model then infers the third slice and the same process is carried out until the last slice. The proposed model is an combination of two Random Forest algorithms: the classical one and a recent one namely Mondrian Forests. We applied our method on human left ventricle segmentation and results are very promising. This method can also be used to generate labels.",0
"In the field of MRI image segmentation, machine learning models have shown impressive outcomes. However, the majority of these models are trained on extensive datasets that require manual labeling by experts. This process is both time-consuming and costly for experts. As a result, there is a high demand to find a way to reduce this expense. This article presents a segmentation technique that leverages the sequential structure of MRI images to reduce the need for manual labeling. Only the initial slice of the image requires manual labeling to train the model. The model then uses this information to infer the segmentation of the next slice, and the process continues for all subsequent slices. Our proposed model combines two Random Forest algorithms, the classical one and the recent Mondrian Forests. We applied this method to segment the human left ventricle and observed promising results. Additionally, this method can also generate labels.",1
"We present a semi-parametric approach to photographic image synthesis from semantic layouts. The approach combines the complementary strengths of parametric and nonparametric techniques. The nonparametric component is a memory bank of image segments constructed from a training set of images. Given a novel semantic layout at test time, the memory bank is used to retrieve photographic references that are provided as source material to a deep network. The synthesis is performed by a deep network that draws on the provided photographic material. Experiments on multiple semantic segmentation datasets show that the presented approach yields considerably more realistic images than recent purely parametric techniques. The results are shown in the supplementary video at https://youtu.be/U4Q98lenGLQ",0
Our approach to generating photographic images from semantic layouts is semi-parametric and utilizes a combination of parametric and nonparametric techniques. The nonparametric aspect of our approach involves creating a memory bank of image segments from a training dataset. This memory bank is used to retrieve photographic references when presented with a new semantic layout during testing. The photographic references are then used by a deep network to synthesize the final image. Our experiments on various semantic segmentation datasets demonstrate that our approach generates significantly more realistic images than recent purely parametric techniques. You can view the results in our supplementary video at https://youtu.be/U4Q98lenGLQ.,1
"This paper introduces a novel algorithm for transductive inference in higher-order MRFs, where the unary energies are parameterized by a variable classifier. The considered task is posed as a joint optimization problem in the continuous classifier parameters and the discrete label variables. In contrast to prior approaches such as convex relaxations, we propose an advantageous decoupling of the objective function into discrete and continuous subproblems and a novel, efficient optimization method related to ADMM. This approach preserves integrality of the discrete label variables and guarantees global convergence to a critical point. We demonstrate the advantages of our approach in several experiments including video object segmentation on the DAVIS data set and interactive image segmentation.",0
"The aim of this paper is to present a new algorithm for transductive inference in higher-order MRFs, which utilizes a variable classifier to parameterize the unary energies. The proposed approach involves solving a joint optimization problem that considers both the discrete label variables and the continuous classifier parameters. Unlike previous methods that rely on convex relaxations, our approach involves decoupling the objective function into discrete and continuous subproblems, and using a novel optimization technique that is similar to ADMM. This method ensures that the discrete label variables remain integral and that global convergence to a critical point is guaranteed. We validate the effectiveness of our approach through various experiments, including interactive image segmentation and video object segmentation using the DAVIS dataset.",1
"Many imaging tasks require global information about all pixels in an image. Conventional bottom-up classification networks globalize information by decreasing resolution; features are pooled and downsampled into a single output. But for semantic segmentation and object detection tasks, a network must provide higher-resolution pixel-level outputs. To globalize information while preserving resolution, many researchers propose the inclusion of sophisticated auxiliary blocks, but these come at the cost of a considerable increase in network size and computational cost. This paper proposes stacked u-nets (SUNets), which iteratively combine features from different resolution scales while maintaining resolution. SUNets leverage the information globalization power of u-nets in a deeper network architectures that is capable of handling the complexity of natural images. SUNets perform extremely well on semantic segmentation tasks using a small number of parameters.",0
"To accomplish many imaging tasks, it is necessary to have a comprehensive understanding of all pixels in an image. Traditional classification networks utilize bottom-up techniques to globalize information by reducing the resolution of features through pooling and downsampling, resulting in a single output. However, for semantic segmentation and object detection, high-resolution pixel-level outputs are required. Although researchers suggest using advanced auxiliary blocks to globalize information while retaining resolution, this comes at the expense of increased network size and computational cost. This paper introduces stacked u-nets (SUNets), which repeatedly combine features from various resolution scales while maintaining resolution. SUNets use the information globalization capabilities of u-nets in deeper network architectures that can handle the complexity of natural images. SUNets achieve exceptional results in semantic segmentation tasks with minimal parameters.",1
"In the fields of nanoscience and nanotechnology, it is important to be able to functionalize surfaces chemically for a wide variety of applications. Scanning tunneling microscopes (STMs) are important instruments in this area used to measure the surface structure and chemistry with better than molecular resolution. Self-assembly is frequently used to create monolayers that redefine the surface chemistry in just a single-molecule-thick layer. Indeed, STM images reveal rich information about the structure of self-assembled monolayers since they convey chemical and physical properties of the studied material.   In order to assist in and to enhance the analysis of STM and other images, we propose and demonstrate an image-processing framework that produces two image segmentations: one is based on intensities (apparent heights in STM images) and the other is based on textural patterns. The proposed framework begins with a cartoon+texture decomposition, which separates an image into its cartoon and texture components. Afterward, the cartoon image is segmented by a modified multiphase version of the local Chan-Vese model, while the texture image is segmented by a combination of 2D empirical wavelet transform and a clustering algorithm. Overall, our proposed framework contains several new features, specifically in presenting a new application of cartoon+texture decomposition and of the empirical wavelet transforms and in developing a specialized framework to segment STM images and other data. To demonstrate the potential of our approach, we apply it to actual STM images of cyanide monolayers on Au\{111\} and present their corresponding segmentation results.",0
"Functionalizing surfaces chemically is crucial in nanoscience and nanotechnology for various applications. Scanning tunneling microscopes (STMs) are pivotal instruments that offer better-than-molecular resolution to measure surface structure and chemistry. Self-assembly is commonly used to create monolayers that alter the surface chemistry in a single-molecule-thick layer. STM images provide valuable information on the structure of self-assembled monolayers, conveying chemical and physical properties of the material. To enhance the analysis of STM and other images, we propose an image-processing framework that produces two image segmentations based on intensities and textural patterns. Our framework involves cartoon+texture decomposition to separate the image into its cartoon and texture components. The cartoon image is segmented using a modified multiphase version of the local Chan-Vese model, while the texture image is segmented using a combination of 2D empirical wavelet transform and a clustering algorithm. Our approach presents several new features, including a new application of cartoon+texture decomposition and empirical wavelet transforms, and a specialized framework for segmenting STM images and other data. We demonstrate the potential of our approach by applying it to actual STM images of cyanide monolayers on Au\{111\} and presenting their corresponding segmentation results.",1
"America has a massive railway system. As of 2006, U.S. freight railroads have 140,490 route- miles of standard gauge, but maintaining such a huge system and eliminating any dangers, like reduced track stability and poor drainage, caused by railway ballast degradation require huge amount of labor. The traditional way to quantify the degradation of ballast is to use an index called Fouling Index (FI) through ballast sampling and sieve analysis. However, determining the FI values in lab is very time-consuming and laborious, but with the help of recent development in the field of computer vision, a novel method for a potential machine-vison based ballast inspection system can be employed that can hopefully replace the traditional mechanical method. The new machine-vision approach analyses the images of the in-service ballasts, and then utilizes image segmentation algorithm to get ballast segments. By comparing the segment results and their corresponding FI values, this novel method produces a machine-vision-based index that has the best-fit relation with FI. The implementation details of how this algorithm works are discussed in this report.",0
"The United States boasts an extensive railway network, with freight railroads covering 140,490 route-miles of standard gauge as of 2006. However, maintaining such a vast system and preventing potential hazards, such as instability and poor drainage caused by ballast degradation, requires significant labor. Traditionally, the Fouling Index (FI) has been used to measure ballast degradation through sampling and analysis in a lab, but this method is time-consuming and labor-intensive. Fortunately, recent developments in computer vision have led to the creation of a potential machine-vision based ballast inspection system that could replace the traditional mechanical method. This approach involves analyzing images of in-service ballasts and using an image segmentation algorithm to identify ballast segments. By comparing the segment results and their corresponding FI values, this new method produces a machine-vision-based index that correlates with FI. This report discusses the implementation details of this algorithm.",1
"Image segmentation needs both local boundary position information and global object context information. The performance of the recent state-of-the-art method, fully convolutional networks, reaches a bottleneck due to the neural network limit after balancing between the two types of information simultaneously in an end-to-end training style. To overcome this problem, we divide the semantic image segmentation into temporal subtasks. First, we find a possible pixel position of some object boundary; then trace the boundary at steps within a limited length until the whole object is outlined. We present the first deep reinforcement learning approach to semantic image segmentation, called DeepOutline, which outperforms other algorithms in Coco detection leaderboard in the middle and large size person category in Coco val2017 dataset. Meanwhile, it provides an insight into a divide and conquer way by reinforcement learning on computer vision problems.",0
"In order to achieve accurate image segmentation, both local boundary position information and global object context information are necessary. However, the current state-of-the-art method, fully convolutional networks, faces limitations due to the neural network's ability to balance these two types of information in an end-to-end training style. To address this issue, we propose a novel approach called DeepOutline, which tackles semantic image segmentation by breaking it down into temporal subtasks. Our method first identifies a potential boundary position and then traces the boundary in steps until the entire object is outlined. By utilizing deep reinforcement learning, DeepOutline outperforms other algorithms in the Coco detection leaderboard for middle and large size person categories in the Coco val2017 dataset. Additionally, our approach showcases the potential of a divide and conquer approach to computer vision problems.",1
"We introduce the concept of derivate-based component-trees for images with an arbitrary number of channels. The approach is a natural extension of the classical component-tree devoted to gray-scale images. The similar structure enables the translation of many gray-level image processing techniques based on the component-tree to hyperspectral and color images. As an example application, we present an image segmentation approach that extracts Maximally Stable Homogeneous Regions (MSHR). The approach very similar to MSER but can be applied to images with an arbitrary number of channels. As opposed to MSER, our approach implicitly segments regions with are both lighter and darker than their background for gray-scale images and can be used in OCR applications where MSER will fail. We introduce a local flooding-based immersion for the derivate-based component-tree construction which is linear in the number of pixels. In the experiments, we show that the runtime scales favorably with an increasing number of channels and may improve algorithms which build on MSER.",0
"We present a novel concept of derivate-based component-trees that can be applied to images with any number of channels, building upon the classical component-tree used for grayscale images. This similarity enables the adaptation of numerous gray-level image processing techniques from the component-tree to hyperspectral and color images. Our method includes an image segmentation approach that extracts Maximally Stable Homogeneous Regions (MSHR), which is similar to MSER but can be used for images with any number of channels. Unlike MSER, our approach implicitly segments regions that are both lighter and darker than their background for grayscale images, making it more useful in OCR applications where MSER is inadequate. We propose a local flooding-based immersion for the construction of the derivate-based component-tree, which scales linearly with the number of pixels. Our experimental results demonstrate that the runtime is favorably scalable with an increasing number of channels, and may improve algorithms that are based on MSER.",1
"We present a multilinear statistical model of the human tongue that captures anatomical and tongue pose related shape variations separately. The model is derived from 3D magnetic resonance imaging data of 11 speakers sustaining speech related vocal tract configurations. The extraction is performed by using a minimally supervised method that uses as basis an image segmentation approach and a template fitting technique. Furthermore, it uses image denoising to deal with possibly corrupt data, palate surface information reconstruction to handle palatal tongue contacts, and a bootstrap strategy to refine the obtained shapes. Our evaluation concludes that limiting the degrees of freedom for the anatomical and speech related variations to 5 and 4, respectively, produces a model that can reliably register unknown data while avoiding overfitting effects. Furthermore, we show that it can be used to generate a plausible tongue animation by tracking sparse motion capture data.",0
"Our study introduces a statistical model for the human tongue that separates anatomical and tongue pose related shape variations. The model is based on 3D magnetic resonance imaging data from 11 speakers who maintained speech-related vocal tract configurations. We employ a minimally supervised technique that utilizes image segmentation and template fitting approaches to extract the model, along with image denoising, palate surface information reconstruction, and a bootstrap strategy to refine the obtained shapes. Our evaluation indicates that limiting the degrees of freedom for anatomical and speech-related variations to 5 and 4, respectively, produces a reliable model that avoids overfitting. Additionally, we demonstrate that it can generate a realistic tongue animation by tracking sparse motion capture data.",1
"We propose a novel locally adaptive learning estimator for enhancing the inter- and intra- discriminative capabilities of Deep Neural Networks, which can be used as improved loss layer for semantic image segmentation tasks. Most loss layers compute pixel-wise cost between feature maps and ground truths, ignoring spatial layouts and interactions between neighboring pixels with same object category, and thus networks cannot be effectively sensitive to intra-class connections. Stride by stride, our method firstly conducts adaptive pooling filter operating over predicted feature maps, aiming to merge predicted distributions over a small group of neighboring pixels with same category, and then it computes cost between the merged distribution vector and their category label. Such design can make groups of neighboring predictions from same category involved into estimations on predicting correctness with respect to their category, and hence train networks to be more sensitive to regional connections between adjacent pixels based on their categories. In the experiments on Pascal VOC 2012 segmentation datasets, the consistently improved results show that our proposed approach achieves better segmentation masks against previous counterparts.",0
"Our proposed method involves a locally adaptive learning estimator that enhances the discriminative capabilities of Deep Neural Networks for semantic image segmentation tasks. Unlike traditional loss layers that compute pixel-wise costs between feature maps and ground truths, our method considers spatial layouts and interactions between neighboring pixels with the same object category. This sensitivity to intra-class connections is achieved through an adaptive pooling filter that merges predicted distributions over a small group of neighboring pixels with the same category. The merged distribution vector is then compared to their category label to improve estimations on predicting correctness with respect to their category. Our experiments on Pascal VOC 2012 segmentation datasets demonstrate that our approach consistently outperforms previous counterparts.",1
"Semantic segmentation is an established while rapidly evolving field in medical imaging. In this paper we focus on the segmentation of brain Magnetic Resonance Images (MRI) into cerebral structures using convolutional neural networks (CNN). CNNs achieve good performance by finding effective high dimensional image features describing the patch content only. In this work, we propose different ways to introduce spatial constraints into the network to further reduce prediction inconsistencies.   A patch based CNN architecture was trained, making use of multiple scales to gather contextual information. Spatial constraints were introduced within the CNN through a distance to landmarks feature or through the integration of a probability atlas. We demonstrate experimentally that using spatial information helps to reduce segmentation inconsistencies.",0
"Medical imaging has an evolving but well-established field called semantic segmentation. Our paper focuses on segmenting cerebral structures from brain Magnetic Resonance Images (MRI) using convolutional neural networks (CNN). CNNs use high dimensional image features to effectively describe patch content and achieve good performance. We propose various methods to introduce spatial constraints into the network to further decrease prediction inconsistencies. To train the patch based CNN architecture, we use multiple scales to obtain contextual information. We introduce spatial constraints through a distance to landmarks feature or the integration of a probability atlas within the CNN. Our experimental results demonstrate that incorporating spatial information helps to decrease segmentation inconsistencies.",1
"This paper presents a novel unsupervised segmentation method for 3D medical images. Convolutional neural networks (CNNs) have brought significant advances in image segmentation. However, most of the recent methods rely on supervised learning, which requires large amounts of manually annotated data. Thus, it is challenging for these methods to cope with the growing amount of medical images. This paper proposes a unified approach to unsupervised deep representation learning and clustering for segmentation. Our proposed method consists of two phases. In the first phase, we learn deep feature representations of training patches from a target image using joint unsupervised learning (JULE) that alternately clusters representations generated by a CNN and updates the CNN parameters using cluster labels as supervisory signals. We extend JULE to 3D medical images by utilizing 3D convolutions throughout the CNN architecture. In the second phase, we apply k-means to the deep representations from the trained CNN and then project cluster labels to the target image in order to obtain the fully segmented image. We evaluated our methods on three images of lung cancer specimens scanned with micro-computed tomography (micro-CT). The automatic segmentation of pathological regions in micro-CT could further contribute to the pathological examination process. Hence, we aim to automatically divide each image into the regions of invasive carcinoma, noninvasive carcinoma, and normal tissue. Our experiments show the potential abilities of unsupervised deep representation learning for medical image segmentation.",0
"This study introduces a new unsupervised segmentation technique for 3D medical images. While convolutional neural networks have made progress in image segmentation, most current methods require extensive manual annotation for supervised learning. This is problematic as the number of medical images continues to grow. To address this, the authors propose a unified approach to unsupervised deep representation learning and clustering for segmentation. The method has two phases: first, deep feature representations of training patches from a target image are learned using joint unsupervised learning (JULE) that clusters representations generated by a CNN and updates CNN parameters using cluster labels as supervisory signals. JULE is extended to 3D medical images through the use of 3D convolutions. In the second phase, k-means is applied to the deep representations from the trained CNN, and cluster labels are projected onto the target image to obtain a fully segmented image. The authors tested their method on three images of lung cancer specimens scanned with micro-CT and aimed to divide each image into regions of invasive carcinoma, noninvasive carcinoma, and normal tissue. The study shows the potential of unsupervised deep representation learning for medical image segmentation.",1
"This paper presents a novel method for unsupervised segmentation of pathology images. Staging of lung cancer is a major factor of prognosis. Measuring the maximum dimensions of the invasive component in a pathology images is an essential task. Therefore, image segmentation methods for visualizing the extent of invasive and noninvasive components on pathology images could support pathological examination. However, it is challenging for most of the recent segmentation methods that rely on supervised learning to cope with unlabeled pathology images. In this paper, we propose a unified approach to unsupervised representation learning and clustering for pathology image segmentation. Our method consists of two phases. In the first phase, we learn feature representations of training patches from a target image using the spherical k-means. The purpose of this phase is to obtain cluster centroids which could be used as filters for feature extraction. In the second phase, we apply conventional k-means to the representations extracted by the centroids and then project cluster labels to the target images. We evaluated our methods on pathology images of lung cancer specimen. Our experiments showed that the proposed method outperforms traditional k-means segmentation and the multithreshold Otsu method both quantitatively and qualitatively with an improved normalized mutual information (NMI) score of 0.626 compared to 0.168 and 0.167, respectively. Furthermore, we found that the centroids can be applied to the segmentation of other slices from the same sample.",0
"The article introduces a unique technique for unsupervised segmentation of pathology images, which is crucial for staging lung cancer and determining prognosis. The measurement of the maximum dimensions of the invasive component in pathology images is essential. However, it is challenging for most segmentation methods to deal with unlabeled pathology images. The proposed method comprises two phases, wherein the first phase involves learning feature representations of training patches using spherical k-means to obtain cluster centroids as filters for feature extraction. The second phase applies conventional k-means to the extracted representations and projects cluster labels to the target images. The study evaluated the proposed method on pathology images of lung cancer specimens and found that it outperforms traditional k-means segmentation and multithreshold Otsu method, with an improved NMI score of 0.626, and that the centroids can be used for the segmentation of other slices from the same sample.",1
"Breast cancer is the most frequently diagnosed cancer and leading cause of cancer-related death among females worldwide. In this article, we investigate the applicability of densely connected convolutional neural networks to the problems of histology image classification and whole slide image segmentation in the area of computer-aided diagnoses for breast cancer. To this end, we study various approaches for transfer learning and apply them to the data set from the 2018 grand challenge on breast cancer histology images (BACH).",0
"Among females worldwide, breast cancer is the most commonly diagnosed cancer and the primary cause of cancer-related death. This article explores the potential of densely connected convolutional neural networks in the field of computer-aided diagnoses for breast cancer, specifically in histology image classification and whole slide image segmentation. Our research focuses on transfer learning techniques and their implementation in the analysis of the data set from the 2018 grand challenge on breast cancer histology images (BACH).",1
"Compared to the general semantic segmentation problem, portrait segmentation has higher precision requirement on boundary area. However, this problem has not been well studied in previous works. In this paper, we propose a boundary-sensitive deep neural network (BSN) for portrait segmentation. BSN introduces three novel techniques. First, an individual boundary-sensitive kernel is proposed by dilating the contour line and assigning the boundary pixels with multi-class labels. Second, a global boundary-sensitive kernel is employed as a position sensitive prior to further constrain the overall shape of the segmentation map. Third, we train a boundary-sensitive attribute classifier jointly with the segmentation network to reinforce the network with semantic boundary shape information. We have evaluated BSN on the current largest public portrait segmentation dataset, i.e, the PFCN dataset, as well as the portrait images collected from other three popular image segmentation datasets: COCO, COCO-Stuff, and PASCAL VOC. Our method achieves the superior quantitative and qualitative performance over state-of-the-arts on all the datasets, especially on the boundary area.",0
"Portrait segmentation necessitates higher precision on the boundary area than the general semantic segmentation problem, although this aspect has not been explored thoroughly in previous works. Our study proposes a boundary-sensitive deep neural network (BSN) that introduces three innovative techniques. Firstly, we define an individual boundary-sensitive kernel that enlarges the contour line and assigns multi-class labels to boundary pixels. Secondly, we use a global boundary-sensitive kernel as a position-sensitive prior to further constrain the segmentation map's overall shape. Thirdly, we jointly train a boundary-sensitive attribute classifier with the segmentation network to reinforce the network with semantic boundary shape information. We evaluated BSN on various datasets, including the PFCN dataset, COCO, COCO-Stuff, and PASCAL VOC, and our method surpasses state-of-the-art results in quantitative and qualitative performance, particularly in the boundary area.",1
"The Jaccard index, also referred to as the intersection-over-union score, is commonly employed in the evaluation of image segmentation results given its perceptual qualities, scale invariance - which lends appropriate relevance to small objects, and appropriate counting of false negatives, in comparison to per-pixel losses. We present a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex Lov\'asz extension of submodular losses. The loss is shown to perform better with respect to the Jaccard index measure than the traditionally used cross-entropy loss. We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset. We evaluate the impact of our method in a semantic segmentation pipeline and show substantially improved intersection-over-union segmentation scores on the Pascal VOC and Cityscapes datasets using state-of-the-art deep learning segmentation architectures.",0
"The Jaccard index, also known as the intersection-over-union score, is frequently used to evaluate image segmentation results due to its perceptual qualities, scale invariance, and accurate counting of false negatives. Our research proposes a new approach for optimizing the mean intersection-over-union loss in neural networks for semantic image segmentation. This method is based on the convex Lov\'asz extension of submodular losses and outperforms the cross-entropy loss in terms of the Jaccard index measure. We compare the results of optimizing the Jaccard index per image versus optimizing it over an entire dataset and demonstrate the positive impact of our method on the Pascal VOC and Cityscapes datasets using cutting-edge deep learning segmentation architectures. Our approach leads to significantly improved intersection-over-union segmentation scores.",1
"In this paper, we present a new image segmentation method based on the concept of sparse subset selection. Starting with an over-segmentation, we adopt local spectral histogram features to encode the visual information of the small segments into high-dimensional vectors, called superpixel features. Then, the superpixel features are fed into a novel convex model which efficiently leverages the features to group the superpixels into a proper number of coherent regions. Our model automatically determines the optimal number of coherent regions and superpixels assignment to shape final segments. To solve our model, we propose a numerical algorithm based on the alternating direction method of multipliers (ADMM), whose iterations consist of two highly parallelizable sub-problems. We show each sub-problem enjoys closed-form solution which makes the ADMM iterations computationally very efficient. Extensive experiments on benchmark image segmentation datasets demonstrate that our proposed method in combination with an over-segmentation can provide high quality and competitive results compared to the existing state-of-the-art methods.",0
"The article presents a novel technique for image segmentation that utilizes sparse subset selection. The process involves starting with an over-segmentation and then employing local spectral histogram features to encode visual information into superpixel features. These features are then fed into a convex model that effectively groups the superpixels into coherent regions, with the optimal number being automatically determined. To solve the model, an algorithm based on the ADMM is proposed, which consists of two sub-problems that enjoy closed-form solutions, making the iterations highly computationally efficient. The experiments conducted on benchmark image segmentation datasets demonstrate that the proposed technique, when used in combination with an over-segmentation, yields high-quality and competitive results compared to existing state-of-the-art methods.",1
"Deep convolutional neural networks (CNNs), especially fully convolutional networks, have been widely applied to automatic medical image segmentation problems, e.g., multi-organ segmentation. Existing CNN-based segmentation methods mainly focus on looking for increasingly powerful network architectures, but pay less attention to data sampling strategies for training networks more effectively. In this paper, we present a simple but effective sample selection method for training multi-organ segmentation networks. Sample selection exhibits an exploitation-exploration strategy, i.e., exploiting hard samples and exploring less frequently visited samples. Based on the fact that very hard samples might have annotation errors, we propose a new sample selection policy, named Relaxed Upper Confident Bound (RUCB). Compared with other sample selection policies, e.g., Upper Confident Bound (UCB), it exploits a range of hard samples rather than being stuck with a small set of very hard ones, which mitigates the influence of annotation errors during training. We apply this new sample selection policy to training a multi-organ segmentation network on a dataset containing 120 abdominal CT scans and show that it boosts segmentation performance significantly.",0
"Convolutional neural networks (CNNs), particularly those that are fully convolutional, are frequently used to solve medical image segmentation problems such as multi-organ segmentation. While current CNN-based segmentation methods focus on developing more powerful network architectures, they do not place enough emphasis on data sampling strategies that can improve network training effectiveness. This study presents a straightforward yet effective sample selection approach for training multi-organ segmentation networks. The sample selection process follows an exploitation-exploration strategy that involves exploiting difficult samples and exploring less frequently visited ones. Because very hard samples may contain annotation errors, a new sample selection policy called Relaxed Upper Confident Bound (RUCB) is proposed. This policy is more effective than other sample selection policies, such as Upper Confident Bound (UCB), which only focuses on a small number of very hard samples. By applying RUCB to train a multi-organ segmentation network on a dataset containing 120 abdominal CT scans, the study found that segmentation performance was significantly improved.",1
"Examining locomotion has improved our basic understanding of motor control and aided in treating motor impairment. Mice and rats are the model system of choice for basic neuroscience studies of human disease. High frame rates are needed to quantify the kinematics of running rodents, due to their high stride frequency. Manual tracking, especially for multiple body landmarks, becomes extremely time-consuming. To overcome these limitations, we proposed the use of superpixels based image segmentation as superpixels utilized both spatial and color information for segmentation. We segmented some parts of body and tested the success of segmentation as a function of color space and SLIC segment size. We used a simple merging function to connect the segmented regions considered as neighbor and having the same intensity value range. In addition, 28 features were extracted, and t-SNE was used to demonstrate how much the methods are capable to differentiate the regions. Finally, we compared the segmented regions to a manually outlined region. The results showed for segmentation, using the RGB image was slightly better compared to the hue channel. For merg- ing and classification, however, the hue representation was better as it captures the relevant color information in a single channel.",0
"The study of locomotion has enhanced our fundamental comprehension of motor control and has contributed to the treatment of motor impairment. To investigate human diseases, mice and rats are the preferred model system for basic neuroscience research. Because of their high stride frequency, analyzing the kinematics of running rodents requires high frame rates. Manual tracking of multiple body landmarks can be extremely time-consuming. To address these issues, we proposed using superpixels based image segmentation, which incorporates both spatial and color information for segmentation. We segmented certain body parts and tested the effectiveness of segmentation as a function of color space and SLIC segment size. We used a simple merging function to connect the segmented regions that were considered neighboring and had the same intensity value range. Additionally, we extracted 28 features, and t-SNE was used to demonstrate the ability of the methods to differentiate the regions. Finally, we compared the segmented regions to a manually outlined region. The results showed that using the RGB image was slightly better for segmentation than the hue channel. However, for merging and classification, the hue representation was better since it captures the relevant color information in a single channel.",1
"Image foreground extraction is a classical problem in image processing and vision, with a large range of applications. In this dissertation, we focus on the extraction of text and graphics in mixed-content images, and design novel approaches for various aspects of this problem.   We first propose a sparse decomposition framework, which models the background by a subspace containing smooth basis vectors, and foreground as a sparse and connected component. We then formulate an optimization framework to solve this problem, by adding suitable regularizations to the cost function to promote the desired characteristics of each component. We present two techniques to solve the proposed optimization problem, one based on alternating direction method of multipliers (ADMM), and the other one based on robust regression. Promising results are obtained for screen content image segmentation using the proposed algorithm.   We then propose a robust subspace learning algorithm for the representation of the background component using training images that could contain both background and foreground components, as well as noise. With the learnt subspace for the background, we can further improve the segmentation results, compared to using a fixed subspace. Lastly, we investigate a different class of signal/image decomposition problem, where only one signal component is active at each signal element. In this case, besides estimating each component, we need to find their supports, which can be specified by a binary mask. We propose a mixed-integer programming problem, that jointly estimates the two components and their supports through an alternating optimization scheme. We show the application of this algorithm on various problems, including image segmentation, video motion segmentation, and also separation of text from textured images.",0
"The extraction of text and graphics from mixed-content images is a well-known problem in image processing and vision, which has numerous applications. Within this thesis, we concentrate on this problem and present innovative approaches to various aspects. Our initial proposal is a sparse decomposition model, which interprets the background as a subspace of smooth basis vectors and the foreground as a sparse and connected component. To solve this issue, we develop an optimization framework with regularizations that encourage the desired characteristics of each element. We present two techniques to solve this problem, one using ADMM and the other based on robust regression, both of which produce promising results for screen content image segmentation. Additionally, we introduce a robust subspace learning algorithm for representing the background element using training images that may contain background, foreground, and noise. This approach improves the segmentation results compared to using a fixed subspace. Finally, we explore a signal/image decomposition problem where only one signal component is active at each element, and the supports are specified by a binary mask. We propose a mixed-integer programming problem that jointly estimates the two components and their supports through an alternating optimization scheme. This algorithm has various applications, including image and video motion segmentation and the separation of text from textured images.",1
"Superpixel-based Higher-order Conditional random fields (SP-HO-CRFs) are known for their effectiveness in enforcing both short and long spatial contiguity for pixelwise labelling in computer vision. However, their higher-order potentials are usually too complex to learn and often incur a high computational cost in performing inference. We propose an new approximation approach to SP-HO-CRFs that resolves these problems. Our approach is a multi-layer CRF framework that inherits the simplicity from pairwise CRFs by formulating both the higher-order and pairwise cues into the same pairwise potentials in the first layer. Essentially, this approach provides accuracy enhancement on the basis of pairwise CRFs without training by reusing their pre-trained parameters and/or weights. The proposed multi-layer approach performs especially well in delineating the boundary details (boarders) of object categories such as ""trees"" and ""bushes"". Multiple sets of experiments conducted on dataset MSRC-21 and PASCAL VOC 2012 validate the effectiveness and efficiency of the proposed methods.",0
"SP-HO-CRFs are well-known for their ability to ensure short and long spatial contiguity in pixelwise labeling for computer vision. However, their higher-order potentials are often too complicated to learn and can result in high computational costs during inference. To address these issues, we have developed a new approximation approach for SP-HO-CRFs. Our approach is a multi-layer CRF framework that combines higher-order and pairwise cues into the same pairwise potentials in the first layer. This approach improves accuracy without requiring additional training by utilizing pre-trained parameters and/or weights from pairwise CRFs. Our multi-layer approach is particularly effective in delineating object category boundaries, such as ""trees"" and ""bushes."" Multiple experiments conducted on datasets MSRC-21 and PASCAL VOC 2012 confirm the effectiveness and efficiency of our proposed methods.",1
"The Normalized Cut (NCut) objective function, widely used in data clustering and image segmentation, quantifies the cost of graph partitioning in a way that biases clusters or segments that are balanced towards having lower values than unbalanced partitionings. However, this bias is so strong that it avoids any singleton partitions, even when vertices are very weakly connected to the rest of the graph. Motivated by the B\""uhler-Hein family of balanced cut costs, we propose the family of Compassionately Conservative Balanced (CCB) Cut costs, which are indexed by a parameter that can be used to strike a compromise between the desire to avoid too many singleton partitions and the notion that all partitions should be balanced. We show that CCB-Cut minimization can be relaxed into an orthogonally constrained $\ell_{\tau}$-minimization problem that coincides with the problem of computing Piecewise Flat Embeddings (PFE) for one particular index value, and we present an algorithm for solving the relaxed problem by iteratively minimizing a sequence of reweighted Rayleigh quotients (IRRQ). Using images from the BSDS500 database, we show that image segmentation based on CCB-Cut minimization provides better accuracy with respect to ground truth and greater variability in region size than NCut-based image segmentation.",0
"The Normalized Cut (NCut) objective function is commonly used for data clustering and image segmentation. It assesses the cost of graph partitioning, favoring partitions that are balanced over unbalanced ones. However, this preference is so strong that it disregards singleton partitions, even when vertices have weak connections to the rest of the graph. To address this issue, we introduce the Compassionately Conservative Balanced (CCB) Cut costs, which are a family of balanced cut costs indexed by a parameter that balances the desire to avoid too many singleton partitions with the need for balanced partitions. We demonstrate that CCB-Cut minimization can be relaxed into an orthogonally constrained $\ell_{\tau}$-minimization problem that coincides with the problem of computing Piecewise Flat Embeddings (PFE) for one particular index value. We present an algorithm for solving this relaxed problem by iteratively minimizing a sequence of reweighted Rayleigh quotients (IRRQ). Our experiments on images from the BSDS500 database show that CCB-Cut-based image segmentation provides better accuracy and greater variability in region size compared to NCut-based segmentation.",1
"One of the most common tasks in medical imaging is semantic segmentation. Achieving this segmentation automatically has been an active area of research, but the task has been proven very challenging due to the large variation of anatomy across different patients. However, recent advances in deep learning have made it possible to significantly improve the performance of image recognition and semantic segmentation methods in the field of computer vision. Due to the data driven approaches of hierarchical feature learning in deep learning frameworks, these advances can be translated to medical images without much difficulty. Several variations of deep convolutional neural networks have been successfully applied to medical images. Especially fully convolutional architectures have been proven efficient for segmentation of 3D medical images. In this article, we describe how to build a 3D fully convolutional network (FCN) that can process 3D images in order to produce automatic semantic segmentations. The model is trained and evaluated on a clinical computed tomography (CT) dataset and shows state-of-the-art performance in multi-organ segmentation.",0
"Semantic segmentation is a common task in medical imaging that has proved challenging due to the diverse anatomy among patients. However, the advancements in deep learning have improved the performance of image recognition and semantic segmentation methods in computer vision. Deep learning frameworks' data-driven approaches have made it possible to apply these advances to medical images without difficulty. Medical images have successfully been segmented using various deep convolutional neural networks, particularly fully convolutional architectures for 3D medical images. This article explains how to construct a 3D fully convolutional network that segments 3D images automatically. The model is trained and evaluated on a clinical CT dataset and achieves state-of-the-art performance in multi-organ segmentation.",1
"Pixelwise segmentation of the left ventricular (LV) myocardium and the four cardiac chambers in 2-D steady state free precession (SSFP) cine sequences is an essential preprocessing step for a wide range of analyses. Variability in contrast, appearance, orientation, and placement of the heart between patients, clinical views, scanners, and protocols makes fully automatic semantic segmentation a notoriously difficult problem. Here, we present ${\Omega}$-Net (Omega-Net): a novel convolutional neural network (CNN) architecture for simultaneous localization, transformation into a canonical orientation, and semantic segmentation. First, an initial segmentation is performed on the input image, second, the features learned during this initial segmentation are used to predict the parameters needed to transform the input image into a canonical orientation, and third, a final segmentation is performed on the transformed image. In this work, ${\Omega}$-Nets of varying depths were trained to detect five foreground classes in any of three clinical views (short axis, SA, four-chamber, 4C, two-chamber, 2C), without prior knowledge of the view being segmented. The architecture was trained on a cohort of patients with hypertrophic cardiomyopathy and healthy control subjects. Network performance as measured by weighted foreground intersection-over-union (IoU) was substantially improved in the best-performing ${\Omega}$- Net compared with U-Net segmentation without localization or orientation. In addition, {\Omega}-Net was retrained from scratch on the 2017 MICCAI ACDC dataset, and achieves state-of-the-art results on the LV and RV bloodpools, and performed slightly worse in segmentation of the LV myocardium. We conclude this architecture represents a substantive advancement over prior approaches, with implications for biomedical image segmentation more generally.",0
"Performing pixelwise segmentation of the left ventricular (LV) myocardium and four cardiac chambers in 2-D steady state free precession (SSFP) cine sequences is a crucial preprocessing step for a variety of analyses. However, due to variations in contrast, appearance, orientation, and heart placement across patients, clinical views, scanners, and protocols, fully automatic semantic segmentation can be a challenging problem. To address this issue, we introduce ${\Omega}$-Net, a novel convolutional neural network (CNN) architecture that can simultaneously perform localization, transformation into a canonical orientation, and semantic segmentation. The network performs an initial segmentation on the input image, utilizes the learned features to predict the parameters required for transforming the input image into a canonical orientation, and finally performs a final segmentation on the transformed image. We trained ${\Omega}$-Nets of varying depths to detect five foreground classes in three clinical views (short axis, SA, four-chamber, 4C, two-chamber, 2C), without prior knowledge of the view being segmented. The architecture was trained on a cohort of patients with hypertrophic cardiomyopathy and healthy control subjects. Our results show that the best-performing ${\Omega}$-Net significantly improves network performance compared to U-Net segmentation without localization or orientation, as measured by weighted foreground intersection-over-union (IoU). Furthermore, ${\Omega}$-Net achieved state-of-the-art results on the LV and RV bloodpools when trained on the 2017 MICCAI ACDC dataset, with slightly worse performance in LV myocardium segmentation. Our findings suggest that ${\Omega}$-Net represents a significant advance in biomedical image segmentation and has broad implications for this field.",1
"Recent advances in 3D fully convolutional networks (FCN) have made it feasible to produce dense voxel-wise predictions of volumetric images. In this work, we show that a multi-class 3D FCN trained on manually labeled CT scans of several anatomical structures (ranging from the large organs to thin vessels) can achieve competitive segmentation results, while avoiding the need for handcrafting features or training class-specific models.   To this end, we propose a two-stage, coarse-to-fine approach that will first use a 3D FCN to roughly define a candidate region, which will then be used as input to a second 3D FCN. This reduces the number of voxels the second FCN has to classify to ~10% and allows it to focus on more detailed segmentation of the organs and vessels.   We utilize training and validation sets consisting of 331 clinical CT images and test our models on a completely unseen data collection acquired at a different hospital that includes 150 CT scans, targeting three anatomical organs (liver, spleen, and pancreas). In challenging organs such as the pancreas, our cascaded approach improves the mean Dice score from 68.5 to 82.2%, achieving the highest reported average score on this dataset. We compare with a 2D FCN method on a separate dataset of 240 CT scans with 18 classes and achieve a significantly higher performance in small organs and vessels. Furthermore, we explore fine-tuning our models to different datasets.   Our experiments illustrate the promise and robustness of current 3D FCN based semantic segmentation of medical images, achieving state-of-the-art results. Our code and trained models are available for download: https://github.com/holgerroth/3Dunet_abdomen_cascade.",0
"Advancements in 3D fully convolutional networks (FCN) have enabled the production of dense voxel-wise predictions of volumetric images. This study demonstrates that a multi-class 3D FCN, trained on manually labeled CT scans of various anatomical structures, can achieve competitive segmentation results without the need for handcrafting features or training class-specific models. The proposed two-stage approach involves using a 3D FCN to define a candidate region, which is then inputted into a second 3D FCN for more detailed segmentation. Training and validation sets of 331 clinical CT images were used, and the models were tested on 150 CT scans from a different hospital targeting three anatomical organs. The cascaded approach improved the mean Dice score from 68.5 to 82.2%, achieving the highest reported average score on this dataset. Comparative analysis with a 2D FCN method on a separate dataset of 240 CT scans with 18 classes demonstrated significantly higher performance in small organs and vessels. Additionally, the study explored fine-tuning the models to different datasets. The results show the potential and robustness of current 3D FCN based semantic segmentation of medical images, achieving state-of-the-art performance. The code and trained models are available for download at https://github.com/holgerroth/3Dunet_abdomen_cascade.",1
"This work presents a region-growing image segmentation approach based on superpixel decomposition. From an initial contour-constrained over-segmentation of the input image, the image segmentation is achieved by iteratively merging similar superpixels into regions. This approach raises two key issues: (1) how to compute the similarity between superpixels in order to perform accurate merging and (2) in which order those superpixels must be merged together. In this perspective, we firstly introduce a robust adaptive multi-scale superpixel similarity in which region comparisons are made both at content and common border level. Secondly, we propose a global merging strategy to efficiently guide the region merging process. Such strategy uses an adpative merging criterion to ensure that best region aggregations are given highest priorities. This allows to reach a final segmentation into consistent regions with strong boundary adherence. We perform experiments on the BSDS500 image dataset to highlight to which extent our method compares favorably against other well-known image segmentation algorithms. The obtained results demonstrate the promising potential of the proposed approach.",0
The paper outlines a technique for segmenting images using region-growing and superpixel decomposition. The segmentation process involves merging similar superpixels iteratively to create larger regions. Two primary challenges are addressed: determining the appropriate similarity metric for merging superpixels and selecting the optimal order for merging them. The authors propose a multi-scale superpixel similarity approach that considers both content and common borders. They also introduce a global merging strategy that prioritizes the best region aggregations. The resulting segmentation method produces consistent regions with strong boundary adherence. The method was evaluated on the BSDS500 image dataset and was found to outperform other commonly used image segmentation algorithms. These results demonstrate the potential of the proposed approach.,1
"State-of-the-art systems for semantic image segmentation use feed-forward pipelines with fixed computational costs. Building an image segmentation system that works across a range of computational budgets is challenging and time-intensive as new architectures must be designed and trained for every computational setting. To address this problem we develop a recurrent neural network that successively improves prediction quality with each iteration. Importantly, the RNN may be deployed across a range of computational budgets by merely running the model for a variable number of iterations. We find that this architecture is uniquely suited for efficiently segmenting videos. By exploiting the segmentation of past frames, the RNN can perform video segmentation at similar quality but reduced computational cost compared to state-of-the-art image segmentation methods. When applied to static images in the PASCAL VOC 2012 and Cityscapes segmentation datasets, the RNN traces out a speed-accuracy curve that saturates near the performance of state-of-the-art segmentation methods.",0
"Current techniques for semantic image segmentation employ fixed computational costs using feed-forward pipelines. Creating an image segmentation system that is effective across various computational budgets is challenging and requires significant time investment, as new architectures must be developed and trained for each computational setting. To overcome this obstacle, we have developed a recurrent neural network that enhances prediction accuracy with each iteration, enabling it to be implemented across a range of computational budgets simply by adjusting the number of iterations. This architecture is especially well-suited for efficiently segmenting videos by leveraging the segmentation of preceding frames. When compared to modern image segmentation methods, our RNN can perform video segmentation at comparable quality but with less computational expenditure. We have observed that when applied to static images in the PASCAL VOC 2012 and Cityscapes segmentation datasets, the RNN's speed-accuracy curve approaches the performance of state-of-the-art segmentation methods.",1
"Modern deep learning algorithms have triggered various image segmentation approaches. However most of them deal with pixel based segmentation. However, superpixels provide a certain degree of contextual information while reducing computation cost. In our approach, we have performed superpixel level semantic segmentation considering 3 various levels as neighbours for semantic contexts. Furthermore, we have enlisted a number of ensemble approaches like max-voting and weighted-average. We have also used the Dempster-Shafer theory of uncertainty to analyze confusion among various classes. Our method has proved to be superior to a number of different modern approaches on the same dataset.",0
"Numerous image segmentation methods have been triggered by contemporary deep learning algorithms. Nevertheless, the majority of these methods are focused on pixel-based segmentation. Despite this, superpixels can reduce computation cost while providing a certain degree of contextual information. In our approach, we have conducted semantic segmentation at the superpixel level, taking into account three different levels as neighboring semantic contexts. Additionally, we have employed several ensemble methods such as max-voting and weighted-average. Moreover, we have utilized the Dempster-Shafer theory of uncertainty to assess confusion among different classes. Our technique has demonstrated superior performance compared to various contemporary approaches on the same dataset.",1
"With pervasive applications of medical imaging in health-care, biomedical image segmentation plays a central role in quantitative analysis, clinical diagno- sis, and medical intervention. Since manual anno- tation su ers limited reproducibility, arduous e orts, and excessive time, automatic segmentation is desired to process increasingly larger scale histopathological data. Recently, deep neural networks (DNNs), par- ticularly fully convolutional networks (FCNs), have been widely applied to biomedical image segmenta- tion, attaining much improved performance. At the same time, quantization of DNNs has become an ac- tive research topic, which aims to represent weights with less memory (precision) to considerably reduce memory and computation requirements of DNNs while maintaining acceptable accuracy. In this paper, we apply quantization techniques to FCNs for accurate biomedical image segmentation. Unlike existing litera- ture on quantization which primarily targets memory and computation complexity reduction, we apply quan- tization as a method to reduce over tting in FCNs for better accuracy. Speci cally, we focus on a state-of- the-art segmentation framework, suggestive annotation [22], which judiciously extracts representative annota- tion samples from the original training dataset, obtain- ing an e ective small-sized balanced training dataset. We develop two new quantization processes for this framework: (1) suggestive annotation with quantiza- tion for highly representative training samples, and (2) network training with quantization for high accuracy. Extensive experiments on the MICCAI Gland dataset show that both quantization processes can improve the segmentation performance, and our proposed method exceeds the current state-of-the-art performance by up to 1%. In addition, our method has a reduction of up to 6.4x on memory usage.",0
"Biomedical image segmentation is essential in health-care for quantitative analysis, clinical diagnosis, and medical intervention due to the widespread use of medical imaging. However, manual annotation has limitations such as limited reproducibility, excessive time and arduous efforts, making automatic segmentation a desired solution, especially for larger scale histopathological data. Fully convolutional networks (FCNs), particularly deep neural networks (DNNs), have shown improved performance in biomedical image segmentation. Additionally, quantization of DNNs has become an active research topic to reduce memory and computation requirements while maintaining acceptable accuracy. This paper applies quantization techniques to FCNs for accurate biomedical image segmentation, specifically focusing on a segmentation framework called suggestive annotation. Two new quantization processes are proposed: suggestive annotation with quantization for highly representative training samples, and network training with quantization for high accuracy. Experiments on the MICCAI Gland dataset show that both quantization processes improve segmentation performance and reduce memory usage by up to 6.4x, with the proposed method exceeding the current state-of-the-art by up to 1%.",1
"Image segmentation is the process of partitioning the image into significant regions easier to analyze. Nowadays, segmentation has become a necessity in many practical medical imaging methods as locating tumors and diseases. Hidden Markov Random Field model is one of several techniques used in image segmentation. It provides an elegant way to model the segmentation process. This modeling leads to the minimization of an objective function. Conjugate Gradient algorithm (CG) is one of the best known optimization techniques. This paper proposes the use of the Conjugate Gradient algorithm (CG) for image segmentation, based on the Hidden Markov Random Field. Since derivatives are not available for this expression, finite differences are used in the CG algorithm to approximate the first derivative. The approach is evaluated using a number of publicly available images, where ground truth is known. The Dice Coefficient is used as an objective criterion to measure the quality of segmentation. The results show that the proposed CG approach compares favorably with other variants of Hidden Markov Random Field segmentation algorithms.",0
"The partitioning of an image into regions that are easier to analyze is known as image segmentation. This process has now become essential in the field of medical imaging for identifying diseases and tumors. A popular technique for image segmentation is the Hidden Markov Random Field model, which provides an elegant way to model the segmentation process by minimizing an objective function. The Conjugate Gradient algorithm (CG) is a well-known optimization technique that is proposed in this paper for image segmentation based on the Hidden Markov Random Field model. As derivatives are not available for this expression, finite differences are used in the CG algorithm to approximate the first derivative. The quality of the segmentation is evaluated using the Dice Coefficient as an objective criterion and is compared with other segmentation algorithms. The results demonstrate that the proposed CG approach is comparable to other variants of Hidden Markov Random Field segmentation algorithms.",1
"As the demand for enabling high-level autonomous driving has increased in recent years and visual perception is one of the critical features to enable fully autonomous driving, in this paper, we introduce an efficient approach for simultaneous object detection, depth estimation and pixel-level semantic segmentation using a shared convolutional architecture. The proposed network model, which we named Driving Scene Perception Network (DSPNet), uses multi-level feature maps and multi-task learning to improve the accuracy and efficiency of object detection, depth estimation and image segmentation tasks from a single input image. Hence, the resulting network model uses less than 850 MiB of GPU memory and achieves 14.0 fps on NVIDIA GeForce GTX 1080 with a 1024x512 input image, and both precision and efficiency have been improved over combination of single tasks.",0
"In this paper, we present a method to enable high-level autonomous driving by addressing the critical feature of visual perception. Our approach involves simultaneous object detection, depth estimation, and pixel-level semantic segmentation using a shared convolutional architecture. The Driving Scene Perception Network (DSPNet) employs multi-level feature maps and multi-task learning to enhance accuracy and efficiency from a single input image. This results in a network model that uses less than 850 MiB GPU memory and achieves 14.0 fps on NVIDIA GeForce GTX 1080 with a 1024x512 input image. The combination of single tasks has been surpassed in terms of both precision and efficiency.",1
"Image segmentation is still an open problem especially when intensities of the interested objects are overlapped due to the presence of intensity inhomogeneity (also known as bias field). To segment images with intensity inhomogeneities, a bias correction embedded level set model is proposed where Inhomogeneities are Estimated by Orthogonal Primary Functions (IEOPF). In the proposed model, the smoothly varying bias is estimated by a linear combination of a given set of orthogonal primary functions. An inhomogeneous intensity clustering energy is then defined and membership functions of the clusters described by the level set function are introduced to rewrite the energy as a data term of the proposed model. Similar to popular level set methods, a regularization term and an arc length term are also included to regularize and smooth the level set function, respectively. The proposed model is then extended to multichannel and multiphase patterns to segment colourful images and images with multiple objects, respectively. It has been extensively tested on both synthetic and real images that are widely used in the literature and public BrainWeb and IBSR datasets. Experimental results and comparison with state-of-the-art methods demonstrate that advantages of the proposed model in terms of bias correction and segmentation accuracy.",0
"The challenge of image segmentation remains unresolved, especially when the intensities of objects of interest overlap due to the presence of intensity inhomogeneity (known as bias field). To address this issue, a level set model that incorporates bias correction is proposed, using Inhomogeneities Estimated by Orthogonal Primary Functions (IEOPF). This model estimates the smoothly varying bias by combining a set of orthogonal primary functions, and defines an inhomogeneous intensity clustering energy. Membership functions of the clusters are introduced to rewrite the energy as a data term. The model also includes a regularization term and an arc length term to smooth and regularize the level set function. The proposed model is extended to handle multichannel and multiphase patterns for segmenting colorful images and images with multiple objects. The model is extensively tested on both synthetic and real images from literature, as well as the public BrainWeb and IBSR datasets. The experimental results demonstrate that the proposed model offers superior performance in terms of bias correction and segmentation accuracy, compared to state-of-the-art methods.",1
"Robust cross-seasonal localization is one of the major challenges in long-term visual navigation of autonomous vehicles. In this paper, we exploit recent advances in semantic segmentation of images, i.e., where each pixel is assigned a label related to the type of object it represents, to attack the problem of long-term visual localization. We show that semantically labeled 3-D point maps of the environment, together with semantically segmented images, can be efficiently used for vehicle localization without the need for detailed feature descriptors (SIFT, SURF, etc.). Thus, instead of depending on hand-crafted feature descriptors, we rely on the training of an image segmenter. The resulting map takes up much less storage space compared to a traditional descriptor based map. A particle filter based semantic localization solution is compared to one based on SIFT-features, and even with large seasonal variations over the year we perform on par with the larger and more descriptive SIFT-features, and are able to localize with an error below 1 m most of the time.",0
"Challenging long-term visual navigation of autonomous vehicles is to achieve strong localization across seasons. To address this issue, we have utilized advancements in semantic segmentation of images, by assigning a label to each pixel that represents the type of object it is. This approach can be used for efficient vehicle localization without the requirement of detailed feature descriptors like SIFT or SURF. We have trained an image segmenter instead of relying on hand-crafted feature descriptors. The resulting map requires less storage space than traditional descriptor-based maps. We have compared a particle filter-based semantic localization solution with one based on SIFT-features, and our method has performed equally well, even with significant seasonal variations throughout the year. We have been able to achieve localization with an error below 1 m most of the time.",1
"We propose a novel traffic sign detection system that simultaneously estimates the location and precise boundary of traffic signs using convolutional neural network (CNN). Estimating the precise boundary of traffic signs is important in navigation systems for intelligent vehicles where traffic signs can be used as 3D landmarks for road environment. Previous traffic sign detection systems, including recent methods based on CNN, only provide bounding boxes of traffic signs as output, and thus requires additional processes such as contour estimation or image segmentation to obtain the precise sign boundary. In this work, the boundary estimation of traffic signs is formulated as a 2D pose and shape class prediction problem, and this is effectively solved by a single CNN. With the predicted 2D pose and the shape class of a target traffic sign in an input image, we estimate the actual boundary of the target sign by projecting the boundary of a corresponding template sign image into the input image plane. By formulating the boundary estimation problem as a CNN-based pose and shape prediction task, our method is end-to-end trainable, and more robust to occlusion and small targets than other boundary estimation methods that rely on contour estimation or image segmentation. The proposed method with architectural optimization provides an accurate traffic sign boundary estimation which is also efficient in compute, showing a detection frame rate higher than 7 frames per second on low-power mobile platforms.",0
"Our innovative traffic sign detection system uses a convolutional neural network (CNN) to estimate both the location and precise boundary of traffic signs. Accurately identifying the boundary of traffic signs is crucial for intelligent vehicle navigation systems, as they can serve as 3D landmarks for road environments. Unlike previous CNN-based methods that only provide bounding boxes for traffic signs, our approach formulates the boundary estimation as a 2D pose and shape class prediction problem, which is solved by a single CNN. By predicting the 2D pose and shape class of a target traffic sign in an input image, we can estimate its actual boundary by projecting the corresponding template sign image's boundary into the input image plane. This end-to-end trainable method is more robust to occlusion and small targets than other boundary estimation techniques that rely on contour estimation or image segmentation. Our optimized architecture provides accurate traffic sign boundary estimation that is computationally efficient, achieving a detection frame rate of over 7 frames per second on low-power mobile platforms.",1
"Medical image analysis, especially segmenting a specific organ, has an important role in developing clinical decision support systems. In cardiac magnetic resonance (MR) imaging, segmenting the left and right ventricles helps physicians diagnose different heart abnormalities. There are challenges for this task, including the intensity and shape similarity between left ventricle and other organs, inaccurate boundaries and presence of noise in most of the images. In this paper we propose an automated method for segmenting the left ventricle in cardiac MR images. We first automatically extract the region of interest, and then employ it as an input of a fully convolutional network. We train the network accurately despite the small number of left ventricle pixels in comparison with the whole image. Thresholding on the output map of the fully convolutional network and selection of regions based on their roundness are performed in our proposed post-processing phase. The Dice score of our method reaches 87.24% by applying this algorithm on the York dataset of heart images.",0
"Developing clinical decision support systems heavily relies on medical image analysis, especially in segmenting specific organs. In the case of cardiac magnetic resonance (MR) imaging, the segmentation of the left and right ventricles plays a crucial role in the diagnosis of various heart abnormalities. However, this task poses several challenges, such as the similarity in intensity and shape between the left ventricle and other organs, inaccuracy in boundary detection, and the presence of noise in most images. In this study, we propose an automated method for segmenting the left ventricle in cardiac MR images. Our approach involves automatic extraction of the region of interest, which we then use as an input for a fully convolutional network. Despite the small number of left ventricle pixels in comparison to the entire image, we train the network accurately. In our proposed post-processing phase, we perform thresholding on the output map of the fully convolutional network and select regions based on their roundness. Applying this algorithm to the York dataset of heart images yields a Dice score of 87.24% for our method.",1
"Image segmentation is the process of partitioning an image into a set of meaningful regions according to some criteria. Hierarchical segmentation has emerged as a major trend in this regard as it favors the emergence of important regions at different scales. On the other hand, many methods allow us to have prior information on the position of structures of interest in the images. In this paper, we present a versatile hierarchical segmentation method that takes into account any prior spatial information and outputs a hierarchical segmentation that emphasizes the contours or regions of interest while preserving the important structures in the image. An application of this method to the weakly-supervised segmentation problem is presented.",0
"The act of dividing an image into significant regions based on certain criteria is known as image segmentation. Hierarchical segmentation is becoming increasingly popular because it highlights important regions at various scales. Additionally, there are several methods that allow us to have prior knowledge about the location of critical structures in the images. This article presents a flexible hierarchical segmentation technique that considers any prior spatial information and produces a hierarchical segmentation that emphasizes the boundaries or regions of importance while preserving significant structures in the image. Furthermore, the paper demonstrates the application of this technique to the weakly-supervised segmentation problem.",1
"Osteoarthritis (OA) is one of the major health issues among the elderly population. MRI is the most popular technology to observe and evaluate the progress of OA course. However, the extreme labor cost of MRI analysis makes the process inefficient and expensive. Also, due to human error and subjective nature, the inter- and intra-observer variability is rather high. Computer-aided knee MRI segmentation is currently an active research field because it can alleviate doctors and radiologists from the time consuming and tedious job, and improve the diagnosis performance which has immense potential for both clinic and scientific research. In the past decades, researchers have investigated automatic/semi-automatic knee MRI segmentation methods extensively. However, to the best of our knowledge, there is no comprehensive survey paper in this field yet. In this survey paper, we classify the existing methods by their principles and discuss the current research status and point out the future research trend in-depth.",0
"Among the elderly population, osteoarthritis (OA) is a significant health concern, and MRI is the most commonly used technology to monitor and assess the progression of OA. However, the high labor costs associated with MRI analysis make the process inefficient and costly. Additionally, inter- and intra-observer variability is high due to human error and subjectivity. As a result, computer-aided knee MRI segmentation is a rapidly growing field of research that has significant potential for improving diagnosis performance in both clinical and scientific settings. Although numerous automatic and semi-automatic segmentation techniques have been studied over the past few decades, a comprehensive survey paper on the subject has yet to be written. This paper aims to classify existing segmentation methods based on their principles, discuss current research status, and identify future research trends in detail.",1
"This paper proposes a generative ScatterNet hybrid deep learning (G-SHDL) network for semantic image segmentation. The proposed generative architecture is able to train rapidly from relatively small labeled datasets using the introduced structural priors. In addition, the number of filters in each layer of the architecture is optimized resulting in a computationally efficient architecture. The G-SHDL network produces state-of-the-art classification performance against unsupervised and semi-supervised learning on two image datasets. Advantages of the G-SHDL network over supervised methods are demonstrated with experiments performed on training datasets of reduced size.",0
"The proposed approach in this paper is the G-SHDL network, which is a hybrid deep learning model that generates ScatterNet. It is designed for semantic image segmentation and can quickly learn from small labeled datasets by using structural priors. Moreover, the network has been optimized to have an efficient architecture with an optimal number of filters in each layer. The G-SHDL network outperforms unsupervised and semi-supervised learning methods on two image datasets. The experiments conducted also demonstrate that the G-SHDL network has advantages over supervised methods, particularly when trained on smaller datasets.",1
"Deep neural networks have been successfully applied to problems such as image segmentation, image super-resolution, coloration and image inpainting. In this work we propose the use of convolutional neural networks (CNN) for image inpainting of large regions in high-resolution textures. Due to limited computational resources processing high-resolution images with neural networks is still an open problem. Existing methods separate inpainting of global structure and the transfer of details, which leads to blurry results and loss of global coherence in the detail transfer step. Based on advances in texture synthesis using CNNs we propose patch-based image inpainting by a CNN that is able to optimize for global as well as detail texture statistics. Our method is capable of filling large inpainting regions, oftentimes exceeding the quality of comparable methods for high-resolution images. For reference patch look-up we propose to use the same summary statistics that are used in the inpainting process.",0
"The successful application of deep neural networks includes solving problems such as image segmentation, image super-resolution, coloration, and image inpainting. The aim of this study is to suggest the use of convolutional neural networks (CNN) for image inpainting of large regions in high-resolution textures. However, processing high-resolution images with neural networks is still a challenge due to limited computational resources. Existing methods are known to separate inpainting of global structure and the transfer of details, resulting in blurry outcomes and loss of global coherence in the detail transfer step. Based on advancements in texture synthesis using CNNs, this study proposes a patch-based image inpainting technique using a CNN that can optimize both global and detail texture statistics. The method is capable of effectively filling large inpainting regions, often exceeding the quality of comparable methods for high-resolution images. Additionally, the study suggests using the same summary statistics used in the inpainting process for reference patch look-up.",1
"This review presents an in-depth study of the literature on segmentation methods applied in dental imaging. Ten segmentation methods were studied and categorized according to the type of the segmentation method (region-based, threshold-based, cluster-based, boundary-based or watershed-based), type of X-ray images used (intra-oral or extra-oral) and characteristics of the dataset used to evaluate the methods in the state-of-the-art works. We found that the literature has primarily focused on threshold-based segmentation methods (54%). 80% of the reviewed papers have used intra-oral X-ray images in their experiments, demonstrating preference to perform segmentation on images of already isolated parts of the teeth, rather than using extra-oral X-rays, which show tooth structure of the mouth and bones of the face. To fill a scientific gap in the field, a novel data set based on extra-oral X-ray images are proposed here. A statistical comparison of the results found with the 10 image segmentation methods over our proposed data set comprised of 1,500 images is also carried out, providing a more comprehensive source of performance assessment. Discussion on limitations of the methods conceived over the past year as well as future perspectives on exploiting learning-based segmentation methods to improve performance are also provided.",0
"The aim of this review is to examine segmentation methods used in dental imaging in detail. The review analyzed ten segmentation methods and categorized them based on the type of method used for segmentation, such as region-based, threshold-based, cluster-based, boundary-based or watershed-based. The review also categorized the type of X-ray images used for segmentation, such as intra-oral or extra-oral. The characteristics of the dataset used to evaluate the methods were also taken into account. The literature was found to primarily focus on threshold-based segmentation methods (54%). Intra-oral X-ray images were preferred over extra-oral X-rays (80%) for segmentation, creating a gap in the field. To address this gap, a new dataset based on extra-oral X-ray images was proposed. The review also conducted a statistical comparison of the results of the ten image segmentation methods on the proposed dataset of 1,500 images. The limitations of the methods developed in the past were discussed, and future perspectives on exploiting learning-based segmentation methods were explored to improve performance.",1
"Image segmentation is an important component of many image understanding systems. It aims to group pixels in a spatially and perceptually coherent manner. Typically, these algorithms have a collection of parameters that control the degree of over-segmentation produced. It still remains a challenge to properly select such parameters for human-like perceptual grouping. In this work, we exploit the diversity of segments produced by different choices of parameters. We scan the segmentation parameter space and generate a collection of image segmentation hypotheses (from highly over-segmented to under-segmented). These are fed into a cost minimization framework that produces the final segmentation by selecting segments that: (1) better describe the natural contours of the image, and (2) are more stable and persistent among all the segmentation hypotheses. We compare our algorithm's performance with state-of-the-art algorithms, showing that we can achieve improved results. We also show that our framework is robust to the choice of segmentation kernel that produces the initial set of hypotheses.",0
"Many systems rely on image segmentation as a crucial element to understand images. The objective is to group pixels in a cohesive manner, both spatially and perceptually. Such algorithms possess various parameters that dictate the level of over-segmentation produced, presenting a challenge to select appropriate parameters for human-like perceptual grouping. This study addresses this issue by utilizing the variety of segments produced by different parameter choices. By scanning the parameter space, we create a range of image segmentation hypotheses, from highly over-segmented to under-segmented. These hypotheses are then analyzed by a cost minimization framework that selects segments based on their ability to represent the image's natural contours and stability across the various segmentation hypotheses. Our approach outperforms other algorithms, and we demonstrate our system's resilience to the segmentation kernel used to generate the initial hypotheses.",1
"Superpixel segmentation has become an important research problem in image processing. In this paper, we propose an Iterative Spanning Forest (ISF) framework, based on sequences of Image Foresting Transforms, where one can choose i) a seed sampling strategy, ii) a connectivity function, iii) an adjacency relation, and iv) a seed pixel recomputation procedure to generate improved sets of connected superpixels (supervoxels in 3D) per iteration. The superpixels in ISF structurally correspond to spanning trees rooted at those seeds. We present five ISF methods to illustrate different choices of its components. These methods are compared with approaches from the state-of-the-art in effectiveness and efficiency. The experiments involve 2D and 3D datasets with distinct characteristics, and a high level application, named sky image segmentation. The theoretical properties of ISF are demonstrated in the supplementary material and the results show that some of its methods are competitive with or superior to the best baselines in effectiveness and efficiency.",0
"The segmentation of superpixels is a crucial issue in image processing, and the Iterative Spanning Forest (ISF) framework is proposed in this article to address it. The ISF framework is built on sequences of Image Foresting Transforms and offers a choice of four key components for generating improved sets of connected superpixels: seed sampling strategy, connectivity function, adjacency relation, and seed pixel recomputation procedure. The resulting superpixels correspond to spanning trees rooted at the chosen seeds. The article presents five ISF methods that demonstrate different component choices and compares their effectiveness and efficiency with other state-of-the-art approaches. The experiments use 2D and 3D datasets with distinct characteristics and a sky image segmentation application. The supplementary material demonstrates the theoretical properties of ISF, and the results show that some of its methods are as good as or even better than the best baselines in terms of effectiveness and efficiency.",1
"Deep convolutional networks for semantic image segmentation typically require large-scale labeled data, e.g. ImageNet and MS COCO, for network pre-training. To reduce annotation efforts, self-supervised semantic segmentation is recently proposed to pre-train a network without any human-provided labels. The key of this new form of learning is to design a proxy task (e.g. image colorization), from which a discriminative loss can be formulated on unlabeled data. Many proxy tasks, however, lack the critical supervision signals that could induce discriminative representation for the target image segmentation task. Thus self-supervision's performance is still far from that of supervised pre-training. In this study, we overcome this limitation by incorporating a ""mix-and-match"" (M&M) tuning stage in the self-supervision pipeline. The proposed approach is readily pluggable to many self-supervision methods and does not use more annotated samples than the original process. Yet, it is capable of boosting the performance of target image segmentation task to surpass fully-supervised pre-trained counterpart. The improvement is made possible by better harnessing the limited pixel-wise annotations in the target dataset. Specifically, we first introduce the ""mix"" stage, which sparsely samples and mixes patches from the target set to reflect rich and diverse local patch statistics of target images. A ""match"" stage then forms a class-wise connected graph, which can be used to derive a strong triplet-based discriminative loss for fine-tuning the network. Our paradigm follows the standard practice in existing self-supervised studies and no extra data or label is required. With the proposed M&M approach, for the first time, a self-supervision method can achieve comparable or even better performance compared to its ImageNet pre-trained counterpart on both PASCAL VOC2012 dataset and CityScapes dataset.",0
"Semantic image segmentation using deep convolutional networks typically requires large amounts of labeled data, such as ImageNet and MS COCO, for pre-training. To reduce the need for human-labeled data, a new technique called self-supervised semantic segmentation has been proposed, which involves designing a proxy task that can generate a discriminative loss on unlabeled data. However, existing proxy tasks lack critical supervision signals that would induce discriminative representation for image segmentation. To address this limitation, we introduce a ""mix-and-match"" tuning stage in the self-supervision pipeline, which is easily adaptable to existing methods and does not require additional annotated samples. This approach uses limited pixel-wise annotations in the target dataset to mix and match patches and form a class-wise connected graph, resulting in a strong triplet-based discriminative loss for fine-tuning the network. Our method achieves comparable or better performance than ImageNet pre-trained models on the PASCAL VOC2012 and CityScapes datasets, without requiring extra data or labels.",1
"This paper reports Deep LOGISMOS approach to 3D tumor segmentation by incorporating boundary information derived from deep contextual learning to LOGISMOS - layered optimal graph image segmentation of multiple objects and surfaces. Accurate and reliable tumor segmentation is essential to tumor growth analysis and treatment selection. A fully convolutional network (FCN), UNet, is first trained using three adjacent 2D patches centered at the tumor, providing contextual UNet segmentation and probability map for each 2D patch. The UNet segmentation is then refined by Gaussian Mixture Model (GMM) and morphological operations. The refined UNet segmentation is used to provide the initial shape boundary to build a segmentation graph. The cost for each node of the graph is determined by the UNet probability maps. Finally, a max-flow algorithm is employed to find the globally optimal solution thus obtaining the final segmentation. For evaluation, we applied the method to pancreatic tumor segmentation on a dataset of 51 CT scans, among which 30 scans were used for training and 21 for testing. With Deep LOGISMOS, DICE Similarity Coefficient (DSC) and Relative Volume Difference (RVD) reached 83.2+-7.8% and 18.6+-17.4% respectively, both are significantly improved (p<0.05) compared with contextual UNet and/or LOGISMOS alone.",0
"In this paper, the authors present the Deep LOGISMOS method for 3D tumor segmentation, which incorporates boundary information derived from deep contextual learning into the LOGISMOS approach. Accurate tumor segmentation is crucial for analyzing tumor growth and selecting appropriate treatment. The authors first train a fully convolutional network (FCN), UNet, using three adjacent 2D patches centered at the tumor to provide contextual UNet segmentation and probability maps for each patch. The UNet segmentation is refined using Gaussian Mixture Model (GMM) and morphological operations before being used to generate an initial shape boundary for building a segmentation graph. The cost for each graph node is determined by the UNet probability maps, and a max-flow algorithm is used to obtain the final segmentation. The Deep LOGISMOS method is evaluated on a dataset of 51 CT scans of pancreatic tumors, with DICE Similarity Coefficient (DSC) and Relative Volume Difference (RVD) reaching 83.2+-7.8% and 18.6+-17.4% respectively. The results show significant improvement (p<0.05) compared to contextual UNet and/or LOGISMOS alone.",1
"Image segmentation is a fundamental problem in medical image analysis. In recent years, deep neural networks achieve impressive performances on many medical image segmentation tasks by supervised learning on large manually annotated data. However, expert annotations on big medical datasets are tedious, expensive or sometimes unavailable. Weakly supervised learning could reduce the effort for annotation but still required certain amounts of expertise. Recently, deep learning shows a potential to produce more accurate predictions than the original erroneous labels. Inspired by this, we introduce a very weakly supervised learning method, for cystic lesion detection and segmentation in lung CT images, without any manual annotation. Our method works in a self-learning manner, where segmentation generated in previous steps (first by unsupervised segmentation then by neural networks) is used as ground truth for the next level of network learning. Experiments on a cystic lung lesion dataset show that the deep learning could perform better than the initial unsupervised annotation, and progressively improve itself after self-learning.",0
"Medical image analysis faces a crucial challenge in image segmentation. With recent advancements in deep neural networks, supervised learning has shown impressive results in medical image segmentation tasks. However, obtaining expert annotations for large medical datasets is often a laborious and expensive process. To reduce the effort for annotation, weakly supervised learning can be employed, though this still requires a certain level of expertise. Recent developments in deep learning have also demonstrated its ability to generate more accurate predictions than the initial erroneous labels. Our approach introduces a novel and very weakly supervised learning method for detecting and segmenting cystic lesions in lung CT images without any manual annotations. Our method operates in a self-learning manner, where the segmentation generated in previous steps is used as ground truth for the next level of network learning. Our experiments on a cystic lung lesion dataset demonstrate that our deep learning approach can perform better than the initial unsupervised annotation while progressively improving after self-learning.",1
"Pixel-wise image segmentation is demanding task in computer vision. Classical U-Net architectures composed of encoders and decoders are very popular for segmentation of medical images, satellite images etc. Typically, neural network initialized with weights from a network pre-trained on a large data set like ImageNet shows better performance than those trained from scratch on a small dataset. In some practical applications, particularly in medicine and traffic safety, the accuracy of the models is of utmost importance. In this paper, we demonstrate how the U-Net type architecture can be improved by the use of the pre-trained encoder. Our code and corresponding pre-trained weights are publicly available at https://github.com/ternaus/TernausNet. We compare three weight initialization schemes: LeCun uniform, the encoder with weights from VGG11 and full network trained on the Carvana dataset. This network architecture was a part of the winning solution (1st out of 735) in the Kaggle: Carvana Image Masking Challenge.",0
"Computer vision faces a challenging task with pixel-wise image segmentation. Encoders and decoders in Classical U-Net architectures are widely used for medical and satellite image segmentation. Pre-training neural networks with large data sets like ImageNet has shown better performance than small data set training. Accuracy is of the utmost importance in medical and traffic safety applications. This paper demonstrates how pre-trained encoders can improve U-Net architecture. The code and pre-trained weights are publicly available at https://github.com/ternaus/TernausNet. Three weight initialization schemes are compared, including LeCun uniform, VGG11 encoder weights, and full network training on the Carvana dataset. This network architecture was a winning solution (1st out of 735) in the Kaggle: Carvana Image Masking Challenge.",1
"Deep fully convolutional neural network (FCN) based architectures have shown great potential in medical image segmentation. However, such architectures usually have millions of parameters and inadequate number of training samples leading to over-fitting and poor generalization. In this paper, we present a novel highly parameter and memory efficient FCN based architecture for medical image analysis. We propose a novel up-sampling path which incorporates long skip and short-cut connections to overcome the feature map explosion in FCN like architectures. In order to processes the input images at multiple scales and view points simultaneously, we propose to incorporate Inception module's parallel structures. We also propose a novel dual loss function whose weighting scheme allows to combine advantages of cross-entropy and dice loss. We have validated our proposed network architecture on two publicly available datasets, namely: (i) Automated Cardiac Disease Diagnosis Challenge (ACDC-2017), (ii) Left Ventricular Segmentation Challenge (LV-2011). Our approach in ACDC-2017 challenge stands second place for segmentation and first place in automated cardiac disease diagnosis tasks with an accuracy of 100%. In the LV-2011 challenge our approach attained 0.74 Jaccard index, which is so far the highest published result in fully automated algorithms. From the segmentation we extracted clinically relevant cardiac parameters and hand-crafted features which reflected the clinical diagnostic analysis to train an ensemble system for cardiac disease classification. Our approach combined both cardiac segmentation and disease diagnosis into a fully automated framework which is computational efficient and hence has the potential to be incorporated in computer-aided diagnosis (CAD) tools for clinical application.",0
"Medical image segmentation using deep fully convolutional neural network (FCN) based architectures has shown promising results. However, these architectures often have a large number of parameters and limited training samples, which can lead to over-fitting and poor generalization. To address these challenges, we propose a novel, highly efficient FCN architecture that incorporates a unique up-sampling path with long skip and short-cut connections to overcome the feature map explosion in FCN-like architectures. We also incorporate Inception module's parallel structures to process input images at multiple scales and viewpoints simultaneously. Our approach also includes a novel dual loss function that combines the advantages of cross-entropy and dice loss. We validated our approach on two publicly available datasets, achieving high accuracy and Jaccard index scores. Furthermore, we extracted clinically relevant cardiac parameters and hand-crafted features to train an ensemble system for cardiac disease classification. Our approach is computationally efficient and has the potential to be incorporated into computer-aided diagnosis (CAD) tools for clinical applications.",1
"Automated cardiac image interpretation has the potential to transform clinical practice in multiple ways including enabling low-cost serial assessment of cardiac function in the primary care and rural setting. We hypothesized that advances in computer vision could enable building a fully automated, scalable analysis pipeline for echocardiogram (echo) interpretation. Our approach entailed: 1) preprocessing; 2) convolutional neural networks (CNN) for view identification, image segmentation, and phasing of the cardiac cycle; 3) quantification of chamber volumes and left ventricular mass; 4) particle tracking to compute longitudinal strain; and 5) targeted disease detection. CNNs accurately identified views (e.g. 99% for apical 4-chamber) and segmented individual cardiac chambers. Cardiac structure measurements agreed with study report values (e.g. mean absolute deviations (MAD) of 7.7 mL/kg/m2 for left ventricular diastolic volume index, 2918 studies). We computed automated ejection fraction and longitudinal strain measurements (within 2 cohorts), which agreed with commercial software-derived values [for ejection fraction, MAD=5.3%, N=3101 studies; for strain, MAD=1.5% (n=197) and 1.6% (n=110)], and demonstrated applicability to serial monitoring of breast cancer patients for trastuzumab cardiotoxicity. Overall, we found that, compared to manual measurements, automated measurements had superior performance across seven internal consistency metrics with an average increase in the Spearman correlation coefficient of 0.05 (p=0.02). Finally, we developed disease detection algorithms for hypertrophic cardiomyopathy and cardiac amyloidosis, with C-statistics of 0.93 and 0.84, respectively. Our pipeline lays the groundwork for using automated interpretation to support point-of-care handheld cardiac ultrasound and large-scale analysis of the millions of echos archived within healthcare systems.",0
"The use of automated interpretation of cardiac images has the potential to revolutionize clinical practice, especially in primary care and rural settings where low-cost serial assessment of cardiac function is required. To achieve this, we employed computer vision technology to build a fully automated analysis pipeline for echocardiogram interpretation. Our approach involved several steps, including preprocessing, convolutional neural networks for view identification, image segmentation, and phasing of the cardiac cycle, quantification of chamber volumes and left ventricular mass, particle tracking to compute longitudinal strain, and targeted disease detection. We found that our approach accurately identified views and segmented individual cardiac chambers, with cardiac structure measurements agreeing with study report values. We also computed automated ejection fraction and longitudinal strain measurements, which agreed with commercial software-derived values and demonstrated applicability to serial monitoring of breast cancer patients for trastuzumab cardiotoxicity. Compared to manual measurements, automated measurements had superior performance across seven internal consistency metrics, and we developed disease detection algorithms for hypertrophic cardiomyopathy and cardiac amyloidosis. Our pipeline sets the foundation for using automated interpretation to support point-of-care handheld cardiac ultrasound and large-scale analysis of the millions of echos archived within healthcare systems.",1
"Unsupervised image segmentation and denoising are two fundamental tasks in image processing. Usually, graph based models such as multicut are used for segmentation and variational models are employed for denoising. Our approach addresses both problems at the same time. We propose a novel ILP formulation of the first derivative Potts model with the $\ell_1$ data term, where binary variables are introduced to deal with the $\ell_0$ norm of the regularization term. The ILP is then solved by a standard off-the-shelf MIP solver. Numerical experiments are compared with the multicut problem.",0
"Image processing involves two crucial tasks, namely unsupervised image segmentation and denoising. Typically, segmentation is accomplished using graph-based models such as multicut, while variational models are utilized for denoising. Our approach simultaneously addresses both issues by presenting a pioneering ILP formulation of the first derivative Potts model with the $\ell_1$ data term, which utilizes binary variables to tackle the $\ell_0$ norm of the regularization term. The ILP is then solved using a standard MIP solver. We conducted numerical experiments and compared the results with the multicut problem.",1
"Breast cancer is one of the leading causes of cancer death among women worldwide. In clinical routine, automatic breast ultrasound (BUS) image segmentation is very challenging and essential for cancer diagnosis and treatment planning. Many BUS segmentation approaches have been studied in the last two decades, and have been proved to be effective on private datasets. Currently, the advancement of BUS image segmentation seems to meet its bottleneck. The improvement of the performance is increasingly challenging, and only few new approaches were published in the last several years. It is the time to look at the field by reviewing previous approaches comprehensively and to investigate the future directions. In this paper, we study the basic ideas, theories, pros and cons of the approaches, group them into categories, and extensively review each category in depth by discussing the principles, application issues, and advantages/disadvantages.",0
"Breast cancer is a major cause of cancer-related deaths in women worldwide. Automatic segmentation of breast ultrasound (BUS) images is crucial for the diagnosis and treatment of cancer, but it is a difficult task in clinical practice. Over the last two decades, various BUS segmentation methods have been developed and demonstrated to be effective on private datasets. However, progress in BUS image segmentation has reached a plateau, with only a few new approaches developed in recent years. Therefore, it is necessary to comprehensively review previous approaches and explore future directions. This paper examines the fundamental concepts, theories, advantages, and drawbacks of BUS segmentation techniques, categorizes them, and provides an in-depth analysis of each category, including principles, application issues, and pros and cons.",1
"Breast ultrasound (BUS) image segmentation is challenging and critical for BUS Computer-Aided Diagnosis (CAD) systems. Many BUS segmentation approaches have been proposed in the last two decades, but the performances of most approaches have been assessed using relatively small private datasets with differ-ent quantitative metrics, which result in discrepancy in performance comparison. Therefore, there is a pressing need for building a benchmark to compare existing methods using a public dataset objectively, and to determine the performance of the best breast tumor segmentation algorithm available today and to investigate what segmentation strategies are valuable in clinical practice and theoretical study. In this work, we will publish a B-mode BUS image segmentation benchmark (BUSIS) with 562 images and compare the performance of five state-of-the-art BUS segmentation methods quantitatively.",0
"Accurately segmenting breast ultrasound (BUS) images is crucial for developing effective BUS Computer-Aided Diagnosis (CAD) systems. Though various approaches have been proposed over the last twenty years, their performance has typically been evaluated using small, private datasets and different metrics, leading to inconsistent comparisons. Therefore, it is imperative to establish a public benchmark dataset to objectively compare existing methods, determine the most effective breast tumor segmentation algorithm available, and identify valuable segmentation strategies for clinical and theoretical use. Our study involves the creation of a B-mode BUS image segmentation benchmark (BUSIS) featuring 562 images, as well as a quantitative comparison of five advanced BUS segmentation methods.",1
"Early and correct diagnosis is a very important aspect of cancer treatment. Detection of tumour in Computed Tomography scan is a tedious and tricky task which requires expert knowledge and a lot of human working hours. As small human error is present in any work he does, it is possible that a CT scan could be misdiagnosed causing the patient to become terminal. This paper introduces a novel fully automated framework which helps to detect and segment tumour, if present in a lung CT scan series. It also provides useful analysis of the detected tumour such as its approximate volume, centre location and more. The framework provides a single click solution which analyses all CT images of a single patient series in one go. It helps to reduce the work of manually going through each CT slice and provides quicker and more accurate tumour diagnosis. It makes use of customized image processing and image segmentation methods, to detect and segment the prospective tumour region from the CT scan. It then uses a trained ensemble classifier to correctly classify the segmented region as being tumour or not. Tumour analysis further computed can then be used to determine malignity of the tumour. With an accuracy of 98.14%, the implemented framework can be used in various practical scenarios, capable of eliminating need of any expert pathologist intervention.",0
"The timely and accurate identification of cancer is a critical factor in its treatment. Identifying tumors in Computed Tomography (CT) scans is a difficult and complex task that requires specialized knowledge and significant human effort. Since errors can occur in any human work, misdiagnosis in a CT scan can result in a patient becoming terminal. This study introduces an innovative and fully automated approach that can detect and segment lung tumors in a CT scan series. The approach provides valuable information about the tumor, such as its volume and location, using a single-click solution to analyze all images in a patient's series. This approach reduces the need for manual analysis and provides faster and more accurate tumor diagnosis. Customized image processing and segmentation techniques are used to identify and segment the tumor region from the CT scan, followed by the use of an ensemble classifier to classify the segmented region accurately. The tumor's malignancy can be determined based on the analysis. With an accuracy rate of 98.14%, this approach can be used in various practical settings, eliminating the need for an expert pathologist's intervention.",1
"Semantic image segmentation is one of the most challenged tasks in computer vision. In this paper, we propose a highly fused convolutional network, which consists of three parts: feature downsampling, combined feature upsampling and multiple predictions. We adopt a strategy of multiple steps of upsampling and combined feature maps in pooling layers with its corresponding unpooling layers. Then we bring out multiple pre-outputs, each pre-output is generated from an unpooling layer by one-step upsampling. Finally, we concatenate these pre-outputs to get the final output. As a result, our proposed network makes highly use of the feature information by fusing and reusing feature maps. In addition, when training our model, we add multiple soft cost functions on pre-outputs and final outputs. In this way, we can reduce the loss reduction when the loss is back propagated. We evaluate our model on three major segmentation datasets: CamVid, PASCAL VOC and ADE20K. We achieve a state-of-the-art performance on CamVid dataset, as well as considerable improvements on PASCAL VOC dataset and ADE20K dataset",0
"The task of semantic image segmentation presents a significant challenge in the field of computer vision. Our study proposes a highly integrated convolutional network that comprises three components: feature downsampling, combined feature upsampling, and multiple predictions. We implement a strategy of multiple upsampling steps and merging feature maps in pooling layers with corresponding unpooling layers. We generate multiple pre-outputs from each unpooling layer through one-step upsampling and then concatenate them to produce the final output. This approach effectively utilizes feature information by merging and reusing feature maps. During model training, we incorporate multiple soft cost functions on pre-outputs and final outputs to minimize loss reduction during back propagation. We evaluate the model on three major segmentation datasets: CamVid, PASCAL VOC, and ADE20K, where we achieve state-of-the-art performance on CamVid and significant improvements on PASCAL VOC and ADE20K.",1
"The automated segmentation of cells in microscopic images is an open research problem that has important implications for studies of the developmental and cancer processes based on in vitro models. In this paper, we present the approach for segmentation of the DIC images of cultured cells using G-neighbor smoothing followed by Kauwahara filtering and local standard deviation approach for boundary detection. NIH FIJI/ImageJ tools are used to create the ground truth dataset. The results of this work indicate that detection of cell boundaries using segmentation approach even in the case of realistic measurement conditions is a challenging problem.",0
"The segmentation of cells in microscopic images is an unsettled research issue that holds significant importance in the investigations of developmental and cancer processes using in vitro models. This article introduces a technique for segmenting DIC images of cultured cells, utilizing G-neighbor smoothing, Kauwahara filtering, and local standard deviation for boundary detection. The ground truth dataset is created through NIH FIJI/ImageJ tools. The findings suggest that detecting cell boundaries through segmentation is a difficult task, even under realistic measurement conditions.",1
"In this paper we present our system for human-in-the-loop video object segmentation. The backbone of our system is a method for one-shot video object segmentation. While fast, this method requires an accurate pixel-level segmentation of one (or several) frames as input. As manually annotating such a segmentation is impractical, we propose a deep interactive image segmentation method, that can accurately segment objects with only a handful of clicks. On the GrabCut dataset, our method obtains 90% IOU with just 3.8 clicks on average, setting the new state of the art. Furthermore, as our method iteratively refines an initial segmentation, it can effectively correct frames where the video object segmentation fails, thus allowing users to quickly obtain high quality results even on challenging sequences. Finally, we investigate usage patterns and give insights in how many steps users take to annotate frames, what kind of corrections they provide, etc., thus giving important insights for further improving interactive video segmentation.",0
"Our paper introduces our system for video object segmentation with human involvement, which is based on a one-shot method. This approach is speedy but necessitates precise pixel-level segmentation of one or more frames as input, which is not feasible to annotate manually. To overcome this, we suggest a deep interactive image segmentation method that can accurately segment objects with only a small number of clicks. Our method achieves a 90% IOU on the GrabCut dataset with an average of 3.8 clicks, surpassing previous records. Our system can also refine the initial segmentation iteratively, allowing users to quickly obtain high-quality results, even on challenging sequences. Finally, we explore how users annotate frames, including the number of steps they take and the types of corrections they make, which provides useful insights for enhancing interactive video segmentation.",1
"A novel multi-atlas based image segmentation method is proposed by integrating a semi-supervised label propagation method and a supervised random forests method in a pattern recognition based label fusion framework. The semi-supervised label propagation method takes into consideration local and global image appearance of images to be segmented and segments the images by propagating reliable segmentation results obtained by the supervised random forests method. Particularly, the random forests method is used to train a regression model based on image patches of atlas images for each voxel of the images to be segmented. The regression model is used to obtain reliable segmentation results to guide the label propagation for the segmentation. The proposed method has been compared with state-of-the-art multi-atlas based image segmentation methods for segmenting the hippocampus in MR images. The experiment results have demonstrated that our method obtained superior segmentation performance.",0
"The article introduces a new method for image segmentation using multiple atlases. This method combines a semi-supervised label propagation approach with a supervised random forests approach, within a pattern recognition-based label fusion framework. The semi-supervised label propagation approach considers both local and global appearance of the images to be segmented, and uses reliable segmentation results obtained from the supervised random forests approach to segment the images. The random forests approach trains a regression model using image patches of atlas images for each voxel of the images to be segmented, which is then used to guide the label propagation for segmentation. The proposed method is compared with existing multi-atlas-based image segmentation methods for segmenting the hippocampus in MR images, and the results show that the proposed method achieves better segmentation performance.",1
"The segmentation of animals from camera-trap images is a difficult task. To illustrate, there are various challenges due to environmental conditions and hardware limitation in these images. We proposed a multi-layer robust principal component analysis (multi-layer RPCA) approach for background subtraction. Our method computes sparse and low-rank images from a weighted sum of descriptors, using color and texture features as case of study for camera-trap images segmentation. The segmentation algorithm is composed of histogram equalization or Gaussian filtering as pre-processing, and morphological filters with active contour as post-processing. The parameters of our multi-layer RPCA were optimized with an exhaustive search. The database consists of camera-trap images from the Colombian forest taken by the Instituto de Investigaci\'on de Recursos Biol\'ogicos Alexander von Humboldt. We analyzed the performance of our method in inherent and therefore challenging situations of camera-trap images. Furthermore, we compared our method with some state-of-the-art algorithms of background subtraction, where our multi-layer RPCA outperformed these other methods. Our multi-layer RPCA reached 76.17 and 69.97% of average fine-grained F-measure for color and infrared sequences, respectively. To our best knowledge, this paper is the first work proposing multi-layer RPCA and using it for camera-trap images segmentation.",0
"It is a challenging task to segment animals from camera-trap images due to various obstacles, such as environmental conditions and hardware limitations. As a solution, we presented a multi-layer robust principal component analysis (multi-layer RPCA) method for background subtraction. This approach employs color and texture features as a case of study to compute sparse and low-rank images from a weighted sum of descriptors. Our segmentation algorithm includes pre-processing techniques such as histogram equalization or Gaussian filtering and post-processing techniques such as morphological filters with active contour. We optimized the parameters of our multi-layer RPCA using an exhaustive search and tested our method on camera-trap images from the Colombian forest. Our multi-layer RPCA outperformed other state-of-the-art algorithms for background subtraction, achieving an average fine-grained F-measure of 76.17% and 69.97% for color and infrared sequences, respectively. This paper is the first to propose the use of multi-layer RPCA for camera-trap image segmentation.",1
"Image segmentation is considered to be one of the critical tasks in hyperspectral remote sensing image processing. Recently, convolutional neural network (CNN) has established itself as a powerful model in segmentation and classification by demonstrating excellent performances. The use of a graphical model such as a conditional random field (CRF) contributes further in capturing contextual information and thus improving the segmentation performance. In this paper, we propose a method to segment hyperspectral images by considering both spectral and spatial information via a combined framework consisting of CNN and CRF. We use multiple spectral cubes to learn deep features using CNN, and then formulate deep CRF with CNN-based unary and pairwise potential functions to effectively extract the semantic correlations between patches consisting of three-dimensional data cubes. Effective piecewise training is applied in order to avoid the computationally expensive iterative CRF inference. Furthermore, we introduce a deep deconvolution network that improves the segmentation masks. We also introduce a new dataset and experimented our proposed method on it along with several widely adopted benchmark datasets to evaluate the effectiveness of our method. By comparing our results with those from several state-of-the-art models, we show the promising potential of our method.",0
"Hyperspectral remote sensing image processing requires accurate image segmentation, which is a challenging task. Convolutional neural network (CNN) has shown remarkable success in segmentation and classification. However, integrating a graphical model such as a conditional random field (CRF) can enhance contextual information and improve segmentation performance. This study proposes a novel approach that combines CNN and CRF to segment hyperspectral images using spectral and spatial information. The method involves using multiple spectral cubes to learn deep features with CNN and formulating deep CRF with CNN-based unary and pairwise potential functions for semantic correlation extraction. The approach uses effective piecewise training to avoid the costly iterative CRF inference. Additionally, a deep deconvolution network is introduced to improve the segmentation masks. A new dataset is also introduced, and the proposed method is evaluated on both the new dataset and several benchmark datasets. The results demonstrate the effectiveness of the proposed approach compared to state-of-the-art models.",1
"We present an end-to-end trainable deep convolutional neural network (DCNN) for semantic segmentation with built-in awareness of semantically meaningful boundaries. Semantic segmentation is a fundamental remote sensing task, and most state-of-the-art methods rely on DCNNs as their workhorse. A major reason for their success is that deep networks learn to accumulate contextual information over very large windows (receptive fields). However, this success comes at a cost, since the associated loss of effecive spatial resolution washes out high-frequency details and leads to blurry object boundaries. Here, we propose to counter this effect by combining semantic segmentation with semantically informed edge detection, thus making class-boundaries explicit in the model, First, we construct a comparatively simple, memory-efficient model by adding boundary detection to the Segnet encoder-decoder architecture. Second, we also include boundary detection in FCN-type models and set up a high-end classifier ensemble. We show that boundary detection significantly improves semantic segmentation with CNNs. Our high-end ensemble achieves > 90% overall accuracy on the ISPRS Vaihingen benchmark.",0
"Our study introduces a deep convolutional neural network (DCNN) that is trainable end-to-end for semantic segmentation while being equipped with knowledge of semantically meaningful boundaries. Semantic segmentation is a crucial task in remote sensing, with most advanced methods relying on DCNNs. These networks are successful in capturing contextual information over large receptive fields, but this comes at a cost of losing spatial resolution and blurring object boundaries. To tackle this issue, we combine semantic segmentation with semantically informed edge detection, making class-boundaries explicit in the model. We propose two models, a memory-efficient Segnet encoder-decoder architecture with boundary detection and an FCN-type model with boundary detection included in a high-end classifier ensemble. Our results show that boundary detection significantly improves semantic segmentation with CNNs, with our ensemble achieving over 90% overall accuracy on the ISPRS Vaihingen benchmark.",1
"The most common paradigm for vision-based multi-object tracking is tracking-by-detection, due to the availability of reliable detectors for several important object categories such as cars and pedestrians. However, future mobile systems will need a capability to cope with rich human-made environments, in which obtaining detectors for every possible object category would be infeasible. In this paper, we propose a model-free multi-object tracking approach that uses a category-agnostic image segmentation method to track objects. We present an efficient segmentation mask-based tracker which associates pixel-precise masks reported by the segmentation. Our approach can utilize semantic information whenever it is available for classifying objects at the track level, while retaining the capability to track generic unknown objects in the absence of such information. We demonstrate experimentally that our approach achieves performance comparable to state-of-the-art tracking-by-detection methods for popular object categories such as cars and pedestrians. Additionally, we show that the proposed method can discover and robustly track a large variety of other objects.",0
"The most popular technique for tracking multiple objects based on vision is tracking-by-detection, which is feasible due to the availability of reliable detectors for important categories like cars and pedestrians. However, mobile systems of the future will require the ability to handle complex environments created by humans, where it would be impractical to obtain detectors for every possible object category. In this paper, we introduce a model-free approach for tracking multiple objects that uses an image segmentation method that is not category-specific. We present an effective tracker based on segmentation masks that links pixel-precise masks provided by the segmentation. Our approach can leverage semantic information for object classification at the track level, where available, while still being able to track generic unknown objects when such information is missing. Our experimental results demonstrate that our approach achieves results comparable to state-of-the-art tracking-by-detection methods for popular object categories like cars and pedestrians. Moreover, we demonstrate that our proposed method can identify and track a wide range of other objects with robustness.",1
"In medicine, visualizing chromosomes is important for medical diagnostics, drug development, and biomedical research. Unfortunately, chromosomes often overlap and it is necessary to identify and distinguish between the overlapping chromosomes. A segmentation solution that is fast and automated will enable scaling of cost effective medicine and biomedical research. We apply neural network-based image segmentation to the problem of distinguishing between partially overlapping DNA chromosomes. A convolutional neural network is customized for this problem. The results achieved intersection over union (IOU) scores of 94.7% for the overlapping region and 88-94% on the non-overlapping chromosome regions.",0
"Visualizing chromosomes plays a crucial role in medical diagnostics, drug development, and biomedical research. However, distinguishing between overlapping chromosomes proves challenging, thus requiring a fast and automated segmentation solution to facilitate cost-effective medicine and research. To address this issue, we utilized a customized convolutional neural network for DNA chromosome segmentation, achieving an intersection over union (IOU) score of 94.7% for the overlapping regions and 88-94% for the non-overlapping chromosome regions.",1
"Gastric cancer is the second leading cause of cancer-related deaths worldwide, and the major hurdle in biomedical image analysis is the determination of the cancer extent. This assignment has high clinical relevance and would generally require vast microscopic assessment by pathologists. Recent advances in deep learning have produced inspiring results on biomedical image segmentation, while its outcome is reliant on comprehensive annotation. This requires plenty of labor costs, for the ground truth must be annotated meticulously by pathologists. In this paper, a reiterative learning framework was presented to train our network on partial annotated biomedical images, and superior performance was achieved without any pre-trained or further manual annotation. We eliminate the boundary error of patch-based model through our overlapped region forecast algorithm. Through these advisable methods, a mean intersection over union coefficient (IOU) of 0.883 and mean accuracy of 91.09% on the partial labeled dataset was achieved, which made us win the 2017 China Big Data & Artificial Intelligence Innovation and Entrepreneurship Competitions.",0
"Biomedical image analysis faces the challenge of determining the extent of gastric cancer, which is the second leading cause of cancer-related deaths globally. This task involves extensive microscopic evaluation by pathologists and incurs significant labor costs for accurate annotation of the ground truth. Recent advancements in deep learning have demonstrated promising results in biomedical image segmentation, but rely on comprehensive annotation. This paper proposes a reiterative learning framework that uses partial annotated biomedical images to train the network and achieve superior performance without pre-training or manual annotation. Our overlapped region forecast algorithm eliminates the boundary error of patch-based models. These methods resulted in a mean intersection over union coefficient (IOU) of 0.883 and mean accuracy of 91.09% on the partially labeled dataset, which won the 2017 China Big Data & Artificial Intelligence Innovation and Entrepreneurship Competitions. The clinical significance of this assignment cannot be overstated.",1
"Precise 3D segmentation of infant brain tissues is an essential step towards comprehensive volumetric studies and quantitative analysis of early brain developement. However, computing such segmentations is very challenging, especially for 6-month infant brain, due to the poor image quality, among other difficulties inherent to infant brain MRI, e.g., the isointense contrast between white and gray matter and the severe partial volume effect due to small brain sizes. This study investigates the problem with an ensemble of semi-dense fully convolutional neural networks (CNNs), which employs T1-weighted and T2-weighted MR images as input. We demonstrate that the ensemble agreement is highly correlated with the segmentation errors. Therefore, our method provides measures that can guide local user corrections. To the best of our knowledge, this work is the first ensemble of 3D CNNs for suggesting annotations within images. Furthermore, inspired by the very recent success of dense networks, we propose a novel architecture, SemiDenseNet, which connects all convolutional layers directly to the end of the network. Our architecture allows the efficient propagation of gradients during training, while limiting the number of parameters, requiring one order of magnitude less parameters than popular medical image segmentation networks such as 3D U-Net. Another contribution of our work is the study of the impact that early or late fusions of multiple image modalities might have on the performances of deep architectures. We report evaluations of our method on the public data of the MICCAI iSEG-2017 Challenge on 6-month infant brain MRI segmentation, and show very competitive results among 21 teams, ranking first or second in most metrics.",0
"Accurately segmenting infant brain tissues in 3D is crucial for comprehensive volumetric studies and quantitative analysis of early brain development. However, this is a challenging task, particularly for 6-month-old infants, due to poor image quality and other difficulties inherent to infant brain MRI, such as isointense contrast between white and gray matter and severe partial volume effects due to small brain sizes. In this study, we address this problem using an ensemble of semi-dense fully convolutional neural networks that take T1-weighted and T2-weighted MR images as input. Our method generates measures that can guide local user corrections and is the first ensemble of 3D CNNs for suggesting annotations within images. Additionally, we propose a novel architecture called SemiDenseNet that efficiently propagates gradients during training while requiring significantly fewer parameters than other popular medical image segmentation networks. We also evaluate the impact of early or late fusions of multiple image modalities on the performance of deep architectures. Our method achieves very competitive results, ranking first or second in most metrics among 21 teams in the public data of the MICCAI iSEG-2017 Challenge on 6-month infant brain MRI segmentation.",1
"We have developed a deep learning network for classification of different flowers. For this, we have used Visual Geometry Group's 102 category flower dataset having 8189 images of 102 different flowers from University of Oxford. The method is basically divided into two parts; Image segmentation and classification. We have compared the performance of two different Convolutional Neural Network architectures GoogLeNet and AlexNet for classification purpose. By keeping the hyper parameters same for both architectures, we have found that the top 1 and top 5 accuracies of GoogLeNet are 47.15% and 69.17% respectively whereas the top 1 and top 5 accuracies of AlexNet are 43.39% and 68.68% respectively. These results are extremely good when compared to random classification accuracy of 0.98%. This method for classification of flowers can be implemented in real time applications and can be used to help botanists for their research as well as camping enthusiasts.",0
"A deep learning network has been developed to classify various types of flowers using the Visual Geometry Group's 102 category flower dataset comprising 8189 images of 102 different flowers from the University of Oxford. The process includes image segmentation and classification, with a comparison of the GoogLeNet and AlexNet Convolutional Neural Network architectures. The study found that with the same hyper parameters, GoogLeNet had a top 1 accuracy of 47.15% and top 5 accuracy of 69.17%, while AlexNet had a top 1 accuracy of 43.39% and top 5 accuracy of 68.68%. These results are significantly better than the random classification accuracy of 0.98% and can be applied to real-time applications, aiding botanists and outdoor enthusiasts in their research.",1
"The goal of this paper is to present a new efficient image segmentation method based on evolutionary computation which is a model inspired from human behavior. Based on this model, a four layer process for image segmentation is proposed using the split/merge approach. In the first layer, an image is split into numerous regions using the watershed algorithm. In the second layer, a co-evolutionary process is applied to form centers of finals segments by merging similar primary regions. In the third layer, a meta-heuristic process uses two operators to connect the residual regions to their corresponding determined centers. In the final layer, an evolutionary algorithm is used to combine the resulted similar and neighbor regions. Different layers of the algorithm are totally independent, therefore for certain applications a specific layer can be changed without constraint of changing other layers. Some properties of this algorithm like the flexibility of its method, the ability to use different feature vectors for segmentation (grayscale, color, texture, etc), the ability to control uniformity and the number of final segments using free parameters and also maintaining small regions, makes it possible to apply the algorithm to different applications. Moreover, the independence of each region from other regions in the second layer, and the independence of centers in the third layer, makes parallel implementation possible. As a result the algorithm speed will increase. The presented algorithm was tested on a standard dataset (BSDS 300) of images, and the region boundaries were compared with different people segmentation contours. Results show the efficiency of the algorithm and its improvement to similar methods. As an instance, in 70% of tested images, results are better than ACT algorithm, besides in 100% of tested images, we had better results in comparison with VSP algorithm.",0
"The aim of this paper is to introduce a novel technique for image segmentation that utilizes evolutionary computation as a basis, inspired by human behavior. The proposed approach involves a four-layer process that employs the split/merge technique. In the first layer, an image is divided into multiple regions using the watershed algorithm. In the second layer, a co-evolutionary process merges similar primary regions to form centers of final segments. The third layer utilizes a meta-heuristic process to connect residual regions to their corresponding determined centers using two operators. Finally, the fourth layer employs an evolutionary algorithm to combine similar and neighboring regions. Each layer of the algorithm is independent, allowing for customization of specific layers for different applications. The algorithm offers flexibility, the ability to use various feature vectors, control of uniformity and final segment number, and maintenance of small regions. Additionally, the second and third layers' independence allows for parallel implementation and increased speed. The algorithm's efficiency was tested on a standard dataset (BSDS 300) and compared to other segmentation methods, demonstrating its superiority in most cases. For example, it outperformed the ACT algorithm in 70% of tested images and the VSP algorithm in 100% of tested images.",1
"In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.",0
"This work focuses on the use of atrous convolution in semantic image segmentation. Atrous convolution is a useful tool for adjusting a filter's field-of-view and controlling the resolution of feature responses in deep convolutional neural networks. To address the challenge of segmenting objects at multiple scales, we introduce modules that use atrous convolution in cascade or in parallel with multiple atrous rates to capture multi-scale context. In addition, we enhance the Atrous Spatial Pyramid Pooling module, which probes convolutional features at various scales, with global image-level features to improve performance. We provide details on implementation and share our training experience. Our proposed 'DeepLabv3' system outperforms our previous DeepLab versions without DenseCRF post-processing and achieves comparable results to other state-of-the-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.",1
"We define and study error detection and correction tasks that are useful for 3D reconstruction of neurons from electron microscopic imagery, and for image segmentation more generally. Both tasks take as input the raw image and a binary mask representing a candidate object. For the error detection task, the desired output is a map of split and merge errors in the object. For the error correction task, the desired output is the true object. We call this object mask pruning, because the candidate object mask is assumed to be a superset of the true object. We train multiscale 3D convolutional networks to perform both tasks. We find that the error-detecting net can achieve high accuracy. The accuracy of the error-correcting net is enhanced if its input object mask is ""advice"" (union of erroneous objects) from the error-detecting net.",0
"Our focus is on examining error detection and correction duties that have applicability in the 3D reconstruction of neurons from electron microscopic imaging and more broadly in image segmentation. To perform both of these duties, we utilize the raw image and a binary mask that represents a potential object. The error detection task aims to produce a detailed map of split and merge errors in the object, while the error correction task seeks to establish the actual object. We refer to this as object mask pruning since the candidate object mask is assumed to include the true object. Multiscale 3D convolutional networks are trained for both tasks. We observed that the error-detecting network attains high accuracy, whereas the accuracy of the error-correcting network increases if its input object mask is the ""advice"" (union of erroneous objects) received from the error-detecting network.",1
"According to the World Health Organization, breast cancer is the most common form of cancer in women. It is the second leading cause of death among women round the world, becoming the most fatal form of cancer. Mammographic image segmentation is a fundamental task to support image analysis and diagnosis, taking into account shape analysis of mammary lesions and their borders. However, mammogram segmentation is a very hard process, once it is highly dependent on the types of mammary tissues. In this work we present a new semi-supervised segmentation algorithm based on the modification of the GrowCut algorithm to perform automatic mammographic image segmentation once a region of interest is selected by a specialist. In our proposal, we used fuzzy Gaussian membership functions to modify the evolution rule of the original GrowCut algorithm, in order to estimate the uncertainty of a pixel being object or background. The main impact of the proposed method is the significant reduction of expert effort in the initialization of seed points of GrowCut to perform accurate segmentation, once it removes the need of selection of background seeds. We also constructed an automatic point selection process based on the simulated annealing optimization method, avoiding the need of human intervention. The proposed approach was qualitatively compared with other state-of-the-art segmentation techniques, considering the shape of segmented regions. In order to validate our proposal, we built an image classifier using a classical multilayer perceptron. We used Zernike moments to extract segmented image features. This analysis employed 685 mammograms from IRMA breast cancer database, using fat and fibroid tissues. Results show that the proposed technique could achieve a classification rate of 91.28\% for fat tissues, evidencing the feasibility of our approach.",0
"Breast cancer is the most prevalent cancer type among women worldwide and the second leading cause of female death. Accurate mammographic image segmentation is crucial for diagnosis and shape analysis of mammary lesions. However, mammogram segmentation is challenging due to its dependence on mammary tissue types. To address this issue, we propose a new semi-supervised segmentation algorithm based on the GrowCut algorithm modification. Our method uses fuzzy Gaussian membership functions to estimate the uncertainty of a pixel being object or background, reducing the need for expert intervention. We also developed an automatic point selection process based on simulated annealing optimization. Our proposed approach was compared with other state-of-the-art segmentation methods, and we built an image classifier using a multilayer perceptron and Zernike moments to extract segmented image features. We validated our approach using 685 mammograms from the IRMA breast cancer database, achieving a classification rate of 91.28\% for fat tissues.",1
"In this paper, we propose a simple but effective method for fast image segmentation. We re-examine the locality-preserving character of spectral clustering by constructing a graph over image regions with both global and local connections. Our novel approach to build graph connections relies on two key observations: 1) local region pairs that co-occur frequently will have a high probability to reside on a common object; 2) spatially distant regions in a common object often exhibit similar visual saliency, which implies their neighborship in a manifold. We present a novel energy function to efficiently conduct graph partitioning. Based on multiple high quality partitions, we show that the generated eigenvector histogram based representation can automatically drive effective unary potentials for a hierarchical random field model to produce multi-class segmentation. Sufficient experiments, on the BSDS500 benchmark, large-scale PASCAL VOC and COCO datasets, demonstrate the competitive segmentation accuracy and significantly improved efficiency of our proposed method compared with other state of the arts.",0
"Our paper introduces a fast image segmentation method that is both simple and effective. We take a fresh look at spectral clustering's locality-preserving aspect and construct a graph using both global and local connections between image regions. To build these connections, we rely on two key observations: 1) frequently co-occurring local region pairs are likely to belong to a common object, and 2) distant regions within an object tend to exhibit similar visual saliency, indicating they are neighbors in a manifold. Our novel energy function enables efficient graph partitioning, resulting in multiple high-quality partitions. These partitions generate an eigenvector histogram-based representation that can effectively drive unary potentials for a hierarchical random field model, producing multi-class segmentation. We demonstrate the competitive segmentation accuracy and significantly improved efficiency of our method compared to other state-of-the-art methods through sufficient experiments on the BSDS500 benchmark, large-scale PASCAL VOC, and COCO datasets.",1
"Spleen volume estimation using automated image segmentation technique may be used to detect splenomegaly (abnormally enlarged spleen) on Magnetic Resonance Imaging (MRI) scans. In recent years, Deep Convolutional Neural Networks (DCNN) segmentation methods have demonstrated advantages for abdominal organ segmentation. However, variations in both size and shape of the spleen on MRI images may result in large false positive and false negative labeling when deploying DCNN based methods. In this paper, we propose the Splenomegaly Segmentation Network (SSNet) to address spatial variations when segmenting extraordinarily large spleens. SSNet was designed based on the framework of image-to-image conditional generative adversarial networks (cGAN). Specifically, the Global Convolutional Network (GCN) was used as the generator to reduce false negatives, while the Markovian discriminator (PatchGAN) was used to alleviate false positives. A cohort of clinically acquired 3D MRI scans (both T1 weighted and T2 weighted) from patients with splenomegaly were used to train and test the networks. The experimental results demonstrated that a mean Dice coefficient of 0.9260 and a median Dice coefficient of 0.9262 using SSNet on independently tested MRI volumes of patients with splenomegaly.",0
"Automated image segmentation techniques can estimate spleen volume and detect splenomegaly in MRI scans. Deep Convolutional Neural Networks have shown advantages in abdominal organ segmentation, but size and shape variations can lead to false labeling. To address this issue, we propose the Splenomegaly Segmentation Network (SSNet) which uses image-to-image conditional generative adversarial networks. The Global Convolutional Network generator reduces false negatives, while the Markovian discriminator PatchGAN alleviates false positives. We trained and tested SSNet on 3D MRI scans from patients with splenomegaly and obtained a mean Dice coefficient of 0.9260 and a median Dice coefficient of 0.9262 on independently tested MRI volumes.",1
"We propose an approach to semantic (image) segmentation that reduces the computational costs by a factor of 25 with limited impact on the quality of results. Semantic segmentation has a number of practical applications, and for most such applications the computational costs are critical. The method follows a typical two-column network structure, where one column accepts an input image, while the other accepts a half-resolution version of that image. By identifying specific regions in the full-resolution image that can be safely ignored, as well as carefully tailoring the network structure, we can process approximately 15 highresolution Cityscapes images (1024x2048) per second using a single GTX 980 video card, while achieving a mean intersection-over-union score of 72.9% on the Cityscapes test set.",0
"Our proposed approach to semantic segmentation of images aims to significantly decrease computational costs by 25 times, while maintaining the quality of the results. Semantic segmentation is widely used in practical applications, where computational costs are a major concern. Our method involves a two-column network structure, where one column receives the input image, and the other receives a half-resolution version of the same image. We have developed a technique to identify regions in the full-resolution image that can be safely ignored, and have also optimized the network structure to achieve high performance. With this approach, we can process 15 high-resolution Cityscapes images (1024x2048) per second on a single GTX 980 video card, while achieving a mean intersection-over-union score of 72.9% on the Cityscapes test set.",1
"There has been a significant increase from 2010 to 2016 in the number of people suffering from spine problems. The automatic image segmentation of the spine obtained from a computed tomography (CT) image is important for diagnosing spine conditions and for performing surgery with computer-assisted surgery systems. The spine has a complex anatomy that consists of 33 vertebrae, 23 intervertebral disks, the spinal cord, and connecting ribs. As a result, the spinal surgeon is faced with the challenge of needing a robust algorithm to segment and create a model of the spine. In this study, we developed an automatic segmentation method to segment the spine, and we compared our segmentation results with reference segmentations obtained by experts. We developed a fully automatic approach for spine segmentation from CT based on a hybrid method. This method combines the convolutional neural network (CNN) and fully convolutional network (FCN), and utilizes class redundancy as a soft constraint to greatly improve the segmentation results. The proposed method was found to significantly enhance the accuracy of the segmentation results and the system processing time. Our comparison was based on 12 measurements: the Dice coefficient (94%), Jaccard index (93%), volumetric similarity (96%), sensitivity (97%), specificity (99%), precision (over segmentation; 8.3 and under segmentation 2.6), accuracy (99%), Matthews correlation coefficient (0.93), mean surface distance (0.16 mm), Hausdorff distance (7.4 mm), and global consistency error (0.02). We experimented with CT images from 32 patients, and the experimental results demonstrated the efficiency of the proposed method.",0
"Between 2010 and 2016, there has been a significant increase in the number of individuals experiencing spine problems. The automatic segmentation of the spine from CT images is crucial for diagnosing spine conditions and conducting computer-assisted surgeries. However, the complexity of the spine's anatomy - comprising 33 vertebrae, 23 intervertebral disks, the spinal cord, and ribs - presents a challenge for surgeons who require a robust algorithm to create a model of the spine. In response, we developed an automatic segmentation method that utilizes a hybrid approach combining the convolutional neural network (CNN) and fully convolutional network (FCN) and class redundancy as a soft constraint. Our research involved comparing our method's results to reference segmentations generated by experts. Our method significantly improved the accuracy of the segmentation results and the system processing time. We conducted 12 measurements to assess our method's performance, including the Dice coefficient (94%), Jaccard index (93%), volumetric similarity (96%), sensitivity (97%), specificity (99%), precision (over-segmentation; 8.3 and under-segmentation 2.6), accuracy (99%), Matthews correlation coefficient (0.93), mean surface distance (0.16 mm), Hausdorff distance (7.4 mm), and global consistency error (0.02). We tested our method on CT images from 32 patients, and the results demonstrated its efficiency.",1
"With the rapidly increasing interest in machine learning based solutions for automatic image annotation, the availability of reference annotations for algorithm training is one of the major bottlenecks in the field. Crowdsourcing has evolved as a valuable option for low-cost and large-scale data annotation; however, quality control remains a major issue which needs to be addressed. To our knowledge, we are the first to analyze the annotation process to improve crowd-sourced image segmentation. Our method involves training a regressor to estimate the quality of a segmentation from the annotator's clickstream data. The quality estimation can be used to identify spam and weight individual annotations by their (estimated) quality when merging multiple segmentations of one image. Using a total of 29,000 crowd annotations performed on publicly available data of different object classes, we show that (1) our method is highly accurate in estimating the segmentation quality based on clickstream data, (2) outperforms state-of-the-art methods for merging multiple annotations. As the regressor does not need to be trained on the object class that it is applied to it can be regarded as a low-cost option for quality control and confidence analysis in the context of crowd-based image annotation.",0
"The field of machine learning based solutions for automatic image annotation is rapidly growing, but a major obstacle is the shortage of reference annotations for algorithm training. Crowdsourcing is an attractive option due to its low cost and ability to scale, but maintaining quality control remains a significant challenge. Our team is the first to investigate the annotation process to enhance crowd-sourced image segmentation. Our approach involves utilizing a regressor to determine the quality of a segmentation based on the clickstream data of the annotator. This quality estimation can then be used to identify spam and weigh individual annotations based on their estimated quality when merging multiple segmentations of an image. We have conducted experiments on publicly available data of various object classes, incorporating a total of 29,000 crowd annotations, and our results demonstrate that (1) our method is highly accurate in estimating segmentation quality based on clickstream data, and (2) it outperforms existing techniques for merging multiple annotations. Because the regressor can be applied across various object classes, it can be considered a cost-effective solution for quality control and confidence analysis in the context of crowd-based image annotation.",1
"We present the 2017 Visual Domain Adaptation (VisDA) dataset and challenge, a large-scale testbed for unsupervised domain adaptation across visual domains. Unsupervised domain adaptation aims to solve the real-world problem of domain shift, where machine learning models trained on one domain must be transferred and adapted to a novel visual domain without additional supervision. The VisDA2017 challenge is focused on the simulation-to-reality shift and has two associated tasks: image classification and image segmentation. The goal in both tracks is to first train a model on simulated, synthetic data in the source domain and then adapt it to perform well on real image data in the unlabeled test domain. Our dataset is the largest one to date for cross-domain object classification, with over 280K images across 12 categories in the combined training, validation and testing domains. The image segmentation dataset is also large-scale with over 30K images across 18 categories in the three domains. We compare VisDA to existing cross-domain adaptation datasets and provide a baseline performance analysis using various domain adaptation models that are currently popular in the field.",0
"Introducing the VisDA challenge and dataset for the year 2017, which serves as a comprehensive platform for unsupervised domain adaptation in the visual domain. The challenge aims to address the issue of domain shift, where machine learning models trained on one domain are required to be adapted to a different visual domain without additional supervision. The VisDA2017 challenge is specifically focused on the simulation-to-reality shift and comprises two tasks: image classification and image segmentation. The objective of both tasks is to train a model on synthetic data in the source domain and adapt it to real image data in the unlabeled test domain. The dataset is currently the largest for cross-domain object classification, with over 280K images across 12 categories distributed in the training, validation, and testing domains. The image segmentation dataset is also sizeable, with over 30K images across 18 categories in the three domains. We compare VisDA to other existing cross-domain adaptation datasets and provide a baseline performance analysis using various domain adaptation models that are currently popular in the field.",1
"We propose a novel Active Learning framework capable to train effectively a convolutional neural network for semantic segmentation of medical imaging, with a limited amount of training labeled data. Our contribution is a practical Cost-Effective Active Learning approach using dropout at test time as Monte Carlo sampling to model the pixel-wise uncertainty and to analyze the image information to improve the training performance. The source code of this project is available at https://marc-gorriz.github.io/CEAL-Medical-Image-Segmentation/ .",0
"Our proposed Active Learning framework is capable of efficiently training a convolutional neural network for semantic segmentation of medical imaging, even with limited labeled data. Our approach is a Cost-Effective Active Learning method that utilizes dropout at test time as Monte Carlo sampling to model pixel-wise uncertainty. This allows us to analyze image information to enhance training performance. The project's source code can be accessed at https://marc-gorriz.github.io/CEAL-Medical-Image-Segmentation/.",1
"This paper presents an efficient automatic color image segmentation method using a seeded region growing and merging method based on square elemental regions. Our segmentation method consists of the three steps: generating seed regions, merging the regions, and applying a pixel-wise boundary determination algorithm to the resultant polygonal regions. The major features of our method are as follows: the use of square elemental regions instead of pixels as the processing unit, a seed generation method based on enhanced gradient values, a seed region growing method exploiting local gradient values, a region merging method using a similarity measure including a homogeneity distance based on Tsallis entropy, and a termination condition of region merging using an estimated desired number of regions. Using square regions as the processing unit substantially reduces the time complexity of the algorithm and makes the performance stable. The experimental results show that our method exhibits stable performance for a variety of natural images, including heavily textured areas, and produces good segmentation results using the same parameter values. The results of our method are fairly comparable to, and in some respects better than, those of existing algorithms.",0
"In this article, we introduce a color image segmentation technique that utilizes a seeded region growing and merging approach based on square elemental regions. Our segmentation technique involves three main stages: generating seed regions, merging the regions, and executing a pixel-wise boundary determination algorithm on the resultant polygonal regions. The key aspects of our approach include utilizing square elemental regions instead of pixels for processing, a seed generation method that relies on enhanced gradient values, a seed region growing method that leverages local gradient values, a region merging method that employs a similarity measure featuring a homogeneity distance based on Tsallis entropy, and a termination condition for region merging that utilizes an estimated desired number of regions. By employing square regions as the processing unit, we greatly reduce the algorithm's time complexity and enhance its overall stability. Our experimental results demonstrate that our approach delivers consistent performance on a range of natural images, even in heavily textured areas, and produces excellent segmentation results using the same parameter values. Our results are quite comparable to, and in some cases surpass, those of existing algorithms.",1
"Convolutional neural networks have shown great success on feature extraction from raw input data such as images. Although convolutional neural networks are invariant to translations on the inputs, they are not invariant to other transformations, including rotation and flip. Recent attempts have been made to incorporate more invariance in image recognition applications, but they are not applicable to dense prediction tasks, such as image segmentation. In this paper, we propose a set of methods based on kernel rotation and flip to enable rotation and flip invariance in convolutional neural networks. The kernel rotation can be achieved on kernels of 3 $\times$ 3, while kernel flip can be applied on kernels of any size. By rotating in eight or four angles, the convolutional layers could produce the corresponding number of feature maps based on eight or four different kernels. By using flip, the convolution layer can produce three feature maps. By combining produced feature maps using maxout, the resource requirement could be significantly reduced while still retain the invariance properties. Experimental results demonstrate that the proposed methods can achieve various invariance at reasonable resource requirements in terms of both memory and time.",0
"Feature extraction from raw input data, particularly images, has been highly successful with the use of convolutional neural networks. Although these networks are able to handle translations in the input, other transformations such as rotation and flip are not easily addressed. While recent efforts have been made to increase invariance in image recognition applications, they do not apply to dense prediction tasks like image segmentation. Therefore, this paper suggests a solution using kernel rotation and flip to provide rotation and flip invariance in convolutional neural networks. By rotating kernels of size 3x3 in eight or four angles, the convolutional layers can produce a corresponding number of feature maps. Additionally, flipping kernels of any size can produce three feature maps. Using maxout to combine the generated feature maps can greatly reduce resource requirements while maintaining invariance properties. Experimental results show the proposed methods can achieve multiple invariances with reasonable resource requirements in terms of memory and time.",1
"While significant attention has been recently focused on designing supervised deep semantic segmentation algorithms for vision tasks, there are many domains in which sufficient supervised pixel-level labels are difficult to obtain. In this paper, we revisit the problem of purely unsupervised image segmentation and propose a novel deep architecture for this problem. We borrow recent ideas from supervised semantic segmentation methods, in particular by concatenating two fully convolutional networks together into an autoencoder--one for encoding and one for decoding. The encoding layer produces a k-way pixelwise prediction, and both the reconstruction error of the autoencoder as well as the normalized cut produced by the encoder are jointly minimized during training. When combined with suitable postprocessing involving conditional random field smoothing and hierarchical segmentation, our resulting algorithm achieves impressive results on the benchmark Berkeley Segmentation Data Set, outperforming a number of competing methods.",0
"The focus on creating supervised deep semantic segmentation algorithms for vision tasks has increased recently, but obtaining enough supervised pixel-level labels is challenging in many domains. This paper revisits the problem of unsupervised image segmentation and proposes a new deep architecture that combines two fully convolutional networks into an autoencoder, similar to supervised semantic segmentation methods. The encoding layer generates a k-way pixelwise prediction, and during training, both the reconstruction error of the autoencoder and the normalized cut produced by the encoder are minimized. When combined with appropriate postprocessing techniques such as conditional random field smoothing and hierarchical segmentation, our algorithm performs exceptionally well on the Berkeley Segmentation Data Set, surpassing several other methods.",1
"Motivated by the important archaeological application of exploring cultural heritage objects, in this paper we study the challenging problem of automatically segmenting curve structures that are very weakly stamped or carved on an object surface in the form of a highly noisy depth map. Different from most classical low-level image segmentation methods that are known to be very sensitive to the noise and occlusions, we propose a new supervised learning algorithm based on Convolutional Neural Network (CNN) to implicitly learn and utilize more curve geometry and pattern information for addressing this challenging problem. More specifically, we first propose a Fully Convolutional Network (FCN) to estimate the skeleton of curve structures and at each skeleton pixel, a scale value is estimated to reflect the local curve width. Then we propose a dense prediction network to refine the estimated curve skeletons. Based on the estimated scale values, we finally develop an adaptive thresholding algorithm to achieve the final segmentation of curve structures. In the experiment, we validate the performance of the proposed method on a dataset of depth images scanned from unearthed pottery sherds dating to the Woodland period of Southeastern North America.",0
"In this paper, we investigate the challenging task of automatically segmenting curve structures that are weakly stamped or carved on an object's surface in the form of a very noisy depth map. Our motivation comes from the important archaeological application of exploring cultural heritage objects. Unlike traditional low-level image segmentation methods that are highly sensitive to noise and occlusions, we propose a novel supervised learning algorithm that utilizes Convolutional Neural Network (CNN) to implicitly learn and incorporate more curve geometry and pattern information. We begin by introducing a Fully Convolutional Network (FCN) that estimates the skeleton of curve structures and derives a scale value at each skeleton pixel to represent the local curve width. We then propose a dense prediction network to refine the estimated curve skeletons. Finally, we develop an adaptive thresholding algorithm based on the estimated scale values to achieve the final segmentation of curve structures. To validate our proposed method, we conduct experiments on a dataset of depth images scanned from pottery sherds dating back to the Woodland period of Southeastern North America.",1
"We present a method for reconstructing images viewed by observers based only on their eye movements. By exploring the relationships between gaze patterns and image stimuli, the ""What Are You Looking At?"" (WAYLA) system learns to synthesize photo-realistic images that are similar to the original pictures being viewed. The WAYLA approach is based on the Conditional Generative Adversarial Network (Conditional GAN) image-to-image translation technique of Isola et al. We consider two specific applications - the first, of reconstructing newspaper images from gaze heat maps, and the second, of detailed reconstruction of images containing only text. The newspaper image reconstruction process is divided into two image-to-image translation operations, the first mapping gaze heat maps into image segmentations, and the second mapping the generated segmentation into a newspaper image. We validate the performance of our approach using various evaluation metrics, along with human visual inspection. All results confirm the ability of our network to perform image generation tasks using eye tracking data.",0
"Our study presents a novel technique for recreating images based solely on the eye movements of an observer. The ""What Are You Looking At?"" (WAYLA) system utilizes gaze patterns and image stimuli correlations to generate high-quality, photorealistic images similar to the original ones viewed. We adopt the Conditional Generative Adversarial Network (Conditional GAN) image-to-image translation approach of Isola et al. We demonstrate two specific use cases - first, by reconstructing newspaper images from gaze heat maps, and second, by precisely recreating images with only text. The reconstruction of newspaper images involves two image-to-image translation operations, mapping gaze heat maps to image segmentations and then segmentations to newspaper images. We evaluate our method using various metrics, including human visual inspection, and confirm its capability to generate images using eye tracking data.",1
"In interactive medical image segmentation, anatomical structures are extracted from reconstructed volumetric images. The first iterations of user interaction traditionally consist of drawing pictorial hints as an initial estimate of the object to extract. Only after this time consuming first phase, the efficient selective refinement of current segmentation results begins. Erroneously labeled seeds, especially near the border of the object, are challenging to detect and replace for a human and may substantially impact the overall segmentation quality. We propose an automatic seeding pipeline as well as a configuration based on saliency recognition, in order to skip the time-consuming initial interaction phase during segmentation. A median Dice score of 68.22% is reached before the first user interaction on the test data set with an error rate in seeding of only 0.088%.",0
"The process of interactive medical image segmentation involves extracting anatomical structures from volumetric images. Typically, users must draw pictorial hints to estimate the object to extract, which is a time-consuming initial phase. Afterwards, selective refinement of the segmentation results can begin. However, identifying and replacing erroneously labeled seeds, particularly near the object's border, is difficult and can significantly affect segmentation quality. To address this issue, we propose an automatic seeding pipeline based on saliency recognition, which eliminates the need for the initial interaction phase. Using this approach, we achieved a median Dice score of 68.22% on the test data set before any user interaction, with only a 0.088% error rate in seeding.",1
"Deep convolutional neural networks (CNNs) have been shown to perform extremely well at a variety of tasks including subtasks of autonomous driving such as image segmentation and object classification. However, networks designed for these tasks typically require vast quantities of training data and long training periods to converge. We investigate the design rationale behind end-to-end driving network designs by proposing and comparing three small and computationally inexpensive deep end-to-end neural network models that generate driving control signals directly from input images. In contrast to prior work that segments the autonomous driving task, our models take on a novel approach to the autonomous driving problem by utilizing deep and thin Fully Convolutional Nets (FCNs) with recurrent neural nets and low parameter counts to tackle a complex end-to-end regression task predicting both steering and acceleration commands. In addition, we include layers optimized for classification to allow the networks to implicitly learn image semantics. We show that the resulting networks use 3x fewer parameters than the most recent comparable end-to-end driving network and 500x fewer parameters than the AlexNet variations and converge both faster and to lower losses while maintaining robustness against overfitting.",0
"Convolutional neural networks (CNNs) have proven to be highly effective in various autonomous driving subtasks, such as image segmentation and object classification. However, these networks necessitate significant amounts of training data and lengthy training periods to converge. Our research examines the design principles behind end-to-end driving network designs, proposing and comparing three small, computationally efficient deep end-to-end neural network models that directly generate driving control signals based on input images. Unlike prior work that segments the autonomous driving task, our models adopt an innovative approach to the problem by utilizing deep and thin Fully Convolutional Nets (FCNs) with recurrent neural nets and low parameter counts to tackle a complex end-to-end regression task predicting steering and acceleration commands. Furthermore, we incorporate layers optimized for classification, enabling the networks to learn image semantics implicitly. Our results demonstrate that the resulting networks use 3x fewer parameters than the most recent comparable end-to-end driving network and 500x fewer parameters than the AlexNet variants, converge faster and to lower losses, and maintain robustness against overfitting.",1
"We present DLTK, a toolkit providing baseline implementations for efficient experimentation with deep learning methods on biomedical images. It builds on top of TensorFlow and its high modularity and easy-to-use examples allow for a low-threshold access to state-of-the-art implementations for typical medical imaging problems. A comparison of DLTK's reference implementations of popular network architectures for image segmentation demonstrates new top performance on the publicly available challenge data ""Multi-Atlas Labeling Beyond the Cranial Vault"". The average test Dice similarity coefficient of $81.5$ exceeds the previously best performing CNN ($75.7$) and the accuracy of the challenge winning method ($79.0$).",0
"DLTK is a toolkit that offers basic implementations for effective experimentation with deep learning techniques on biomedical images. It is built on top of TensorFlow and its high modularity and user-friendly examples provide easy access to up-to-date implementations for typical medical imaging issues. By comparing DLTK's reference implementations of popular network architectures for image segmentation, it has been demonstrated that it achieves new top performance on the publicly available challenge data ""Multi-Atlas Labeling Beyond the Cranial Vault"". The average test Dice similarity coefficient of $81.5$ surpasses the previous best performing CNN ($75.7$) and the accuracy of the challenge-winning method ($79.0$).",1
"We explore encoding brain symmetry into a neural network for a brain tumor segmentation task. A healthy human brain is symmetric at a high level of abstraction, and the high-level asymmetric parts are more likely to be tumor regions. Paying more attention to asymmetries has the potential to boost the performance in brain tumor segmentation. We propose a method to encode brain symmetry into existing neural networks and apply the method to a state-of-the-art neural network for medical imaging segmentation. We evaluate our symmetry-encoded network on the dataset from a brain tumor segmentation challenge and verify that the new model extracts information in the training images more efficiently than the original model.",0
"In our study, we investigate the incorporation of brain symmetry into a neural network to enhance brain tumor segmentation. Typically, a healthy brain is symmetric, and the areas that deviate from this symmetry are often indicative of tumor regions. By focusing on these asymmetrical regions, we can potentially improve the accuracy of brain tumor segmentation. To achieve this, we propose a technique for encoding brain symmetry into an existing neural network, which we then apply to a cutting-edge medical imaging segmentation network. We assess the performance of our symmetry-encoded network using a dataset from a brain tumor segmentation challenge and find that it achieves more efficient information extraction from the training images compared to the original model.",1
"In this paper, a novel 3D deep learning network is proposed for brain MR image segmentation with randomized connection, which can decrease the dependency between layers and increase the network capacity. The convolutional LSTM and 3D convolution are employed as network units to capture the long-term and short-term 3D properties respectively. To assemble these two kinds of spatial-temporal information and refine the deep learning outcomes, we further introduce an efficient graph-based node selection and label inference method. Experiments have been carried out on two publicly available databases and results demonstrate that the proposed method can obtain competitive performances as compared with other state-of-the-art methods.",0
"This paper introduces a new 3D deep learning network for segmenting brain MR images. The network includes randomized connections to reduce layer dependency and increase capacity. The network units consist of convolutional LSTM and 3D convolution to capture long-term and short-term 3D properties. To refine the deep learning outcomes, a graph-based node selection and label inference method is introduced. The proposed method is tested on two public databases and yields comparable results to other advanced methods.",1
"Accurate and reliable brain tumor segmentation is a critical component in cancer diagnosis, treatment planning, and treatment outcome evaluation. Build upon successful deep learning techniques, a novel brain tumor segmentation method is developed by integrating fully convolutional neural networks (FCNNs) and Conditional Random Fields (CRFs) in a unified framework to obtain segmentation results with appearance and spatial consistency. We train a deep learning based segmentation model using 2D image patches and image slices in following steps: 1) training FCNNs using image patches; 2) training CRFs as Recurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNs fixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices. Particularly, we train 3 segmentation models using 2D image patches and slices obtained in axial, coronal and sagittal views respectively, and combine them to segment brain tumors using a voting based fusion strategy. Our method could segment brain images slice-by-slice, much faster than those based on image patches. We have evaluated our method based on imaging data provided by the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015 and BRATS 2016. The experimental results have demonstrated that our method could build a segmentation model with Flair, T1c, and T2 scans and achieve competitive performance as those built with Flair, T1, T1c, and T2 scans.",0
"The precise and dependable segmentation of brain tumors is a crucial element in diagnosing cancer, planning treatment, and evaluating treatment outcomes. A new method for segmenting brain tumors is developed by combining fully convolutional neural networks (FCNNs) and Conditional Random Fields (CRFs) in a unified framework to achieve segmentation results with appearance and spatial consistency. The deep learning-based segmentation model is trained using 2D image patches and image slices through a three-step process: 1) training FCNNs with image patches; 2) training CRFs as Recurrent Neural Networks (CRF-RNN) with fixed FCNN parameters using image slices; and 3) fine-tuning FCNNs and CRF-RNN with image slices. Our method trains three segmentation models using 2D image patches and slices obtained in axial, coronal, and sagittal views, respectively. These models are combined using a voting-based fusion strategy to segment brain tumors. Our approach segments brain images slice-by-slice, which is much faster than using image patches. We evaluated our method using imaging data from the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015, and BRATS 2016. The experimental results show that our method can build a segmentation model with Flair, T1c, and T2 scans, achieving competitive performance as models built with Flair, T1, T1c, and T2 scans.",1
"In this paper, a novel label fusion method is proposed for brain magnetic resonance image segmentation. This label fusion method is formulated on a graph, which embraces both label priors from atlases and anatomical priors from target image. To represent a pixel in a comprehensive way, three kinds of feature vectors are generated, including intensity, gradient and structural signature. To select candidate atlas nodes for fusion, rather than exact searching, randomized k-d tree with spatial constraint is introduced as an efficient approximation for high-dimensional feature matching. Feature Sensitive Label Prior (FSLP), which takes both the consistency and variety of different features into consideration, is proposed to gather atlas priors. As FSLP is a non-convex problem, one heuristic approach is further designed to solve it efficiently. Moreover, based on the anatomical knowledge, parts of the target pixels are also employed as graph seeds to assist the label fusion process and an iterative strategy is utilized to gradually update the label map. The comprehensive experiments carried out on two publicly available databases give results to demonstrate that the proposed method can obtain better segmentation quality.",0
"This article introduces a fresh approach to magnetic resonance image segmentation of the brain using a novel label fusion method. The method utilizes a graph to combine label priors from atlases and anatomical priors from the target image. To ensure a thorough representation of each pixel, three types of feature vectors are generated: intensity, gradient, and structural signature. The selection of candidate atlas nodes for fusion is accomplished through a randomized k-d tree with spatial constraint, which approximates high-dimensional feature matching efficiently. The Feature Sensitive Label Prior (FSLP) is proposed to collect atlas priors, taking into account the consistency and variety of different features. A heuristic approach is designed to solve the non-convex problem of FSLP efficiently. Additionally, the method employs parts of the target pixels as graph seeds based on anatomical knowledge to aid the label fusion process. An iterative strategy updates the label map gradually. Comprehensive experiments on publicly available databases show that the proposed method achieves better segmentation quality.",1
"In this work, we propose a new segmentation algorithm for images containing convex objects present in multiple shapes with a high degree of overlap. The proposed algorithm is carried out in two steps, first we identify the visible contours, segment them using concave points and finally group the segments belonging to the same object. The next step is to assign a shape identity to these grouped contour segments. For images containing objects in multiple shapes we begin first by identifying shape classes of the contours followed by assigning a shape entity to these classes. We provide a comprehensive experimentation of our algorithm on two crystal image datasets. One dataset comprises of images containing objects in multiple shapes overlapping each other and the other dataset contains standard images with objects present in a single shape. We test our algorithm against two baselines, with our proposed algorithm outperforming both the baselines.",0
"We introduce a novel segmentation algorithm for images that feature convex objects with high overlap and diverse shapes. Our approach consists of two stages: initially detecting visible contours and segmenting them using concave points, then grouping contour segments that belong to the same object and assigning them a shape identity. For images with multiple shapes, we start by identifying shape classes of the contours, followed by assigning a shape entity to these classes. To evaluate our algorithm, we conduct comprehensive experiments on two crystal image datasets. One dataset contains images with objects in multiple shapes overlapping each other, while the other dataset consists of standard images with objects in a single shape. Our proposed algorithm outperforms two baselines, as demonstrated by the results.",1
"Image analysis using more than one modality (i.e. multi-modal) has been increasingly applied in the field of biomedical imaging. One of the challenges in performing the multimodal analysis is that there exist multiple schemes for fusing the information from different modalities, where such schemes are application-dependent and lack a unified framework to guide their designs. In this work we firstly propose a conceptual architecture for the image fusion schemes in supervised biomedical image analysis: fusing at the feature level, fusing at the classifier level, and fusing at the decision-making level. Further, motivated by the recent success in applying deep learning for natural image analysis, we implement the three image fusion schemes above based on the Convolutional Neural Network (CNN) with varied structures, and combined into a single framework. The proposed image segmentation framework is capable of analyzing the multi-modality images using different fusing schemes simultaneously. The framework is applied to detect the presence of soft tissue sarcoma from the combination of Magnetic Resonance Imaging (MRI), Computed Tomography (CT) and Positron Emission Tomography (PET) images. It is found from the results that while all the fusion schemes outperform the single-modality schemes, fusing at the feature level can generally achieve the best performance in terms of both accuracy and computational cost, but also suffers from the decreased robustness in the presence of large errors in any image modalities.",0
"The use of multi-modal image analysis has become increasingly popular in the biomedical imaging field. One of the challenges of this approach is that there are numerous methods available for combining information from different modalities, and they lack a unified framework for designing them. In this study, we propose a conceptual architecture for creating image fusion schemes in supervised biomedical image analysis. We suggest fusing at the feature level, classifier level, and decision-making level. To implement these fusion schemes, we utilize Convolutional Neural Networks (CNN) with varied structures, and combine them into a single framework. Our proposed framework can analyze multi-modality images simultaneously. We apply this framework to detect soft tissue sarcoma using Magnetic Resonance Imaging (MRI), Computed Tomography (CT), and Positron Emission Tomography (PET) images. The results show that all fusion schemes outperform single-modality schemes, but fusing at the feature level typically achieves the best performance in terms of both accuracy and computational cost, despite being less robust in the presence of large errors in any image modalities.",1
"We propose an attention mechanism for 3D medical image segmentation. The method, named segmentation-by-detection, is a cascade of a detection module followed by a segmentation module. The detection module enables a region of interest to come to attention and produces a set of object region candidates which are further used as an attention model. Rather than dealing with the entire volume, the segmentation module distills the information from the potential region. This scheme is an efficient solution for volumetric data as it reduces the influence of the surrounding noise which is especially important for medical data with low signal-to-noise ratio. Experimental results on 3D ultrasound data of the femoral head shows superiority of the proposed method when compared with a standard fully convolutional network like the U-Net.",0
"Our proposal involves an attention mechanism for 3D medical image segmentation known as segmentation-by-detection. This method utilizes a detection module followed by a segmentation module in a cascade fashion. The detection module identifies a region of interest and generates a set of object region candidates that serve as an attention model. By applying this approach, the segmentation module can extract relevant information only from the potential region instead of the entire volume, resulting in a more efficient solution for volumetric data. This is particularly useful for medical data with low signal-to-noise ratio as it minimizes the impact of surrounding noise. We conducted experiments on 3D ultrasound data of the femoral head and found that our proposed method outperforms a standard fully convolutional network like the U-Net.",1
"State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions.   Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train.   In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets.   Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py",0
"Convolutional Neural Networks (CNNs) are the foundation for advanced semantic image segmentation techniques. The typical segmentation model includes a downsampling path to capture coarse semantic features, followed by an upsampling path that restores the input image resolution at the output of the model. Additional post-processing modules, such as Conditional Random Fields, can refine model predictions. Recently, DenseNets, a new CNN architecture, has demonstrated exceptional performance in image classification tasks. DenseNets utilize a feed-forward connection approach between layers to improve accuracy and ease of training. In this study, we apply DenseNets to address semantic segmentation challenges. Our approach achieves outstanding results on urban scene benchmark datasets, such as CamVid and Gatech, without requiring additional post-processing or pretraining. Furthermore, our model has fewer parameters than the current best entries for these datasets. Access to the code used in our experiments is available at https://github.com/SimJeg/FC-DenseNet/blob/master/train.py.",1
"Two of the most popular modelling paradigms in computer vision are feed-forward neural networks (FFNs) and probabilistic graphical models (GMs). Various connections between the two have been studied in recent works, such as e.g. expressing mean-field based inference in a GM as an FFN. This paper establishes a new connection between FFNs and GMs. Our key observation is that any FFN implements a certain approximation of a corresponding Bayesian network (BN). We characterize various benefits of having this connection. In particular, it results in a new learning algorithm for BNs. We validate the proposed methods for a classification problem on CIFAR-10 dataset and for binary image segmentation on Weizmann Horse dataset. We show that statistically learned BNs improve performance, having at the same time essentially better generalization capability, than their FFN counterparts.",0
"Computer vision has two popular modelling paradigms, namely feed-forward neural networks (FFNs) and probabilistic graphical models (GMs). Previous research has explored different connections between these two models, such as expressing mean-field based inference in a GM as an FFN. This paper introduces a new connection between FFNs and GMs, where it is observed that any FFN can approximate a corresponding Bayesian network (BN). The benefits of this connection are characterized, including a new learning algorithm for BNs. The proposed methods are validated through a classification problem on CIFAR-10 dataset and binary image segmentation on Weizmann Horse dataset, demonstrating that statistically learned BNs outperform FFN counterparts in terms of performance and generalization capability.",1
"The quantitative analysis of 3D confocal microscopy images of the shoot apical meristem helps understanding the growth process of some plants. Cell segmentation in these images is crucial for computational plant analysis and many automated methods have been proposed. However, variations in signal intensity across the image mitigate the effectiveness of those approaches with no easy way for user correction. We propose a web-based collaborative 3D image segmentation application, SEGMENT3D, to leverage automatic segmentation results. The image is divided into 3D tiles that can be either segmented interactively from scratch or corrected from a pre-existing segmentation. Individual segmentation results per tile are then automatically merged via consensus analysis and then stitched to complete the segmentation for the entire image stack. SEGMENT3D is a comprehensive application that can be applied to other 3D imaging modalities and general objects. It also provides an easy way to create supervised data to advance segmentation using machine learning models.",0
"In order to comprehend the growth process of certain plants, it is necessary to quantitatively analyze 3D confocal microscopy images of the shoot apical meristem. However, the effectiveness of automated methods for cell segmentation is mitigated by variations in signal intensity across the image, with no simple way for user correction. To address this issue, we propose SEGMENT3D, a web-based collaborative 3D image segmentation application that leverages automatic segmentation results. The image is divided into 3D tiles that can be segmented interactively or corrected from a pre-existing segmentation. Individual segmentation results per tile are then merged via consensus analysis and stitched to complete the segmentation for the entire image stack. SEGMENT3D is a comprehensive application that can be utilized for other 3D imaging modalities and general objects. Additionally, it offers a straightforward way to create supervised data to advance segmentation through machine learning models.",1
"Quantitative image analysis often depends on accurate classification of pixels through a segmentation process. However, imaging artifacts such as the partial volume effect and sensor noise complicate the classification process. These effects increase the pixel intensity variance of each constituent class, causing intensities from one class to overlap with another. This increased variance makes threshold based segmentation methods insufficient due to ambiguous overlap regions in the pixel intensity distributions. The class ambiguity becomes even more complex for systems with more than two constituents, such as unsaturated moist granular media. In this paper, we propose an image processing workflow that improves segmentation accuracy for multiphase systems. First, the ambiguous transition regions between classes are identified and removed, which allows for global thresholding of single-class regions. Then the transition regions are classified using a distance function, and finally both segmentations are combined into one classified image. This workflow includes three methodologies for identifying transition pixels and we demonstrate on a variety of synthetic images that these approaches are able to accurately separate the ambiguous transition pixels from the single-class regions. For situations with typical amounts of image noise, misclassification errors and area differences calculated between each class of the synthetic images and the resultant segmented images range from 0.69-1.48% and 0.01-0.74%, respectively, showing the segmentation accuracy of this approach. We demonstrate that we are able to accurately segment x-ray microtomography images of moist granular media using these computationally efficient methodologies.",0
"Accurate pixel classification through segmentation is crucial for quantitative image analysis. However, the process is complicated by imaging artifacts such as sensor noise and the partial volume effect. These effects cause overlapping intensities between different classes, resulting in ambiguous overlap regions in pixel intensity distributions that make threshold-based segmentation methods insufficient. The problem is even more complex for unsaturated moist granular media and other systems with multiple constituents. In this paper, we propose an image processing workflow that addresses this problem by identifying and removing ambiguous transition regions between classes, allowing for global thresholding of single-class regions. The transition regions are then classified using a distance function, and the two segmentations are combined into one classified image. We demonstrate the effectiveness of our approach on synthetic images, which show minimal misclassification errors and area differences between each class of the synthetic images and the resultant segmented images. We also demonstrate the accuracy of our approach on x-ray microtomography images of moist granular media.",1
"This paper proposes a novel image segmentation approachthat integrates fully convolutional networks (FCNs) with a level setmodel. Compared with a FCN, the integrated method can incorporatesmoothing and prior information to achieve an accurate segmentation.Furthermore, different than using the level set model as a post-processingtool, we integrate it into the training phase to fine-tune the FCN. Thisallows the use of unlabeled data during training in a semi-supervisedsetting. Using two types of medical imaging data (liver CT and left ven-tricle MRI data), we show that the integrated method achieves goodperformance even when little training data is available, outperformingthe FCN or the level set model alone.",0
"This paper introduces a new technique for image segmentation that combines fully convolutional networks (FCNs) with a level set model. Unlike using only an FCN, the combined approach incorporates smoothing and prior knowledge to achieve precise segmentation. Moreover, the level set model is not used as a post-processing tool but is instead integrated into the training phase to fine-tune the FCN. This allows for the use of unlabeled data in a semi-supervised setting during training. By evaluating the method on two types of medical imaging data, liver CT and left ventricle MRI data, we demonstrate that it performs well even with limited training data, surpassing the performance of either an FCN or a level set model used alone.",1
"In this paper, we present a novel approach to perform deep neural networks layer-wise weight initialization using Linear Discriminant Analysis (LDA). Typically, the weights of a deep neural network are initialized with: random values, greedy layer-wise pre-training (usually as Deep Belief Network or as auto-encoder) or by re-using the layers from another network (transfer learning). Hence, many training epochs are needed before meaningful weights are learned, or a rather similar dataset is required for seeding a fine-tuning of transfer learning. In this paper, we describe how to turn an LDA into either a neural layer or a classification layer. We analyze the initialization technique on historical documents. First, we show that an LDA-based initialization is quick and leads to a very stable initialization. Furthermore, for the task of layout analysis at pixel level, we investigate the effectiveness of LDA-based initialization and show that it outperforms state-of-the-art random weight initialization methods.",0
"This paper proposes a new method for initializing deep neural network weights using Linear Discriminant Analysis (LDA). Generally, deep neural network weights are initialized through random values, greedy layer-wise pre-training, or transfer learning from another network, which often requires numerous training epochs or a similar dataset for fine-tuning. However, the authors demonstrate how an LDA can be utilized as either a neural layer or a classification layer for weight initialization. They evaluate this technique on historical documents and find that it is a quick and stable method for initialization. Additionally, they compare the performance of LDA-based initialization to state-of-the-art random weight initialization methods for layout analysis at the pixel level, concluding that LDA-based initialization outperforms them.",1
"In this paper we present a new framework for the solution of active contour models on graphs. With the use of the Finite Element Method we generalize active contour models on graphs and reduce the problem from a partial differential equation to the solution of a sparse non-linear system. Additionally, we extend the proposed framework to solve models where the curve evolution is locally constrained around its current location. Based on the previous extension, we propose a fast algorithm for the solution of a wide range active contour models. Last, we present a supervised extension of Geodesic Active Contours for image segmentation and provide experimental evidence for the effectiveness of our framework.",0
"A novel approach to address active contour models on graphs is presented in this paper. The Finite Element Method is utilized to expand the scope of active contour models on graphs and simplify the issue from a partial differential equation to addressing a sparse non-linear system. Furthermore, the framework is expanded to tackle models where the curve evolution is restricted in the vicinity of its present position. Using this expansion, a rapid algorithm is suggested to solve a broad range of active contour models. Finally, a supervised extension of Geodesic Active Contours for image segmentation is proposed, and our framework's efficacy is supported by experimental evidence.",1
"Convolutional neural networks (CNNs) have achieved state-of-the-art performance for automatic medical image segmentation. However, they have not demonstrated sufficiently accurate and robust results for clinical use. In addition, they are limited by the lack of image-specific adaptation and the lack of generalizability to previously unseen object classes. To address these problems, we propose a novel deep learning-based framework for interactive segmentation by incorporating CNNs into a bounding box and scribble-based segmentation pipeline. We propose image-specific fine-tuning to make a CNN model adaptive to a specific test image, which can be either unsupervised (without additional user interactions) or supervised (with additional scribbles). We also propose a weighted loss function considering network and interaction-based uncertainty for the fine-tuning. We applied this framework to two applications: 2D segmentation of multiple organs from fetal MR slices, where only two types of these organs were annotated for training; and 3D segmentation of brain tumor core (excluding edema) and whole brain tumor (including edema) from different MR sequences, where only tumor cores in one MR sequence were annotated for training. Experimental results show that 1) our model is more robust to segment previously unseen objects than state-of-the-art CNNs; 2) image-specific fine-tuning with the proposed weighted loss function significantly improves segmentation accuracy; and 3) our method leads to accurate results with fewer user interactions and less user time than traditional interactive segmentation methods.",0
"While convolutional neural networks (CNNs) have achieved remarkable performance in automatic medical image segmentation, their results are not sufficiently accurate or robust for clinical use. They are also limited in their adaptability to new image classes and lack of image-specific adaptation. To overcome these issues, we propose a novel deep learning-based framework that incorporates CNNs into a bounding box and scribble-based segmentation pipeline. We suggest using image-specific fine-tuning to make the CNN model adaptable to a specific test image, either unsupervised or supervised with additional scribbles. The proposed weighted loss function considers network and interaction-based uncertainty for fine-tuning. We applied this framework to two applications: 2D segmentation of multiple organs from fetal MR slices and 3D segmentation of brain tumor core and whole brain tumor from different MR sequences. Our experimental results demonstrate that our model is more robust than state-of-the-art CNNs in segmenting previously unseen objects, image-specific fine-tuning with the proposed weighted loss function significantly improves segmentation accuracy, and our method requires fewer user interactions and less user time than traditional interactive segmentation methods.",1
"Accurate segmentation of the heart is an important step towards evaluating cardiac function. In this paper, we present a fully automated framework for segmentation of the left (LV) and right (RV) ventricular cavities and the myocardium (Myo) on short-axis cardiac MR images. We investigate various 2D and 3D convolutional neural network architectures for this task. We investigate the suitability of various state-of-the art 2D and 3D convolutional neural network architectures, as well as slight modifications thereof, for this task. Experiments were performed on the ACDC 2017 challenge training dataset comprising cardiac MR images of 100 patients, where manual reference segmentations were made available for end-diastolic (ED) and end-systolic (ES) frames. We find that processing the images in a slice-by-slice fashion using 2D networks is beneficial due to a relatively large slice thickness. However, the exact network architecture only plays a minor role. We report mean Dice coefficients of $0.950$ (LV), $0.893$ (RV), and $0.899$ (Myo), respectively with an average evaluation time of 1.1 seconds per volume on a modern GPU.",0
"This paper presents a fully automated framework for accurately segmenting the left and right ventricular cavities and the myocardium on short-axis cardiac MR images, which is crucial for evaluating cardiac function. The suitability of various state-of-the-art 2D and 3D convolutional neural network architectures, as well as modifications thereof, were investigated. The experiments were conducted on the ACDC 2017 challenge training dataset consisting of 100 patients' cardiac MR images with manual reference segmentations for end-diastolic and end-systolic frames. Processing the images in a slice-by-slice manner using 2D networks was found to be beneficial due to the relatively thick slices, but the exact network architecture had only a minor effect. Mean Dice coefficients of 0.950, 0.893, and 0.899 were achieved for LV, RV, and Myo, respectively, with an average evaluation time of 1.1 seconds per volume on a modern GPU.",1
"Digital image segmentation is the process of assigning distinct labels to different objects in a digital image, and the fuzzy segmentation algorithm has been successfully used in the segmentation of images from a wide variety of sources. However, the traditional fuzzy segmentation algorithm fails to segment objects that are characterized by textures whose patterns cannot be successfully described by simple statistics computed over a very restricted area. In this paper, we propose an extension of the fuzzy segmentation algorithm that uses adaptive textural affinity functions to perform the segmentation of such objects on bidimensional images. The adaptive affinity functions compute their appropriate neighborhood size as they compute the texture descriptors surrounding the seed spels (spatial elements), according to the characteristics of the texture being processed. The algorithm then segments the image with an appropriate neighborhood for each object. We performed experiments on mosaic images that were composed using images from the Brodatz database, and compared our results with the ones produced by a recently published texture segmentation algorithm, showing the applicability of our method.",0
"The process of assigning unique labels to various objects in a digital image is known as digital image segmentation. The fuzzy segmentation algorithm has been effectively utilized in segmenting images from diverse sources. However, the traditional fuzzy segmentation algorithm is unsuccessful in segmenting objects that possess textures with intricate patterns that cannot be described by simple statistics computed over a limited area. This paper proposes an extension to the fuzzy segmentation algorithm that utilizes adaptive textural affinity functions to segment such objects in bidimensional images. The adaptive affinity functions determine the appropriate neighborhood size as they compute the texture descriptors surrounding the seed spels (spatial elements) based on the texture's characteristics. The algorithm segments the image with the right neighborhood for each object. We conducted experiments on mosaic images created using pictures from the Brodatz database and compared our results with a recently published texture segmentation algorithm, demonstrating the applicability of our approach.",1
"Efficient and real time segmentation of color images has a variety of importance in many fields of computer vision such as image compression, medical imaging, mapping and autonomous navigation. Being one of the most computationally expensive operation, it is usually done through software imple- mentation using high-performance processors. In robotic systems, however, with the constrained platform dimensions and the need for portability, low power consumption and simultaneously the need for real time image segmentation, we envision hardware parallelism as the way forward to achieve higher acceleration. Field-programmable gate arrays (FPGAs) are among the best suited for this task as they provide high computing power in a small physical area. They exceed the computing speed of software based implementations by breaking the paradigm of sequential execution and accomplishing more per clock cycle operations by enabling hardware level parallelization at an architectural level. In this paper, we propose three novel architectures of a well known Efficient Graph based Image Segmentation algorithm. These proposed implementations optimizes time and power consumption when compared to software implementations. The hybrid design proposed, has notable furtherance of acceleration capabilities delivering atleast 2X speed gain over other implemen- tations, which henceforth allows real time image segmentation that can be deployed on Mobile Robotic systems.",0
"Real time segmentation of color images is crucial in various fields of computer vision, including image compression, medical imaging, mapping, and autonomous navigation. However, it is a computationally expensive operation, often performed through software implementation using high-performance processors. In robotic systems, where platform size is limited and portability, low power consumption, and real-time image segmentation are necessary, hardware parallelism is the way forward for achieving higher acceleration. Field-programmable gate arrays (FPGAs) are well-suited for this task as they offer high computing power in a small physical area and exceed the computing speed of software-based implementations by enabling hardware level parallelization. This paper proposes three novel architectures of the Efficient Graph-based Image Segmentation algorithm, which optimize time and power consumption when compared to software implementations. The hybrid design proposed offers a notable advancement in acceleration capabilities, delivering at least a 2X speed gain over other implementations, thereby allowing real-time image segmentation that can be deployed on Mobile Robotic systems.",1
"In this paper, we propose spatial propagation networks for learning the affinity matrix for vision tasks. We show that by constructing a row/column linear propagation model, the spatially varying transformation matrix exactly constitutes an affinity matrix that models dense, global pairwise relationships of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix, where all elements can be the output from a deep CNN, but (b) results in a dense affinity matrix that effectively models any task-specific pairwise similarity matrix. Instead of designing the similarity kernels according to image features of two points, we can directly output all the similarities in a purely data-driven manner. The spatial propagation network is a generic framework that can be applied to many affinity-related tasks, including but not limited to image matting, segmentation and colorization, to name a few. Essentially, the model can learn semantically-aware affinity values for high-level vision tasks due to the powerful learning capability of the deep neural network classifier. We validate the framework on the task of refinement for image segmentation boundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks show that the spatial propagation network provides a general, effective and efficient solution for generating high-quality segmentation results.",0
"The paper proposes the use of spatial propagation networks to learn the affinity matrix for vision tasks. The networks utilize a row/column linear propagation model that generates an affinity matrix representing dense, global pairwise relationships in an image. The model employs a three-way connection to create a sparse transformation matrix, with all elements derived from a deep CNN, that results in an effective dense affinity matrix for task-specific pairwise similarity. By using a data-driven approach, the model can output all similarities without designing similarity kernels for image features. The spatial propagation network is a versatile framework for affinity-related tasks, such as image matting, segmentation, and colorization. The model can learn semantically-aware affinity values for high-level vision tasks due to its deep neural network classifier's powerful learning capability. The study validates the framework's effectiveness in refining image segmentation boundaries, with experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks showing that the spatial propagation network is a general, efficient, and effective solution for generating high-quality segmentation results.",1
"Marking tumors and organs is a challenging task suffering from both inter- and intra-observer variability. The literature quantifies observer variability by generating consensus among multiple experts when they mark the same image. Automatically building consensus contours to establish quality assurance for image segmentation is presently absent in the clinical practice. As the \emph{big data} becomes more and more available, techniques to access a large number of existing segments of multiple experts becomes possible. Fast algorithms are, hence, required to facilitate the search for similar cases. The present work puts forward a potential framework that tested with small datasets (both synthetic and real images) displays the reliability of finding similar images. In this paper, the idea of content-based barcodes is used to retrieve similar cases in order to build consensus contours in medical image segmentation. This approach may be regarded as an extension of the conventional atlas-based segmentation that generally works with rather small atlases due to required computational expenses. The fast segment-retrieval process via barcodes makes it possible to create and use large atlases, something that directly contributes to the quality of the consensus building. Because the accuracy of experts' contours must be measured, we first used 500 synthetic prostate images with their gold markers and delineations by 20 simulated users. The fast barcode-guided computed consensus delivered an average error of $8\%\!\pm\!5\%$ compared against the gold standard segments. Furthermore, we used magnetic resonance images of prostates from 15 patients delineated by 5 oncologists and selected the best delineations to serve as the gold-standard segments. The proposed barcode atlas achieved a Jaccard overlap of $87\%\!\pm\!9\%$ with the contours of the gold-standard segments.",0
"The task of marking tumors and organs is difficult due to variations between and within observers. Consensus among multiple experts is used to quantify observer variability in the literature. However, there is currently no automated method for establishing quality assurance for image segmentation in clinical practice. With the availability of big data, it is now possible to access a large number of existing segments from multiple experts. To facilitate the search for similar cases, fast algorithms are required. This study proposes a framework for using content-based barcodes to retrieve similar cases and build consensus contours in medical image segmentation. This approach extends the conventional atlas-based segmentation and allows for the use of large atlases, which contributes to the quality of consensus building. The proposed method was tested using small datasets (both synthetic and real images) and achieved good results in comparison to gold-standard segments.",1
"Side-look synthetic aperture sonar (SAS) can produce very high quality images of the sea-floor. When viewing this imagery, a human observer can often easily identify various sea-floor textures such as sand ripple, hard-packed sand, sea grass and rock. In this paper, we present the Possibilistic Fuzzy Local Information C-Means (PFLICM) approach to segment SAS imagery into sea-floor regions that exhibit these various natural textures. The proposed PFLICM method incorporates fuzzy and possibilistic clustering methods and leverages (local) spatial information to perform soft segmentation. Results are shown on several SAS scenes and compared to alternative segmentation approaches.",0
"The use of side-look synthetic aperture sonar (SAS) can yield high-quality images of the ocean floor which enable human observers to easily distinguish between various textures such as hard-packed sand, sea grass, sand ripple, and rock. This study introduces the Possibilistic Fuzzy Local Information C-Means (PFLICM) technique to segment SAS images into distinct regions according to the natural textures present. The PFLICM method utilizes both fuzzy and possibilistic clustering techniques and considers local spatial information for soft segmentation. The effectiveness of the proposed method is demonstrated through its application on multiple SAS scenes and compared to alternative segmentation approaches.",1
"Automatic skin lesion segmentation on dermoscopic images is an essential step in computer-aided diagnosis of melanoma. However, this task is challenging due to significant variations of lesion appearances across different patients. This challenge is further exacerbated when dealing with a large amount of image data. In this paper, we extended our previous work by developing a deeper network architecture with smaller kernels to enhance its discriminant capacity. In addition, we explicitly included color information from multiple color spaces to facilitate network training and thus to further improve the segmentation performance. We extensively evaluated our method on the ISBI 2017 skin lesion segmentation challenge. By training with the 2000 challenge training images, our method achieved an average Jaccard Index (JA) of 0.765 on the 600 challenge testing images, which ranked itself in the first place in the challenge",0
"The segmentation of skin lesions in dermoscopic images is a critical component of the computer-aided diagnosis of melanoma. However, due to variations in lesion appearance across patients and the large amount of image data, this task is particularly challenging. In this study, we expanded on our previous work by designing a deeper network architecture with smaller kernels to improve its discriminative capabilities. Additionally, we incorporated color information from multiple color spaces to aid in network training and enhance segmentation performance. Our method was extensively evaluated in the ISBI 2017 skin lesion segmentation challenge, where it achieved an average Jaccard Index (JA) of 0.765 on the 600 challenge testing images, placing it first in the challenge after training with the 2000 challenge training images.",1
"In this research, we propose a deep learning based approach for speeding up the topology optimization methods. The problem we seek to solve is the layout problem. The main novelty of this work is to state the problem as an image segmentation task. We leverage the power of deep learning methods as the efficient pixel-wise image labeling technique to perform the topology optimization. We introduce convolutional encoder-decoder architecture and the overall approach of solving the above-described problem with high performance. The conducted experiments demonstrate the significant acceleration of the optimization process. The proposed approach has excellent generalization properties. We demonstrate the ability of the application of the proposed model to other problems. The successful results, as well as the drawbacks of the current method, are discussed.",0
"Our research proposes a technique for accelerating topology optimization methods using deep learning. Specifically, we focus on the layout problem and introduce an innovative approach of treating it as an image segmentation task. By utilizing the efficiency of deep learning methods in pixel-wise image labeling, we present a convolutional encoder-decoder architecture to solve the problem with high performance. Our experiments demonstrate notable improvements in the optimization process, and the proposed approach exhibits strong generalization properties. Furthermore, we investigate the potential applicability of our model to other problems and discuss both its successes and limitations.",1
"An autonomous robot should be able to evaluate the affordances that are offered by a given situation. Here we address this problem by designing a system that can densely predict affordances given only a single 2D RGB image. This is achieved with a convolutional neural network (ResNet), which we combine with refinement modules recently proposed for addressing semantic image segmentation. We define a novel cost function, which is able to handle (potentially multiple) affordances of objects and their parts in a pixel-wise manner even in the case of incomplete data. We perform qualitative as well as quantitative evaluations with simulated and real data assessing 15 different affordances. In general, we find that affordances, which are well-enough represented in the training data, are correctly recognized with a substantial fraction of correctly assigned pixels. Furthermore, we show that our model outperforms several baselines. Hence, this method can give clear action guidelines for a robot.",0
"The ability of an independent robot to assess the opportunities presented by a particular situation is crucial. To tackle this issue, we have developed a system that can accurately predict affordances from a single 2D RGB image. Our approach involves using a convolutional neural network (ResNet) in combination with refinement modules designed for addressing semantic image segmentation. We have introduced a novel cost function that can handle the potential multiple affordances of objects and their parts in a pixel-wise manner, even when data is incomplete. Through qualitative and quantitative evaluations using simulated and real data, we have assessed 15 different affordances. Our findings indicate that affordances that are well-represented in the training data are correctly recognized with a significant percentage of correctly assigned pixels. Furthermore, we have demonstrated that our method outperforms several baseline models, providing clear action guidelines for a robot.",1
"This paper proposes an end-to-end trainable network, SegFlow, for simultaneously predicting pixel-wise object segmentation and optical flow in videos. The proposed SegFlow has two branches where useful information of object segmentation and optical flow is propagated bidirectionally in a unified framework. The segmentation branch is based on a fully convolutional network, which has been proved effective in image segmentation task, and the optical flow branch takes advantage of the FlowNet model. The unified framework is trained iteratively offline to learn a generic notion, and fine-tuned online for specific objects. Extensive experiments on both the video object segmentation and optical flow datasets demonstrate that introducing optical flow improves the performance of segmentation and vice versa, against the state-of-the-art algorithms.",0
"In this paper, SegFlow is introduced as a network that can be trained end-to-end in order to predict object segmentation and optical flow simultaneously in video. SegFlow consists of two branches that work together to gather information about object segmentation and optical flow using a single framework. The segmentation branch is based on a fully convolutional network, which is effective in image segmentation, while the optical flow branch uses the FlowNet model. The framework is trained offline to learn a general concept and then fine-tuned online for specific objects. Extensive experiments on both video object segmentation and optical flow datasets show that the inclusion of optical flow improves segmentation performance and vice versa, surpassing current state-of-the-art algorithms.",1
"Accurate medical image segmentation is essential for diagnosis, surgical planning and many other applications. Convolutional Neural Networks (CNNs) have become the state-of-the-art automatic segmentation methods. However, fully automatic results may still need to be refined to become accurate and robust enough for clinical use. We propose a deep learning-based interactive segmentation method to improve the results obtained by an automatic CNN and to reduce user interactions during refinement for higher accuracy. We use one CNN to obtain an initial automatic segmentation, on which user interactions are added to indicate mis-segmentations. Another CNN takes as input the user interactions with the initial segmentation and gives a refined result. We propose to combine user interactions with CNNs through geodesic distance transforms, and propose a resolution-preserving network that gives a better dense prediction. In addition, we integrate user interactions as hard constraints into a back-propagatable Conditional Random Field. We validated the proposed framework in the context of 2D placenta segmentation from fetal MRI and 3D brain tumor segmentation from FLAIR images. Experimental results show our method achieves a large improvement from automatic CNNs, and obtains comparable and even higher accuracy with fewer user interventions and less time compared with traditional interactive methods.",0
"Accurate segmentation of medical images is crucial for various applications, such as diagnosis and surgical planning. Although Convolutional Neural Networks (CNNs) are the leading automatic segmentation methods, fully automatic results may require further refinement for clinical use. To address this, we propose a deep learning-based interactive segmentation approach to enhance the results of an automatic CNN and minimize the need for user interactions for greater accuracy. Our method involves using one CNN to generate an initial segmentation, followed by user interactions to correct any mis-segmentations. Another CNN then refines the segmentation based on the user interactions. We introduce geodesic distance transforms to combine user interactions with CNNs and a resolution-preserving network for better dense prediction. Moreover, we integrate user interactions as hard constraints into a back-propagatable Conditional Random Field. We tested our framework on 2D placenta segmentation from fetal MRI and 3D brain tumor segmentation from FLAIR images, obtaining significant improvements in accuracy over automatic CNNs and comparable or better results than traditional interactive methods with fewer user interventions and less time.",1
"In the isointense stage, the accurate volumetric image segmentation is a challenging task due to the low contrast between tissues. In this paper, we propose a novel very deep network architecture based on a densely convolutional network for volumetric brain segmentation. The proposed network architecture provides a dense connection between layers that aims to improve the information flow in the network. By concatenating features map of fine and coarse dense blocks, it allows capturing multi-scale contextual information. Experimental results demonstrate significant advantages of the proposed method over existing methods, in terms of both segmentation accuracy and parameter efficiency in MICCAI grand challenge on 6-month infant brain MRI segmentation.",0
"During the isointense stage, accurately segmenting volumetric images can be difficult due to the low tissue contrast. This paper introduces a novel deep network architecture based on densely convolutional networks for brain segmentation. The network's design focuses on improving information flow by creating dense connections between layers. By combining fine and coarse dense blocks, multi-scale contextual information can be captured. The proposed method outperforms existing methods in both segmentation accuracy and parameter efficiency, as demonstrated in the MICCAI grand challenge on 6-month infant brain MRI segmentation.",1
"For complex segmentation tasks, fully automatic systems are inherently limited in their achievable accuracy for extracting relevant objects. Especially in cases where only few data sets need to be processed for a highly accurate result, semi-automatic segmentation techniques exhibit a clear benefit for the user. One area of application is medical image processing during an intervention for a single patient. We propose a learning-based cooperative segmentation approach which includes the computing entity as well as the user into the task. Our system builds upon a state-of-the-art fully convolutional artificial neural network (FCN) as well as an active user model for training. During the segmentation process, a user of the trained system can iteratively add additional hints in form of pictorial scribbles as seed points into the FCN system to achieve an interactive and precise segmentation result. The segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches can yield superior results compared to networks without the user input channel component, due to a consistent improvement in segmentation quality after each interaction.",0
"Fully automatic systems have limitations in achieving high accuracy for extracting relevant objects in complex segmentation tasks. In such cases, where only a few data sets need to be processed for a highly accurate result, semi-automatic segmentation techniques offer clear benefits to users. A prime example of this is medical image processing during intervention for a single patient. Our proposed learning-based cooperative segmentation approach involves both the computing entity and the user in the task. The system employs a state-of-the-art fully convolutional artificial neural network (FCN) alongside an active user model for training. During the segmentation process, the trained system allows users to add additional hints in the form of pictorial scribbles as seed points into the FCN system, resulting in an interactive and precise segmentation result. We evaluate the segmentation quality of interactive FCNs and find that iterative FCN approaches produce superior results compared to networks without the user input channel component because of a consistent improvement in segmentation quality after each interaction.",1
"Low-shot learning methods for image classification support learning from sparse data. We extend these techniques to support dense semantic image segmentation. Specifically, we train a network that, given a small set of annotated images, produces parameters for a Fully Convolutional Network (FCN). We use this FCN to perform dense pixel-level prediction on a test image for the new semantic class. Our architecture shows a 25% relative meanIoU improvement compared to the best baseline methods for one-shot segmentation on unseen classes in the PASCAL VOC 2012 dataset and is at least 3 times faster.",0
"We broaden the application of low-shot learning approaches from image classification to dense semantic image segmentation. Our approach involves training a network that generates parameters for a Fully Convolutional Network (FCN) based on a limited number of annotated images. This FCN is then used to make dense pixel-level predictions on a new semantic class within a test image. Our method outperforms the best baseline methods for one-shot segmentation on previously unseen classes in the PASCAL VOC 2012 dataset by a relative meanIoU improvement of 25%, and is at least three times faster.",1
"Semantic image segmentation is an important computer vision task that is difficult because it consists of both recognition and segmentation. The task is often cast as a structured output problem on an exponentially large output-space, which is typically modeled by a discrete probabilistic model. The best segmentation is found by inferring the Maximum a-Posteriori (MAP) solution over the output distribution defined by the model. Due to limitations in optimization, the model cannot be arbitrarily complex. This leads to a trade-off: devise a more accurate model that incorporates rich high-order interactions between image elements at the cost of inaccurate and possibly intractable optimization OR leverage a tractable model which produces less accurate MAP solutions but may contain high quality solutions as other modes of its output distribution.   This thesis investigates the latter and presents a two stage approach to semantic segmentation. In the first stage a tractable segmentation model outputs a set of high probability segmentations from the underlying distribution that are not just minor perturbations of each other. Critically the output of this stage is a diverse set of plausible solutions and not just a single one. In the second stage, a discriminatively trained re-ranking model selects the best segmentation from this set. The re-ranking stage can use much more complex features than what could be tractably used in the segmentation model, allowing a better exploration of the solution space than simply returning the MAP solution. The formulation is agnostic to the underlying segmentation model (e.g. CRF, CNN, etc.) and optimization algorithm, which makes it applicable to a wide range of models and inference methods. Evaluation of the approach on a number of semantic image segmentation benchmark datasets highlight its superiority over inferring the MAP solution.",0
"Semantic image segmentation is a challenging task in computer vision that involves both recognition and segmentation. To address this problem, a discrete probabilistic model is typically used to model the exponentially large output-space, and the best segmentation is found by inferring the Maximum a-Posteriori (MAP) solution. However, due to optimization limitations, the model cannot be too complex, resulting in a trade-off between accuracy and tractability. This thesis explores the use of a tractable segmentation model in the first stage, which outputs a diverse set of plausible solutions, followed by a discriminatively trained re-ranking model in the second stage, which selects the best segmentation from this set. This approach is applicable to a wide range of models and inference methods and has been shown to outperform inferring the MAP solution on several semantic image segmentation benchmark datasets.",1
"We introduce a dynamic multiscale tree (DMT) architecture that learns how to leverage the strengths of different existing classifiers for supervised multi-label image segmentation. Unlike previous works that simply aggregate or cascade classifiers for addressing image segmentation and labeling tasks, we propose to embed strong classifiers into a tree structure that allows bi-directional flow of information between its classifier nodes to gradually improve their performances. Our DMT is a generic classification model that inherently embeds different cascades of classifiers while enhancing learning transfer between them to boost up their classification accuracies. Specifically, each node in our DMT can nest a Structured Random Forest (SRF) classifier or a Bayesian Network (BN) classifier. The proposed SRF-BN DMT architecture has several appealing properties. First, while SRF operates at a patch-level (regular image region), BN operates at the super-pixel level (irregular image region), thereby enabling the DMT to integrate multi-level image knowledge in the learning process. Second, although BN is powerful in modeling dependencies between image elements (superpixels, edges) and their features, the learning of its structure and parameters is challenging. On the other hand, SRF may fail to accurately detect very irregular object boundaries. The proposed DMT robustly overcomes these limitations for both classifiers through the ascending and descending flow of contextual information between each parent node and its children nodes. Third, we train DMT using different scales, where we progressively decrease the patch and superpixel sizes as we go deeper along the tree edges nearing its leaf nodes. Last, DMT demonstrates its outperformance in comparison to several state-of-the-art segmentation methods for multi-labeling of brain images with gliomas.",0
"Our team has developed a dynamic multiscale tree (DMT) architecture that improves the accuracy of multi-label image segmentation by incorporating various existing classifiers. Unlike previous approaches that simply combine or sequence classifiers, our solution embeds strong classifiers within a tree structure that permits information flow in both directions, enhancing their performance over time. The DMT is a versatile classification model that comprises a range of classifier cascades, each of which learns from the others to improve classification accuracy. Each node in the DMT can contain either a Structured Random Forest (SRF) classifier or a Bayesian Network (BN) classifier. Our proposed SRF-BN DMT architecture has several benefits, including the ability to integrate multi-level image knowledge in the learning process by operating at different patch and super-pixel levels. Additionally, the DMT overcomes the limitations of each classifier by exchanging contextual information between parent and child nodes. Finally, DMT outperforms several state-of-the-art segmentation methods for multi-labeling of brain images with gliomas.",1
"Longitudinal reproducibility is an essential concern in automated medical image segmentation, yet has proven to be an elusive objective as manual brain structure tracings have shown more than 10% variability. To improve reproducibility, lon-gitudinal segmentation (4D) approaches have been investigated to reconcile tem-poral variations with traditional 3D approaches. In the past decade, multi-atlas la-bel fusion has become a state-of-the-art segmentation technique for 3D image and many efforts have been made to adapt it to a 4D longitudinal fashion. However, the previous methods were either limited by using application specified energy function (e.g., surface fusion and multi model fusion) or only considered tem-poral smoothness on two consecutive time points (t and t+1) under sparsity as-sumption. Therefore, a 4D multi-atlas label fusion theory for general label fusion purpose and simultaneously considering temporal consistency on all time points is appealing. Herein, we propose a novel longitudinal label fusion algorithm, called 4D joint label fusion (4DJLF), to incorporate the temporal consistency modeling via non-local patch-intensity covariance models. The advantages of 4DJLF include: (1) 4DJLF is under the general label fusion framework by simul-taneously incorporating the spatial and temporal covariance on all longitudinal time points. (2) The proposed algorithm is a longitudinal generalization of a lead-ing joint label fusion method (JLF) that has proven adaptable to a wide variety of applications. (3) The spatial temporal consistency of atlases is modeled in a prob-abilistic model inspired from both voting based and statistical fusion. The pro-posed approach improves the consistency of the longitudinal segmentation while retaining sensitivity compared with original JLF approach using the same set of atlases. The method is available online in open-source.",0
"Automated medical image segmentation requires longitudinal reproducibility, which has been difficult to achieve due to over 10% variability in manual brain structure tracings. To improve reproducibility, researchers have explored longitudinal segmentation approaches that combine traditional 3D methods with temporal variations. Multi-atlas label fusion has emerged as a state-of-the-art segmentation technique for 3D images, but efforts to adapt it to a 4D longitudinal fashion have been limited by application-specific energy functions or sparsity assumptions. Therefore, a 4D multi-atlas label fusion theory that considers temporal consistency on all time points is needed. In this study, we propose a novel longitudinal label fusion algorithm called 4D joint label fusion (4DJLF), which incorporates temporal consistency modeling through non-local patch-intensity covariance models. The advantages of 4DJLF include its general label fusion framework that incorporates both spatial and temporal covariance, its adaptability to a wide variety of applications, and its probabilistic model that improves the consistency of longitudinal segmentation while retaining sensitivity. The method is available online as open-source software.",1
"In this paper, we cast the scribble-based interactive image segmentation as a semi-supervised learning problem. Our novel approach alleviates the need to solve an expensive generalized eigenvector problem by approximating the eigenvectors using efficiently computed eigenfunctions. The smoothness operator defined on feature densities at the limit n tends to infinity recovers the exact eigenvectors of the graph Laplacian, where n is the number of nodes in the graph. To further reduce the computational complexity without scarifying our accuracy, we select pivots pixels from user annotations. In our experiments, we evaluate our approach using both human scribble and ""robot user"" annotations to guide the foreground/background segmentation. We developed a new unbiased collection of five annotated images datasets to standardize the evaluation procedure for any scribble-based segmentation method. We experimented with several variations, including different feature vectors, pivot count and the number of eigenvectors. Experiments are carried out on datasets that contain a wide variety of natural images. We achieve better qualitative and quantitative results compared to state-of-the-art interactive segmentation algorithms.",0
"The scribble-based interactive image segmentation is approached as a semi-supervised learning issue in this study. Our approach avoids the necessity of solving a costly generalized eigenvector problem by approximating the eigenvectors using efficiently computed eigenfunctions. The smoothness operator described on feature densities when the number of nodes in the graph tends to infinity recovers the precise eigenvectors of the graph Laplacian. To decrease the computational complexity without compromising accuracy, we choose pivot pixels from user annotations. We assess our method using both human scribble and ""robot user"" annotations to guide the foreground/background segmentation in our experiments. We also created a new unbiased collection of five annotated image datasets to standardize the evaluation procedure for any scribble-based segmentation technique. We experimented with different feature vectors, pivot count, and number of eigenvectors, among other variations. The experiments were conducted on datasets containing a wide range of natural images. In comparison to state-of-the-art interactive segmentation algorithms, we achieve superior qualitative and quantitative results.",1
"In many computer vision tasks, we expect a particular behavior of the output with respect to rotations of the input image. If this relationship is explicitly encoded, instead of treated as any other variation, the complexity of the problem is decreased, leading to a reduction in the size of the required model. In this paper, we propose the Rotation Equivariant Vector Field Networks (RotEqNet), a Convolutional Neural Network (CNN) architecture encoding rotation equivariance, invariance and covariance. Each convolutional filter is applied at multiple orientations and returns a vector field representing magnitude and angle of the highest scoring orientation at every spatial location. We develop a modified convolution operator relying on this representation to obtain deep architectures. We test RotEqNet on several problems requiring different responses with respect to the inputs' rotation: image classification, biomedical image segmentation, orientation estimation and patch matching. In all cases, we show that RotEqNet offers extremely compact models in terms of number of parameters and provides results in line to those of networks orders of magnitude larger.",0
"The output behavior in computer vision tasks concerning input image rotations is often expected to be a certain way. Explicitly encoding this relationship instead of treating it as a regular variation can decrease the complexity of the problem, resulting in a smaller required model size. This paper introduces the Rotation Equivariant Vector Field Networks (RotEqNet), a Convolutional Neural Network (CNN) architecture that encodes rotation equivariance, invariance, and covariance. Each convolutional filter is applied at multiple orientations and produces a vector field that represents the magnitude and angle of the highest scoring orientation at each spatial location. We have modified the convolution operator based on this representation to create deep architectures. We have tested RotEqNet on various problems that require different responses concerning input rotation, such as image classification, biomedical image segmentation, orientation estimation, and patch matching. In all scenarios, we have demonstrated that RotEqNet offers highly compact models in terms of the number of parameters and provides results that are comparable to those of networks that are much larger.",1
"One of the most straightforward, direct and efficient approaches to Image Segmentation is Image Thresholding. Multi-level Image Thresholding is an essential viewpoint in many image processing and Pattern Recognition based real-time applications which can effectively and efficiently classify the pixels into various groups denoting multiple regions in an Image. Thresholding based Image Segmentation using fuzzy entropy combined with intelligent optimization approaches are commonly used direct methods to properly identify the thresholds so that they can be used to segment an Image accurately. In this paper a novel approach for multi-level image thresholding is proposed using Type II Fuzzy sets combined with Adaptive Plant Propagation Algorithm (APPA). Obtaining the optimal thresholds for an image by maximizing the entropy is extremely tedious and time consuming with increase in the number of thresholds. Hence, Adaptive Plant Propagation Algorithm (APPA), a memetic algorithm based on plant intelligence, is used for fast and efficient selection of optimal thresholds. This fact is reasonably justified by comparing the accuracy of the outcomes and computational time consumed by other modern state-of-the-art algorithms such as Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA) and Genetic Algorithm (GA).",0
"Image Thresholding is a simple and effective method for Image Segmentation. It is commonly used in real-time applications to classify pixels into various groups, indicating multiple regions in an Image. Fuzzy entropy combined with intelligent optimization approaches is often used to identify the thresholds required for accurate segmentation. However, obtaining optimal thresholds using this method can be time-consuming. To address this issue, we propose a novel approach using Type II Fuzzy sets and Adaptive Plant Propagation Algorithm (APPA) for multi-level image thresholding. APPA is a memetic algorithm that mimics plant intelligence and quickly selects optimal thresholds. We compared our approach with other modern algorithms like Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA), and Genetic Algorithm (GA) in terms of accuracy and computational time, and our approach proved to be superior.",1
"Inspired by the combination of feedforward and iterative computations in the virtual cortex, and taking advantage of the ability of denoising autoencoders to estimate the score of a joint distribution, we propose a novel approach to iterative inference for capturing and exploiting the complex joint distribution of output variables conditioned on some input variables. This approach is applied to image pixel-wise segmentation, with the estimated conditional score used to perform gradient ascent towards a mode of the estimated conditional distribution. This extends previous work on score estimation by denoising autoencoders to the case of a conditional distribution, with a novel use of a corrupted feedforward predictor replacing Gaussian corruption. An advantage of this approach over more classical ways to perform iterative inference for structured outputs, like conditional random fields (CRFs), is that it is not any more necessary to define an explicit energy function linking the output variables. To keep computations tractable, such energy function parametrizations are typically fairly constrained, involving only a few neighbors of each of the output variables in each clique. We experimentally find that the proposed iterative inference from conditional score estimation by conditional denoising autoencoders performs better than comparable models based on CRFs or those not using any explicit modeling of the conditional joint distribution of outputs.",0
"Our approach to iterative inference is based on the virtual cortex's combination of feedforward and iterative computations, and the denoising autoencoder's ability to estimate the score of a joint distribution. By utilizing this, we have proposed a novel method to capture and exploit the complex joint distribution of output variables based on input variables. Our method was applied to image pixel-wise segmentation, where the estimated conditional score was used to perform gradient ascent towards a mode of the estimated conditional distribution. This approach differs from classical methods like conditional random fields (CRFs) because it does not require defining an explicit energy function linking the output variables, which can be computationally limiting. Our experiments show that the proposed iterative inference from conditional score estimation by conditional denoising autoencoders outperforms comparable models based on CRFs or those not using any explicit modeling of the conditional joint distribution of outputs. Additionally, we introduce a novel use of a corrupted feedforward predictor instead of Gaussian corruption in the score estimation, extending previous work on score estimation by denoising autoencoders to the case of a conditional distribution.",1
"Learning rich and diverse representations is critical for the performance of deep convolutional neural networks (CNNs). In this paper, we consider how to use privileged information to promote inherent diversity of a single CNN model such that the model can learn better representations and offer stronger generalization ability. To this end, we propose a novel group orthogonal convolutional neural network (GoCNN) that learns untangled representations within each layer by exploiting provided privileged information and enhances representation diversity effectively. We take image classification as an example where image segmentation annotations are used as privileged information during the training process. Experiments on two benchmark datasets -- ImageNet and PASCAL VOC -- clearly demonstrate the strong generalization ability of our proposed GoCNN model. On the ImageNet dataset, GoCNN improves the performance of state-of-the-art ResNet-152 model by absolute value of 1.2% while only uses privileged information of 10% of the training images, confirming effectiveness of GoCNN on utilizing available privileged knowledge to train better CNNs.",0
"Deep convolutional neural networks (CNNs) require rich and diverse representations to perform well. This study explores the use of privileged information to enhance the inherent diversity of a single CNN model, which can lead to better representation learning and stronger generalization ability. The proposed solution is a group orthogonal CNN (GoCNN) that learns untangled representations within each layer by leveraging privileged information. The study uses image classification as an example, where image segmentation annotations are used as privileged information during training. Experiments on ImageNet and PASCAL VOC datasets show that GoCNN outperforms the state-of-the-art ResNet-152 model by 1.2% on ImageNet, while using privileged information from only 10% of the training images. This confirms the effectiveness of GoCNN in using available privileged knowledge to train better CNNs.",1
"In this paper, we propose a novel medical image segmentation using iterative deep learning framework. We have combined an iterative learning approach and an encoder-decoder network to improve segmentation results, which enables to precisely localize the regions of interest (ROIs) including complex shapes or detailed textures of medical images in an iterative manner. The proposed iterative deep convolutional encoder-decoder network consists of two main paths: convolutional encoder path and convolutional decoder path with iterative learning. Experimental results show that the proposed iterative deep learning framework is able to yield excellent medical image segmentation performances for various medical images. The effectiveness of the proposed method has been proved by comparing with other state-of-the-art medical image segmentation methods.",0
"Our paper introduces a new approach to medical image segmentation through an iterative deep learning framework. By combining an iterative learning technique and an encoder-decoder network, we are able to enhance the accuracy of segmentation results, particularly in localizing regions of interest with complex shapes or detailed textures. Our proposed framework includes two key components: a convolutional encoder path and a convolutional decoder path, both with iterative learning. In our experiments, we found that our iterative deep convolutional encoder-decoder network produced superior results compared to other state-of-the-art segmentation methods across a range of medical images.",1
"Slack and margin rescaling are variants of the structured output SVM, which is frequently applied to problems in computer vision such as image segmentation, object localization, and learning parts based object models. They define convex surrogates to task specific loss functions, which, when specialized to non-additive loss functions for multi-label problems, yield extensions to increasing set functions. We demonstrate in this paper that we may use these concepts to define polynomial time convex extensions of arbitrary supermodular functions, providing an analysis framework for the tightness of these surrogates. This analysis framework shows that, while neither margin nor slack rescaling dominate the other, known bounds on supermodular functions can be used to derive extensions that dominate both of these, indicating possible directions for defining novel structured output prediction surrogates. In addition to the analysis of structured prediction loss functions, these results imply an approach to supermodular minimization in which margin rescaling is combined with non-polynomial time convex extensions to compute a sequence of LP relaxations reminiscent of a cutting plane method. This approach is applied to the problem of selecting representative exemplars from a set of images, validating our theoretical contributions.",0
"Structured output SVM has two variants, namely slack and margin rescaling, which are commonly used in computer vision for tasks like image segmentation, object localization, and parts-based object model learning. These variants provide convex surrogates to task-specific loss functions, enabling extensions to increasing set functions for multi-label problems. Through our research, we have shown that these concepts can be utilized to define polynomial time convex extensions of arbitrary supermodular functions. Our analysis framework indicates that neither margin nor slack rescaling dominates the other, but known bounds on supermodular functions can be used to derive extensions that dominate both. This suggests that there may be opportunities to develop new structured output prediction surrogates. We also demonstrate how this framework can be used for supermodular minimization by combining margin rescaling with non-polynomial time convex extensions to create a sequence of LP relaxations. This approach is applied to the exemplar selection problem in image sets, validating our theoretical contributions.",1
"Color image segmentation is a very emerging research topic in the area of color image analysis and pattern recognition. Many state-of-the-art algorithms have been developed for this purpose. But, often the segmentation results of these algorithms seem to be suffering from miss-classifications and over-segmentation. The reasons behind these are the degradation of image quality during the acquisition, transmission and color space conversion. So, here arises the need of an efficient image enhancement technique which can remove the redundant pixels or noises from the color image before proceeding for final segmentation. In this paper, an effort has been made to study and analyze different image enhancement techniques and thereby finding out the better one for color image segmentation. Also, this comparative study is done on two well-known color spaces HSV and LAB separately to find out which color space supports segmentation task more efficiently with respect to those enhancement techniques.",0
"The research of color image segmentation is a rapidly developing field in the realm of color image analysis and pattern recognition. A variety of advanced algorithms have been created to achieve this goal. However, these techniques often produce segmentation results that suffer from errors and excessive segmentation. The cause of these issues can be attributed to the deterioration of image quality that occurs during image acquisition, transmission, and color space conversion. Thus, there is a demand for an effective image enhancement method that can eliminate redundant pixels or noise from the color image before completing the final segmentation. This paper aims to investigate and assess various image enhancement techniques to identify the optimal one for color image segmentation. Additionally, a comparative analysis is conducted on the two commonly used color spaces, HSV and LAB, to determine which color space is better suited for efficient segmentation with respect to these enhancement techniques.",1
"We approach structured output prediction by optimizing a deep value network (DVN) to precisely estimate the task loss on different output configurations for a given input. Once the model is trained, we perform inference by gradient descent on the continuous relaxations of the output variables to find outputs with promising scores from the value network. When applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar estimating the intersection over union between the input and ground truth masks. For multi-label classification, the DVN's objective is to correctly predict the F1 score for any potential label configuration. The DVN framework achieves the state-of-the-art results on multi-label prediction and image segmentation benchmarks.",0
"Our approach to structured output prediction involves the optimization of a deep value network (DVN) to accurately estimate the task loss for different output configurations based on a given input. Once the model is trained, we use gradient descent on the continuous relaxations of the output variables during inference to identify outputs with high scores from the value network. In the case of image segmentation, the value network takes an image and a segmentation mask as inputs and generates a scalar that estimates the intersection over union between the input and ground truth masks. In multi-label classification, the DVN's objective is to predict the F1 score for any potential label configuration. Our DVN framework has achieved state-of-the-art results in both multi-label prediction and image segmentation benchmarks.",1
"Selective weeding is one of the key challenges in the field of agriculture robotics. To accomplish this task, a farm robot should be able to accurately detect plants and to distinguish them between crop and weeds. Most of the promising state-of-the-art approaches make use of appearance-based models trained on large annotated datasets. Unfortunately, creating large agricultural datasets with pixel-level annotations is an extremely time consuming task, actually penalizing the usage of data-driven techniques. In this paper, we face this problem by proposing a novel and effective approach that aims to dramatically minimize the human intervention needed to train the detection and classification algorithms. The idea is to procedurally generate large synthetic training datasets randomizing the key features of the target environment (i.e., crop and weed species, type of soil, light conditions). More specifically, by tuning these model parameters, and exploiting a few real-world textures, it is possible to render a large amount of realistic views of an artificial agricultural scenario with no effort. The generated data can be directly used to train the model or to supplement real-world images. We validate the proposed methodology by using as testbed a modern deep learning based image segmentation architecture. We compare the classification results obtained using both real and synthetic images as training data. The reported results confirm the effectiveness and the potentiality of our approach.",0
"Agriculture robotics faces a significant challenge in selective weeding, which requires a farm robot to accurately distinguish between crop and weeds. Currently, most state-of-the-art approaches rely on appearance-based models trained on large annotated datasets. However, the creation of such datasets is arduous, which hinders the use of data-driven techniques. In this study, we propose a novel approach to minimize human intervention in training detection and classification algorithms. We generate large synthetic training datasets by randomizing key features of the target environment, such as crop and weed species, type of soil, and light conditions. This approach requires minimal effort and can be used to supplement real-world images. We validate this methodology using a deep learning-based image segmentation architecture and obtain promising results in both real and synthetic image classification.",1
"In this paper we are interested in the problem of image segmentation given natural language descriptions, i.e. referring expressions. Existing works tackle this problem by first modeling images and sentences independently and then segment images by combining these two types of representations. We argue that learning word-to-image interaction is more native in the sense of jointly modeling two modalities for the image segmentation task, and we propose convolutional multimodal LSTM to encode the sequential interactions between individual words, visual information, and spatial information. We show that our proposed model outperforms the baseline model on benchmark datasets. In addition, we analyze the intermediate output of the proposed multimodal LSTM approach and empirically explain how this approach enforces a more effective word-to-image interaction.",0
"The focus of our research is on solving the issue of image segmentation using natural language descriptions, also known as referring expressions. Current methods address this problem by initially representing images and sentences separately and then combining them to segment images. We contend that it is more appropriate to learn the interaction between words and images by jointly modeling both modalities for image segmentation. To accomplish this, we suggest using a convolutional multimodal LSTM to encode the sequential interactions between words, visual data, and spatial information. Our results demonstrate that our proposed model outperforms the baseline model on standard datasets. Furthermore, we analyze the intermediate output of the proposed multimodal LSTM technique and provide empirical evidence on its effectiveness in enforcing a better word-to-image interaction.",1
"This manuscript introduces the problem of prominent object detection and recognition inspired by the fact that human seems to priorities perception of scene elements. The problem deals with finding the most important region of interest, segmenting the relevant item/object in that area, and assigning it an object class label. In other words, we are solving the three problems of saliency modeling, saliency detection, and object recognition under one umbrella. The motivation behind such a problem formulation is (1) the benefits to the knowledge representation-based vision pipelines, and (2) the potential improvements in emulating bio-inspired vision systems by solving these three problems together. We are foreseeing extending this problem formulation to fully semantically segmented scenes with instance object priority for high-level inferences in various applications including assistive vision. Along with a new problem definition, we also propose a method to achieve such a task. The proposed model predicts the most important area in the image, segments the associated objects, and labels them. The proposed problem and method are evaluated against human fixations, annotated segmentation masks, and object class categories. We define a chance level for each of the evaluation criterion to compare the proposed algorithm with. Despite the good performance of the proposed baseline, the overall evaluations indicate that the problem of prominent object detection and recognition is a challenging task that is still worth investigating further.",0
"The manuscript presented in this text outlines the issue of detecting and recognizing prominent objects, which is inspired by the human brain's tendency to prioritize the perception of scene elements. The problem involves identifying the most significant area of interest, segmenting the relevant object within that area, and assigning it an object class label. Essentially, this manuscript addresses the three problems of saliency modeling, saliency detection, and object recognition simultaneously. The motivation behind this approach is twofold: firstly, to benefit knowledge representation-based vision pipelines, and secondly, to emulate bio-inspired vision systems by solving these three problems in conjunction. The authors hope to extend this problem formulation to fully semantically segmented scenes with instance object priority, which could have applications in assistive vision and high-level inferences. The authors also propose a method to achieve this task, which is evaluated against human fixations, annotated segmentation masks, and object class categories. Although the proposed baseline performs well, the overall evaluations indicate that the problem of detecting and recognizing prominent objects is still a challenging task that requires further investigation.",1
"Image segmentation of touching objects plays a key role in providing accurate classification for computer vision technologies. A new line profile based imaging segmentation algorithm has been developed to provide a robust and accurate segmentation of a group of touching corns. The performance of the line profile based algorithm has been compared to a watershed based imaging segmentation algorithm. Both algorithms are tested on three different patterns of images, which are isolated corns, single-lines, and random distributed formations. The experimental results show that the algorithm can segment a large number of touching corn kernels efficiently and accurately.",0
"Accurate classification for computer vision technologies heavily relies on image segmentation of objects that are touching. A new algorithm has been created that is based on line profiles, allowing for a reliable and precise segmentation of a cluster of corns that are touching. A comparison was made between the line profile based algorithm and a watershed based imaging segmentation algorithm. The algorithms were tested on three different image patterns: isolated corns, single-lines, and random distributed formations. The results of the experiment indicate that the algorithm can efficiently and accurately segment a substantial number of touching corn kernels.",1
"Non-invasive detection of cardiovascular disorders from radiology scans requires quantitative image analysis of the heart and its substructures. There are well-established measurements that radiologists use for diseases assessment such as ejection fraction, volume of four chambers, and myocardium mass. These measurements are derived as outcomes of precise segmentation of the heart and its substructures. The aim of this paper is to provide such measurements through an accurate image segmentation algorithm that automatically delineates seven substructures of the heart from MRI and/or CT scans. Our proposed method is based on multi-planar deep convolutional neural networks (CNN) with an adaptive fusion strategy where we automatically utilize complementary information from different planes of the 3D scans for improved delineations. For CT and MRI, we have separately designed three CNNs (the same architectural configuration) for three planes, and have trained the networks from scratch for voxel-wise labeling for the following cardiac structures: myocardium of left ventricle (Myo), left atrium (LA), left ventricle (LV), right atrium (RA), right ventricle (RV), ascending aorta (Ao), and main pulmonary artery (PA). We have evaluated the proposed method with 4-fold-cross validation on the multi-modality whole heart segmentation challenge (MM-WHS 2017) dataset. The precision and dice index of 0.93 and 0.90, and 0.87 and 0.85 were achieved for CT and MR images, respectively. While a CT volume was segmented about 50 seconds, an MRI scan was segmented around 17 seconds with the GPUs/CUDA implementation.",0
"To detect cardiovascular disorders non-invasively from radiology scans, precise measurements of the heart and its substructures are necessary. Radiologists use established measurements such as ejection fraction, volume of four chambers, and myocardium mass, which are the outcomes of accurate segmentation of the heart and its substructures. The objective of this study is to provide these measurements through an image segmentation algorithm that delineates seven substructures of the heart from MRI and/or CT scans. The proposed method uses multi-planar deep convolutional neural networks (CNN) with an adaptive fusion strategy that automatically combines information from different 3D scan planes to improve delineations. Three CNNs were designed for CT and MRI scans separately, and trained from scratch for voxel-wise labeling of the left ventricle myocardium (Myo), left atrium (LA), left ventricle (LV), right atrium (RA), right ventricle (RV), ascending aorta (Ao), and main pulmonary artery (PA). The proposed method was evaluated on the MM-WHS 2017 dataset using 4-fold-cross validation, achieving precision and dice index scores of 0.93 and 0.90 for CT images, and 0.87 and 0.85 for MR images, respectively. The segmentation time for a CT volume was about 50 seconds, while an MRI scan took around 17 seconds with GPUs/CUDA implementation.",1
"While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversarial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work has focused on image classification, this work proposes attacks against semantic image segmentation: we present an approach for generating (universal) adversarial perturbations that make the network yield a desired target segmentation as output. We show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. Furthermore, we also show the existence of universal noise which removes a target class (e.g., all pedestrians) from the segmentation while leaving the segmentation mostly unchanged otherwise.",0
"Although deep learning is highly effective in tasks related to perception, it can be easily deceived by adversarial alterations made to the input. These changes refer to adding noise to the input, which is created to trick the system while being almost undetectable to humans. In some cases, universal perturbations can deceive the network on most inputs, regardless of their type. While previous research has mainly concentrated on image classification, this study proposes a method for attacking semantic image segmentation. We introduce a technique for generating (universal) adversarial perturbations that prompt the network to produce a target segmentation as the output. We demonstrate through experiments that there are universal noise patterns that are hardly noticeable but lead to almost identical predicted segmentation for any input. Additionally, we exhibit the existence of universal noise that eliminates a particular class (such as all pedestrians) from the segmentation, leaving the segmentation mostly unchanged in other respects.",1
"Convolutional Neural Networks have been highly successful in performing a host of computer vision tasks such as object recognition, object detection, image segmentation and texture synthesis. In 2015, Gatys et. al [7] show how the style of a painter can be extracted from an image of the painting and applied to another normal photograph, thus recreating the photo in the style of the painter. The method has been successfully applied to a wide range of images and has since spawned multiple applications and mobile apps. In this paper, the neural style transfer algorithm is applied to fashion so as to synthesize new custom clothes. We construct an approach to personalize and generate new custom clothes based on a users preference and by learning the users fashion choices from a limited set of clothes from their closet. The approach is evaluated by analyzing the generated images of clothes and how well they align with the users fashion style.",0
"A range of computer vision tasks such as object recognition, object detection, image segmentation, and texture synthesis have been accomplished effectively by Convolutional Neural Networks. In 2015, Gatys et. al [7] demonstrated how to extract a painter's style from an image of their painting and apply it to a normal photograph, effectively transforming the photo into the painter's style. This method has been widely used with success in various applications and mobile apps. The current study utilizes the neural style transfer algorithm to create new custom clothes in the fashion industry. The approach involves personalizing and generating new custom clothes based on a user's preference by learning their fashion choices from a limited set of clothes in their closet. The study evaluates the effectiveness of the approach by analyzing the generated images of clothes and their alignment with the user's fashion style.",1
"In this paper, we develop a 2D and 3D segmentation pipelines for fully automated cardiac MR image segmentation using Deep Convolutional Neural Networks (CNN). Our models are trained end-to-end from scratch using the ACD Challenge 2017 dataset comprising of 100 studies, each containing Cardiac MR images in End Diastole and End Systole phase. We show that both our segmentation models achieve near state-of-the-art performance scores in terms of distance metrics and have convincing accuracy in terms of clinical parameters. A comparative analysis is provided by introducing a novel dice loss function and its combination with cross entropy loss. By exploring different network structures and comprehensive experiments, we discuss several key insights to obtain optimal model performance, which also is central to the theme of this challenge.",0
"This article presents our development of 2D and 3D segmentation pipelines for automated cardiac MR image segmentation using Deep Convolutional Neural Networks (CNN). We train our models from scratch using the ACD Challenge 2017 dataset, which includes 100 studies with Cardiac MR images in End Diastole and End Systole phase. Our segmentation models exhibit impressive performance scores in terms of distance metrics and clinical parameters. To provide a comparative analysis, we introduce a novel dice loss function and combine it with cross entropy loss. Through exploration of different network structures and extensive experiments, we discuss key insights for achieving optimal model performance, which is integral to the challenge's theme.",1
"Computer Vision is growing day by day in terms of user specific applications. The first step of any such application is segmenting an image. In this paper, we propose a novel and grass-root level image segmentation algorithm for cases in which the background has uniform color distribution. This algorithm can be used for images of flowers, birds, insects and many more where such background conditions occur. By image segmentation, the visualization of a computer increases manifolds and it can even attain near-human accuracy during classification.",0
"The application of Computer Vision is experiencing continuous expansion with respect to custom user needs. The initial stage of any such application involves the division of an image into segments. Our paper presents an innovative and fundamental approach to image segmentation algorithms that caters to scenarios where the background has uniform color distribution. This algorithm can be effectively employed for images containing flowers, birds, insects, and many more, where homogeneous background conditions prevail. By facilitating image segmentation, the potential of computer visualization multiplies, and it can even achieve accuracy levels comparable to that of humans during classification.",1
"Rapid growth in the field of quantitative digital image analysis is paving the way for researchers to make precise measurements about objects in an image. To compute quantities from the image such as the density of compressed materials or the velocity of a shockwave, we must determine object boundaries. Images containing regions that each have a spatial trend in intensity are of particular interest. We present a supervised image segmentation method that incorporates spatial information to locate boundaries between regions with overlapping intensity histograms. The segmentation of a pixel is determined by comparing its intensity to distributions from local, nearby pixel intensities. Because of the statistical nature of the algorithm, we use maximum likelihood estimation theory to quantify uncertainty about each boundary. We demonstrate the success of this algorithm on a radiograph of a multicomponent cylinder and on an optical image of a laser-induced shockwave, and we provide final boundary locations with associated bands of uncertainty.",0
"The field of quantitative digital image analysis is experiencing rapid growth, allowing researchers to make precise measurements about objects in images. In order to calculate quantities such as the density of compressed materials or the velocity of a shockwave, it is necessary to determine object boundaries. Images that exhibit regions with a spatial intensity trend are of particular interest. To address this, we propose a supervised image segmentation method that incorporates spatial information to locate boundaries between regions that have overlapping intensity histograms. The segmentation process involves comparing the intensity of a pixel to distributions from local, nearby pixel intensities. Because the algorithm is statistical in nature, we utilize maximum likelihood estimation theory to quantify uncertainty about each boundary. We demonstrate the effectiveness of this algorithm on a radiograph of a multicomponent cylinder and an optical image of a laser-induced shockwave, providing final boundary locations along with associated bands of uncertainty.",1
"This paper presents GridNet, a new Convolutional Neural Network (CNN) architecture for semantic image segmentation (full scene labelling). Classical neural networks are implemented as one stream from the input to the output with subsampling operators applied in the stream in order to reduce the feature maps size and to increase the receptive field for the final prediction. However, for semantic image segmentation, where the task consists in providing a semantic class to each pixel of an image, feature maps reduction is harmful because it leads to a resolution loss in the output prediction. To tackle this problem, our GridNet follows a grid pattern allowing multiple interconnected streams to work at different resolutions. We show that our network generalizes many well known networks such as conv-deconv, residual or U-Net networks. GridNet is trained from scratch and achieves competitive results on the Cityscapes dataset.",0
"The article introduces GridNet, which is a new architecture of Convolutional Neural Network (CNN) designed specifically for semantic image segmentation, or full scene labeling. While traditional neural networks use a single stream from input to output with subsampling operators, this approach can reduce feature maps and cause a loss of resolution in output prediction for semantic image segmentation. To address this challenge, GridNet uses a grid pattern, enabling interconnected streams to work at different resolutions. The article demonstrates that GridNet offers a generalization of well-known networks, such as conv-deconv, residual, and U-Net networks. GridNet is trained from scratch and produces competitive results on the Cityscapes dataset.",1
"Deep learning usually requires large amounts of labeled training data, but annotating data is costly and tedious. The framework of semi-supervised learning provides the means to use both labeled data and arbitrary amounts of unlabeled data for training. Recently, semi-supervised deep learning has been intensively studied for standard CNN architectures. However, Fully Convolutional Networks (FCNs) set the state-of-the-art for many image segmentation tasks. To the best of our knowledge, there is no existing semi-supervised learning method for such FCNs yet. We lift the concept of auxiliary manifold embedding for semi-supervised learning to FCNs with the help of Random Feature Embedding. In our experiments on the challenging task of MS Lesion Segmentation, we leverage the proposed framework for the purpose of domain adaptation and report substantial improvements over the baseline model.",0
"Typically, deep learning requires a large amount of labeled training data, which is expensive and laborious to annotate. Semi-supervised learning offers a solution by utilizing both labeled and unlabeled data for training. While semi-supervised deep learning has been thoroughly researched for standard CNN architectures, Fully Convolutional Networks (FCNs) are currently the most advanced for image segmentation tasks. Unfortunately, there is currently no existing semi-supervised learning method for FCNs. Our approach employs Random Feature Embedding to apply the concept of auxiliary manifold embedding for semi-supervised learning to FCNs. In our experiments, we utilize this framework for domain adaptation in the challenging task of MS Lesion Segmentation, resulting in significant improvements over the baseline model.",1
"Convolutional neural networks have been applied to a wide variety of computer vision tasks. Recent advances in semantic segmentation have enabled their application to medical image segmentation. While most CNNs use two-dimensional kernels, recent CNN-based publications on medical image segmentation featured three-dimensional kernels, allowing full access to the three-dimensional structure of medical images. Though closely related to semantic segmentation, medical image segmentation includes specific challenges that need to be addressed, such as the scarcity of labelled data, the high class imbalance found in the ground truth and the high memory demand of three-dimensional images. In this work, a CNN-based method with three-dimensional filters is demonstrated and applied to hand and brain MRI. Two modifications to an existing CNN architecture are discussed, along with methods on addressing the aforementioned challenges. While most of the existing literature on medical image segmentation focuses on soft tissue and the major organs, this work is validated on data both from the central nervous system as well as the bones of the hand.",0
"Convolutional neural networks have been utilized in a broad range of computer vision tasks and have recently been employed in medical image segmentation thanks to advancements in semantic segmentation. In contrast to most CNNs that employ two-dimensional kernels, CNN-based studies on medical image segmentation have implemented three-dimensional kernels to fully capture the three-dimensional structure of medical images. Medical image segmentation presents unique challenges, including a scarcity of labelled data, a high class imbalance in the ground truth, and the high memory demand of three-dimensional images, which must be addressed. This study introduces a CNN-based approach that employs three-dimensional filters and is applied to MRI scans of the brain and hand. The paper discusses two modifications to an existing CNN architecture and outlines strategies for overcoming the aforementioned challenges. While the majority of prior research in medical image segmentation has focused on major organs and soft tissue, this study is validated on data from both the bones of the hand and the central nervous system.",1
"Delineation of line patterns in images is a basic step required in various applications such as blood vessel detection in medical images, segmentation of rivers or roads in aerial images, detection of cracks in walls or pavements, etc. In this paper we present trainable B-COSFIRE filters, which are a model of some neurons in area V1 of the primary visual cortex, and apply it to the delineation of line patterns in different kinds of images. B-COSFIRE filters are trainable as their selectivity is determined in an automatic configuration process given a prototype pattern of interest. They are configurable to detect any preferred line structure (e.g. segments, corners, cross-overs, etc.), so usable for automatic data representation learning. We carried out experiments on two data sets, namely a line-network data set from INRIA and a data set of retinal fundus images named IOSTAR. The results that we achieved confirm the robustness of the proposed approach and its effectiveness in the delineation of line structures in different kinds of images.",0
"Various applications require the basic step of delineating line patterns in images, such as medical images for blood vessel detection, aerial images for segmentation of rivers or roads, and detecting cracks in walls or pavements. This paper introduces trainable B-COSFIRE filters, which model neurons in area V1 of the primary visual cortex, and applies them to detect line patterns in various types of images. The filters are trainable and their selectivity is determined through an automatic configuration process based on a prototype pattern of interest. These filters can detect any preferred line structure, making them useful for automatic data representation learning. The proposed approach was tested on two data sets, INRIA's line-network data set and the IOSTAR data set of retinal fundus images, and achieved robust and effective results in delineating line structures in different types of images.",1
"There is a vast body of theoretical research on lifted inference in probabilistic graphical models (PGMs). However, few demonstrations exist where lifting is applied in conjunction with top of the line applied algorithms. We pursue the applicability of lifted inference for computer vision (CV), with the insight that a globally optimal (MAP) labeling will likely have the same label for two symmetric pixels. The success of our approach lies in efficiently handling a distinct unary potential on every node (pixel), typical of CV applications. This allows us to lift the large class of algorithms that model a CV problem via PGM inference. We propose a generic template for coarse-to-fine (C2F) inference in CV, which progressively refines an initial coarsely lifted PGM for varying quality-time trade-offs. We demonstrate the performance of C2F inference by developing lifted versions of two near state-of-the-art CV algorithms for stereo vision and interactive image segmentation. We find that, against flat algorithms, the lifted versions have a much superior anytime performance, without any loss in final solution quality.",0
"Despite a wealth of theoretical research on lifted inference in probabilistic graphical models (PGMs), there are few instances where lifting has been utilized in conjunction with cutting-edge applied algorithms. Our goal is to explore the practicality of lifted inference for computer vision (CV) by recognizing that symmetric pixels are likely to have the same globally optimal (MAP) labeling. The key to our success lies in efficiently managing a distinct unary potential on every node (pixel), a common feature of CV applications, which allows us to lift a broad range of algorithms that model CV problems through PGM inference. We introduce a general template for coarse-to-fine (C2F) inference in CV that progressively improves an initial coarsely lifted PGM for different quality-time trade-offs. To demonstrate C2F inference's effectiveness, we developed lifted versions of two nearly state-of-the-art CV algorithms for stereo vision and interactive image segmentation. We discovered that, compared to flat algorithms, the lifted versions had significantly improved anytime performance without sacrificing final solution quality.",1
"This study deals with semantic segmentation of high-resolution (aerial) images where a semantic class label is assigned to each pixel via supervised classification as a basis for automatic map generation. Recently, deep convolutional neural networks (CNNs) have shown impressive performance and have quickly become the de-facto standard for semantic segmentation, with the added benefit that task-specific feature design is no longer necessary. However, a major downside of deep learning methods is that they are extremely data-hungry, thus aggravating the perennial bottleneck of supervised classification, to obtain enough annotated training data. On the other hand, it has been observed that they are rather robust against noise in the training labels. This opens up the intriguing possibility to avoid annotating huge amounts of training data, and instead train the classifier from existing legacy data or crowd-sourced maps which can exhibit high levels of noise. The question addressed in this paper is: can training with large-scale, publicly available labels replace a substantial part of the manual labeling effort and still achieve sufficient performance? Such data will inevitably contain a significant portion of errors, but in return virtually unlimited quantities of it are available in larger parts of the world. We adapt a state-of-the-art CNN architecture for semantic segmentation of buildings and roads in aerial images, and compare its performance when using different training data sets, ranging from manually labeled, pixel-accurate ground truth of the same city to automatic training data derived from OpenStreetMap data from distant locations. We report our results that indicate that satisfying performance can be obtained with significantly less manual annotation effort, by exploiting noisy large-scale training data.",0
"This study focuses on semantic segmentation of high-resolution images, assigning a semantic class label to each pixel through supervised classification to generate maps automatically. Convolutional neural networks (CNNs) have become the standard for semantic segmentation due to their impressive performance and elimination of task-specific feature design. However, deep learning methods require significant amounts of annotated training data, creating a bottleneck for supervised classification. Nonetheless, they are robust against noise in training labels, allowing for the possibility of avoiding annotating large amounts of data in favor of training the classifier with existing data or crowd-sourced maps with high noise levels. This paper explores the ability of training with publicly available labels to replace manual labeling efforts while achieving sufficient performance. Although such data may contain errors, it is available in larger parts of the world in virtually unlimited quantities. The study adapts a CNN architecture for semantic segmentation of buildings and roads in aerial images and compares its performance using different training data sets. The results suggest that satisfactory performance can be achieved with less manual annotation effort by utilizing noisy large-scale training data.",1
"Calcium imaging is a technique for observing neuron activity as a series of images showing indicator fluorescence over time. Manually segmenting neurons is time-consuming, leading to research on automated calcium imaging segmentation (ACIS). We evaluated several deep learning models for ACIS on the Neurofinder competition datasets and report our best model: U-Net2DS, a fully convolutional network that operates on 2D mean summary images. U-Net2DS requires minimal domain-specific pre/post-processing and parameter adjustment, and predictions are made on full $512\times512$ images at $\approx$9K images per minute. It ranks third in the Neurofinder competition ($F_1=0.569$) and is the best model to exclusively use deep learning. We also demonstrate useful segmentations on data from outside the competition. The model's simplicity, speed, and quality results make it a practical choice for ACIS and a strong baseline for more complex models in the future.",0
"The process of observing neuron activity through calcium imaging involves capturing a sequence of images that display fluorescent signals produced by an indicator over time. However, manually identifying and separating neurons in the images can be a time-consuming task. For this reason, researchers have explored automated calcium imaging segmentation (ACIS) techniques. Our study involved testing different deep learning models for ACIS using Neurofinder competition datasets. The most effective model we found was U-Net2DS, which is a fully convolutional network that operates on 2D mean summary images. U-Net2DS is simple to use, requires minimal domain-specific pre/post-processing and parameter adjustment, and can make predictions on full $512\times512$ images at approximately 9K images per minute. It ranked third in the Neurofinder competition with an $F_1$ value of 0.569 and is the best model that uses only deep learning. We also applied this model to data outside the competition and obtained useful segmentations. Given its simplicity, speed, and quality results, U-Net2DS is a practical option for ACIS and can serve as a strong baseline for more complex models in the future.",1
"The pairwise influence matrix of Dobrushin has long been used as an analytical tool to bound the rate of convergence of Gibbs sampling. In this work, we use Dobrushin influence as the basis of a practical tool to certify and efficiently improve the quality of a discrete Gibbs sampler. Our Dobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection orders for a given sampling budget and variable subset of interest, explicit bounds on total variation distance to stationarity, and certifiable improvements over the standard systematic and uniform random scan Gibbs samplers. In our experiments with joint image segmentation and object recognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising model inference, DoGS consistently deliver higher-quality inferences with significantly smaller sampling budgets than standard Gibbs samplers.",0
"For a long time, the Dobrushin pairwise influence matrix has been utilized as an analytical tool for bounding the rate of convergence of Gibbs sampling. In this study, we have employed the Dobrushin influence as the foundation of an effective instrument to verify and enhance the quality of a discrete Gibbs sampler in a practical manner. Our approach, known as Dobrushin-optimized Gibbs samplers (DoGS), provides tailor-made variable selection orders for a specific sampling budget and variable subset of interest, explicit limits on the total variation distance to stationarity, and verifiable enhancements over standard systematic and uniform random scan Gibbs samplers. In our experiments on joint image segmentation and object recognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising model inference, DoGS consistently deliver superior inferences with much smaller sampling budgets than standard Gibbs samplers.",1
"Inspired by classic generative adversarial networks (GAN), we propose a novel end-to-end adversarial neural network, called SegAN, for the task of medical image segmentation. Since image segmentation requires dense, pixel-level labeling, the single scalar real/fake output of a classic GAN's discriminator may be ineffective in producing stable and sufficient gradient feedback to the networks. Instead, we use a fully convolutional neural network as the segmentor to generate segmentation label maps, and propose a novel adversarial critic network with a multi-scale $L_1$ loss function to force the critic and segmentor to learn both global and local features that capture long- and short-range spatial relationships between pixels. In our SegAN framework, the segmentor and critic networks are trained in an alternating fashion in a min-max game: The critic takes as input a pair of images, (original_image $*$ predicted_label_map, original_image $*$ ground_truth_label_map), and then is trained by maximizing a multi-scale loss function; The segmentor is trained with only gradients passed along by the critic, with the aim to minimize the multi-scale loss function. We show that such a SegAN framework is more effective and stable for the segmentation task, and it leads to better performance than the state-of-the-art U-net segmentation method. We tested our SegAN method using datasets from the MICCAI BRATS brain tumor segmentation challenge. Extensive experimental results demonstrate the effectiveness of the proposed SegAN with multi-scale loss: on BRATS 2013 SegAN gives performance comparable to the state-of-the-art for whole tumor and tumor core segmentation while achieves better precision and sensitivity for Gd-enhance tumor core segmentation; on BRATS 2015 SegAN achieves better performance than the state-of-the-art in both dice score and precision.",0
"Our innovative SegAN neural network, created for medical image segmentation, draws inspiration from traditional generative adversarial networks (GAN). In contrast to GANs, where the single scalar output of the discriminator may not be effective for pixel-level labeling, we utilize a fully convolutional neural network as the segmentor to generate segmentation label maps. To capture both global and local features that account for spatial relationships between pixels, we propose a novel adversarial critic network with a multi-scale $L_1$ loss function. In our SegAN framework, the critic and segmentor networks are trained in an alternating fashion in a min-max game, where the critic is trained by maximizing the multi-scale loss function, while the segmentor is trained with only gradients passed along by the critic, aiming to minimize the same loss function. Our SegAN approach proves more effective and stable than the U-net segmentation method, as evidenced by extensive experimental results on the MICCAI BRATS brain tumor segmentation challenge dataset. SegAN's multi-scale loss function yields comparable performance to state-of-the-art methods for whole tumor and tumor core segmentation, and better precision and sensitivity for Gd-enhance tumor core segmentation. Furthermore, on BRATS 2015, SegAN outperforms the state-of-the-art in both dice score and precision.",1
"Image segmentation has come a long way since the early days of computer vision, and still remains a challenging task. Modern variations of the classical (purely bottom-up) approach, involve, e.g., some form of user assistance (interactive segmentation) or ask for the simultaneous segmentation of two or more images (co-segmentation). At an abstract level, all these variants can be thought of as ""constrained"" versions of the original formulation, whereby the segmentation process is guided by some external source of information. In this paper, we propose a new approach to tackle this kind of problems in a unified way. Our work is based on some properties of a family of quadratic optimization problems related to dominant sets, a well-known graph-theoretic notion of a cluster which generalizes the concept of a maximal clique to edge-weighted graphs. In particular, we show that by properly controlling a regularization parameter which determines the structure and the scale of the underlying problem, we are in a position to extract groups of dominant-set clusters that are constrained to contain predefined elements. In particular, we shall focus on interactive segmentation and co-segmentation (in both the unsupervised and the interactive versions). The proposed algorithm can deal naturally with several type of constraints and input modality, including scribbles, sloppy contours, and bounding boxes, and is able to robustly handle noisy annotations on the part of the user. Experiments on standard benchmark datasets show the effectiveness of our approach as compared to state-of-the-art algorithms on a variety of natural images under several input conditions and constraints.",0
"Despite advancements in image segmentation within computer vision, it still poses a challenge. Current variations of the traditional bottom-up approach often require some form of user assistance or segmentation of multiple images simultaneously. These variants can be considered constrained versions of the original formulation, where external information guides the segmentation process. This paper presents a new approach to address these problems in a unified manner. The approach relies on properties of quadratic optimization problems related to dominant sets, a graph-theoretic concept of a cluster. By controlling a regularization parameter, the algorithm extracts dominant-set clusters that contain predefined elements. The approach focuses on interactive and co-segmentation, handling various types of constraints and input modalities, and handling noisy annotations. Experiments on benchmark datasets demonstrate the effectiveness of the approach compared to state-of-the-art algorithms under different input conditions and constraints.",1
"Subspace learning is an important problem, which has many applications in image and video processing. It can be used to find a low-dimensional representation of signals and images. But in many applications, the desired signal is heavily distorted by outliers and noise, which negatively affect the learned subspace. In this work, we present a novel algorithm for learning a subspace for signal representation, in the presence of structured outliers and noise. The proposed algorithm tries to jointly detect the outliers and learn the subspace for images. We present an alternating optimization algorithm for solving this problem, which iterates between learning the subspace and finding the outliers. This algorithm has been trained on a large number of image patches, and the learned subspace is used for image segmentation, and is shown to achieve better segmentation results than prior methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu, and shape primitive extraction and coding algorithm.",0
"The problem of subspace learning is significant in image and video processing, allowing for a low-dimensional representation of signals and images. However, outliers and noise can distort the desired signal, negatively impacting the learned subspace. This study presents a new algorithm for subspace learning in the presence of structured outliers and noise. The algorithm detects outliers and learns the subspace for images, utilizing an alternating optimization approach that alternates between learning the subspace and finding outliers. The algorithm is trained on numerous image patches and used for image segmentation, producing superior results compared to prior methods such as least absolute deviation fitting, k-means clustering based segmentation in DjVu, and shape primitive extraction and coding algorithm.",1
"Convolutional neural networks (CNNs) have been applied to various automatic image segmentation tasks in medical image analysis, including brain MRI segmentation. Generative adversarial networks have recently gained popularity because of their power in generating images that are difficult to distinguish from real images.   In this study we use an adversarial training approach to improve CNN-based brain MRI segmentation. To this end, we include an additional loss function that motivates the network to generate segmentations that are difficult to distinguish from manual segmentations. During training, this loss function is optimised together with the conventional average per-voxel cross entropy loss.   The results show improved segmentation performance using this adversarial training procedure for segmentation of two different sets of images and using two different network architectures, both visually and in terms of Dice coefficients.",0
"Various automatic image segmentation tasks in medical image analysis, such as brain MRI segmentation, have utilized convolutional neural networks (CNNs). Recently, generative adversarial networks have gained popularity due to their ability to create images that closely resemble real images. In this study, a new approach was implemented by incorporating an additional loss function in the CNN-based brain MRI segmentation process. This loss function motivates the network to generate segmentations that are difficult to differentiate from manual segmentations, and is optimized alongside the conventional average per-voxel cross entropy loss. The results demonstrate that this adversarial training approach improves the segmentation performance of two different sets of images and two different network architectures, both in terms of Dice coefficients and visual quality.",1
"The technology of image segmentation is widely used in medical image processing, face recognition pedestrian detection, etc. The current image segmentation techniques include region-based segmentation, edge detection segmentation, segmentation based on clustering, segmentation based on weakly-supervised learning in CNN, etc. This paper analyzes and summarizes these algorithms of image segmentation, and compares the advantages and disadvantages of different algorithms. Finally, we make a prediction of the development trend of image segmentation with the combination of these algorithms.",0
"The use of image segmentation technology is prevalent in various fields such as medical image processing, pedestrian detection, and face recognition. Image segmentation techniques currently in use include region-based segmentation, edge detection segmentation, clustering-based segmentation, and weakly-supervised learning segmentation in CNN. This paper evaluates and summarizes these algorithms, comparing their strengths and weaknesses. In conclusion, we predict the future development of image segmentation through the integration of these algorithms.",1
"Deep convolutional neural networks are powerful tools for learning visual representations from images. However, designing efficient deep architectures to analyse volumetric medical images remains challenging. This work investigates efficient and flexible elements of modern convolutional networks such as dilated convolution and residual connection. With these essential building blocks, we propose a high-resolution, compact convolutional network for volumetric image segmentation. To illustrate its efficiency of learning 3D representation from large-scale image data, the proposed network is validated with the challenging task of parcellating 155 neuroanatomical structures from brain MR images. Our experiments show that the proposed network architecture compares favourably with state-of-the-art volumetric segmentation networks while being an order of magnitude more compact. We consider the brain parcellation task as a pretext task for volumetric image segmentation; our trained network potentially provides a good starting point for transfer learning. Additionally, we show the feasibility of voxel-level uncertainty estimation using a sampling approximation through dropout.",0
"Learning visual representations from images is a powerful application of deep convolutional neural networks. However, designing deep architectures that can efficiently analyze volumetric medical images is still challenging. To address this issue, this study explores the use of efficient and flexible elements in modern convolutional networks, such as dilated convolution and residual connection. Using these building blocks, we propose a compact convolutional network that can achieve high resolution volumetric image segmentation. We demonstrate the effectiveness of this network by successfully parcellating 155 neuroanatomical structures from brain MR images. Our experiments indicate that our proposed architecture is more compact than state-of-the-art volumetric segmentation networks while still achieving comparable results. Furthermore, we consider the brain parcellation task as a pretext task for volumetric image segmentation, and our trained network can serve as a starting point for transfer learning. Finally, we show that it is possible to estimate voxel-level uncertainty using a sampling approximation through dropout.",1
"This paper presents a robust regression approach for image binarization under significant background variations and observation noises. The work is motivated by the need of identifying foreground regions in noisy microscopic image or degraded document images, where significant background variation and severe noise make an image binarization challenging. The proposed method first estimates the background of an input image, subtracts the estimated background from the input image, and apply a global thresholding to the subtracted outcome for achieving a binary image of foregrounds. A robust regression approach was proposed to estimate the background intensity surface with minimal effects of foreground intensities and noises, and a global threshold selector was proposed on the basis of a model selection criterion in a sparse regression. The proposed approach was validated using 26 test images and the corresponding ground truths, and the outcomes of the proposed work were compared with those from nine existing image binarization methods. The approach was also combined with three state-of-the-art morphological segmentation methods to show how the proposed approach can improve their image segmentation outcomes.",0
"In this paper, a new method for image binarization is presented that can effectively handle significant background variations and observation noises. The motivation behind this work is to accurately identify foreground regions in challenging images such as noisy microscopic or degraded document images. The proposed method involves estimating the background of the input image, subtracting it from the input image, and applying global thresholding to achieve a binary image of foregrounds. To estimate the background intensity surface, a robust regression approach is proposed that is minimally affected by foreground intensities and noises. A global threshold selector is also proposed using a model selection criterion in a sparse regression. The effectiveness of the proposed approach is evaluated using 26 test images and compared against nine existing image binarization methods. Additionally, the proposed approach is combined with three state-of-the-art morphological segmentation methods to demonstrate its effectiveness in improving image segmentation outcomes.",1
"This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions.",0
"The proposal of this paper is a deep learning structure that utilizes Residual Network and has the capability to adapt the number of layers executed based on different image regions. This architecture is trainable from start to finish, consistent, and can be applied without alterations to various computer vision problems such as image classification, object detection, and image segmentation. Through experimental results, the model has shown to enhance the computational efficiency of Residual Networks in challenging datasets like ImageNet classification and COCO object detection. Furthermore, we assessed the computation time maps on the cat2000 visual saliency dataset and discovered a surprising correlation with human eye fixation positions.",1
"Dirichlet processes (DP) are widely applied in Bayesian nonparametric modeling. However, in their basic form they do not directly integrate dependency information among data arising from space and time. In this paper, we propose location dependent Dirichlet processes (LDDP) which incorporate nonparametric Gaussian processes in the DP modeling framework to model such dependencies. We develop the LDDP in the context of mixture modeling, and develop a mean field variational inference algorithm for this mixture model. The effectiveness of the proposed modeling framework is shown on an image segmentation task.",0
"The application of Dirichlet processes (DP) in Bayesian nonparametric modeling is widespread. However, their fundamental form does not account for the integration of dependency information among data that is associated with space and time. To address this issue, we introduce location dependent Dirichlet processes (LDDP), which utilize nonparametric Gaussian processes within the DP modeling framework to capture such dependencies. Our focus is on developing the LDDP within a mixture modeling context, and we present a mean field variational inference algorithm for this mixture model. The effectiveness of this modeling framework is demonstrated through an image segmentation task.",1
"Training deep fully convolutional neural networks (F-CNNs) for semantic image segmentation requires access to abundant labeled data. While large datasets of unlabeled image data are available in medical applications, access to manually labeled data is very limited. We propose to automatically create auxiliary labels on initially unlabeled data with existing tools and to use them for pre-training. For the subsequent fine-tuning of the network with manually labeled data, we introduce error corrective boosting (ECB), which emphasizes parameter updates on classes with lower accuracy. Furthermore, we introduce SkipDeconv-Net (SD-Net), a new F-CNN architecture for brain segmentation that combines skip connections with the unpooling strategy for upsampling. The SD-Net addresses challenges of severe class imbalance and errors along boundaries. With application to whole-brain MRI T1 scan segmentation, we generate auxiliary labels on a large dataset with FreeSurfer and fine-tune on two datasets with manual annotations. Our results show that the inclusion of auxiliary labels and ECB yields significant improvements. SD-Net segments a 3D scan in 7 secs in comparison to 30 hours for the closest multi-atlas segmentation method, while reaching similar performance. It also outperforms the latest state-of-the-art F-CNN models.",0
"To train F-CNNs for semantic image segmentation, abundant labeled data is needed, but manual labeling is limited in medical applications despite the availability of large unlabeled datasets. Our proposed solution is to automatically create auxiliary labels on initially unlabeled data using existing tools and use them for pre-training. For fine-tuning, we introduce ECB, which prioritizes parameter updates on classes with lower accuracy. We also present SD-Net, a new F-CNN architecture for brain segmentation that addresses challenges of class imbalance and boundary errors by combining skip connections with the unpooling strategy for upsampling. Using whole-brain MRI T1 scans, we generate auxiliary labels with FreeSurfer and fine-tune on two datasets with manual annotations, resulting in significant improvements. SD-Net segments a 3D scan in 7 secs, outperforming the closest multi-atlas segmentation method and the latest state-of-the-art F-CNN models.",1
"Video-based eye tracking is a valuable technique in various research fields. Numerous open-source eye tracking algorithms have been developed in recent years, primarily designed for general application with many different camera types. These algorithms do not, however, capitalize on the high frame rate of eye tracking cameras often employed in psychophysical studies. We present a pupil detection method that utilizes this high-speed property to obtain reliable predictions through recursive estimation about certain pupil characteristics in successive camera frames. These predictions are subsequently used to carry out novel image segmentation and classification routines to improve pupil detection performance. Based on results from hand-labelled eye images, our approach was found to have a greater detection rate, accuracy and speed compared to other recently published open-source pupil detection algorithms. The program's source code, together with a graphical user interface, can be downloaded at https://github.com/tbrouns/eyestalker",0
"The use of video-based eye tracking is a valuable technique in diverse research fields. Various open-source eye tracking algorithms have been developed in recent years, which are designed for general application with different camera types. However, these algorithms do not take advantage of the high frame rate of eye tracking cameras, often used in psychophysical studies. We have presented a pupil detection method that utilizes this high-speed feature to obtain reliable predictions about certain pupil characteristics in successive camera frames through recursive estimation. We then use these predictions to conduct novel image segmentation and classification routines, which enhance pupil detection performance. Based on results from hand-labelled eye images, our approach has exhibited a higher detection rate, accuracy, and speed than other recently published open-source pupil detection algorithms. The program's source code, along with a graphical user interface, can be downloaded at https://github.com/tbrouns/eyestalker.",1
"Fully convolutional deep neural networks carry out excellent potential for fast and accurate image segmentation. One of the main challenges in training these networks is data imbalance, which is particularly problematic in medical imaging applications such as lesion segmentation where the number of lesion voxels is often much lower than the number of non-lesion voxels. Training with unbalanced data can lead to predictions that are severely biased towards high precision but low recall (sensitivity), which is undesired especially in medical applications where false negatives are much less tolerable than false positives. Several methods have been proposed to deal with this problem including balanced sampling, two step training, sample re-weighting, and similarity loss functions. In this paper, we propose a generalized loss function based on the Tversky index to address the issue of data imbalance and achieve much better trade-off between precision and recall in training 3D fully convolutional deep neural networks. Experimental results in multiple sclerosis lesion segmentation on magnetic resonance images show improved F2 score, Dice coefficient, and the area under the precision-recall curve in test data. Based on these results we suggest Tversky loss function as a generalized framework to effectively train deep neural networks.",0
"Fast and accurate image segmentation can be achieved through fully convolutional deep neural networks. However, data imbalance poses a significant challenge in training these networks, especially in medical imaging applications such as lesion segmentation where the number of non-lesion voxels is much higher than the number of lesion voxels. Training with unbalanced data can lead to biased predictions with high precision but low sensitivity, which is unacceptable in medical applications where false negatives are less tolerable than false positives. To address this issue, various methods have been proposed, including balanced sampling, two-step training, sample re-weighting, and similarity loss functions. In this study, we introduce a generalized loss function based on the Tversky index, which achieves a better balance between precision and recall in training 3D fully convolutional deep neural networks. Our experimental results in multiple sclerosis lesion segmentation on magnetic resonance images demonstrate improved F2 score, Dice coefficient, and area under the precision-recall curve in test data. Therefore, we suggest the Tversky loss function as a generalized framework for effectively training deep neural networks.",1
"Image segmentation is a fundamental problem in biomedical image analysis. Recent advances in deep learning have achieved promising results on many biomedical image segmentation benchmarks. However, due to large variations in biomedical images (different modalities, image settings, objects, noise, etc), to utilize deep learning on a new application, it usually needs a new set of training data. This can incur a great deal of annotation effort and cost, because only biomedical experts can annotate effectively, and often there are too many instances in images (e.g., cells) to annotate. In this paper, we aim to address the following question: With limited effort (e.g., time) for annotation, what instances should be annotated in order to attain the best performance? We present a deep active learning framework that combines fully convolutional network (FCN) and active learning to significantly reduce annotation effort by making judicious suggestions on the most effective annotation areas. We utilize uncertainty and similarity information provided by FCN and formulate a generalized version of the maximum set cover problem to determine the most representative and uncertain areas for annotation. Extensive experiments using the 2015 MICCAI Gland Challenge dataset and a lymph node ultrasound image segmentation dataset show that, using annotation suggestions by our method, state-of-the-art segmentation performance can be achieved by using only 50% of training data.",0
"Biomedical image analysis involves the critical issue of image segmentation. Deep learning has made significant progress in this regard, yielding impressive results on various biomedical image segmentation benchmarks. However, the diverse range of biomedical images, including various modalities, image settings, objects, and noise, requires a new training dataset to use deep learning for a new application. This incurs a considerable amount of annotation effort and cost as only experts can annotate effectively, and there are often many instances in images (such as cells) to annotate. This paper aims to answer the question of how to achieve the best performance with limited annotation effort. The authors propose a deep active learning framework that combines fully convolutional network (FCN) and active learning to suggest the most effective annotation areas, utilizing the uncertainty and similarity information provided by FCN. They formulate a generalized version of the maximum set cover problem to identify the most representative and uncertain areas for annotation. Experiments using the 2015 MICCAI Gland Challenge dataset and a lymph node ultrasound image segmentation dataset show that state-of-the-art segmentation performance can be achieved using only 50% of the training data with annotation suggestions by this method.",1
"Background: Cardiac MRI derived biventricular mass and function parameters, such as end-systolic volume (ESV), end-diastolic volume (EDV), ejection fraction (EF), stroke volume (SV), and ventricular mass (VM) are clinically well established. Image segmentation can be challenging and time-consuming, due to the complex anatomy of the human heart.   Objectives: This study introduces $\nu$-net (/nju:n$\varepsilon$t/) -- a deep learning approach allowing for fully-automated high quality segmentation of right (RV) and left ventricular (LV) endocardium and epicardium for extraction of cardiac function parameters.   Methods: A set consisting of 253 manually segmented cases has been used to train a deep neural network. Subsequently, the network has been evaluated on 4 different multicenter data sets with a total of over 1000 cases.   Results: For LV EF the intraclass correlation coefficient (ICC) is 98, 95, and 80 % (95 %), and for RV EF 96, and 87 % (80 %) on the respective data sets (human expert ICCs reported in parenthesis). The LV VM ICC is 95, and 94 % (84 %), and the RV VM ICC is 83, and 83 % (54 %). This study proposes a simple adjustment procedure, allowing for the adaptation to distinct segmentation philosophies. $\nu$-net exhibits state of-the-art performance in terms of dice coefficient.   Conclusions: Biventricular mass and function parameters can be determined reliably in high quality by applying a deep neural network for cardiac MRI segmentation, especially in the anatomically complex right ventricle. Adaption to individual segmentation styles by applying a simple adjustment procedure is viable, allowing for the processing of novel data without time-consuming additional training.",0
"The use of cardiac MRI to derive biventricular mass and function parameters, such as ESV, EDV, EF, SV, and VM, is a well-established clinical practice. However, image segmentation can be difficult and time-consuming due to the complex anatomy of the heart. In this study, a deep learning approach called $\nu$-net was introduced to allow for fully-automated, high-quality segmentation of the right and left ventricular endocardium and epicardium. The deep neural network was trained on a set of 253 manually segmented cases and evaluated on four different multicenter datasets with over 1000 cases. The results showed that $\nu$-net had state-of-the-art performance in terms of dice coefficient, and reliable determination of biventricular mass and function parameters was achieved, particularly in the anatomically complex right ventricle. Furthermore, a simple adjustment procedure was proposed to adapt to individual segmentation styles, allowing for the processing of novel data without additional training.",1
"Parkinson's disease (PD) is a degenerative condition of the nervous system, which manifests itself primarily as muscle stiffness, hypokinesia, bradykinesia, and tremor. In patients suffering from advanced stages of PD, Deep Brain Stimulation neurosurgery (DBS) is the best alternative to medical treatment, especially when they become tolerant to the drugs. This surgery produces a neuronal activity, a result from electrical stimulation, whose quantification is known as Volume of Tissue Activated (VTA). To locate correctly the VTA in the cerebral volume space, one should be aware exactly the location of the tip of the DBS electrodes, as well as their spatial projection.   In this paper, we automatically locate DBS electrodes using a threshold-based medical imaging segmentation methodology, determining the optimal value of this threshold adaptively. The proposed methodology allows the localization of DBS electrodes in Computed Tomography (CT) images, with high noise tolerance, using automatic threshold detection methods.",0
"Parkinson's disease is a neurodegenerative disorder that causes muscle stiffness, hypokinesia, bradykinesia, and tremors. Advanced stage PD patients may opt for Deep Brain Stimulation (DBS) surgery as a more effective alternative to medication when they become resistant to drugs. DBS surgery involves the electrical stimulation of neurons, which produces a Volume of Tissue Activated (VTA). To accurately locate the VTA in the brain, the precise spatial projection and location of the DBS electrodes must be known. This study proposes a threshold-based medical imaging segmentation methodology that automatically locates DBS electrodes in CT images with high noise tolerance by determining an optimal threshold value adaptively.",1
"Purpose: To improve kidney segmentation in clinical ultrasound (US) images, we develop a new graph cuts based method to segment kidney US images by integrating original image intensity information and texture feature maps extracted using Gabor filters. Methods: To handle large appearance variation within kidney images and improve computational efficiency, we build a graph of image pixels close to kidney boundary instead of building a graph of the whole image. To make the kidney segmentation robust to weak boundaries, we adopt localized regional information to measure similarity between image pixels for computing edge weights to build the graph of image pixels. The localized graph is dynamically updated and the GC based segmentation iteratively progresses until convergence. The proposed method has been evaluated and compared with state of the art image segmentation methods based on clinical kidney US images of 85 subjects. We randomly selected US images of 20 subjects as training data for tuning the parameters, and validated the methods based on US images of the remaining 65 subjects. The segmentation results have been quantitatively analyzed using 3 metrics, including Dice Index, Jaccard Index, and Mean Distance. Results: Experiment results demonstrated that the proposed method obtained segmentation results for bilateral kidneys of 65 subjects with average Dice index of 0.9581, Jaccard index of 0.9204, and Mean Distance of 1.7166, better than other methods under comparison (p<10-19, paired Wilcoxon rank sum tests). Conclusions: The proposed method achieved promising performance for segmenting kidneys in US images, better than segmentation methods that built on any single channel of image information. This method will facilitate extraction of kidney characteristics that may predict important clinical outcomes such progression chronic kidney disease.",0
"The objective of this study is to enhance kidney segmentation in clinical ultrasound images by developing a novel graph cuts-based technique that integrates original image intensity information and texture feature maps extracted using Gabor filters. To address the large appearance variation within kidney images and improve computational efficiency, a graph of image pixels close to the kidney boundary is built instead of the whole image. Localized regional information is adopted to measure similarity between image pixels to compute edge weights to construct the graph of image pixels, which is dynamically updated. The proposed method is evaluated and compared with other state-of-the-art image segmentation methods using clinical kidney ultrasound images of 85 subjects. The results indicate that the proposed method outperforms other methods based on three metrics, including Dice Index, Jaccard Index, and Mean Distance. The proposed method achieved promising performance for segmenting kidneys in ultrasound images, better than segmentation methods that built on any single channel of image information. This technique will facilitate the extraction of kidney characteristics that may predict important clinical outcomes such as the progression of chronic kidney disease.",1
"The key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this work, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained. We apply the proposed networks on natural and biological image segmentation tasks and show superior performance is achieved in comparison to baseline methods.",0
"Current deep learning methods for dense prediction involve applying a model to a regular patch centered on each pixel to make predictions. However, these methods are limited because the patch shapes are determined by the network architecture rather than learned from the data. To address this, we introduce the dense transformer networks, which can learn patch shapes and sizes from the data. These networks use an encoder-decoder architecture with a pair of dense transformer modules inserted into each path. Our work provides technical solutions for learning patch shapes and sizes from data and restoring spatial correspondence for dense prediction. The proposed modules are differentiable, allowing for training of the entire network. We evaluate the proposed networks on natural and biological image segmentation tasks and demonstrate superior performance compared to baseline methods.",1
"A major challenge in brain tumor treatment planning and quantitative evaluation is determination of the tumor extent. The noninvasive magnetic resonance imaging (MRI) technique has emerged as a front-line diagnostic tool for brain tumors without ionizing radiation. Manual segmentation of brain tumor extent from 3D MRI volumes is a very time-consuming task and the performance is highly relied on operator's experience. In this context, a reliable fully automatic segmentation method for the brain tumor segmentation is necessary for an efficient measurement of the tumor extent. In this study, we propose a fully automatic method for brain tumor segmentation, which is developed using U-Net based deep convolutional networks. Our method was evaluated on Multimodal Brain Tumor Image Segmentation (BRATS 2015) datasets, which contain 220 high-grade brain tumor and 54 low-grade tumor cases. Cross-validation has shown that our method can obtain promising segmentation efficiently.",0
"Determining the extent of a brain tumor is a significant obstacle in treatment planning and quantitative evaluation. Magnetic resonance imaging (MRI) is a noninvasive and radiation-free diagnostic tool commonly used in brain tumor diagnosis. However, manually segmenting the tumor extent from 3D MRI volumes is a time-consuming task that heavily relies on the operator's experience. To overcome this challenge, a reliable fully automatic segmentation method is necessary to efficiently measure the tumor extent. In this study, we propose a fully automatic segmentation method for brain tumors that uses U-Net based deep convolutional networks. We evaluated our method on Multimodal Brain Tumor Image Segmentation (BRATS 2015) datasets, which included 220 high-grade and 54 low-grade brain tumor cases. Our cross-validation results demonstrate that our method can achieve efficient and promising segmentation.",1
"Imaging the atmosphere using ground-based sky cameras is a popular approach to study various atmospheric phenomena. However, it usually focuses on the daytime. Nighttime sky/cloud images are darker and noisier, and thus harder to analyze. An accurate segmentation of sky/cloud images is already challenging because of the clouds' non-rigid structure and size, and the lower and less stable illumination of the night sky increases the difficulty. Nonetheless, nighttime cloud imaging is essential in certain applications, such as continuous weather analysis and satellite communication.   In this paper, we propose a superpixel-based method to segment nighttime sky/cloud images. We also release the first nighttime sky/cloud image segmentation database to the research community. The experimental results show the efficacy of our proposed algorithm for nighttime images.",0
"Using ground-based sky cameras to study various atmospheric phenomena is a popular method, but it is mostly focused on daytime imaging. Nighttime sky/cloud images pose a greater challenge due to their darker and noisier nature, making them more difficult to analyze. Additionally, the non-rigid structure and size of clouds, coupled with the lower and less stable illumination of the night sky, make accurate segmentation of nighttime cloud images even more challenging. However, nighttime cloud imaging is vital in certain scenarios, such as continuous weather analysis and satellite communication. This paper introduces a superpixel-based approach for segmenting nighttime sky/cloud images and provides the first nighttime sky/cloud image segmentation database for research purposes. The experimental results demonstrate the effectiveness of our proposed algorithm for nighttime images.",1
"Shape priors have been widely utilized in medical image segmentation to improve segmentation accuracy and robustness. A major way to encode such a prior shape model is to use a mesh representation, which is prone to causing self-intersection or mesh folding. Those problems require complex and expensive algorithms to mitigate. In this paper, we propose a novel shape prior directly embedded in the voxel grid space, based on gradient vector flows of a pre-segmentation. The flexible and powerful prior shape representation is ready to be extended to simultaneously segmenting multiple interacting objects with minimum separation distance constraint. The problem is formulated as a Markov random field problem whose exact solution can be efficiently computed with a single minimum s-t cut in an appropriately constructed graph. The proposed algorithm is validated on two multi-object segmentation applications: the brain tissue segmentation in MRI images, and the bladder/prostate segmentation in CT images. Both sets of experiments show superior or competitive performance of the proposed method to other state-of-the-art methods.",0
"Medical image segmentation often utilizes shape priors to improve the accuracy and robustness of the process. Mesh representations have traditionally been used to encode the prior shape model, but they are prone to issues such as self-intersection and mesh folding, which require complex and costly algorithms to fix. This paper presents a new approach, which embeds the shape prior directly into the voxel grid space through gradient vector flows of a pre-segmentation. This method allows for a flexible and powerful prior shape representation that can be extended to simultaneously segment multiple objects while adhering to minimum separation distance constraints. The problem is formulated as a Markov random field problem, and its solution can be efficiently computed with a single minimum s-t cut in a graph. The proposed algorithm is validated on two applications; brain tissue segmentation in MRI images, and bladder/prostate segmentation in CT images. Results show that the proposed method outperforms other state-of-the-art methods.",1
"Anatomical and biophysical modeling of left atrium (LA) and proximal pulmonary veins (PPVs) is important for clinical management of several cardiac diseases. Magnetic resonance imaging (MRI) allows qualitative assessment of LA and PPVs through visualization. However, there is a strong need for an advanced image segmentation method to be applied to cardiac MRI for quantitative analysis of LA and PPVs. In this study, we address this unmet clinical need by exploring a new deep learning-based segmentation strategy for quantification of LA and PPVs with high accuracy and heightened efficiency. Our approach is based on a multi-view convolutional neural network (CNN) with an adaptive fusion strategy and a new loss function that allows fast and more accurate convergence of the backpropagation based optimization. After training our network from scratch by using more than 60K 2D MRI images (slices), we have evaluated our segmentation strategy to the STACOM 2013 cardiac segmentation challenge benchmark. Qualitative and quantitative evaluations, obtained from the segmentation challenge, indicate that the proposed method achieved the state-of-the-art sensitivity (90%), specificity (99%), precision (94%), and efficiency levels (10 seconds in GPU, and 7.5 minutes in CPU).",0
"The clinical management of various cardiac diseases requires anatomical and biophysical modeling of the left atrium (LA) and proximal pulmonary veins (PPVs). Magnetic resonance imaging (MRI) can provide a qualitative assessment of LA and PPVs through visualization, but a more advanced image segmentation method is necessary for a quantitative analysis. To address this issue, we present a new deep learning-based segmentation strategy that utilizes a multi-view convolutional neural network (CNN) with an adaptive fusion strategy and a new loss function for faster and more accurate convergence. Our network was trained from scratch using over 60,000 2D MRI images (slices), and we evaluated our segmentation strategy to the STACOM 2013 cardiac segmentation challenge benchmark. Our method achieved state-of-the-art sensitivity (90%), specificity (99%), precision (94%), and efficiency levels (10 seconds in GPU and 7.5 minutes in CPU) as demonstrated through qualitative and quantitative evaluations obtained from the segmentation challenge.",1
"In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed ""DeepLab"" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.",0
"The objective of our study is to utilize Deep Learning for semantic image segmentation, and we have identified three key contributions that have proven to be practically beneficial. First, we have highlighted the effectiveness of atrous convolution, or convolution with upsampled filters, as a powerful tool in predicting dense images. This method allows us to control the resolution at which feature responses are computed within Deep Convolutional Neural Networks, and also enables us to incorporate larger context without increasing the number of parameters or computation. Second, we have introduced the atrous spatial pyramid pooling (ASPP) method, which allows for the robust segmentation of objects at multiple scales by probing an incoming convolutional feature layer with filters at varying sampling rates and effective fields-of-views. Third, we have improved the localization of object boundaries by combining techniques from DCNNs and probabilistic graphical models. Our proposed ""DeepLab"" system has achieved state-of-the-art performance in the PASCAL VOC-2012 semantic image segmentation task, with a 79.7% mIOU score in the test set, and has advanced results in three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. Additionally, we have made all of our code publicly available online.",1
"Most current semantic segmentation methods rely on fully convolutional networks (FCNs). However, their use of large receptive fields and many pooling layers cause low spatial resolution inside the deep layers. This leads to predictions with poor localization around the boundaries. Prior work has attempted to address this issue by post-processing predictions with CRFs or MRFs. But such models often fail to capture semantic relationships between objects, which causes spatially disjoint predictions. To overcome these problems, recent methods integrated CRFs or MRFs into an FCN framework. The downside of these new models is that they have much higher complexity than traditional FCNs, which renders training and testing more challenging.   In this work we introduce a simple, yet effective Convolutional Random Walk Network (RWN) that addresses the issues of poor boundary localization and spatially fragmented predictions with very little increase in model complexity. Our proposed RWN jointly optimizes the objectives of pixelwise affinity and semantic segmentation. It combines these two objectives via a novel random walk layer that enforces consistent spatial grouping in the deep layers of the network. Our RWN is implemented using standard convolution and matrix multiplication. This allows an easy integration into existing FCN frameworks and it enables end-to-end training of the whole network via standard back-propagation. Our implementation of RWN requires just $131$ additional parameters compared to the traditional FCNs, and yet it consistently produces an improvement over the FCNs on semantic segmentation and scene labeling.",0
"Current methods for semantic segmentation mainly rely on fully convolutional networks (FCNs), which use large receptive fields and many pooling layers. However, these features result in low spatial resolution within the deep layers, leading to poor localization around boundaries. Previous solutions, such as using CRFs or MRFs for post-processing, have failed to capture semantic relationships between objects and can result in disjoint predictions. To address these issues, recent methods have integrated CRFs or MRFs into FCN frameworks, but this increases model complexity and makes training and testing more challenging. To overcome these problems, we propose a simple yet effective solution called the Convolutional Random Walk Network (RWN), which optimizes pixelwise affinity and semantic segmentation objectives. The RWN includes a random walk layer that enforces consistent spatial grouping in the deep layers, using standard convolution and matrix multiplication for easy integration into existing FCN frameworks and end-to-end training via back-propagation. Our implementation of the RWN requires only 131 additional parameters compared to traditional FCNs, but consistently improves semantic segmentation and scene labeling results.",1
"State-of-the-art semantic image segmentation methods are mostly based on training deep convolutional neural networks (CNNs). In this work, we proffer to improve semantic segmentation with the use of contextual information. In particular, we explore `patch-patch' context and `patch-background' context in deep CNNs. We formulate deep structured models by combining CNNs and Conditional Random Fields (CRFs) for learning the patch-patch context between image regions. Specifically, we formulate CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied in order to avoid repeated expensive CRF inference during the course of back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image inputs and sliding pyramid pooling is very effective for improving performance. We perform comprehensive evaluation of the proposed method. We achieve new state-of-the-art performance on a number of challenging semantic segmentation datasets including $NYUDv2$, $PASCAL$-$VOC2012$, $Cityscapes$, $PASCAL$-$Context$, $SUN$-$RGBD$, $SIFT$-$flow$, and $KITTI$ datasets. Particularly, we report an intersection-over-union score of $77.8$ on the $PASCAL$-$VOC2012$ dataset.",0
"Current methods for semantic image segmentation rely heavily on the use of deep convolutional neural networks (CNNs). This study proposes a new approach to improve semantic segmentation by leveraging contextual information. Specifically, the study examines the impact of `patch-patch' and `patch-background' context on deep CNNs. The researchers develop deep structured models that combine CNNs with Conditional Random Fields (CRFs) to learn the patch-patch context between image regions. To capture patch-background context, the study employs a network design with traditional multi-scale image inputs and sliding pyramid pooling. The researchers conduct a comprehensive evaluation of the proposed method and achieve new state-of-the-art performance on multiple challenging semantic segmentation datasets, including $NYUDv2$, $PASCAL$-$VOC2012$, $Cityscapes$, $PASCAL$-$Context$, $SUN$-$RGBD$, $SIFT$-$flow$, and $KITTI$ datasets. Notably, the study reports an intersection-over-union score of $77.8$ on the $PASCAL$-$VOC2012$ dataset.",1
"We present Convolutional Oriented Boundaries (COB), which produces multiscale oriented contours and region hierarchies starting from generic image classification Convolutional Neural Networks (CNNs). COB is computationally efficient, because it requires a single CNN forward pass for multi-scale contour detection and it uses a novel sparse boundary representation for hierarchical segmentation; it gives a significant leap in performance over the state-of-the-art, and it generalizes very well to unseen categories and datasets. Particularly, we show that learning to estimate not only contour strength but also orientation provides more accurate results. We perform extensive experiments for low-level applications on BSDS, PASCAL Context, PASCAL Segmentation, and NYUD to evaluate boundary detection performance, showing that COB provides state-of-the-art contours and region hierarchies in all datasets. We also evaluate COB on high-level tasks when coupled with multiple pipelines for object proposals, semantic contours, semantic segmentation, and object detection on MS-COCO, SBD, and PASCAL; showing that COB also improves the results for all tasks.",0
"We introduce Convolutional Oriented Boundaries (COB), a method that generates multiscale oriented contours and region hierarchies using generic image classification Convolutional Neural Networks (CNNs). COB is highly efficient, as it only requires one CNN forward pass for multi-scale contour detection and uses a sparse boundary representation for hierarchical segmentation. It surpasses the state-of-the-art and is highly adaptable to new categories and datasets. Our research demonstrates that learning to estimate both contour strength and orientation produces more accurate outcomes. We conducted extensive experiments on BSDS, PASCAL Context, PASCAL Segmentation, and NYUD to assess boundary detection performance, proving that COB delivers state-of-the-art contours and region hierarchies across all datasets. We also evaluated COB in conjunction with multiple pipelines for object proposals, semantic contours, semantic segmentation, and object detection on MS-COCO, SBD, and PASCAL for high-level tasks, demonstrating that COB enhances results across all tasks.",1
"Color and intensity are two important components in an image. Usually, groups of image pixels, which are similar in color or intensity, are an informative representation for an object. They are therefore particularly suitable for computer vision tasks, such as saliency detection and object proposal generation. However, image pixels, which share a similar real-world color, may be quite different since colors are often distorted by intensity. In this paper, we reinvestigate the affinity matrices originally used in image segmentation methods based on spectral clustering. A new affinity matrix, which is robust to color distortions, is formulated for object discovery. Moreover, a Cohesion Measurement (CM) for object regions is also derived based on the formulated affinity matrix. Based on the new Cohesion Measurement, a novel object discovery method is proposed to discover objects latent in an image by utilizing the eigenvectors of the affinity matrix. Then we apply the proposed method to both saliency detection and object proposal generation. Experimental results on several evaluation benchmarks demonstrate that the proposed CM based method has achieved promising performance for these two tasks.",0
"In an image, color and intensity are significant components. When image pixels are similar in color or intensity, they are an informative representation of an object, making them suitable for computer vision tasks like saliency detection and object proposal generation. However, real-world colors can be distorted by intensity, resulting in differences among similar-color image pixels. In this paper, we revisit affinity matrices used in image segmentation methods based on spectral clustering and formulate a new, color-distortion-resistant affinity matrix for object discovery. Additionally, we derive a Cohesion Measurement (CM) for object regions based on the new affinity matrix. Using this CM, we propose a novel object discovery method that utilizes the eigenvectors of the affinity matrix to uncover hidden objects in an image. Finally, we apply the proposed method to saliency detection and object proposal generation and demonstrate through experimentation that the CM-based method performs well for both tasks on multiple evaluation benchmarks.",1
"We propose an effective framework for multi-phase image segmentation and semi-supervised data clustering by introducing a novel region force term into the Potts model. Assume the probability that a pixel or a data point belongs to each class is known a priori. We show that the corresponding indicator function obeys the Bernoulli distribution and the new region force function can be computed as the negative log-likelihood function under the Bernoulli distribution. We solve the Potts model by the primal-dual hybrid gradient method and the augmented Lagrangian method, which are based on two different dual problems of the same primal problem. Empirical evaluations of the Potts model with the new region force function on benchmark problems show that it is competitive with existing variational methods in both image segmentation and semi-supervised data clustering.",0
"Our proposal presents a robust framework for multi-phase image segmentation and semi-supervised data clustering. We achieve this by incorporating a fresh region force term into the Potts model. Assuming the probability distribution of each pixel or data point belonging to a given class is known beforehand, we demonstrate that the corresponding indicator function can be modeled using the Bernoulli distribution. We derive the new region force function as the negative log-likelihood function for the Bernoulli distribution. We utilize the primal-dual hybrid gradient method and the augmented Lagrangian method to solve the Potts model based on two different dual problems from the same primal problem. Our empirical evaluations of the Potts model with the new region force function on benchmark problems indicate that it is comparable to existing variational methods in both image segmentation and semi-supervised data clustering.",1
"We propose a novel superpixel-based multi-view convolutional neural network for semantic image segmentation. The proposed network produces a high quality segmentation of a single image by leveraging information from additional views of the same scene. Particularly in indoor videos such as captured by robotic platforms or handheld and bodyworn RGBD cameras, nearby video frames provide diverse viewpoints and additional context of objects and scenes. To leverage such information, we first compute region correspondences by optical flow and image boundary-based superpixels. Given these region correspondences, we propose a novel spatio-temporal pooling layer to aggregate information over space and time. We evaluate our approach on the NYU--Depth--V2 and the SUN3D datasets and compare it to various state-of-the-art single-view and multi-view approaches. Besides a general improvement over the state-of-the-art, we also show the benefits of making use of unlabeled frames during training for multi-view as well as single-view prediction.",0
"Our proposal is a new convolutional neural network that uses superpixels to achieve semantic image segmentation. By incorporating information from multiple views of the same scene, our network produces high-quality segmentation results for individual images. This is particularly useful in indoor videos, where different viewpoints and contexts can be captured by robotic platforms or handheld and bodyworn RGBD cameras. To utilize this information, we use optical flow and image boundary-based superpixels to establish region correspondences. We also introduce a novel spatio-temporal pooling layer to combine information across space and time. Our approach is evaluated on the NYU-Depth-V2 and SUN3D datasets and compared to existing single-view and multi-view approaches. In addition to outperforming the state-of-the-art, we demonstrate the advantages of using unlabeled frames during training for both multi-view and single-view prediction.",1
"Healthcare sector is totally different from other industry. It is on high priority sector and people expect highest level of care and services regardless of cost. It did not achieve social expectation even though it consume huge percentage of budget. Mostly the interpretations of medical data is being done by medical expert. In terms of image interpretation by human expert, it is quite limited due to its subjectivity, the complexity of the image, extensive variations exist across different interpreters, and fatigue. After the success of deep learning in other real world application, it is also providing exciting solutions with good accuracy for medical imaging and is seen as a key method for future applications in health secotr. In this chapter, we discussed state of the art deep learning architecture and its optimization used for medical image segmentation and classification. In the last section, we have discussed the challenges deep learning based methods for medical imaging and open research issue.",0
"The healthcare industry is unique and holds a high level of importance to society, with people expecting top-notch care and services regardless of cost. Despite consuming a large portion of the budget, it has not met social expectations. Medical data interpretation is mainly conducted by experts, but image interpretation by human experts is limited due to subjectivity, image complexity, variations among interpreters, and fatigue. However, deep learning has proven successful in other areas and is now providing accurate solutions for medical imaging, making it a crucial method for future healthcare applications. This chapter explores the state of the art deep learning architecture and optimization techniques used for medical image segmentation and classification, as well as the challenges and open research issues surrounding deep learning-based methods for medical imaging.",1
"Fully convolutional neural networks (FCNNs) trained on a large number of images with strong pixel-level annotations have become the new state of the art for the semantic segmentation task. While there have been recent attempts to learn FCNNs from image-level weak annotations, they need additional constraints, such as the size of an object, to obtain reasonable performance. To address this issue, we present motion-CNN (M-CNN), a novel FCNN framework which incorporates motion cues and is learned from video-level weak annotations. Our learning scheme to train the network uses motion segments as soft constraints, thereby handling noisy motion information. When trained on weakly-annotated videos, our method outperforms the state-of-the-art EM-Adapt approach on the PASCAL VOC 2012 image segmentation benchmark. We also demonstrate that the performance of M-CNN learned with 150 weak video annotations is on par with state-of-the-art weakly-supervised methods trained with thousands of images. Finally, M-CNN substantially outperforms recent approaches in a related task of video co-localization on the YouTube-Objects dataset.",0
"The latest technique for achieving semantic segmentation is by using fully convolutional neural networks (FCNNs) that are trained on a vast number of images with pixel-level annotations. Previously, there have been attempts to learn FCNNs using image-level weak annotations, but they require additional constraints, like object size, to perform well. To overcome this problem, we introduce motion-CNN (M-CNN), a new FCNN framework that incorporates motion cues and is learned from video-level weak annotations. Our learning approach uses motion segments as soft constraints to address noisy motion information. By training M-CNN on weakly-annotated videos, we surpass the state-of-the-art EM-Adapt method on the PASCAL VOC 2012 image segmentation benchmark. Moreover, we prove that M-CNN's performance is comparable to state-of-the-art weakly-supervised approaches that are trained with thousands of images, with only 150 weak video annotations. Finally, M-CNN performs markedly better than recent methods in a related task of video co-localization on the YouTube-Objects dataset.",1
"In application domains such as robotics, it is useful to represent the uncertainty related to the robot's belief about the state of its environment. Algorithms that only yield a single ""best guess"" as a result are not sufficient. In this paper, we propose object proposal generation based on non-parametric Bayesian inference that allows quantification of the likelihood of the proposals. We apply Markov chain Monte Carlo to draw samples of image segmentations via the distance dependent Chinese restaurant process. Our method achieves state-of-the-art performance on an indoor object discovery data set, while additionally providing a likelihood term for each proposal. We show that the likelihood term can effectively be used to rank proposals according to their quality.",0
"Representing uncertainty related to a robot's perception of its environment is crucial in fields such as robotics. Algorithms that provide only one outcome are insufficient. This study introduces a non-parametric Bayesian inference-based method for generating object proposals that quantifies the likelihood of each proposal. Using Markov chain Monte Carlo, image segmentations are sampled through the distance-dependent Chinese restaurant process. Our method outperforms others on an indoor object discovery data set and offers a likelihood term for each proposal. We demonstrate that the likelihood term can be used to rank proposals based on their quality.",1
"Automatic segmentation of medical images is an important task for many clinical applications. In practice, a wide range of anatomical structures are visualised using different imaging modalities. In this paper, we investigate whether a single convolutional neural network (CNN) can be trained to perform different segmentation tasks.   A single CNN is trained to segment six tissues in MR brain images, the pectoral muscle in MR breast images, and the coronary arteries in cardiac CTA. The CNN therefore learns to identify the imaging modality, the visualised anatomical structures, and the tissue classes.   For each of the three tasks (brain MRI, breast MRI and cardiac CTA), this combined training procedure resulted in a segmentation performance equivalent to that of a CNN trained specifically for that task, demonstrating the high capacity of CNN architectures. Hence, a single system could be used in clinical practice to automatically perform diverse segmentation tasks without task-specific training.",0
"Many clinical applications require automatic segmentation of medical images. These images can show various anatomical structures through different imaging techniques. This study aims to determine if a single convolutional neural network (CNN) can be trained to perform various segmentation tasks. By training a single CNN to segment six tissues in MR brain images, the pectoral muscle in MR breast images, and the coronary arteries in cardiac CTA, the CNN can identify imaging modality, visualized anatomical structures, and tissue classes. The combined training procedure resulted in a segmentation performance similar to a CNN trained specifically for each task, showcasing the high capacity of CNN architectures. Therefore, a single system could perform diverse segmentation tasks automatically without task-specific training, making it useful in clinical practice.",1
"We introduce a novel loss max-pooling concept for handling imbalanced training data distributions, applicable as alternative loss layer in the context of deep neural networks for semantic image segmentation. Most real-world semantic segmentation datasets exhibit long tail distributions with few object categories comprising the majority of data and consequently biasing the classifiers towards them. Our method adaptively re-weights the contributions of each pixel based on their observed losses, targeting under-performing classification results as often encountered for under-represented object classes. Our approach goes beyond conventional cost-sensitive learning attempts through adaptive considerations that allow us to indirectly address both, inter- and intra-class imbalances. We provide a theoretical justification of our approach, complementary to experimental analyses on benchmark datasets. In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach.",0
"We have developed a new technique called loss max-pooling to address the issue of imbalanced training data distributions in deep neural networks used for semantic image segmentation. Typically, real-world datasets for semantic segmentation have a long tail distribution, with a few object categories making up the majority of the data. This can bias the results towards those categories. Our method adjusts the weights of each pixel based on their observed losses, focusing on under-performing classifications for less represented object classes. Our approach is more advanced than traditional cost-sensitive learning as it considers both inter- and intra-class imbalances. The efficacy of our approach has been demonstrated through theoretical justification and experimental analyses on benchmark datasets such as Cityscapes and Pascal VOC 2012. We consistently achieved better results with our approach.",1
"Automatic cell image segmentation methods in connectomics produce merge and split errors, which require correction through proofreading. Previous research has identified the visual search for these errors as the bottleneck in interactive proofreading. To aid error correction, we develop two classifiers that automatically recommend candidate merges and splits to the user. These classifiers use a convolutional neural network (CNN) that has been trained with errors in automatic segmentations against expert-labeled ground truth. Our classifiers detect potentially-erroneous regions by considering a large context region around a segmentation boundary. Corrections can then be performed by a user with yes/no decisions, which reduces variation of information 7.5x faster than previous proofreading methods. We also present a fully-automatic mode that uses a probability threshold to make merge/split decisions. Extensive experiments using the automatic approach and comparing performance of novice and expert users demonstrate that our method performs favorably against state-of-the-art proofreading methods on different connectomics datasets.",0
"Connectomics' automatic cell image segmentation methods result in merge and split errors that necessitate proofreading. The visual search for these mistakes has been identified as a bottleneck in interactive proofreading by preceding research. To assist with error correction, we have created two classifiers that suggest potential merges and splits to the user automatically. These classifiers employ a convolutional neural network (CNN) which has been trained using errors in automatic segmentation against ground truth that has been labeled by experts. Our classifiers detect potentially erroneous regions by taking into account a large context area surrounding a segmentation boundary. Corrections may be made by a user using yes/no decisions, resulting in 7.5x faster reduction of variation of information than previous proofreading approaches. We also provide a fully automatic mode that employs a probability threshold to make merge/split decisions. Our approach has been extensively tested using the automatic method and comparing the performance of novice and expert users, and it has been demonstrated that our technique outperforms state-of-the-art proofreading methods across various connectomics datasets.",1
"Tissue segmentation is an important pre-requisite for efficient and accurate diagnostics in digital pathology. However, it is well known that whole-slide scanners can fail in detecting all tissue regions, for example due to the tissue type, or due to weak staining because their tissue detection algorithms are not robust enough. In this paper, we introduce two different convolutional neural network architectures for whole slide image segmentation to accurately identify the tissue sections. We also compare the algorithms to a published traditional method. We collected 54 whole slide images with differing stains and tissue types from three laboratories to validate our algorithms. We show that while the two methods do not differ significantly they outperform their traditional counterpart (Jaccard index of 0.937 and 0.929 vs. 0.870, p < 0.01).",0
"Efficient and accurate diagnostics in digital pathology require tissue segmentation as a crucial first step. However, it is widely acknowledged that whole-slide scanners may fail to detect all tissue regions due to factors such as tissue type or weak staining, as their tissue detection algorithms lack robustness. This paper presents two convolutional neural network architectures that accurately identify tissue sections in whole slide images. We compare the performance of these algorithms to a traditional method previously published and validate them using 54 whole slide images with varying stains and tissue types from three laboratories. Our results demonstrate that although the two methods do not differ significantly, they both outperform their traditional counterpart (with Jaccard index values of 0.937 and 0.929 compared to 0.870, p < 0.01).",1
"The aim of this paper is to give an overview of domain adaptation and transfer learning with a specific view on visual applications. After a general motivation, we first position domain adaptation in the larger transfer learning problem. Second, we try to address and analyze briefly the state-of-the-art methods for different types of scenarios, first describing the historical shallow methods, addressing both the homogeneous and the heterogeneous domain adaptation methods. Third, we discuss the effect of the success of deep convolutional architectures which led to new type of domain adaptation methods that integrate the adaptation within the deep architecture. Fourth, we overview the methods that go beyond image categorization, such as object detection or image segmentation, video analyses or learning visual attributes. Finally, we conclude the paper with a section where we relate domain adaptation to other machine learning solutions.",0
"The purpose of this paper is to present an outline of domain adaptation and transfer learning, with a focus on visual applications. After providing a general introduction, we outline the position of domain adaptation in the broader transfer learning context. We then briefly explore the state-of-the-art techniques for various scenarios, including historical shallow methods, homogeneous and heterogeneous domain adaptation methods. We also examine the impact of deep convolutional architectures and their integration into domain adaptation methods. Additionally, we review methods that extend beyond image categorization, such as object detection, image segmentation, video analysis, and learning visual attributes. Finally, we conclude by discussing the relationship between domain adaptation and other machine learning approaches.",1
"Reconstruction based on the stereo camera has received considerable attention recently, but two particular challenges still remain. The first concerns the need to aggregate similar pixels in an effective approach, and the second is to maintain as much of the available information as possible while ensuring sufficient accuracy. To overcome these issues, we propose a new 3D representation method, namely, planecell, that extracts planarity from the depth-assisted image segmentation and then projects these depth planes into the 3D world. An energy function formulated from Conditional Random Field that generalizes the planar relationships is maximized to merge coplanar segments. We evaluate our method with a variety of reconstruction baselines on both KITTI and Middlebury datasets, and the results indicate the superiorities compared to other 3D space representation methods in accuracy, memory requirements and further applications.",0
"Recently, there has been significant interest in using stereo cameras for reconstruction. However, two challenges have yet to be overcome. The first challenge involves effectively aggregating similar pixels, while the second involves maintaining the maximum amount of information possible while ensuring accuracy. To address these issues, we propose a new 3D representation method called ""planecell."" This method extracts planarity from depth-assisted image segmentation and projects these planes into the 3D world. An energy function, formulated from Conditional Random Field, is used to maximize planar relationships and merge coplanar segments. We evaluated our method on KITTI and Middlebury datasets and found superior accuracy, memory requirements, and further applications compared to other 3D space representation methods.",1
"The task of counting eucalyptus trees from aerial images collected by unmanned aerial vehicles (UAVs) has been frequently explored by techniques of estimation of the basal area, i.e, by determining the expected number of trees based on sampling techniques. An alternative is the use of machine learning to identify patterns that represent a tree unit, and then search for the occurrence of these patterns throughout the image. This strategy depends on a supervised image segmentation step to define predefined interest regions. Thus, it is possible to automate the counting of eucalyptus trees in these images, thereby increasing the efficiency of the eucalyptus forest inventory management. In this paper, we evaluated 20 different classifiers for the image segmentation task. A real sample was used to analyze the counting trees task considering a practical environment. The results show that it possible to automate this task with 0.7% counting error, in particular, by using strategies based on a combination of classifiers. Moreover, we present some performance considerations about each classifier that can be useful as a basis for decision-making in future tasks.",0
"Various methods have been employed to count eucalyptus trees from aerial images captured by unmanned aerial vehicles (UAVs). One approach involves estimating the basal area to calculate the expected number of trees using sampling techniques. Alternatively, machine learning can be utilized to identify tree patterns and locate them throughout the image. This method requires a supervised image segmentation step to define areas of interest, resulting in automated tree counting and improved efficiency in eucalyptus forest inventory management. Our study evaluated 20 different classifiers for image segmentation, using a real sample to analyze tree counting in a practical setting. Results showed a 0.7% counting error rate through a combination of classifiers, with performance considerations provided for future decision-making.",1
"We propose a new approach to image segmentation, which exploits the advantages of both conditional random fields (CRFs) and decision trees. In the literature, the potential functions of CRFs are mostly defined as a linear combination of some pre-defined parametric models, and then methods like structured support vector machines (SSVMs) are applied to learn those linear coefficients. We instead formulate the unary and pairwise potentials as nonparametric forests---ensembles of decision trees, and learn the ensemble parameters and the trees in a unified optimization problem within the large-margin framework. In this fashion, we easily achieve nonlinear learning of potential functions on both unary and pairwise terms in CRFs. Moreover, we learn class-wise decision trees for each object that appears in the image. Due to the rich structure and flexibility of decision trees, our approach is powerful in modelling complex data likelihoods and label relationships. The resulting optimization problem is very challenging because it can have exponentially many variables and constraints. We show that this challenging optimization can be efficiently solved by combining a modified column generation and cutting-planes techniques. Experimental results on both binary (Graz-02, Weizmann horse, Oxford flower) and multi-class (MSRC-21, PASCAL VOC 2012) segmentation datasets demonstrate the power of the learned nonlinear nonparametric potentials.",0
"Our proposed method for image segmentation combines the benefits of conditional random fields (CRFs) and decision trees. CRFs are often defined using linear combinations of pre-set models, with structured support vector machines (SSVMs) used to learn the linear coefficients. Instead, we use nonparametric forests, or ensembles of decision trees, to formulate unary and pairwise potentials. We learn the parameters and trees together in a unified optimization problem within a large-margin framework, allowing for nonlinear learning of potential functions on both unary and pairwise terms in CRFs. Our approach also includes class-wise decision trees for each object in the image. This method is effective in modeling complex data likelihoods and label relationships due to the flexibility of decision trees. The optimization problem resulting from this approach is challenging, but we show that it can be efficiently solved using a modified combination of column generation and cutting-planes techniques. Our experimental results on binary and multi-class segmentation datasets demonstrate the power of learned nonlinear nonparametric potentials.",1
"Paleness or pallor is a manifestation of blood loss or low hemoglobin concentrations in the human blood that can be caused by pathologies such as anemia. This work presents the first automated screening system that utilizes pallor site images, segments, and extracts color and intensity-based features for multi-class classification of patients with high pallor due to anemia-like pathologies, normal patients and patients with other abnormalities. This work analyzes the pallor sites of conjunctiva and tongue for anemia screening purposes. First, for the eye pallor site images, the sclera and conjunctiva regions are automatically segmented for regions of interest. Similarly, for the tongue pallor site images, the inner and outer tongue regions are segmented. Then, color-plane based feature extraction is performed followed by machine learning algorithms for feature reduction and image level classification for anemia. In this work, a suite of classification algorithms image-level classifications for normal (class 0), pallor (class 1) and other abnormalities (class 2). The proposed method achieves 86% accuracy, 85% precision and 67% recall in eye pallor site images and 98.2% accuracy and precision with 100% recall in tongue pallor site images for classification of images with pallor. The proposed pallor screening system can be further fine-tuned to detect the severity of anemia-like pathologies using controlled set of local images that can then be used for future benchmarking purposes.",0
"The occurrence of paleness or pallor is linked to low hemoglobin levels or blood loss in humans, often caused by ailments such as anemia. This study introduces the first automated screening system that utilizes images of pallor sites, segments, and extracts color and intensity-based features for the classification of patients with high pallor caused by anemia-like pathologies, normal patients, and patients with other abnormalities. The pallor sites of the conjunctiva and tongue are analyzed for anemia screening. The sclera and conjunctiva regions are automatically segmented in eye pallor site images, while the inner and outer tongue regions are segmented in tongue pallor site images. Color-plane based feature extraction is performed, followed by machine learning algorithms for feature reduction and image level classification for anemia. The proposed method utilizes a suite of classification algorithms for image-level classifications for normal, pallor, and other abnormalities. The proposed system achieves 86% accuracy, 85% precision, and 67% recall in eye pallor site images and 98.2% accuracy, precision, and recall in tongue pallor site images for the classification of images with pallor. The proposed pallor screening system can be fine-tuned to detect the severity of anemia-like pathologies using a controlled set of local images that can be used for future benchmarking purposes.",1
"Image segmentation is the process of partitioning an image into a set of meaningful regions according to some criteria. Hierarchical segmentation has emerged as a major trend in this regard as it favors the emergence of important regions at different scales. On the other hand, many methods allow us to have prior information on the position of structures of interest in the images. In this paper, we present a versatile hierarchical segmentation method that takes into account any prior spatial information and outputs a hierarchical segmentation that emphasizes the contours or regions of interest while preserving the important structures in the image. Several applications are presented that illustrate the method versatility and efficiency.",0
"The act of dividing an image into significant sections based on specific standards is referred to as image segmentation. Hierarchical segmentation has become a prominent practice in this field since it encourages the emergence of crucial regions at varying scales. Meanwhile, numerous techniques allow us to possess preliminary knowledge concerning the positioning of essential structures in the images. This article introduces a flexible hierarchical segmentation approach that considers any prior spatial data and produces a hierarchical segmentation that highlights the outlines or significant areas while conserving the image's crucial structures. Several applications are provided to demonstrate the method's adaptability and effectiveness.",1
"The state-of-the-art method for automatically segmenting white matter bundles in diffusion-weighted MRI is tractography in conjunction with streamline cluster selection. This process involves long chains of processing steps which are not only computationally expensive but also complex to setup and tedious with respect to quality control. Direct bundle segmentation methods treat the task as a traditional image segmentation problem. While they so far did not deliver competitive results, they can potentially mitigate many of the mentioned issues. We present a novel supervised approach for direct tract segmentation that shows major performance gains. It builds upon a stacked U-Net architecture which is trained on manual bundle segmentations from Human Connectome Project subjects. We evaluate our approach \textit{in vivo} as well as \textit{in silico} using the ISMRM 2015 Tractography Challenge phantom dataset. We achieve human segmentation performance and a major performance gain over previous pipelines. We show how the learned spatial priors efficiently guide the segmentation even at lower image qualities with little quality loss.",0
"The current method for segmenting white matter bundles in diffusion-weighted MRI involves tractography and streamline cluster selection. However, this process involves several computationally expensive and complex steps, making it tedious for quality control. Alternatively, direct bundle segmentation methods treat the task as a traditional image segmentation problem and can potentially solve these issues. Despite their lack of competitive results, we present a new supervised approach for direct tract segmentation, which employs a stacked U-Net architecture trained on manual bundle segmentations from Human Connectome Project subjects. We evaluate our method both in vivo and in silico using the ISMRM 2015 Tractography Challenge phantom dataset. Our approach outperforms previous pipelines and achieves human segmentation performance. Furthermore, our learned spatial priors guide the segmentation process efficiently, even at lower image qualities, with minimal quality loss.",1
"Machine learning methods in general and Deep Neural Networks in particular have shown to be vulnerable to adversarial perturbations. So far this phenomenon has mainly been studied in the context of whole-image classification. In this contribution, we analyse how adversarial perturbations can affect the task of semantic segmentation. We show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible adversarial perturbations that lead a deep network to misclassify almost all pixels of a chosen class while leaving network prediction nearly unchanged outside this class.",0
"Adversarial perturbations have been found to be a weakness in machine learning, particularly in Deep Neural Networks. While this has mostly been studied in regard to overall image classification, this study focuses on how adversarial perturbations can impact semantic segmentation. Our analysis demonstrates that existing adversarial attackers can be applied to this task and that it is possible to create subtle adversarial perturbations that cause a deep network to misclassify nearly all pixels of a specific class, while having little impact on network prediction outside of that class.",1
"This paper proposes a novel approach for uncertainty quantification in dense Conditional Random Fields (CRFs). The presented approach, called Perturb-and-MPM, enables efficient, approximate sampling from dense multi-label CRFs via random perturbations. An analytic error analysis was performed which identified the main cause of approximation error as well as showed that the error is bounded. Spatial uncertainty maps can be derived from the Perturb-and-MPM model, which can be used to visualize uncertainty in image segmentation results. The method is validated on synthetic and clinical Magnetic Resonance Imaging data. The effectiveness of the approach is demonstrated on the challenging problem of segmenting the tumor core in glioblastoma. We found that areas of high uncertainty correspond well to wrongly segmented image regions. Furthermore, we demonstrate the potential use of uncertainty maps to refine imaging biomarkers in the case of extent of resection and residual tumor volume in brain tumor patients.",0
"In this paper, a new method for quantifying uncertainty in dense Conditional Random Fields (CRFs) is proposed. The approach, which is called Perturb-and-MPM, involves using random perturbations to achieve efficient, approximate sampling from dense multi-label CRFs. An analytic error analysis was conducted, which identified the primary source of approximation error and demonstrated that the error is bounded. The Perturb-and-MPM model can be used to generate spatial uncertainty maps, which are helpful in visualizing uncertainty in image segmentation outcomes. The method was tested on synthetic and clinical Magnetic Resonance Imaging data, with a focus on the challenging task of glioblastoma tumor core segmentation. Results indicate that regions of high uncertainty correspond to incorrectly segmented image regions, and uncertainty maps can be used to refine imaging biomarkers related to extent of resection and residual tumor volume in brain tumor patients.",1
We consider the problem of semantic image segmentation using deep convolutional neural networks. We propose a novel network architecture called the label refinement network that predicts segmentation labels in a coarse-to-fine fashion at several resolutions. The segmentation labels at a coarse resolution are used together with convolutional features to obtain finer resolution segmentation labels. We define loss functions at several stages in the network to provide supervisions at different stages. Our experimental results on several standard datasets demonstrate that the proposed model provides an effective way of producing pixel-wise dense image labeling.,0
"Our focus is on utilizing deep convolutional neural networks to tackle semantic image segmentation. We introduce a unique network structure, the label refinement network, which gradually predicts segmentation labels at various resolutions. We leverage the coarse resolution segmentation labels and convolutional features to generate more refined segmentation labels. Additionally, we establish loss functions at multiple phases in the network to offer supervisions at different stages. Through our testing on multiple standard datasets, we show that our model is a successful method for producing pixel-wise dense image labeling.",1
"This abstract briefly describes a segmentation algorithm developed for the ISIC 2017 Skin Lesion Detection Competition hosted at [ref]. The objective of the competition is to perform a segmentation (in the form of a binary mask image) of skin lesions in dermoscopic images as close as possible to a segmentation performed by trained clinicians, which is taken as ground truth. This project only takes part in the segmentation phase of the challenge. The other phases of the competition (feature extraction and lesion identification) are not considered.   The proposed algorithm consists of 4 steps: (1) lesion image preprocessing, (2) image segmentation using k-means clustering of pixel colors, (3) calculation of a set of features describing the properties of each segmented region, and (4) calculation of a final score for each region, representing the likelihood of corresponding to a suitable lesion segmentation. The scores in step (4) are obtained by averaging the results of 2 different regression models using the scores of each region as input. Before using the algorithm these regression models must be trained using the training set of images and ground truth masks provided by the Competition. Steps 2 to 4 are repeated with an increasing number of clusters (and therefore the image is segmented into more regions) until there is no further improvement of the calculated scores.",0
"This abstract provides a concise overview of a segmentation algorithm that was created for the ISIC 2017 Skin Lesion Detection Competition. The aim of the competition was to produce a binary mask image that accurately represents a segmentation of skin lesions in dermoscopic images, which has been determined by trained clinicians as the ground truth. This project solely focuses on the segmentation phase of the competition, disregarding feature extraction and lesion identification. The algorithm consists of four steps, including lesion image preprocessing, image segmentation by utilizing k-means clustering of pixel colors, feature calculation for each segmented region, and the computation of a final score for every region that indicates its likelihood of being a suitable lesion segmentation. The final scores are attained by averaging the outcomes of two distinct regression models that use the scores of each region as input. Prior to utilizing the algorithm, the regression models need to be trained using the training images and ground truth masks provided by the competition. Steps 2 to 4 are repeated with an increasing number of clusters until the scores are no longer improved.",1
"Mandible bone segmentation from computed tomography (CT) scans is challenging due to mandible's structural irregularities, complex shape patterns, and lack of contrast in joints. Furthermore, connections of teeth to mandible and mandible to remaining parts of the skull make it extremely difficult to identify mandible boundary automatically. This study addresses these challenges by proposing a novel framework where we define the segmentation as two complementary tasks: recognition and delineation. For recognition, we use random forest regression to localize mandible in 3D. For delineation, we propose to use 3D gradient-based fuzzy connectedness (FC) image segmentation algorithm, operating on the recognized mandible sub-volume. Despite heavy CT artifacts and dental fillings, consisting half of the CT image data in our experiments, we have achieved highly accurate detection and delineation results. Specifically, detection accuracy more than 96% (measured by union of intersection (UoI)), the delineation accuracy of 91% (measured by dice similarity coefficient), and less than 1 mm in shape mismatch (Hausdorff Distance) were found.",0
"Segmenting the mandible bone from CT scans is a difficult task due to irregularities in its structure, complex shape patterns, and lack of contrast in joints. Additionally, identifying the mandible boundary automatically is particularly challenging due to the connections of the teeth to the mandible and the mandible to the rest of the skull. To address these challenges, this study proposes a novel framework that defines the segmentation as two complementary tasks: recognition and delineation. In the recognition task, a random forest regression is used to localize the mandible in 3D. In the delineation task, a 3D gradient-based fuzzy connectedness (FC) image segmentation algorithm is proposed, operating on the recognized mandible sub-volume. Despite the presence of heavy CT artifacts and dental fillings, which made up half of the CT image data in the experiments, the results showed highly accurate detection and delineation. Specifically, the detection accuracy was over 96% (measured by union of intersection (UoI)), the delineation accuracy was 91% (measured by dice similarity coefficient), and the shape mismatch was less than 1 mm (measured by Hausdorff Distance).",1
"We propose a novel label fusion technique as well as a crowdsourcing protocol to efficiently obtain accurate epithelial cell segmentations from non-expert crowd workers. Our label fusion technique simultaneously estimates the true segmentation, the performance levels of individual crowd workers, and an image segmentation model in the form of a pairwise Markov random field. We term our approach image-aware STAPLE (iaSTAPLE) since our image segmentation model seamlessly integrates into the well-known and widely used STAPLE approach. In an evaluation on a light microscopy dataset containing more than 5000 membrane labeled epithelial cells of a fly wing, we show that iaSTAPLE outperforms STAPLE in terms of segmentation accuracy as well as in terms of the accuracy of estimated crowd worker performance levels, and is able to correctly segment 99% of all cells when compared to expert segmentations. These results show that iaSTAPLE is a highly useful tool for crowd sourcing image segmentation.",0
"Our proposal introduces a unique method of label fusion and a crowdsourcing protocol that enables efficient and precise epithelial cell segmentation by non-expert crowd workers. Our label fusion technique accomplishes multiple tasks simultaneously, such as estimating the true segmentation, individual crowd workers' performance levels, and an image segmentation model in the form of a pairwise Markov random field. Our approach is called image-aware STAPLE (iaSTAPLE), as it seamlessly incorporates our image segmentation model into the widely used STAPLE approach. In an evaluation of over 5000 membrane-labeled epithelial cells in a fly wing's light microscopy dataset, we demonstrate that iaSTAPLE surpasses STAPLE in terms of segmentation accuracy and estimated crowd worker performance levels, accurately segmenting 99% of all cells compared to expert segmentations. These results highlight the usefulness of iaSTAPLE as a valuable tool for crowd sourcing image segmentation.",1
"This paper introduces Progressively Diffused Networks (PDNs) for unifying multi-scale context modeling with deep feature learning, by taking semantic image segmentation as an exemplar application. Prior neural networks, such as ResNet, tend to enhance representational power by increasing the depth of architectures and driving the training objective across layers. However, we argue that spatial dependencies in different layers, which generally represent the rich contexts among data elements, are also critical to building deep and discriminative representations. To this end, our PDNs enables to progressively broadcast information over the learned feature maps by inserting a stack of information diffusion layers, each of which exploits multi-dimensional convolutional LSTMs (Long-Short-Term Memory Structures). In each LSTM unit, a special type of atrous filters are designed to capture the short range and long range dependencies from various neighbors to a certain site of the feature map and pass the accumulated information to the next layer. From the extensive experiments on semantic image segmentation benchmarks (e.g., ImageNet Parsing, PASCAL VOC2012 and PASCAL-Part), our framework demonstrates the effectiveness to substantially improve the performances over the popular existing neural network models, and achieves state-of-the-art on ImageNet Parsing for large scale semantic segmentation.",0
"The aim of this paper is to present Progressively Diffused Networks (PDNs), a new approach to combining multi-scale context modeling with deep feature learning, using semantic image segmentation as an example. While existing neural networks like ResNet tend to focus on increasing architecture depth to enhance representational power and optimize training objectives, the authors argue that spatial dependencies across different layers, which reflect the rich contexts between data elements, are equally important in building deep and discriminative representations. To address this, PDNs utilize a series of information diffusion layers featuring multi-dimensional convolutional LSTMs (Long-Short-Term Memory Structures) that progressively broadcast information across feature maps. These LSTMs employ specialized atrous filters that capture short and long-range dependencies from various neighbors, accumulating and passing on information to the next layer. Through extensive experimentation on benchmarks for semantic image segmentation, including ImageNet Parsing, PASCAL VOC2012, and PASCAL-Part, PDNs prove to be highly effective, improving performance significantly compared to existing neural network models and achieving state-of-the-art results for large-scale semantic segmentation on ImageNet Parsing.",1
"Cell nuclei segmentation is one of the most important tasks in the analysis of biomedical images. With ever-growing sizes and amounts of three-dimensional images to be processed, there is a need for better and faster segmentation methods. Graph-based image segmentation has seen a rise in popularity in recent years, but is seen as very costly with regard to computational demand. We propose a new segmentation algorithm which overcomes these limitations. Our method uses recursive balanced graph partitioning to segment foreground components of a fast and efficient binarization. We construct a model for the cell nuclei to guide the partitioning process. Our algorithm is compared to other state-of-the-art segmentation algorithms in an experimental evaluation on two sets of realistically simulated inputs. Our method is faster, has similar or better quality and an acceptable memory overhead.",0
"Segmenting cell nuclei is a crucial task in biomedical image analysis. As the number and size of three-dimensional images continues to grow, there is a need for faster and more efficient segmentation methods. Graph-based segmentation has gained popularity, but is often considered computationally expensive. However, we have developed a new algorithm that overcomes these limitations. Our method utilizes recursive balanced graph partitioning to segment foreground components, with a focus on speedy and effective binarization. We have created a model for cell nuclei to guide the partitioning process. Our algorithm was tested against other advanced segmentation algorithms using two sets of realistically simulated inputs. Our results show that our method is faster, has similar or better quality, and a reasonable memory overhead.",1
"In this paper, we introduce a simple, yet powerful pipeline for medical image segmentation that combines Fully Convolutional Networks (FCNs) with Fully Convolutional Residual Networks (FC-ResNets). We propose and examine a design that takes particular advantage of recent advances in the understanding of both Convolutional Neural Networks as well as ResNets. Our approach focuses upon the importance of a trainable pre-processing when using FC-ResNets and we show that a low-capacity FCN model can serve as a pre-processor to normalize medical input data. In our image segmentation pipeline, we use FCNs to obtain normalized images, which are then iteratively refined by means of a FC-ResNet to generate a segmentation prediction. As in other fully convolutional approaches, our pipeline can be used off-the-shelf on different image modalities. We show that using this pipeline, we exhibit state-of-the-art performance on the challenging Electron Microscopy benchmark, when compared to other 2D methods. We improve segmentation results on CT images of liver lesions, when contrasting with standard FCN methods. Moreover, when applying our 2D pipeline on a challenging 3D MRI prostate segmentation challenge we reach results that are competitive even when compared to 3D methods. The obtained results illustrate the strong potential and versatility of the pipeline by achieving highly accurate results on multi-modality images from different anatomical regions and organs.",0
"This paper presents a medical image segmentation pipeline that combines Fully Convolutional Networks (FCNs) with Fully Convolutional Residual Networks (FC-ResNets). Our approach takes advantage of recent developments in Convolutional Neural Networks and ResNets to create a trainable pre-processing step using a low-capacity FCN model that normalizes medical input data. In our pipeline, FCNs obtain normalized images, which are then refined iteratively by FC-ResNets to generate segmentation predictions. Our off-the-shelf pipeline can be used on various image modalities and exhibits state-of-the-art performance on the Electron Microscopy benchmark and CT images of liver lesions. Additionally, our 2D pipeline achieves competitive results on a challenging 3D MRI prostate segmentation challenge, demonstrating the potential and versatility of the approach across different anatomical regions and organs.",1
"Several supermodular losses have been shown to improve the perceptual quality of image segmentation in a discriminative framework such as a structured output support vector machine (SVM). These loss functions do not necessarily have the same structure as the one used by the segmentation inference algorithm, and in general, we may have to resort to generic submodular minimization algorithms for loss augmented inference. Although these come with polynomial time guarantees, they are not practical to apply to image scale data. Many supermodular losses come with strong optimization guarantees, but are not readily incorporated in a loss augmented graph cuts procedure. This motivates our strategy of employing the alternating direction method of multipliers (ADMM) decomposition for loss augmented inference. In doing so, we create a new API for the structured SVM that separates the maximum a posteriori (MAP) inference of the model from the loss augmentation during training. In this way, we gain computational efficiency, making new choices of loss functions practical for the first time, while simultaneously making the inference algorithm employed during training closer to the test time procedure. We show improvement both in accuracy and computational performance on the Microsoft Research Grabcut database and a brain structure segmentation task, empirically validating the use of several supermodular loss functions during training, and the improved computational properties of the proposed ADMM approach over the Fujishige-Wolfe minimum norm point algorithm.",0
"The perceptual quality of image segmentation in a structured output support vector machine (SVM) can be improved by using various supermodular losses. However, these loss functions may not have the same structure as the segmentation inference algorithm, so generic submodular minimization algorithms are often used for loss augmented inference. Although these algorithms have polynomial time guarantees, they are impractical for image scale data. Additionally, many supermodular losses have strong optimization guarantees but are not easily incorporated into a loss augmented graph cuts procedure. To address these issues, we propose using the alternating direction method of multipliers (ADMM) decomposition for loss augmented inference. This new approach separates the maximum a posteriori (MAP) inference of the model from the loss augmentation during training, resulting in improved computational efficiency. We demonstrate the effectiveness of this approach on the Microsoft Research Grabcut database and a brain structure segmentation task, showing improvement in both accuracy and computational performance. Our results validate the use of several supermodular loss functions during training and the improved computational properties of the proposed ADMM approach compared to the Fujishige-Wolfe minimum norm point algorithm.",1
"We propose a method for semi-supervised training of structured-output neural networks. Inspired by the framework of Generative Adversarial Networks (GAN), we train a discriminator network to capture the notion of a quality of network output. To this end, we leverage the qualitative difference between outputs obtained on the labelled training data and unannotated data. We then use the discriminator as a source of error signal for unlabelled data. This effectively boosts the performance of a network on a held out test set. Initial experiments in image segmentation demonstrate that the proposed framework enables achieving the same network performance as in a fully supervised scenario, while using two times less annotations.",0
"Our proposal outlines a technique for training structured-output neural networks in a semi-supervised manner. Drawing inspiration from the Generative Adversarial Networks (GAN) framework, we teach a discriminator network how to determine the quality of network output. This is achieved by comparing the quality of outputs obtained from labelled and unlabelled training data. The discriminator then serves as an error signal for unlabelled data, leading to improved performance on a held-out test set. Our preliminary tests on image segmentation demonstrate that our approach enables us to achieve the same level of performance as a fully supervised method, while requiring only half the amount of annotations.",1
"Semantic image segmentation is a fundamental task in image understanding. Per-pixel semantic labelling of an image benefits greatly from the ability to consider region consistency both locally and globally. However, many Fully Convolutional Network based methods do not impose such consistency, which may give rise to noisy and implausible predictions. We address this issue by proposing a dense multi-label network module that is able to encourage the region consistency at different levels. This simple but effective module can be easily integrated into any semantic segmentation systems. With comprehensive experiments, we show that the dense multi-label can successfully remove the implausible labels and clear the confusion so as to boost the performance of semantic segmentation systems.",0
"The comprehension of images relies heavily on semantic image segmentation, which involves assigning semantic labels to each pixel in an image. For accurate labelling, it is crucial to consider both local and global region consistency. However, several Fully Convolutional Network based methods do not prioritize this consistency, leading to inaccurate and unreliable predictions. To combat this issue, we propose a dense multi-label network module that promotes region consistency across multiple levels. This module is an easy addition to existing systems and effectively removes implausible labels and confusion, thereby improving the performance of semantic segmentation systems. Our experiments prove the effectiveness of the dense multi-label module.",1
"Edge detection is a classic problem in the field of image processing, which lays foundations for other tasks such as image segmentation. Conventionally, this operation is performed using gradient operators such as the Roberts or Sobel operator, which can discover local changes in intensity levels. These operators, however, perform poorly on low contrast images. In this paper, we propose an edge detector architecture for color images based on fuzzy theory and the Sobel operator. First, the R, G and B channels are extracted from an image and enhanced using fuzzy methods, in order to suppress noise and improve the contrast between the background and the objects. The Sobel operator is then applied to each of the channels, which are finally combined into an edge map of the origin image. Experimental results obtained through an FPGA-based implementation have proved the proposed method effective.",0
"The classic problem of edge detection in image processing serves as a foundation for other tasks like image segmentation. The conventional approach to this problem utilizes gradient operators, such as the Roberts or Sobel operator, to detect local changes in intensity levels. However, these operators do not perform well on low contrast images. This paper suggests an edge detector architecture for color images that utilizes fuzzy theory and the Sobel operator. The R, G, and B channels are initially extracted from the image and improved using fuzzy methods to diminish noise and amplify the contrast between the background and objects. Subsequently, the Sobel operator is applied to each channel, and the resulting maps are combined to form an edge map of the original image. An FPGA-based implementation confirms the effectiveness of the proposed method through experimental results.",1
"In this paper, we develop a new weakly-supervised learning algorithm to learn to segment cancerous regions in histopathology images. Our work is under a multiple instance learning framework (MIL) with a new formulation, deep weak supervision (DWS); we also propose an effective way to introduce constraints to our neural networks to assist the learning process. The contributions of our algorithm are threefold: (1) We build an end-to-end learning system that segments cancerous regions with fully convolutional networks (FCN) in which image-to-image weakly-supervised learning is performed. (2) We develop a deep week supervision formulation to exploit multi-scale learning under weak supervision within fully convolutional networks. (3) Constraints about positive instances are introduced in our approach to effectively explore additional weakly-supervised information that is easy to obtain and enjoys a significant boost to the learning process. The proposed algorithm, abbreviated as DWS-MIL, is easy to implement and can be trained efficiently. Our system demonstrates state-of-the-art results on large-scale histopathology image datasets and can be applied to various applications in medical imaging beyond histopathology images such as MRI, CT, and ultrasound images.",0
"This paper presents a novel algorithm for weakly-supervised learning to identify cancerous regions in histopathology images. Our approach utilizes a multiple instance learning framework, with a new formulation called deep weak supervision (DWS), and incorporates constraints to assist with the learning process. Our algorithm offers three significant contributions: (1) an end-to-end learning system that leverages fully convolutional networks (FCN) for image-to-image weakly-supervised learning, (2) a DWS formulation that allows for multi-scale learning under weak supervision within FCNs, and (3) the introduction of constraints about positive instances to obtain additional weakly-supervised information, which significantly enhances the learning process. Our algorithm, referred to as DWS-MIL, is simple to implement and can be trained efficiently. We demonstrate that our system outperforms existing methods on large-scale histopathology image datasets and can be extended to other medical imaging applications such as MRI, CT, and ultrasound images.",1
"Topic models (e.g., pLSA, LDA, sLDA) have been widely used for segmenting imagery. However, these models are confined to crisp segmentation, forcing a visual word (i.e., an image patch) to belong to one and only one topic. Yet, there are many images in which some regions cannot be assigned a crisp categorical label (e.g., transition regions between a foggy sky and the ground or between sand and water at a beach). In these cases, a visual word is best represented with partial memberships across multiple topics. To address this, we present a partial membership latent Dirichlet allocation (PM-LDA) model and an associated parameter estimation algorithm. This model can be useful for imagery where a visual word may be a mixture of multiple topics. Experimental results on visual and sonar imagery show that PM-LDA can produce both crisp and soft semantic image segmentations; a capability previous topic modeling methods do not have.",0
"The use of topic models, such as pLSA, LDA, and sLDA, has become widespread in the segmentation of imagery. However, these models are limited to providing precise segmentation, where an image patch is assigned to only one topic. This is problematic for images with areas that do not fit neatly into a single category, such as regions between a foggy sky and the ground or sand and water at a beach. To overcome this limitation, we propose a new model called partial membership latent Dirichlet allocation (PM-LDA) and offer an associated parameter estimation algorithm. PM-LDA allows for partial membership across multiple topics, making it useful for imagery where visual words may be a mixture of topics. Our experimental results on visual and sonar imagery demonstrate that PM-LDA can produce both precise and soft semantic image segmentations, a capability that previous topic modeling methods lack.",1
"Electron microscopic connectomics is an ambitious research direction with the goal of studying comprehensive brain connectivity maps by using high-throughput, nano-scale microscopy. One of the main challenges in connectomics research is developing scalable image analysis algorithms that require minimal user intervention. Recently, deep learning has drawn much attention in computer vision because of its exceptional performance in image classification tasks. For this reason, its application to connectomic analyses holds great promise, as well. In this paper, we introduce a novel deep neural network architecture, FusionNet, for the automatic segmentation of neuronal structures in connectomics data. FusionNet leverages the latest advances in machine learning, such as semantic segmentation and residual neural networks, with the novel introduction of summation-based skip connections to allow a much deeper network architecture for a more accurate segmentation. We demonstrate the performance of the proposed method by comparing it with state-of-the-art electron microscopy (EM) segmentation methods from the ISBI EM segmentation challenge. We also show the segmentation results on two different tasks including cell membrane and cell body segmentation and a statistical analysis of cell morphology.",0
"The study of comprehensive brain connectivity maps using high-throughput, nano-scale microscopy is known as electron microscopic connectomics. A major challenge in connectomics research is developing scalable image analysis algorithms that require minimal user intervention. Deep learning has recently gained popularity in computer vision due to its exceptional performance in image classification tasks, making it a promising approach for connectomic analyses. This paper presents a new deep neural network architecture, called FusionNet, for automatically segmenting neuronal structures in connectomics data. FusionNet uses semantic segmentation and residual neural networks, combined with summation-based skip connections to allow a deeper network architecture for more accurate segmentation. The proposed method is compared to state-of-the-art electron microscopy (EM) segmentation methods from the ISBI EM segmentation challenge, and its performance is demonstrated through cell membrane and cell body segmentation, as well as statistical analysis of cell morphology.",1
"Sparse decomposition has been widely used for different applications, such as source separation, image classification and image denoising. This paper presents a new algorithm for segmentation of an image into background and foreground text and graphics using sparse decomposition. First, the background is represented using a suitable smooth model, which is a linear combination of a few smoothly varying basis functions, and the foreground text and graphics are modeled as a sparse component overlaid on the smooth background. Then the background and foreground are separated using a sparse decomposition framework and imposing some prior information, which promote the smoothness of background, and the sparsity and connectivity of foreground pixels. This algorithm has been tested on a dataset of images extracted from HEVC standard test sequences for screen content coding, and is shown to outperform prior methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu, and shape primitive extraction and coding algorithm.",0
"Sparse decomposition has found extensive applications in diverse fields such as image classification, source separation, and image denoising. In this paper, a new technique for segmenting images into foreground text and graphics and background is presented using sparse decomposition. Firstly, the background is represented by a smooth model, which is a combination of a few smoothly varying basis functions. Further, the foreground text and graphics are modeled as a sparse component superimposed over the smooth background. Subsequently, the sparse decomposition framework is employed to separate the background and foreground by enforcing prior information that promotes the smoothness of the background and the sparsity and connectivity of the foreground pixels. The algorithm's efficacy is evaluated on a dataset of images extracted from HEVC standard test sequences for screen content coding, and it is demonstrated to outperform previous methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu, and shape primitive extraction and coding algorithm.",1
"Image segmentation is a popular area of research in computer vision that has many applications in automated image processing. A recent technique called piecewise flat embeddings (PFE) has been proposed for use in image segmentation; PFE transforms image pixel data into a lower dimensional representation where similar pixels are pulled close together and dissimilar pixels are pushed apart. This technique has shown promising results, but its original formulation is not computationally feasible for large images. We propose two improvements to the algorithm for computing PFE: first, we reformulate portions of the algorithm to enable various linear algebra operations to be performed in parallel; second, we propose utilizing an iterative linear solver (preconditioned conjugate gradient) to quickly solve a linear least-squares problem that occurs in the inner loop of a nested iteration. With these two computational improvements, we show on a publicly available image database that PFE can be sped up by an order of magnitude without sacrificing segmentation performance. Our results make this technique more practical for use on large data sets, not only for image segmentation, but for general data clustering problems.",0
"Computer vision research has shown great interest in image segmentation due to its numerous applications in automated image processing. A recent approach, known as piecewise flat embeddings (PFE), has been proposed for image segmentation. PFE transforms pixel data into a lower dimensional representation, where similar pixels are clustered together and dissimilar ones are separated. Although PFE has shown promising results, its original formulation is not suitable for processing large images. To address this issue, we propose two enhancements to the PFE algorithm. Firstly, we have restructured certain portions of the algorithm to allow for parallel linear algebra operations. Secondly, we suggest using an iterative linear solver (preconditioned conjugate gradient) to solve a linear least-squares problem that arises in the inner loop of a nested iteration. Our computational improvements have resulted in a tenfold acceleration of PFE while maintaining segmentation performance. This makes PFE a more practical technique for use on large datasets, not only for image segmentation, but also for general data clustering purposes. Our findings have significant implications for the field of computer vision and automated image processing.",1
"Since 2006, deep learning (DL) has become a rapidly growing research direction, redefining state-of-the-art performances in a wide range of areas such as object recognition, image segmentation, speech recognition and machine translation. In modern manufacturing systems, data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the Internet. Meanwhile, deep learning provides useful tools for processing and analyzing these big machinery data. The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring. After the brief introduction of deep learning techniques, the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines (DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). Finally, some new trends of DL-based machine health monitoring methods are discussed.",0
"Starting from 2006, deep learning (DL) has rapidly grown into a significant research field, revolutionizing state-of-the-art performances in various domains such as object recognition, speech recognition, image segmentation, and machine translation. In the present times, data-driven machine health monitoring is gaining immense popularity in modern manufacturing systems due to the widespread deployment of low-cost sensors that are connected to the Internet. Meanwhile, deep learning offers valuable tools for the processing and analysis of large machinery data. This paper aims to provide a comprehensive overview of the emerging research work of deep learning on machine health monitoring. The study begins with a brief introduction of deep learning techniques, followed by a review of their applications in machine health monitoring systems. The applications are mainly discussed through various aspects such as Auto-encoder (AE) and its variants, Restricted Boltzmann Machines, Deep Belief Network (DBN), Deep Boltzmann Machines (DBM), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN). Finally, the study concludes by discussing the new trends of DL-based machine health monitoring methods.",1
"Image segmentation has many applications which range from machine learning to medical diagnosis. In this paper, we propose a framework for the segmentation of images based on super-pixels and algorithms for community identification in graphs. The super-pixel pre-segmentation step reduces the number of nodes in the graph, rendering the method the ability to process large images. Moreover, community detection algorithms provide more accurate segmentation than traditional approaches, such as those based on spectral graph partition. We also compare our method with two algorithms: a) the graph-based approach by Felzenszwalb and Huttenlocher and b) the contour-based method by Arbelaez. Results have shown that our method provides more precise segmentation and is faster than both of them.",0
"Various fields, including machine learning and medical diagnosis, utilize image segmentation. This research introduces a framework for image segmentation that utilizes super-pixels and algorithms for graph community identification. The implementation of super-pixel pre-segmentation reduces the graph's node count, allowing for efficient processing of large images. Additionally, community detection algorithms offer improved segmentation accuracy compared to traditional methods, such as spectral graph partition. The proposed method is compared to two algorithms: Felzenszwalb and Huttenlocher's graph-based approach and Arbelaez's contour-based method. The results demonstrate that the proposed method is more precise and efficient than both.",1
"This paper presents an efficient approach to image segmentation that approximates the piecewise-smooth (PS) functional in [12] with explicit solutions. By rendering some rational constraints on the initial conditions and the final solutions of the PS functional, we propose two novel formulations which can be approximated to be the explicit solutions of the evolution partial differential equations (PDEs) of the PS model, in which only one PDE needs to be solved efficiently. Furthermore, an energy term that regularizes the level set function to be a signed distance function is incorporated into our evolution formulation, and the time-consuming re-initialization is avoided. Experiments on synthetic and real images show that our method is more efficient than both the PS model and the local binary fitting (LBF) model [4], while having similar segmentation accuracy as the LBF model.",0
"In this paper, we present a new method for image segmentation that involves approximating the piecewise-smooth (PS) functional described in [12] using explicit solutions. By imposing certain rational constraints on the initial and final conditions of the PS functional, we propose two innovative formulations that can be approximated as explicit solutions to the evolution partial differential equations (PDEs) of the PS model. This means that only one PDE needs to be solved efficiently. Additionally, we incorporate an energy term into our evolution formulation that regulates the level set function to be a signed distance function, thereby eliminating the need for time-consuming re-initialization. Our experiments on both synthetic and real images demonstrate that our approach is more efficient than both the PS model and the local binary fitting (LBF) model [4], while achieving similar segmentation accuracy to the LBF model.",1
"Semantic image segmentation is an essential component of modern autonomous driving systems, as an accurate understanding of the surrounding scene is crucial to navigation and action planning. Current state-of-the-art approaches in semantic image segmentation rely on pre-trained networks that were initially developed for classifying images as a whole. While these networks exhibit outstanding recognition performance (i.e., what is visible?), they lack localization accuracy (i.e., where precisely is something located?). Therefore, additional processing steps have to be performed in order to obtain pixel-accurate segmentation masks at the full image resolution. To alleviate this problem we propose a novel ResNet-like architecture that exhibits strong localization and recognition performance. We combine multi-scale context with pixel-level accuracy by using two processing streams within our network: One stream carries information at the full image resolution, enabling precise adherence to segment boundaries. The other stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals. Without additional processing steps and without pre-training, our approach achieves an intersection-over-union score of 71.8% on the Cityscapes dataset.",0
"Autonomous driving systems rely on semantic image segmentation to accurately understand the environment and plan actions. However, current approaches using pre-trained networks for image classification lack precise localization accuracy. To address this, we propose a novel ResNet-like architecture that combines multi-scale context and pixel-level accuracy through two processing streams. One stream maintains full image resolution for precise adherence to segment boundaries, while the other stream undergoes pooling operations for robust recognition features. Our approach achieves an intersection-over-union score of 71.8% on the Cityscapes dataset without pre-training or additional processing steps.",1
"Boundary incompleteness raises great challenges to automatic prostate segmentation in ultrasound images. Shape prior can provide strong guidance in estimating the missing boundary, but traditional shape models often suffer from hand-crafted descriptors and local information loss in the fitting procedure. In this paper, we attempt to address those issues with a novel framework. The proposed framework can seamlessly integrate feature extraction and shape prior exploring, and estimate the complete boundary with a sequential manner. Our framework is composed of three key modules. Firstly, we serialize the static 2D prostate ultrasound images into dynamic sequences and then predict prostate shapes by sequentially exploring shape priors. Intuitively, we propose to learn the shape prior with the biologically plausible Recurrent Neural Networks (RNNs). This module is corroborated to be effective in dealing with the boundary incompleteness. Secondly, to alleviate the bias caused by different serialization manners, we propose a multi-view fusion strategy to merge shape predictions obtained from different perspectives. Thirdly, we further implant the RNN core into a multiscale Auto-Context scheme to successively refine the details of the shape prediction map. With extensive validation on challenging prostate ultrasound images, our framework bridges severe boundary incompleteness and achieves the best performance in prostate boundary delineation when compared with several advanced methods. Additionally, our approach is general and can be extended to other medical image segmentation tasks, where boundary incompleteness is one of the main challenges.",0
"Automatic segmentation of the prostate in ultrasound images is a difficult task due to boundary incompleteness. While traditional shape models can provide guidance in estimating the missing boundary, they are limited by hand-crafted descriptors and loss of local information during fitting. To address these issues, we propose a novel framework that seamlessly integrates feature extraction and shape prior exploration, allowing for sequential estimation of the complete boundary. Our framework comprises three modules: (1) serialization of static 2D ultrasound images into dynamic sequences, with shape prior learning using Recurrent Neural Networks (RNNs), (2) multi-view fusion to merge shape predictions obtained from different perspectives and alleviate bias, and (3) RNN core implantation into a multiscale Auto-Context scheme for refining shape prediction map details. Our approach achieves the best performance in prostate boundary delineation compared to several advanced methods and is generalizable to other medical image segmentation tasks where boundary incompleteness is a challenge.",1
"We propose a new modeling approach that is a generalization of generative and discriminative models. The core idea is to use an implicit parameterization of a joint probability distribution by specifying only the conditional distributions. The proposed scheme combines the advantages of both worlds -- it can use powerful complex discriminative models as its parts, having at the same time better generalization capabilities. We thoroughly evaluate the proposed method for a simple classification task with artificial data and illustrate its advantages for real-word scenarios on a semantic image segmentation problem.",0
"Our proposition is a novel modeling technique that expands on both generative and discriminative models. The fundamental concept is to utilize an implicit parameterization of a joint probability distribution by only specifying the conditional distributions. This approach amalgamates the benefits of both models, as it can integrate robust and intricate discriminative models while maintaining superior generalization capabilities. We conducted a comprehensive assessment of the proposed approach on a basic classification task using synthetic data and demonstrated its advantages in real-world situations with a semantic image segmentation problem.",1
"Three-dimensional Ultrasound image segmentation methods are surveyed in this paper. The focus of this report is to investigate applications of these techniques and a review of the original ideas and concepts. Although many two-dimensional image segmentation in the literature have been considered as a three-dimensional approach by mistake but we review them as a three-dimensional technique. We select the studies that have addressed the problem of medical three-dimensional Ultrasound image segmentation utilizing their proposed techniques. The evaluation methods and comparison between them are presented and tabulated in terms of evaluation techniques, interactivity, and robustness.",0
"This paper presents a survey of methods for segmenting three-dimensional Ultrasound images. The goal of this report is to explore the uses of these techniques and examine their underlying ideas and concepts. While some studies in the literature mistakenly classify two-dimensional image segmentation as three-dimensional, we analyze them as three-dimensional techniques. We focus on research that addresses the challenge of medical three-dimensional Ultrasound image segmentation, and evaluate and compare different approaches based on their evaluation techniques, interactivity, and robustness. Our findings are presented in a tabular format.",1
"In this work we propose a structured prediction technique that combines the virtues of Gaussian Conditional Random Fields (G-CRF) with Deep Learning: (a) our structured prediction task has a unique global optimum that is obtained exactly from the solution of a linear system (b) the gradients of our model parameters are analytically computed using closed form expressions, in contrast to the memory-demanding contemporary deep structured prediction approaches that rely on back-propagation-through-time, (c) our pairwise terms do not have to be simple hand-crafted expressions, as in the line of works building on the DenseCRF, but can rather be `discovered' from data through deep architectures, and (d) out system can trained in an end-to-end manner. Building on standard tools from numerical analysis we develop very efficient algorithms for inference and learning, as well as a customized technique adapted to the semantic segmentation task. This efficiency allows us to explore more sophisticated architectures for structured prediction in deep learning: we introduce multi-resolution architectures to couple information across scales in a joint optimization framework, yielding systematic improvements. We demonstrate the utility of our approach on the challenging VOC PASCAL 2012 image segmentation benchmark, showing substantial improvements over strong baselines. We make all of our code and experiments available at {https://github.com/siddharthachandra/gcrf}",0
"The article proposes a method for structured prediction that combines Gaussian Conditional Random Fields (G-CRF) with Deep Learning. Our proposed technique has several advantages: (a) it has a unique global optimum obtained precisely from a linear system solution, (b) the model parameter gradients are analytically computed using closed-form expressions instead of memory-intensive back-propagation-through-time, (c) our pairwise terms can be discovered from data through deep architectures, unlike the simple hand-crafted expressions in DenseCRF, and (d) our system can be trained end-to-end. We use numerical analysis tools to develop efficient algorithms for inference and learning, including a customized technique for semantic segmentation. Our efficiency allows us to explore more sophisticated architectures for structured prediction in deep learning, such as multi-resolution architectures that couple information across scales in a joint optimization framework, resulting in significant improvements. Our approach is demonstrated on the challenging VOC PASCAL 2012 image segmentation benchmark, where we show substantial improvements over strong baselines. All code and experiments are available at {https://github.com/siddharthachandra/gcrf}.",1
"Size uniformity is one of the main criteria of superpixel methods. But size uniformity rarely conforms to the varying content of an image. The chosen size of the superpixels therefore represents a compromise - how to obtain the fewest superpixels without losing too much important detail. We propose that a more appropriate criterion for creating image segments is information uniformity. We introduce a novel method for segmenting an image based on this criterion. Since information is a natural way of measuring image complexity, our proposed algorithm leads to image segments that are smaller and denser in areas of high complexity and larger in homogeneous regions, thus simplifying the image while preserving its details. Our algorithm is simple and requires just one input parameter - a threshold on the information content. On segmentation comparison benchmarks it proves to be superior to the state-of-the-art. In addition, our method is computationally very efficient, approaching real-time performance, and is easily extensible to three-dimensional image stacks and video volumes.",0
"Superpixel methods typically prioritize size uniformity as a key factor. However, this approach often fails to account for the varying content of an image. As a result, choosing the size of superpixels becomes a compromise between minimizing their quantity while retaining important details. We suggest that a more appropriate criterion for creating image segments is information uniformity. To that end, we propose a novel method for segmenting images based on this criterion. Our algorithm simplifies the image by producing smaller and denser segments in areas of high complexity and larger segments in homogeneous regions while preserving its details. This approach is both simple and efficient, requiring only one input parameter, a threshold on the information content. On segmentation comparison benchmarks, our method outperforms the state-of-the-art. Furthermore, our algorithm can easily be extended to three-dimensional image stacks and video volumes, and its computational efficiency allows for real-time performance.",1
"Semantic segmentation has recently witnessed major progress, where fully convolutional neural networks have shown to perform well. However, most of the previous work focused on improving single image segmentation. To our knowledge, no prior work has made use of temporal video information in a recurrent network. In this paper, we introduce a novel approach to implicitly utilize temporal data in videos for online semantic segmentation. The method relies on a fully convolutional network that is embedded into a gated recurrent architecture. This design receives a sequence of consecutive video frames and outputs the segmentation of the last frame. Convolutional gated recurrent networks are used for the recurrent part to preserve spatial connectivities in the image. Our proposed method can be applied in both online and batch segmentation. This architecture is tested for both binary and semantic video segmentation tasks. Experiments are conducted on the recent benchmarks in SegTrack V2, Davis, CityScapes, and Synthia. Using recurrent fully convolutional networks improved the baseline network performance in all of our experiments. Namely, 5% and 3% improvement of F-measure in SegTrack2 and Davis respectively, 5.7% improvement in mean IoU in Synthia and 3.5% improvement in categorical mean IoU in CityScapes. The performance of the RFCN network depends on its baseline fully convolutional network. Thus RFCN architecture can be seen as a method to improve its baseline segmentation network by exploiting spatiotemporal information in videos.",0
"Recently, there has been significant progress in semantic segmentation, particularly with fully convolutional neural networks. However, previous work has mainly focused on enhancing single image segmentation and has not utilized temporal video information in a recurrent network. In this study, we present a new approach for online semantic segmentation that implicitly makes use of temporal data in videos. Our method involves a fully convolutional network embedded into a gated recurrent architecture that receives a sequence of consecutive video frames and outputs the segmentation of the last frame. Convolutional gated recurrent networks are used for the recurrent part to preserve spatial connectivities in the image. Our proposed method can be applied in both online and batch segmentation and has been tested on binary and semantic video segmentation tasks using recent benchmarks in SegTrack V2, Davis, CityScapes, and Synthia. Results showed that using recurrent fully convolutional networks improved the baseline network performance in all our experiments, with improvements in F-measure, mean IoU, and categorical mean IoU. Therefore, the RFCN architecture can be seen as a means of improving the baseline segmentation network by exploiting spatiotemporal information in videos.",1
In this paper we introduce a novel method for segmentation that can benefit from general semantics of Convolutional Neural Network (CNN). Our segmentation proposes visually and semantically coherent image segments. We use binary encoding of CNN features to overcome the difficulty of the clustering on the high-dimensional CNN feature space. These binary encoding can be embedded into the CNN as an extra layer at the end of the network. This results in real-time segmentation. To the best of our knowledge our method is the first attempt on general semantic image segmentation using CNN. All the previous papers were limited to few number of category of the images (e.g. PASCAL VOC). Experiments show that our segmentation algorithm outperform the state-of-the-art non-semantic segmentation methods by a large margin.,0
"This paper presents an innovative approach to segmentation that utilizes the general semantics of Convolutional Neural Network (CNN) for visually and semantically coherent image segments. By employing binary encoding of CNN features, we overcome the challenge of clustering in the high-dimensional CNN feature space. These binary encodings can be integrated into the CNN as an additional layer at the end of the network, resulting in real-time segmentation. Our method represents the first attempt at general semantic image segmentation using CNN, as previous papers were limited to a small number of image categories such as PASCAL VOC. Our experiments demonstrate that our segmentation algorithm significantly outperforms the state-of-the-art non-semantic segmentation methods.",1
"Deep residual networks have recently emerged as the state-of-the-art architecture in image segmentation and object detection. In this paper, we propose new image features (called ResFeats) extracted from the last convolutional layer of deep residual networks pre-trained on ImageNet. We propose to use ResFeats for diverse image classification tasks namely, object classification, scene classification and coral classification and show that ResFeats consistently perform better than their CNN counterparts on these classification tasks. Since the ResFeats are large feature vectors, we propose to use PCA for dimensionality reduction. Experimental results are provided to show the effectiveness of ResFeats with state-of-the-art classification accuracies on Caltech-101, Caltech-256 and MLC datasets and a significant performance improvement on MIT-67 dataset compared to the widely used CNN features.",0
"Recently, deep residual networks have become the leading architecture for image segmentation and object detection. Our paper introduces ResFeats, a novel set of image features extracted from the final convolutional layer of deep residual networks trained on ImageNet. We propose using ResFeats for various image classification tasks, such as object, scene, and coral classification, and demonstrate that they outperform their CNN counterparts consistently. Since ResFeats are large feature vectors, we suggest using PCA to reduce their dimensionality. We provide experimental results that show ResFeats' effectiveness, with state-of-the-art classification accuracies on datasets like Caltech-101, Caltech-256, and MLC, and a significant performance improvement on MIT-67 dataset compared to widely used CNN features.",1
"Unsupervised evaluation of segmentation quality is a crucial step in image segmentation applications. Previous unsupervised evaluation methods usually lacked the adaptability to multi-scale segmentation. A scale-constrained evaluation method that evaluates segmentation quality according to the specified target scale is proposed in this paper. First, regional saliency and merging cost are employed to describe intra-region homogeneity and inter-region heterogeneity, respectively. Subsequently, both of them are standardized into equivalent spectral distances of a predefined region. Finally, by analyzing the relationship between image characteristics and segmentation quality, we establish the evaluation model. Experimental results show that the proposed method outperforms four commonly used unsupervised methods in multi-scale evaluation tasks.",0
"The evaluation of segmentation quality without supervision is a vital aspect of image segmentation applications. However, previous unsupervised evaluation methods have not been able to adapt to multi-scale segmentation. This paper proposes a scale-constrained evaluation method that assesses segmentation quality based on a specified target scale. The method employs regional saliency and merging cost to describe intra-region homogeneity and inter-region heterogeneity, respectively. Both factors are standardized into equivalent spectral distances of a predefined region. By analyzing the correlation between image characteristics and segmentation quality, an evaluation model is established. Experimental results demonstrate that the proposed method outperforms four commonly used unsupervised methods in multi-scale evaluation tasks.",1
"Segmenting images of low quality or with missing data is a challenging problem. Integrating statistical prior information about the shapes to be segmented can improve the segmentation results significantly. Most shape-based segmentation algorithms optimize an energy functional and find a point estimate for the object to be segmented. This does not provide a measure of the degree of confidence in that result, neither does it provide a picture of other probable solutions based on the data and the priors. With a statistical view, addressing these issues would involve the problem of characterizing the posterior densities of the shapes of the objects to be segmented. For such characterization, we propose a Markov chain Monte Carlo (MCMC) sampling-based image segmentation algorithm that uses statistical shape priors. In addition to better characterization of the statistical structure of the problem, such an approach would also have the potential to address issues with getting stuck at local optima, suffered by existing shape-based segmentation methods. Our approach is able to characterize the posterior probability density in the space of shapes through its samples, and to return multiple solutions, potentially from different modes of a multimodal probability density, which would be encountered, e.g., in segmenting objects from multiple shape classes. We present promising results on a variety of data sets. We also provide an extension for segmenting shapes of objects with parts that can go through independent shape variations. This extension involves the use of local shape priors on object parts and provides robustness to limitations in shape training data size.",0
"Segmenting low quality or incomplete images presents a difficult challenge. However, incorporating statistical prior knowledge about the shapes being segmented can enhance the accuracy of the segmentation results. Most segmentation algorithms based on shape optimize an energy functional and provide a single estimate for the segmented object, without conveying the degree of confidence in the result or alternative solutions. To address these issues, we propose a Markov chain Monte Carlo (MCMC) sampling-based algorithm that utilizes statistical shape priors to characterize the posterior densities of the objects being segmented. Our approach can generate multiple solutions from different modes of a multimodal probability density, which is useful when segmenting objects from multiple shape classes. This approach also improves the robustness of the segmentation method by incorporating local shape priors on object parts. Our results on various datasets demonstrate the effectiveness of this approach.",1
"Spectral graph theory is well known and widely used in computer vision. In this paper, we analyze image segmentation algorithms that are based on spectral graph theory, e.g., normalized cut, and show that there is a natural connection between spectural graph theory based image segmentationand and edge preserving filtering. Based on this connection we show that the normalized cut algorithm is equivalent to repeated iterations of bilateral filtering. Then, using this equivalence we present and implement a fast normalized cut algorithm for image segmentation. Experiments show that our implementation can solve the original optimization problem in the normalized cut algorithm 10 to 100 times faster. Furthermore, we present a new algorithm called conditioned normalized cut for image segmentation that can easily incorporate color image patches and demonstrate how this segmentation problem can be solved with edge preserving filtering.",0
"In computer vision, spectral graph theory is a widely used approach. This study focuses on image segmentation algorithms that utilize spectral graph theory, particularly the normalized cut method. It is observed that there exists a natural correlation between image segmentation using spectral graph theory and edge preserving filtering. As a result, the study demonstrates that the normalized cut technique can be achieved through repetitive iterations of bilateral filtering. A swift implementation of this algorithm is presented, which can solve the original optimization issue 10 to 100 times faster. Additionally, a new algorithm known as conditioned normalized cut is introduced for image segmentation. This algorithm can effortlessly integrate color image patches and is solved using edge preserving filtering.",1
"State-of-the-art image segmentation algorithms generally consist of at least two successive and distinct computations: a boundary detection process that uses local image information to classify image locations as boundaries between objects, followed by a pixel grouping step such as watershed or connected components that clusters pixels into segments. Prior work has varied the complexity and approach employed in these two steps, including the incorporation of multi-layer neural networks to perform boundary prediction, and the use of global optimizations during pixel clustering. We propose a unified and end-to-end trainable machine learning approach, flood-filling networks, in which a recurrent 3d convolutional network directly produces individual segments from a raw image. The proposed approach robustly segments images with an unknown and variable number of objects as well as highly variable object sizes. We demonstrate the approach on a challenging 3d image segmentation task, connectomic reconstruction from volume electron microscopy data, on which flood-filling neural networks substantially improve accuracy over other state-of-the-art methods. The proposed approach can replace complex multi-step segmentation pipelines with a single neural network that is learned end-to-end.",0
"Typically, modern methods for image segmentation involve two distinct processes: boundary detection, which identifies object boundaries using local image data, and pixel grouping, which clusters pixels into segments using methods such as watershed or connected components. Previous research has used various approaches to these processes, including neural networks for boundary prediction and global optimizations during clustering. To address the limitations of these approaches, we propose a single, end-to-end trainable machine learning method called flood-filling networks. This approach uses a recurrent 3D convolutional network to directly produce segments from raw images, allowing for robust segmentation of images with varying object sizes and numbers. We demonstrate the effectiveness of our approach on a challenging task involving connectomic reconstruction from volume electron microscopy data, achieving superior accuracy compared to other state-of-the-art methods. By replacing complex multi-step segmentation pipelines with a single neural network, our approach offers a streamlined and efficient solution.",1
"Image segmentation is an important step in most visual tasks. While convolutional neural networks have shown to perform well on single image segmentation, to our knowledge, no study has been been done on leveraging recurrent gated architectures for video segmentation. Accordingly, we propose a novel method for online segmentation of video sequences that incorporates temporal data. The network is built from fully convolutional element and recurrent unit that works on a sliding window over the temporal data. We also introduce a novel convolutional gated recurrent unit that preserves the spatial information and reduces the parameters learned. Our method has the advantage that it can work in an online fashion instead of operating over the whole input batch of video frames. The network is tested on the change detection dataset, and proved to have 5.5\% improvement in F-measure over a plain fully convolutional network for per frame segmentation. It was also shown to have improvement of 1.4\% for the F-measure compared to our baseline network that we call FCN 12s.",0
"Most visual tasks require image segmentation, and while convolutional neural networks have been successful in single image segmentation, there has been no research on using recurrent gated architectures for video segmentation. Therefore, we propose a new approach for real-time segmentation of video sequences that incorporates temporal data. Our network is composed of a fully convolutional element and a recurrent unit that operates on a sliding window over the temporal data. Furthermore, we introduce a convolutional gated recurrent unit that preserves spatial information and reduces the number of learned parameters. Our method operates in real-time, unlike traditional batch processing, and we tested it on the change detection dataset. Our results showed a 5.5\% improvement in F-measure compared to a plain fully convolutional network, and a 1.4\% improvement compared to our baseline network (FCN 12s).",1
"We present an interactive approach to train a deep neural network pixel classifier for the segmentation of neuronal structures. An interactive training scheme reduces the extremely tedious manual annotation task that is typically required for deep networks to perform well on image segmentation problems. Our proposed method employs a feedback loop that captures sparse annotations using a graphical user interface, trains a deep neural network based on recent and past annotations, and displays the prediction output to users in almost real-time. Our implementation of the algorithm also allows multiple users to provide annotations in parallel and receive feedback from the same classifier. Quick feedback on classifier performance in an interactive setting enables users to identify and label examples that are more important than others for segmentation purposes. Our experiments show that an interactively-trained pixel classifier produces better region segmentation results on Electron Microscopy (EM) images than those generated by a network of the same architecture trained offline on exhaustive ground-truth labels.",0
"We introduce an interactive method for training a deep neural network pixel classifier to segment neuronal structures. This approach reduces the laborious task of manual annotation typically required for deep networks to perform well in image segmentation. Our method employs a feedback loop that captures sparse annotations via a graphical user interface, trains a deep neural network based on recent and past annotations, and displays the prediction output to users in real-time. Our algorithm also enables multiple users to annotate in parallel and receive feedback from the same classifier. The interactive setting provides users with quick feedback on classifier performance, allowing them to identify and label important examples for segmentation. Our experiments show that an interactively-trained pixel classifier outperforms a network of the same architecture trained offline on exhaustive ground-truth labels in segmenting Electron Microscopy (EM) images.",1
"Active contour models based on partial differential equations have proved successful in image segmentation, yet the study of their geometric formulation on arbitrary geometric graphs is still at an early stage. In this paper, we introduce geometric approximations of gradient and curvature, which are used in the geodesic active contour model. We prove convergence in probability of our gradient approximation to the true gradient value and derive an asymptotic upper bound for the error of this approximation for the class of random geometric graphs. Two different approaches for the approximation of curvature are presented and both are also proved to converge in probability in the case of random geometric graphs. We propose neighborhood-based filtering on graphs to improve the accuracy of the aforementioned approximations and define two variants of Gaussian smoothing on graphs which include normalization in order to adapt to graph non-uniformities. The performance of our active contour framework on graphs is demonstrated in the segmentation of regular images and geographical data defined on arbitrary graphs.",0
"Although active contour models based on partial differential equations have been successful in image segmentation, research on their geometric formulation on arbitrary geometric graphs is in its early stages. This study introduces geometric approximations of gradient and curvature for the geodesic active contour model. The paper proves the convergence in probability of the gradient approximation to the true gradient value and derives an asymptotic upper bound for the error of this approximation for random geometric graphs. The study presents two different approaches for the approximation of curvature, proving convergence in probability for both in the case of random geometric graphs. The paper proposes neighborhood-based filtering on graphs to improve the accuracy of the approximations and defines two variants of Gaussian smoothing on graphs that include normalization to adapt to graph non-uniformities. Finally, the active contour framework's efficacy on graphs is demonstrated in the segmentation of regular images and geographical data defined on arbitrary graphs.",1
"A novel model for image segmentation is proposed, which is inspired by the carrier immigration mechanism in physical P-N junction. The carrier diffusing and drifting are simulated in the proposed model, which imitates the physical self-balancing mechanism in P-N junction. The effect of virtual carrier immigration in digital images is analyzed and studied by experiments on test images and real world images. The sign distribution of net carrier at the model's balance state is exploited for region segmentation. The experimental results for both test images and real-world images demonstrate self-adaptive and meaningful gathering of pixels to suitable regions, which prove the effectiveness of the proposed method for image region segmentation.",0
"A new approach to segmenting images is introduced, drawing inspiration from the carrier immigration mechanism observed in physical P-N junctions. To simulate this mechanism, the model employs carrier diffusion and drifting, which replicate the self-balancing behavior found in P-N junctions. The impact of virtual carrier immigration on digital images is explored through experiments on test and real-world images. The model's balanced state's net carrier sign distribution is leveraged to segment regions. The results of experiments on both test and real-world images demonstrate the model's ability to adapt and effectively group pixels into appropriate regions, indicating its efficacy for image segmentation.",1
"This paper addresses the problem of natural image segmentation by extracting information from a multi-layer array which is constructed based on color, gradient, and statistical properties of the local neighborhoods in an image. A Gaussian Mixture Model (GMM) is used to improve the effectiveness of local spectral histogram features. Grouping these features leads to forming a rough initial over-segmented layer which contains coherent regions of pixels. The regions are merged by using two proposed functions for calculating the distance between two neighboring regions and making decisions about their merging. Extensive experiments are performed on the Berkeley Segmentation Dataset to evaluate the performance of our proposed method and compare the results with the recent state-of-the-art methods. The experimental results indicate that our method achieves higher level of accuracy for natural images compared to recent methods.",0
"This article tackles the issue of segmenting natural images. It does so by utilizing a multi-layer array that takes into account color, gradient, and statistical attributes of local neighborhoods within an image. To improve the effectiveness of local spectral histogram features, a Gaussian Mixture Model (GMM) is employed. This helps to create a preliminary over-segmented layer that contains coherent regions of pixels. The merging of these regions is accomplished by using two functions that calculate the distance between neighboring regions and make decisions about their merging. Through extensive experimentation on the Berkeley Segmentation Dataset, our method is evaluated and compared with recent state-of-the-art methods. The results demonstrate that our approach achieves a higher level of accuracy for natural images than recent methods.",1
"We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance.   SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.",0
"Introducing SegNet, a new and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation. This segmentation engine consists of an encoder network, a corresponding decoder network, and a pixel-wise classification layer. The encoder network architecture is identical to the 13 convolutional layers found in the VGG16 network. The decoder network is responsible for mapping low-resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. What sets SegNet apart is the way in which the decoder upsamples its lower-resolution input feature maps. By using pooling indices computed in the max-pooling step of the corresponding encoder, the decoder performs non-linear upsampling, which eliminates the need for learning to upsample. The upsampled maps are sparse and convolved with trainable filters to produce dense feature maps. We compared SegNet to other architectures, such as FCN, DeconvNet, and DeepLab-LargeFOV, and found a memory versus accuracy trade-off in achieving good segmentation performance. SegNet is designed to be efficient in terms of memory and computational time during inference and has fewer trainable parameters than other competing architectures. We conducted a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We found that SegNet provides good performance with competitive inference time and efficiency memory-wise compared to other architectures. We also offer a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.",1
"In this paper, a novel model of 3D elastic mesh is presented for image segmentation. The model is inspired by stress and strain in physical elastic objects, while the repulsive force and elastic force in the model are defined slightly different from the physical force to suit the segmentation problem well. The self-balancing mechanism in the model guarantees the stability of the method in segmentation. The shape of the elastic mesh at balance state is used for region segmentation, in which the sign distribution of the points'z coordinate values is taken as the basis for segmentation. The effectiveness of the proposed method is proved by analysis and experimental results for both test images and real world images.",0
"This paper introduces a new 3D elastic mesh model that can be used for image segmentation. The model draws inspiration from the stress and strain present in physical elastic objects, though the repulsive and elastic forces are defined differently to better suit the segmentation problem. The model also incorporates a self-balancing mechanism that ensures the stability of the segmentation method. The shape of the elastic mesh in its balanced state is utilized for region segmentation, with the distribution of points' z coordinate values serving as the basis for segmentation. The proposed method's effectiveness is demonstrated through analysis and experimental results for both test and real world images.",1
"We investigate in this work a versatile convex framework for multiple image segmentation, relying on the regularized optimal mass transport theory. In this setting, several transport cost functions are considered and used to match statistical distributions of features. In practice, global multidimensional histograms are estimated from the segmented image regions, and are compared to referring models that are either fixed histograms given a priori, or directly inferred in the non-supervised case. The different convex problems studied are solved efficiently using primal-dual algorithms. The proposed approach is generic and enables multi-phase segmentation as well as co-segmentation of multiple images.",0
"In this study, we explore a flexible convex framework for segmenting multiple images by utilizing the regulated optimal mass transport theory. Various transport cost functions are examined and employed to match statistical feature distributions. To obtain global multidimensional histograms, the segmented image regions are estimated and compared to fixed histograms or inferred models in the non-supervised scenario. Primal-dual algorithms are utilized to efficiently solve the different convex problems studied. The proposed method is general, allowing for multi-phase segmentation and co-segmentation of multiple images.",1
"Segmentation is a fundamental task for extracting semantically meaningful regions from an image. The goal of segmentation algorithms is to accurately assign object labels to each image location. However, image-noise, shortcomings of algorithms, and image ambiguities cause uncertainty in label assignment. Estimating the uncertainty in label assignment is important in multiple application domains, such as segmenting tumors from medical images for radiation treatment planning. One way to estimate these uncertainties is through the computation of posteriors of Bayesian models, which is computationally prohibitive for many practical applications. On the other hand, most computationally efficient methods fail to estimate label uncertainty. We therefore propose in this paper the Active Mean Fields (AMF) approach, a technique based on Bayesian modeling that uses a mean-field approximation to efficiently compute a segmentation and its corresponding uncertainty. Based on a variational formulation, the resulting convex model combines any label-likelihood measure with a prior on the length of the segmentation boundary. A specific implementation of that model is the Chan-Vese segmentation model (CV), in which the binary segmentation task is defined by a Gaussian likelihood and a prior regularizing the length of the segmentation boundary. Furthermore, the Euler-Lagrange equations derived from the AMF model are equivalent to those of the popular Rudin-Osher-Fatemi (ROF) model for image denoising. Solutions to the AMF model can thus be implemented by directly utilizing highly-efficient ROF solvers on log-likelihood ratio fields. We qualitatively assess the approach on synthetic data as well as on real natural and medical images. For a quantitative evaluation, we apply our approach to the icgbench dataset.",0
"The extraction of semantically meaningful regions from an image is a crucial task known as segmentation. The main objective of segmentation algorithms is to assign accurate object labels to each image location; however, uncertainties in label assignment may arise due to image-noise, algorithmic limitations, and image ambiguities. In various fields, such as radiation treatment planning, it is essential to estimate this uncertainty. Although computing posteriors of Bayesian models is a way to achieve this, it is not feasible for most practical applications. In contrast, most computationally efficient methods fail to estimate label uncertainty. This paper proposes the Active Mean Fields (AMF) approach, which uses a mean-field approximation to efficiently compute a segmentation and its corresponding uncertainty. This technique is based on a variational formulation and combines any label-likelihood measure with a prior on the length of the segmentation boundary. The Chan-Vese segmentation model is a specific implementation of this model, which defines the binary segmentation task using a Gaussian likelihood and a prior on the length of the segmentation boundary. The Euler-Lagrange equations derived from the AMF model are equivalent to those of the popular Rudin-Osher-Fatemi (ROF) model for image denoising. Thus, solutions to the AMF model can be implemented using highly-efficient ROF solvers on log-likelihood ratio fields. The proposed approach is qualitatively assessed on synthetic data and real natural and medical images. To quantitatively evaluate the approach, it is applied to the icgbench dataset.",1
In this paper we introduce a novel method for general semantic segmentation that can benefit from general semantics of Convolutional Neural Network (CNN). Our segmentation proposes visually and semantically coherent image segments. We use binary encoding of CNN features to overcome the difficulty of the clustering on the high-dimensional CNN feature space. These binary codes are very robust against noise and non-semantic changes in the image. These binary encoding can be embedded into the CNN as an extra layer at the end of the network. This results in real-time segmentation. To the best of our knowledge our method is the first attempt on general semantic image segmentation using CNN. All the previous papers were limited to few number of category of the images (e.g. PASCAL VOC). Experiments show that our segmentation algorithm outperform the state-of-the-art non-semantic segmentation methods by large margin.,0
"A new technique for general semantic segmentation is presented in this paper, which leverages the general semantics of Convolutional Neural Network (CNN). Our segmentation approach generates visually and semantically consistent image segments. To address the challenge of clustering in the high-dimensional CNN feature space, we employ binary encoding of CNN features, which is highly resilient to noise and non-semantic changes in the image. These binary codes can be incorporated into the CNN as an additional layer, resulting in real-time segmentation. To the best of our knowledge, this is the first effort to achieve general semantic image segmentation using CNN, as prior research was limited to only a few image categories (e.g. PASCAL VOC). Our experimental results demonstrate that our segmentation algorithm significantly outperforms state-of-the-art non-semantic segmentation methods.",1
"Deep Learning is considered to be a quite young in the area of machine learning research, found its effectiveness in dealing complex yet high dimensional dataset that includes but limited to images, text and speech etc. with multiple levels of representation and abstraction. As there are a plethora of research on these datasets by various researchers , a win over them needs lots of attention. Careful setting of Deep learning parameters is of paramount importance in order to avoid the overfitting unlike conventional methods with limited parameter settings. Deep Convolutional neural network (DCNN) with multiple layers of compositions and appropriate settings might be is an efficient machine learning method that can outperform the conventional methods in a great way. However, due to its slow adoption in learning, there are also always a chance of overfitting during feature selection process, which can be addressed by employing a regularization method called dropout. Fast Random Forest (FRF) is a powerful ensemble classifier especially when the datasets are noisy and when the number of attributes is large in comparison to the number of instances, as is the case of Bioinformatics datasets. Several publicly available Bioinformatics dataset, Handwritten digits recognition and Image segmentation dataset are considered for evaluation of the proposed approach. The excellent performance obtained by the proposed DCNN based feature selection with FRF classifier on high dimensional datasets makes it a fast and accurate classifier in comparison the state-of-the-art.",0
"Deep Learning, a relatively new field in machine learning research, has proven its effectiveness in handling complex and highly dimensional datasets such as images, text, and speech. As multiple researchers have conducted extensive studies on these datasets, achieving success in this area requires great attention. It is crucial to carefully set the parameters of Deep Learning to avoid overfitting, which is not an issue in conventional methods with limited parameter settings. Deep Convolutional Neural Networks (DCNN) with multiple layers and appropriate settings have shown great potential to outperform conventional methods. However, slow adoption in learning may lead to overfitting during the feature selection process, which can be addressed by using dropout regularization. Fast Random Forest (FRF) is a powerful ensemble classifier that is particularly suitable for noisy datasets with a high number of attributes. The proposed approach has been evaluated on several publicly available Bioinformatics datasets, Handwritten Digits Recognition, and Image Segmentation datasets. The proposed DCNN-based feature selection with FRF classifier has achieved excellent performance on high dimensional datasets, making it a fast and accurate classifier compared to the state-of-the-art.",1
"Image segmentation is a fundamental problem in computational vision and medical imaging. Designing a generic, automated method that works for various objects and imaging modalities is a formidable task. Instead of proposing a new specific segmentation algorithm, we present a general design principle on how to integrate user interactions from the perspective of feedback control theory. Impulsive control and Lyapunov stability analysis are employed to design and analyze an interactive segmentation system. Then stabilization conditions are derived to guide algorithm design. Finally, the effectiveness and robustness of proposed method are demonstrated.",0
"The segmentation of images is a critical issue in medical imaging and computational vision. Developing a universal, automated technique that applies to different imaging modalities and objects is an incredibly challenging undertaking. Rather than introducing a novel segmentation algorithm, we offer a broad framework for incorporating user input using feedback control theory. The interactive segmentation system is created and examined utilizing impulsive control and Lyapunov stability analysis. The algorithm design is guided by the resulting stabilization conditions. Ultimately, we demonstrate the efficiency and resilience of the suggested approach.",1
"We introduce a new machine learning approach for image segmentation that uses a neural network to model the conditional energy of a segmentation given an image. Our approach, combinatorial energy learning for image segmentation (CELIS) places a particular emphasis on modeling the inherent combinatorial nature of dense image segmentation problems. We propose efficient algorithms for learning deep neural networks to model the energy function, and for local optimization of this energy in the space of supervoxel agglomerations. We extensively evaluate our method on a publicly available 3-D microscopy dataset with 25 billion voxels of ground truth data. On an 11 billion voxel test set, we find that our method improves volumetric reconstruction accuracy by more than 20% as compared to two state-of-the-art baseline methods: graph-based segmentation of the output of a 3-D convolutional neural network trained to predict boundaries, as well as a random forest classifier trained to agglomerate supervoxels that were generated by a 3-D convolutional neural network.",0
"Our team has developed a novel approach to image segmentation utilizing machine learning. By using a neural network to model the conditional energy of a segmentation based on an image, our approach - Combinatorial Energy Learning for Image Segmentation (CELIS) - places a strong focus on modeling the combinatorial nature of dense image segmentation problems. We have devised efficient algorithms for deep neural network learning to model the energy function, as well as for local optimization of this energy in the supervoxel agglomeration space. Our method has been extensively evaluated on a large, publicly available 3-D microscopy dataset featuring 25 billion voxels of ground truth data. We have discovered that our approach enhances volumetric reconstruction accuracy by over 20% compared to two leading baseline methods - graph-based segmentation of the output of a 3-D convolutional neural network trained to predict boundaries, and a random forest classifier trained to agglomerate supervoxels generated by a 3-D convolutional neural network - when tested on an 11 billion voxel set.",1
"In this paper, we study the influence of both long and short skip connections on Fully Convolutional Networks (FCN) for biomedical image segmentation. In standard FCNs, only long skip connections are used to skip features from the contracting path to the expanding path in order to recover spatial information lost during downsampling. We extend FCNs by adding short skip connections, that are similar to the ones introduced in residual networks, in order to build very deep FCNs (of hundreds of layers). A review of the gradient flow confirms that for a very deep FCN it is beneficial to have both long and short skip connections. Finally, we show that a very deep FCN can achieve near-to-state-of-the-art results on the EM dataset without any further post-processing.",0
"The aim of this paper is to investigate the impact of utilizing both long and short skip connections on Fully Convolutional Networks (FCN) for biomedical image segmentation. The conventional FCNs only employ long skip connections to transfer features from the contracting path to the expanding path, with the objective of recovering spatial information that was lost during downsampling. To achieve greater depth in FCNs, we introduce short skip connections, similar to those in residual networks. A comprehensive analysis of the gradient flow validates the benefits of both long and short skip connections in very deep FCNs. Finally, we demonstrate that a very deep FCN can produce nearly state-of-the-art results on the EM dataset without any additional post-processing.",1
"Deep Matching (DM) is a popular high-quality method for quasi-dense image matching. Despite its name, however, the original DM formulation does not yield a deep neural network that can be trained end-to-end via backpropagation. In this paper, we remove this limitation by rewriting the complete DM algorithm as a convolutional neural network. This results in a novel deep architecture for image matching that involves a number of new layer types and that, similar to recent networks for image segmentation, has a U-topology. We demonstrate the utility of the approach by improving the performance of DM by learning it end-to-end on an image matching task.",0
"The technique called Deep Matching (DM) is a widely used and effective method for matching quasi-dense images. Despite its name, the original DM approach does not produce a deep neural network that can be trained through backpropagation. However, in this study, we have overcome this limitation by re-engineering the entire DM algorithm as a convolutional neural network. The outcome is an innovative deep architecture for image matching, which comprises several new types of layers and features a U-topology like the recent networks for image segmentation. By demonstrating the effectiveness of our approach, we have improved the performance of DM by training it end-to-end on an image matching task.",1
"Optical coherence tomography (OCT) is a non-invasive imaging technique that can produce images of the eye at the microscopic level. OCT image segmentation to localise retinal layer boundaries is a fundamental procedure for diagnosing and monitoring the progression of retinal and optical nerve disorders. In this paper, we introduce a novel and accurate geodesic distance method (GDM) for OCT segmentation of both healthy and pathological images in either two- or three-dimensional spaces. The method uses a weighted geodesic distance by an exponential function, taking into account both horizontal and vertical intensity variations. The weighted geodesic distance is efficiently calculated from an Eikonal equation via the fast sweeping method. The segmentation is then realised by solving an ordinary differential equation with the geodesic distance. The results of the GDM are compared with manually segmented retinal layer boundaries/surfaces. Extensive experiments demonstrate that the proposed GDM is robust to complex retinal structures with large curvatures and irregularities and it outperforms the parametric active contour algorithm as well as the graph theoretic based approaches for delineating the retinal layers in both healthy and pathological images.",0
"The technique of optical coherence tomography (OCT) is utilized to obtain microscopic level images of the eye in a non-invasive manner. Accurate OCT image segmentation is necessary for detecting and monitoring the progression of retinal and optical nerve disorders. This study introduces a novel geodesic distance method (GDM) that can be used for both healthy and pathological images in either two- or three-dimensional spaces. The method employs a weighted geodesic distance that considers horizontal and vertical intensity variations and is efficiently calculated by an Eikonal equation via the fast sweeping method. The segmentation is achieved by solving an ordinary differential equation with the geodesic distance. The results of the GDM are compared with manually segmented retinal layer boundaries/surfaces. The proposed GDM is shown to be robust to complex retinal structures with large curvatures and irregularities, and it outperforms other approaches for delineating the retinal layers in both healthy and pathological images.",1
"Segmentation of 3D images is a fundamental problem in biomedical image analysis. Deep learning (DL) approaches have achieved state-of-the-art segmentation perfor- mance. To exploit the 3D contexts using neural networks, known DL segmentation methods, including 3D convolution, 2D convolution on planes orthogonal to 2D image slices, and LSTM in multiple directions, all suffer incompatibility with the highly anisotropic dimensions in common 3D biomedical images. In this paper, we propose a new DL framework for 3D image segmentation, based on a com- bination of a fully convolutional network (FCN) and a recurrent neural network (RNN), which are responsible for exploiting the intra-slice and inter-slice contexts, respectively. To our best knowledge, this is the first DL framework for 3D image segmentation that explicitly leverages 3D image anisotropism. Evaluating using a dataset from the ISBI Neuronal Structure Segmentation Challenge and in-house image stacks for 3D fungus segmentation, our approach achieves promising results comparing to the known DL-based 3D segmentation approaches.",0
"Biomedical image analysis requires segmentation of 3D images, which is a crucial task. Deep learning (DL) methods have demonstrated remarkable performance in segmentation. However, conventional DL segmentation techniques such as 3D convolution, 2D convolution on orthogonal planes, and LSTM in multiple directions are not compatible with the highly anisotropic dimensions of common 3D biomedical images. This paper proposes a novel DL framework for 3D image segmentation that employs a combination of a fully convolutional network (FCN) and a recurrent neural network (RNN). The FCN and RNN exploit intra-slice and inter-slice contexts, respectively, and explicitly leverage 3D image anisotropism. Our approach is the first DL framework for 3D image segmentation that does so. We evaluate our approach using ISBI Neuronal Structure Segmentation Challenge and in-house 3D fungus segmentation datasets. Our approach shows promising results compared to known DL-based 3D segmentation techniques.",1
"This study proposes an approach to segment human object from a depth image based on histogram of depth values. The region of interest is first extracted based on a predefined threshold for histogram regions. A region growing process is then employed to separate multiple human bodies with the same depth interval. Our contribution is the identification of an adaptive growth threshold based on the detected histogram region. To demonstrate the effectiveness of the proposed method, an application in driver distraction detection was introduced. After successfully extracting the driver's position inside the car, we came up with a simple solution to track the driver motion. With the analysis of the difference between initial and current frame, a change of cluster position or depth value in the interested region, which cross the preset threshold, is considered as a distracted activity. The experiment results demonstrated the success of the algorithm in detecting typical distracted driving activities such as using phone for calling or texting, adjusting internal devices and drinking in real time.",0
"The study suggests a method for segmenting human objects in a depth image using a histogram of depth values. Firstly, the region of interest is extracted based on a pre-established threshold for histogram regions. Next, a region growing process is utilized to separate multiple human bodies with the same depth interval. The study introduces an innovative approach by identifying an adaptive growth threshold based on the detected histogram region. To prove the efficacy of the proposed technique, the study presents an application in driver distraction detection. Once the driver's position in the car is successfully extracted, a simple solution is introduced to track their motion. The algorithm considers a distracted activity if there is a change in cluster position or depth value in the interested region, crossing the preset threshold. The experiment results show that the algorithm can detect common distracted driving activities such as phone usage, internal device adjustment, and drinking in real-time.",1
"Image segmentation from referring expressions is a joint vision and language modeling task, where the input is an image and a textual expression describing a particular region in the image; and the goal is to localize and segment the specific image region based on the given expression. One major difficulty to train such language-based image segmentation systems is the lack of datasets with joint vision and text annotations. Although existing vision datasets such as MS COCO provide image captions, there are few datasets with region-level textual annotations for images, and these are often smaller in scale. In this paper, we explore how existing large scale vision-only and text-only datasets can be utilized to train models for image segmentation from referring expressions. We propose a method to address this problem, and show in experiments that our method can help this joint vision and language modeling task with vision-only and text-only data and outperforms previous results.",0
"The task of image segmentation from referring expressions requires joint modeling of vision and language. It involves inputting an image and a textual description of a particular region in the image, and the aim is to accurately locate and segment the region described. An obstacle to training such language-based segmentation systems is the shortage of datasets with joint vision and text annotations. Although datasets like MS COCO offer image captions, there are limited datasets with region-specific textual annotations, and these are often on a smaller scale. This study investigates how current large-scale vision and text datasets can be utilized to train models for image segmentation from referring expressions. A proposed method is presented to address this problem, and experiments demonstrate that it outperforms previous results, using both vision-only and text-only data.",1
"We introduce a new light-field dataset of materials, and take advantage of the recent success of deep learning to perform material recognition on the 4D light-field. Our dataset contains 12 material categories, each with 100 images taken with a Lytro Illum, from which we extract about 30,000 patches in total. To the best of our knowledge, this is the first mid-size dataset for light-field images. Our main goal is to investigate whether the additional information in a light-field (such as multiple sub-aperture views and view-dependent reflectance effects) can aid material recognition. Since recognition networks have not been trained on 4D images before, we propose and compare several novel CNN architectures to train on light-field images. In our experiments, the best performing CNN architecture achieves a 7% boost compared with 2D image classification (70% to 77%). These results constitute important baselines that can spur further research in the use of CNNs for light-field applications. Upon publication, our dataset also enables other novel applications of light-fields, including object detection, image segmentation and view interpolation.",0
"We have created a new dataset of materials using light-field technology and utilized deep learning techniques to recognize these materials in the 4D light-field. Our dataset comprises 12 material categories, with 100 images captured using a Lytro Illum, resulting in approximately 30,000 patches extracted from the images. This is the first mid-sized dataset for light-field images that we are aware of. Our primary objective is to investigate if the additional information available in light-fields, such as multiple sub-aperture views and view-dependent reflectance effects, can enhance material recognition. We propose and compare several novel convolutional neural network (CNN) architectures to train on light-field images, as recognition networks have not been trained on 4D images before. Our experiments demonstrate that the best performing CNN architecture yields a 7% improvement in classification accuracy compared to 2D image classification, increasing from 70% to 77%. These results provide important benchmarks for future research in CNNs for light-field applications. Furthermore, the publication of our dataset enables novel applications of light-fields, such as object detection, image segmentation, and view interpolation.",1
"Recently deep residual learning with residual units for training very deep neural networks advanced the state-of-the-art performance on 2D image recognition tasks, e.g., object detection and segmentation. However, how to fully leverage contextual representations for recognition tasks from volumetric data has not been well studied, especially in the field of medical image computing, where a majority of image modalities are in volumetric format. In this paper we explore the deep residual learning on the task of volumetric brain segmentation. There are at least two main contributions in our work. First, we propose a deep voxelwise residual network, referred as VoxResNet, which borrows the spirit of deep residual learning in 2D image recognition tasks, and is extended into a 3D variant for handling volumetric data. Second, an auto-context version of VoxResNet is proposed by seamlessly integrating the low-level image appearance features, implicit shape information and high-level context together for further improving the volumetric segmentation performance. Extensive experiments on the challenging benchmark of brain segmentation from magnetic resonance (MR) images corroborated the efficacy of our proposed method in dealing with volumetric data. We believe this work unravels the potential of 3D deep learning to advance the recognition performance on volumetric image segmentation.",0
"The latest advancements in deep residual learning, which utilizes residual units to train complex neural networks, have significantly improved the state-of-the-art performance in recognizing 2D images. However, the use of contextual representations for recognition tasks in volumetric data remains unexplored, particularly in the field of medical image computing where the majority of image modalities are in volumetric format. In this study, we investigate the application of deep residual learning to volumetric brain segmentation. Our work introduces two main contributions. Firstly, we propose the VoxResNet, a deep voxelwise residual network that extends the 2D residual learning approach to handle volumetric data. Secondly, we develop an auto-context version of VoxResNet that seamlessly integrates low-level image appearance features, implicit shape information, and high-level context to further enhance the volumetric segmentation performance. Through extensive experiments on the challenging benchmark of brain segmentation from MR images, we demonstrate the effectiveness of our proposed method in dealing with volumetric data. Our study highlights the potential of 3D deep learning in advancing the recognition performance of volumetric image segmentation.",1
"We consider log-supermodular models on binary variables, which are probabilistic models with negative log-densities which are submodular. These models provide probabilistic interpretations of common combinatorial optimization tasks such as image segmentation. In this paper, we focus primarily on parameter estimation in the models from known upper-bounds on the intractable log-partition function. We show that the bound based on separable optimization on the base polytope of the submodular function is always inferior to a bound based on ""perturb-and-MAP"" ideas. Then, to learn parameters, given that our approximation of the log-partition function is an expectation (over our own randomization), we use a stochastic subgradient technique to maximize a lower-bound on the log-likelihood. This can also be extended to conditional maximum likelihood. We illustrate our new results in a set of experiments in binary image denoising, where we highlight the flexibility of a probabilistic model to learn with missing data.",0
"The focus of this paper is on log-supermodular models with binary variables, which are probabilistic models that have negative log-densities that are submodular. These models offer a way to interpret common combinatorial optimization tasks, such as image segmentation. Our primary objective is to estimate parameters in these models using known upper-bounds on the intractable log-partition function. We compare two different approaches to bound estimation and show that ""perturb-and-MAP"" ideas yield better results than separable optimization on the base polytope of the submodular function. To learn parameters, we utilize a stochastic subgradient technique to maximize a lower-bound on the log-likelihood. This technique can also be extended to conditional maximum likelihood. In our experiments on binary image denoising, we demonstrate the flexibility of probabilistic models to learn with missing data.",1
"Video games are a compelling source of annotated data as they can readily provide fine-grained groundtruth for diverse tasks. However, it is not clear whether the synthetically generated data has enough resemblance to the real-world images to improve the performance of computer vision models in practice. We present experiments assessing the effectiveness on real-world data of systems trained on synthetic RGB images that are extracted from a video game. We collected over 60000 synthetic samples from a modern video game with similar conditions to the real-world CamVid and Cityscapes datasets. We provide several experiments to demonstrate that the synthetically generated RGB images can be used to improve the performance of deep neural networks on both image segmentation and depth estimation. These results show that a convolutional network trained on synthetic data achieves a similar test error to a network that is trained on real-world data for dense image classification. Furthermore, the synthetically generated RGB images can provide similar or better results compared to the real-world datasets if a simple domain adaptation technique is applied. Our results suggest that collaboration with game developers for an accessible interface to gather data is potentially a fruitful direction for future work in computer vision.",0
"Video games have the potential to provide highly detailed groundtruth data that can be used for a variety of tasks. However, it is uncertain if this synthetic data can sufficiently improve the performance of computer vision models, as it may not closely resemble real-world images. To address this issue, we conducted experiments using over 60,000 synthetic RGB samples extracted from a modern video game, which closely mimics the conditions of the CamVid and Cityscapes datasets. Our experiments demonstrate that deep neural networks trained on synthetic data can effectively improve image segmentation and depth estimation performance. We found that the test error of a convolutional network trained on synthetic data is similar to that of a network trained on real-world data for dense image classification. Additionally, applying a simple domain adaptation technique can produce similar or better results than real-world datasets. These results suggest that collaborating with game developers to gather data could be a promising avenue for future research in computer vision.",1
"Region-based methods have proven necessary for improving segmentation accuracy of neuronal structures in electron microscopy (EM) images. Most region-based segmentation methods use a scoring function to determine region merging. Such functions are usually learned with supervised algorithms that demand considerable ground truth data, which are costly to collect. We propose a semi-supervised approach that reduces this demand. Based on a merge tree structure, we develop a differentiable unsupervised loss term that enforces consistent predictions from the learned function. We then propose a Bayesian model that combines the supervised and the unsupervised information for probabilistic learning. The experimental results on three EM data sets demonstrate that by using a subset of only 3% to 7% of the entire ground truth data, our approach consistently performs close to the state-of-the-art supervised method with the full labeled data set, and significantly outperforms the supervised method with the same labeled subset.",0
"The use of region-based methods has been found to be crucial in enhancing the accuracy of segmentation for neuronal structures in electron microscopy (EM) images. Typically, region-based segmentation methods employ scoring functions to determine the merging of regions. However, these functions are often learned through supervised algorithms which require a substantial amount of ground truth data that can be expensive to collect. To address this challenge, we present a semi-supervised approach that reduces the need for a large amount of labeled data. Our approach is based on a merge tree structure, and we develop a differentiable unsupervised loss term that promotes consistent predictions from the trained function. We also introduce a Bayesian model that combines the supervised and unsupervised information for probabilistic learning. The results of our experiments on three EM data sets demonstrate that our approach consistently performs as well as the state-of-the-art supervised method with a subset of only 3% to 7% of the entire ground truth data. Additionally, our method significantly outperforms the supervised method when using the same subset of labeled data.",1
"Semantic image segmentation is a principal problem in computer vision, where the aim is to correctly classify each individual pixel of an image into a semantic label. Its widespread use in many areas, including medical imaging and autonomous driving, has fostered extensive research in recent years. Empirical improvements in tackling this task have primarily been motivated by successful exploitation of Convolutional Neural Networks (CNNs) pre-trained for image classification and object recognition. However, the pixel-wise labelling with CNNs has its own unique challenges: (1) an accurate deconvolution, or upsampling, of low-resolution output into a higher-resolution segmentation mask and (2) an inclusion of global information, or context, within locally extracted features. To address these issues, we propose a novel architecture to conduct the equivalent of the deconvolution operation globally and acquire dense predictions. We demonstrate that it leads to improved performance of state-of-the-art semantic segmentation models on the PASCAL VOC 2012 benchmark, reaching 74.0% mean IU accuracy on the test set.",0
"The task of semantic image segmentation, which involves assigning a semantic label to each pixel of an image, is a crucial problem in computer vision. It has found widespread application in various fields, such as medical imaging and autonomous driving, leading to extensive research in recent years. The use of pre-trained Convolutional Neural Networks (CNNs) for image classification and object recognition has resulted in significant progress in this area. However, using CNNs for pixel-wise labelling presents unique challenges, such as the need for accurate deconvolution and inclusion of global context in locally extracted features. In this paper, we propose a new architecture to address these challenges by performing a global deconvolution operation and acquiring dense predictions. Our approach improves the performance of state-of-the-art semantic segmentation models on the PASCAL VOC 2012 benchmark, achieving a mean IU accuracy of 74.0% on the test set.",1
We proposed an efficient iterative thresholding method for multi-phase image segmentation. The algorithm is based on minimizing piecewise constant Mumford-Shah functional in which the contour length (or perimeter) is approximated by a non-local multi-phase energy. The minimization problem is solved by an iterative method. Each iteration consists of computing simple convolutions followed by a thresholding step. The algorithm is easy to implement and has the optimal complexity $O(N \log N)$ per iteration. We also show that the iterative algorithm has the total energy decaying property. We present some numerical results to show the efficiency of our method.,0
"Our proposed method for multi-phase image segmentation involves an iterative thresholding technique that is highly efficient. The algorithm is centered around minimizing the Mumford-Shah functional, which approximates the contour length through a non-local multi-phase energy. To solve this minimization problem, we utilize an iterative approach that involves basic convolution operations and a thresholding step. This algorithm is easy to implement and has an optimal complexity of $O(N \log N)$ per iteration. Additionally, we demonstrate that the iterative algorithm reduces the total energy. To showcase the effectiveness of our method, we provide several numerical results.",1
"Recent years have seen tremendous progress in still-image segmentation; however the na\""ive application of these state-of-the-art algorithms to every video frame requires considerable computation and ignores the temporal continuity inherent in video. We propose a video recognition framework that relies on two key observations: 1) while pixels may change rapidly from frame to frame, the semantic content of a scene evolves more slowly, and 2) execution can be viewed as an aspect of architecture, yielding purpose-fit computation schedules for networks. We define a novel family of ""clockwork"" convnets driven by fixed or adaptive clock signals that schedule the processing of different layers at different update rates according to their semantic stability. We design a pipeline schedule to reduce latency for real-time recognition and a fixed-rate schedule to reduce overall computation. Finally, we extend clockwork scheduling to adaptive video processing by incorporating data-driven clocks that can be tuned on unlabeled video. The accuracy and efficiency of clockwork convnets are evaluated on the Youtube-Objects, NYUD, and Cityscapes video datasets.",0
"In recent years, impressive advancements have been made in the field of still-image segmentation. However, applying these algorithms to every single frame of a video is a time-consuming process that overlooks the significance of temporal continuity within video. Our proposed solution is a video recognition framework based on two crucial observations: firstly, the semantic content of a scene changes more slowly than the pixels from frame to frame and secondly, execution can be seen as part of architecture, which allows for a purpose-built computation schedule for networks. We introduce a new type of ""clockwork"" convnets, driven by adaptive or fixed clock signals that schedule the processing of different layers based on their semantic stability. A pipeline schedule has been designed to reduce latency for real-time recognition, while a fixed-rate schedule minimizes overall computation. Furthermore, we have extended clockwork scheduling to incorporate data-driven clocks for adaptive video processing on unlabeled video. The accuracy and efficiency of clockwork convnets are evaluated on video datasets such as Youtube-Objects, NYUD, and Cityscapes.",1
"Over the last decade, it has been demonstrated that many systems in science and engineering can be modeled more accurately by fractional-order than integer-order derivatives, and many methods are developed to solve the problem of fractional systems. Due to the extra free parameter order, fractional-order based methods provide additional degree of freedom in optimization performance. Not surprisingly, many fractional-order based methods have been used in image processing field. Herein recent studies are reviewed in ten sub-fields, which include image enhancement, image denoising, image edge detection, image segmentation, image registration, image recognition, image fusion, image encryption, image compression and image restoration. In sum, it is well proved that as a fundamental mathematic tool, fractional-order derivative shows great success in image processing.",0
"In the past ten years, it has become evident that fractional-order derivatives are a more accurate way of modeling many systems in science and engineering, and multiple techniques have been developed to address fractional systems. With an additional free parameter order, fractional-order methods offer increased optimization performance. As a result, these methods have been widely used in the field of image processing. This review examines recent studies in ten sub-fields of image processing, including image enhancement, denoising, edge detection, segmentation, registration, recognition, fusion, encryption, compression, and restoration. Overall, the success of fractional-order derivatives as a fundamental mathematical tool in image processing is well established.",1
"This study introduced a novel system, called Gaze2Segment, integrating biological and computer vision techniques to support radiologists' reading experience with an automatic image segmentation task. During diagnostic assessment of lung CT scans, the radiologists' gaze information were used to create a visual attention map. This map was then combined with a computer-derived saliency map, extracted from the gray-scale CT images. The visual attention map was used as an input for indicating roughly the location of a object of interest. With computer-derived saliency information, on the other hand, we aimed at finding foreground and background cues for the object of interest. At the final step, these cues were used to initiate a seed-based delineation process. Segmentation accuracy of the proposed Gaze2Segment was found to be 86% with dice similarity coefficient and 1.45 mm with Hausdorff distance. To the best of our knowledge, Gaze2Segment is the first true integration of eye-tracking technology into a medical image segmentation task without the need for any further user-interaction.",0
"The Gaze2Segment system was developed to enhance radiologists' reading experience by automating image segmentation using a combination of biological and computer vision techniques. To create a visual attention map during the diagnostic assessment of lung CT scans, the system utilized the radiologists' gaze information. This map was then merged with a computer-generated saliency map from the gray-scale CT images to locate the object of interest. The computer-derived saliency information was utilized to identify foreground and background cues for the object. Finally, the cues were used to initiate a seed-based delineation process resulting in a segmentation accuracy of 86% using the dice similarity coefficient and 1.45 mm with the Hausdorff distance. Gaze2Segment is the first system to integrate eye-tracking technology into a medical image segmentation task without requiring any further user-interaction.",1
"In this paper we propose a CNN architecture for semantic image segmentation. We introduce a new 'bilateral inception' module that can be inserted in existing CNN architectures and performs bilateral filtering, at multiple feature-scales, between superpixels in an image. The feature spaces for bilateral filtering and other parameters of the module are learned end-to-end using standard backpropagation techniques. The bilateral inception module addresses two issues that arise with general CNN segmentation architectures. First, this module propagates information between (super) pixels while respecting image edges, thus using the structured information of the problem for improved results. Second, the layer recovers a full resolution segmentation result from the lower resolution solution of a CNN. In the experiments, we modify several existing CNN architectures by inserting our inception module between the last CNN (1x1 convolution) layers. Empirical results on three different datasets show reliable improvements not only in comparison to the baseline networks, but also in comparison to several dense-pixel prediction techniques such as CRFs, while being competitive in time.",0
"Our paper presents a CNN architecture designed for semantic image segmentation, which includes a novel 'bilateral inception' module that enables bilateral filtering across multiple feature-scales between superpixels within an image. Using standard backpropagation techniques, the feature spaces for bilateral filtering and other module parameters can be learned end-to-end. Incorporating this module into existing CNN segmentation architectures addresses two issues: firstly, the module propagates information between pixels while preserving image edges, which enhances results by leveraging structured information. Secondly, this layer recovers a full resolution segmentation result from a lower resolution CNN solution. Through experimentation, we inserted our inception module between the last CNN (1x1 convolution) layers of multiple existing architectures, and achieved reliable improvements, outperforming not only baseline networks but also several dense-pixel prediction techniques like CRFs while remaining competitive in terms of time.",1
"We present in this paper an image segmentation approach that combines a fuzzy semantic region classification and a context based region-growing. Input image is first over-segmented. Then, prior domain knowledge is used to perform a fuzzy classification of these regions to provide a fuzzy semantic labeling. This allows the proposed approach to operate at high level instead of using low-level features and consequently to remedy to the problem of the semantic gap. Each over-segmented region is represented by a vector giving its corresponding membership degrees to the different thematic labels and the whole image is therefore represented by a Regions Partition Matrix. The segmentation is achieved on this matrix instead of the image pixels through two main phases: focusing and propagation. The focusing aims at selecting seeds regions from which information propagation will be performed. Thepropagation phase allows to spread toward others regions and using fuzzy contextual information the needed knowledge ensuring the semantic segmentation. An application of the proposed approach on mammograms shows promising results",0
"In this paper, we propose an approach for image segmentation that combines fuzzy semantic region classification and context-based region-growing. Firstly, the input image is over-segmented, and then fuzzy classification using prior domain knowledge is performed on these regions to provide a fuzzy semantic labeling. This allows our approach to operate at a high level and remedy the problem of the semantic gap. Each over-segmented region is represented by a vector of its corresponding membership degrees to different thematic labels, and the entire image is represented by a Regions Partition Matrix. Segmentation is achieved on this matrix instead of the image pixels through two main phases: focusing and propagation. Focusing selects seed regions for information propagation, while the propagation phase spreads towards other regions and uses fuzzy contextual information to ensure semantic segmentation. We apply this approach to mammograms and obtain promising results.",1
"We introduce a new loss function for the weakly-supervised training of semantic image segmentation models based on three guiding principles: to seed with weak localization cues, to expand objects based on the information about which classes can occur in an image, and to constrain the segmentations to coincide with object boundaries. We show experimentally that training a deep convolutional neural network using the proposed loss function leads to substantially better segmentations than previous state-of-the-art methods on the challenging PASCAL VOC 2012 dataset. We furthermore give insight into the working mechanism of our method by a detailed experimental study that illustrates how the segmentation quality is affected by each term of the proposed loss function as well as their combinations.",0
"A novel loss function is presented for the weakly-supervised training of semantic image segmentation models. The approach is based on three key principles: employing weak localization cues to initiate the process, expanding objects using information about the possible classes in an image, and restricting segmentations to coincide with object boundaries. Experimental results demonstrate that utilizing this loss function in training a deep convolutional neural network leads to significantly improved segmentations compared to previous state-of-the-art methods on the challenging PASCAL VOC 2012 dataset. Additionally, a detailed experimental analysis is conducted to elucidate how each term of the proposed loss function and their combinations affect the quality of segmentation.",1
"Interactive image segmentation is a challenging task and receives increasing attention recently; however, two major drawbacks exist in interactive segmentation approaches. First, the segmentation performance of ROI-based methods is sensitive to the initial ROI: different ROIs may produce results with great difference. Second, most seed-based methods need intense interactions, and are not applicable in many cases. In this work, we generalize the Neutro-Connectedness (NC) to be independent of top-down priors of objects and to model image topology with indeterminacy measurement on image regions, propose a novel method for determining object and background regions, which is applied to exclude isolated background regions and enforce label consistency, and put forward a hybrid interactive segmentation method, Neutro-Connectedness Cut (NC-Cut), which can overcome the above two problems by utilizing both pixel-wise appearance information and region-based NC properties. We evaluate the proposed NC-Cut by employing two image datasets (265 images), and demonstrate that the proposed approach outperforms state-of-the-art interactive image segmentation methods (Grabcut, MILCut, One-Cut, MGC_max^sum and pPBC).",0
"Recently, interactive image segmentation has gained increasing attention as a challenging task. However, there are two major drawbacks in interactive segmentation approaches. Firstly, ROI-based methods are sensitive to the initial ROI and may produce vastly different results with different ROIs. Secondly, most seed-based methods require intense interactions, limiting their applicability. This work proposes a novel approach that generalizes the Neutro-Connectedness (NC) method to model image topology and indeterminacy measurement on image regions independently of top-down priors of objects. The proposed method determines object and background regions, excludes isolated background regions, and enforces label consistency. Additionally, a hybrid interactive segmentation method, Neutro-Connectedness Cut (NC-Cut), is proposed, which uses both pixel-wise appearance information and region-based NC properties to overcome the above two problems. The proposed approach was evaluated using two image datasets comprising 265 images, and it was found to outperform state-of-the-art interactive image segmentation methods such as Grabcut, MILCut, One-Cut, MGC_max^sum, and pPBC.",1
"We propose a new approach to interactive image segmentation based on some properties of a family of quadratic optimization problems related to dominant sets, a well-known graph-theoretic notion of a cluster which generalizes the concept of a maximal clique to edge-weighted graphs. In particular, we show that by properly controlling a regularization parameter which determines the structure and the scale of the underlying problem, we are in a position to extract groups of dominant-set clusters which are constrained to contain user-selected elements. The resulting algorithm can deal naturally with any type of input modality, including scribbles, sloppy contours, and bounding boxes, and is able to robustly handle noisy annotations on the part of the user. Experiments on standard benchmark datasets show the effectiveness of our approach as compared to state-of-the-art algorithms on a variety of natural images under several input conditions.",0
"Our proposal introduces a fresh method for interactive image segmentation. It is founded on a family of quadratic optimization problems that are linked to dominant sets. Dominant sets are well-established clusters in graph theory that extend maximal cliques to edge-weighted graphs. By adjusting the regularization parameter, we can control the structure and scale of the problem, enabling the extraction of dominant-set clusters that contain user-selected elements. Our approach can handle various input modalities, like scribbles, sloppy contours, and bounding boxes, and can handle noisy annotations from users. Our experiments on standard benchmark datasets show that our algorithm is more effective than state-of-the-art methods for a range of natural images under different input conditions.",1
"This paper investigates one of the most fundamental computer vision problems: image segmentation. We propose a supervised hierarchical approach to object-independent image segmentation. Starting with over-segmenting superpixels, we use a tree structure to represent the hierarchy of region merging, by which we reduce the problem of segmenting image regions to finding a set of label assignment to tree nodes. We formulate the tree structure as a constrained conditional model to associate region merging with likelihoods predicted using an ensemble boundary classifier. Final segmentations can then be inferred by finding globally optimal solutions to the model efficiently. We also present an iterative training and testing algorithm that generates various tree structures and combines them to emphasize accurate boundaries by segmentation accumulation. Experiment results and comparisons with other very recent methods on six public data sets demonstrate that our approach achieves the state-of-the-art region accuracy and is very competitive in image segmentation without semantic priors.",0
"The main objective of this research is to explore image segmentation, a crucial problem in computer vision. To achieve this, we suggest a supervised hierarchical method for object-independent image segmentation. Our approach commences with over-segmenting superpixels and uses a tree structure to illustrate the hierarchy of region merging. This method simplifies the process of segmenting image regions, and we achieve this by finding a set of label assignment to tree nodes. We apply a constrained conditional model to formulate the tree structure and associate region merging with likelihoods predicted using an ensemble boundary classifier. The final segmentations are obtained by finding globally optimal solutions to the model efficiently. We also present an iterative training and testing algorithm that generates various tree structures, and we combine them to emphasize accurate boundaries by segmentation accumulation. To validate our approach, we conducted experiments and compared our approach with other very recent methods on six public data sets. The results show that our approach achieves the state-of-the-art region accuracy and is highly competitive in image segmentation without semantic priors.",1
"Localizing functional regions of objects or affordances is an important aspect of scene understanding. In this work, we cast the problem of affordance segmentation as that of semantic image segmentation. In order to explore various levels of supervision, we introduce a pixel-annotated affordance dataset of 3090 images containing 9916 object instances with rich contextual information in terms of human-object interactions. We use a deep convolutional neural network within an expectation maximization framework to take advantage of weakly labeled data like image level annotations or keypoint annotations. We show that a further reduction in supervision is possible with a minimal loss in performance when human pose is used as context.",0
"Understanding scenes requires identifying functional regions and affordances of objects. In this study, we approach the segmentation of affordances by using semantic image segmentation. To examine different levels of supervision, we created a dataset of 3090 images with 9916 object instances, each with rich contextual information regarding human-object interactions. We incorporated a deep convolutional neural network into an expectation maximization framework to utilize weakly labeled data, such as image level annotations or keypoint annotations. Our findings indicate that the use of human pose as context allows for a reduction in supervision with only minimal loss in performance.",1
"Sparse decomposition has been widely used for different applications, such as source separation, image classification, image denoising and more. This paper presents a new algorithm for segmentation of an image into background and foreground text and graphics using sparse decomposition and total variation minimization. The proposed method is designed based on the assumption that the background part of the image is smoothly varying and can be represented by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics can be modeled with a sparse component overlaid on the smooth background. The background and foreground are separated using a sparse decomposition framework regularized with a few suitable regularization terms which promotes the sparsity and connectivity of foreground pixels. This algorithm has been tested on a dataset of images extracted from HEVC standard test sequences for screen content coding, and is shown to have superior performance over some prior methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu and shape primitive extraction and coding (SPEC) algorithm.",0
"Sparse decomposition has been extensively utilized in various applications, including but not limited to source separation, image classification, and image denoising. In this study, a novel algorithm is introduced for segmenting an image into its background and foreground components, employing sparse decomposition and total variation minimization. The proposed technique operates under the assumption that the background section of the image is smoothly changing and can be represented by a linear combination of few basis functions, while the foreground text and graphics can be modeled as a sparse component overlaid on the smooth background. The separation of background and foreground is accomplished through a sparse decomposition framework, employing appropriate regularization terms that encourage sparsity and connectivity of the foreground pixels. The algorithm's efficacy was evaluated on a dataset of images extracted from HEVC standard test sequences for screen content coding, and it outperformed previous methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu, and shape primitive extraction and coding algorithm (SPEC).",1
"The semantic image segmentation task presents a trade-off between test time accuracy and training-time annotation cost. Detailed per-pixel annotations enable training accurate models but are very time-consuming to obtain, image-level class labels are an order of magnitude cheaper but result in less accurate models. We take a natural step from image-level annotation towards stronger supervision: we ask annotators to point to an object if one exists. We incorporate this point supervision along with a novel objectness potential in the training loss function of a CNN model. Experimental results on the PASCAL VOC 2012 benchmark reveal that the combined effect of point-level supervision and objectness potential yields an improvement of 12.9% mIOU over image-level supervision. Further, we demonstrate that models trained with point-level supervision are more accurate than models trained with image-level, squiggle-level or full supervision given a fixed annotation budget.",0
"The task of semantic image segmentation requires a balance between accuracy during testing and the cost of annotation during training. While detailed per-pixel annotations lead to accurate models, they are time-consuming to obtain. On the other hand, image-level class labels are cheaper but result in less accurate models. To address this issue, we propose moving from image-level annotation to stronger supervision by asking annotators to point out the objects. We incorporate this point supervision and a novel objectness potential into the training loss function of a CNN model. Our experiments on the PASCAL VOC 2012 benchmark show that combining point-level supervision and objectness potential results in a 12.9% mIOU improvement over image-level supervision. Additionally, we demonstrate that models trained with point-level supervision are more accurate than those trained with other annotation levels, given a fixed annotation budget.",1
"We introduce a deep learning image segmentation framework that is extremely robust to missing imaging modalities. Instead of attempting to impute or synthesize missing data, the proposed approach learns, for each modality, an embedding of the input image into a single latent vector space for which arithmetic operations (such as taking the mean) are well defined. Points in that space, which are averaged over modalities available at inference time, can then be further processed to yield the desired segmentation. As such, any combinatorial subset of available modalities can be provided as input, without having to learn a combinatorial number of imputation models. Evaluated on two neurological MRI datasets (brain tumors and MS lesions), the approach yields state-of-the-art segmentation results when provided with all modalities; moreover, its performance degrades remarkably gracefully when modalities are removed, significantly more so than alternative mean-filling or other synthesis approaches.",0
"Our proposed deep learning image segmentation framework overcomes the challenge of missing imaging modalities. Instead of generating or filling in missing data, our approach creates an embedding for each modality that maps the input image to a single latent vector space. The space allows for arithmetic operations, such as averaging, to be accurately defined. This enables us to process points in the space, which are combined from available modalities during inference, to generate the desired segmentation. Our approach eliminates the need to learn multiple imputation models for different combinations of modalities, as any subset can be used as input. We evaluated our method on two neurological MRI datasets, brain tumors and MS lesions, and achieved state-of-the-art segmentation results with all modalities. Additionally, our approach gracefully handles missing modalities, outperforming alternative methods that rely on mean-filling or synthesis.",1
"The random walker (RW) algorithm is used for both image segmentation and registration, and possesses several useful properties that make it popular in medical imaging, such as being globally optimizable, allowing user interaction, and providing uncertainty information. The RW algorithm defines a weighted graph over an image and uses the graph's Laplacian matrix to regularize its solutions. This regularization reduces to solving a large system of equations, which may be excessively time consuming in some applications, such as when interacting with a human user. Techniques have been developed that precompute eigenvectors of a Laplacian offline, after image acquisition but before any analysis, in order speed up the RW algorithm online, when segmentation or registration is being performed. However, precomputation requires certain algorithm parameters be fixed offline, limiting their flexibility. In this paper, we develop techniques to update the precomputed data online when RW parameters are altered. Specifically, we dynamically determine the number of eigenvectors needed for a desired accuracy based on user input, and derive update equations for the eigenvectors when the edge weights or topology of the image graph are changed. We present results demonstrating that our techniques make RW with precomputation much more robust to offline settings while only sacrificing minimal accuracy.",0
"The RW algorithm is widely used in medical imaging for image segmentation and registration due to its beneficial features such as global optimizability, user interaction, and uncertainty information. This algorithm establishes a weighted graph over an image and employs the graph's Laplacian matrix to regulate its solutions. However, the regularization process can be time-consuming, especially when interacting with a human user. To overcome this issue, researchers have developed techniques to precompute eigenvectors of a Laplacian offline, which can speed up the online RW algorithm during segmentation or registration. Nevertheless, this precomputation method has limitations as it requires fixed algorithm parameters offline. In this study, we propose a dynamic approach to update the precomputed data online when RW parameters are modified. Our method determines the number of eigenvectors required for a desired accuracy based on user input and derives update equations for the eigenvectors when the edge weights or topology of the image graph are altered. Our experimental results demonstrate that our approach enhances the robustness of RW with precomputation, while sacrificing minimal accuracy.",1
"This paper considers how to separate text and/or graphics from smooth background in screen content and mixed document images and proposes two approaches to perform this segmentation task. The proposed methods make use of the fact that the background in each block is usually smoothly varying and can be modeled well by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity. The algorithms separate the background and foreground pixels by trying to fit background pixel values in the block into a smooth function using two different schemes. One is based on robust regression, where the inlier pixels will be considered as background, while remaining outlier pixels will be considered foreground. The second approach uses a sparse decomposition framework where the background and foreground layers are modeled with a smooth and sparse components respectively. These algorithms have been tested on images extracted from HEVC standard test sequences for screen content coding, and are shown to have superior performance over previous approaches. The proposed methods can be used in different applications such as text extraction, separate coding of background and foreground for compression of screen content, and medical image segmentation.",0
"In this article, the focus is on how to distinguish text and/or graphics from a smooth background in mixed document images and screen content, and two methods are suggested to achieve this segmentation task. The methods rely on the fact that the background in each block is typically smoothly varying and can be accurately modeled by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics show sharp discontinuity. The algorithms separate the pixels in the background and foreground by attempting to fit the background pixel values in the block into a smooth function using two different approaches. The first is based on robust regression, where inlier pixels are considered to be part of the background, and outlier pixels are considered to be part of the foreground. The second approach uses a sparse decomposition framework, where smooth and sparse components are used to model the background and foreground layers, respectively. These algorithms have been tested on images extracted from HEVC standard test sequences for screen content coding, and have been shown to outperform previous methods. The suggested methods can have various applications, including text extraction, separate coding of background and foreground for compression of screen content, and medical image segmentation.",1
"Medical image segmentation, the task of partitioning an image into meaningful parts, is an important step toward automating medical image analysis and is at the crux of a variety of medical imaging applications, such as computer aided diagnosis, therapy planning and delivery, and computer aided interventions. However, the existence of noise, low contrast and objects' complexity in medical images are critical obstacles that stand in the way of achieving an ideal segmentation system. Incorporating prior knowledge into image segmentation algorithms has proven useful for obtaining more accurate and plausible results. This paper surveys the different types of prior knowledge that have been utilized in different segmentation frameworks. We focus our survey on optimization-based methods that incorporate prior information into their frameworks. We review and compare these methods in terms of the types of prior employed, the domain of formulation (continuous vs. discrete), and the optimization techniques (global vs. local). We also created an interactive online database of existing works and categorized them based on the type of prior knowledge they use. Our website is interactive so that researchers can contribute to keep the database up to date. We conclude the survey by discussing different aspects of designing an energy functional for image segmentation, open problems, and future perspectives.",0
"The segmentation of medical images is a crucial step towards automating medical image analysis, which is vital for a range of medical imaging applications. These applications include computer aided diagnosis, therapy planning, delivery, and interventions. However, medical images present significant obstacles such as noise, low contrast, and object complexity, which hinder the development of an ideal segmentation system. To address this issue, incorporating prior knowledge into image segmentation algorithms has proven effective in gaining more accurate and plausible results. This research paper surveys the various types of prior knowledge utilized in different segmentation frameworks, focusing on optimization-based methods that incorporate prior information into their frameworks. This survey reviews and compares these methods in terms of the types of prior employed, the domain of formulation (continuous vs. discrete), and the optimization techniques (global vs. local). Additionally, an interactive online database has been created to categorize existing works based on the type of prior knowledge they use, and researchers can contribute to keeping the database up to date. Finally, the survey concludes by discussing various aspects of designing an energy functional for image segmentation, open problems, and future perspectives.",1
"Autonomous driving is a challenging topic that requires complex solutions in perception tasks such as recognition of road, lanes, traffic signs or lights, vehicles and pedestrians. Through years of research, computer vision has grown capable of tackling these tasks with monocular detectors that can provide remarkable detection rates with relatively low processing times. However, the recent appearance of Convolutional Neural Networks (CNNs) has revolutionized the computer vision field and has made possible approaches to perform full pixel-wise semantic segmentation in times close to real time (even on hardware that can be carried on a vehicle). In this paper, we propose to use full image segmentation as an approach to simplify and unify most of the detection tasks required in the perception module of an autonomous vehicle, analyzing major concerns such as computation time and detection performance.",0
"The topic of autonomous driving is complex and requires solutions for perception tasks such as identifying roads, traffic signs, vehicles, pedestrians, and lanes. Over the years, computer vision has improved and can now accomplish these tasks with monocular detectors that have high detection rates and low processing times. However, the introduction of Convolutional Neural Networks (CNNs) has transformed the computer vision field and enabled pixel-wise semantic segmentation in near real-time, even on portable hardware. This paper proposes the use of full image segmentation to simplify and unify detection tasks in the perception module of autonomous vehicles, while considering factors such as computation time and detection performance.",1
"Level set methods are widely used for image segmentation because of their capability to handle topological changes. In this paper, we propose a novel parametric level set method called Disjunctive Normal Level Set (DNLS), and apply it to both two phase (single object) and multiphase (multi-object) image segmentations. The DNLS is formed by union of polytopes which themselves are formed by intersections of half-spaces. The proposed level set framework has the following major advantages compared to other level set methods available in the literature. First, segmentation using DNLS converges much faster. Second, the DNLS level set function remains regular throughout its evolution. Third, the proposed multiphase version of the DNLS is less sensitive to initialization, and its computational cost and memory requirement remains almost constant as the number of objects to be simultaneously segmented grows. The experimental results show the potential of the proposed method.",0
"Due to their ability to handle topological changes, level set methods are commonly used for image segmentation. This study introduces a new parametric level set method called Disjunctive Normal Level Set (DNLS) and applies it to both single and multi-object image segmentations. DNLS is composed of polytopes formed by intersecting half-spaces. Compared to other level set methods in literature, DNLS offers several advantages. First, it converges much faster. Second, its level set function remains regular throughout its evolution. Third, the proposed multiphase version of DNLS is less sensitive to initialization, and its computational cost and memory requirement remain almost constant as the number of objects to be segmented simultaneously grows. The results of our experiments demonstrate the potential of this new method.",1
"Recent advances in AI and robotics have claimed many incredible results with deep learning, yet no work to date has applied deep learning to the problem of liquid perception and reasoning. In this paper, we apply fully-convolutional deep neural networks to the tasks of detecting and tracking liquids. We evaluate three models: a single-frame network, multi-frame network, and a LSTM recurrent network. Our results show that the best liquid detection results are achieved when aggregating data over multiple frames, in contrast to standard image segmentation. They also show that the LSTM network outperforms the other two in both tasks. This suggests that LSTM-based neural networks have the potential to be a key component for enabling robots to handle liquids using robust, closed-loop controllers.",0
"Although AI and robotics have made great strides in deep learning, no research has yet focused on applying this technology to the challenge of liquid perception and reasoning. This study employs fully-convolutional deep neural networks to detect and track liquids. Three models were analyzed: a single-frame network, a multi-frame network, and an LSTM recurrent network. Results indicate that liquid detection is most successful when using data from multiple frames, as opposed to standard image segmentation. Furthermore, the LSTM network outperforms the other two models in both tasks, suggesting that LSTM-based neural networks could be a critical component for enabling robots to handle liquids with reliable, closed-loop controllers.",1
"Image segmentation and 3D pose estimation are two key cogs in any algorithm for scene understanding. However, state-of-the-art CRF-based models for image segmentation rely mostly on 2D object models to construct top-down high-order potentials. In this paper, we propose new top-down potentials for image segmentation and pose estimation based on the shape and volume of a 3D object model. We show that these complex top-down potentials can be easily decomposed into standard forms for efficient inference in both the segmentation and pose estimation tasks. Experiments on a car dataset show that knowledge of segmentation helps perform pose estimation better and vice versa.",0
"In any algorithm for scene understanding, both image segmentation and 3D pose estimation play vital roles. However, the current state-of-the-art CRF-based models for image segmentation rely mainly on 2D object models to generate high-order potentials from a top-down perspective. This paper introduces novel top-down potentials for image segmentation and pose estimation, which are based on the shape and volume of a 3D object model. The proposed complex top-down potentials can be easily decomposed into standard forms for efficient inference in both segmentation and pose estimation tasks. The experiments conducted on a car dataset demonstrate that knowledge of segmentation and pose estimation mutually benefit each other.",1
"Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.",0
"Recently, Convolutional Neural Networks (CNNs) have been utilized to address problems in both computer vision and medical image analysis. Despite their popularity, most CNN approaches can only handle 2D images. However, most medical data used in clinical practice consists of 3D volumes. In this study, we present a method for 3D image segmentation that employs a volumetric, fully convolutional neural network. Our CNN is trained end-to-end on MRI volumes portraying the prostate and can predict segmentation for the entire volume simultaneously. To tackle the issue of a significant imbalance between foreground and background voxels, we introduce a novel objective function, optimized using the Dice coefficient. To overcome the scarcity of annotated volumes available for training, we augment the data utilizing random non-linear transformations and histogram matching. Our experimental evaluation demonstrates that our approach achieves excellent results on challenging test data while requiring only a fraction of the processing time used by previous methods.",1
"Sky/cloud images captured by ground-based cameras (a.k.a. whole sky imagers) are increasingly used nowadays because of their applications in a number of fields, including climate modeling, weather prediction, renewable energy generation, and satellite communications. Due to the wide variety of cloud types and lighting conditions in such images, accurate and robust segmentation of clouds is challenging. In this paper, we present a supervised segmentation framework for ground-based sky/cloud images based on a systematic analysis of different color spaces and components, using partial least squares (PLS) regression. Unlike other state-of-the-art methods, our proposed approach is entirely learning-based and does not require any manually-defined parameters. In addition, we release the Singapore Whole Sky IMaging SEGmentation Database (SWIMSEG), a large database of annotated sky/cloud images, to the research community.",0
"The use of whole sky imagers, which are ground-based cameras, has become increasingly popular due to their application in various fields such as climate modeling, weather prediction, renewable energy generation, and satellite communications. However, accurate and robust segmentation of clouds from sky/cloud images obtained from these cameras poses a challenge due to the diversity of cloud types and lighting conditions. This paper presents a supervised segmentation framework based on a thorough analysis of different color spaces and components using partial least squares (PLS) regression. Unlike existing methods, this approach is entirely learning-based and does not require manual parameterization. Additionally, we introduce the Singapore Whole Sky IMaging SEGmentation Database (SWIMSEG), which is a vast collection of annotated sky/cloud images, to the research community.",1
"Recently there has been an increasing trend to use deep learning frameworks for both 2D consumer images and for 3D medical images. However, there has been little effort to use deep frameworks for volumetric vascular segmentation. We wanted to address this by providing a freely available dataset of 12 annotated two-photon vasculature microscopy stacks. We demonstrated the use of deep learning framework consisting both 2D and 3D convolutional filters (ConvNet). Our hybrid 2D-3D architecture produced promising segmentation result. We derived the architectures from Lee et al. who used the ZNN framework initially designed for electron microscope image segmentation. We hope that by sharing our volumetric vasculature datasets, we will inspire other researchers to experiment with vasculature dataset and improve the used network architectures.",0
"Lately, there has been a growing trend in utilizing deep learning frameworks for 2D consumer images and 3D medical images. However, deep frameworks have not been extensively employed in volumetric vascular segmentation. In an effort to address this shortcoming, we have created a dataset comprising 12 annotated two-photon vasculature microscopy stacks that is available for free. Our approach involved utilizing a deep learning framework that included both 2D and 3D convolutional filters (ConvNet). Our hybrid architecture integrating 2D and 3D components achieved promising segmentation results. We based our architectures on Lee et al.'s approach, which employed the ZNN framework initially developed for electron microscope image segmentation. Sharing our volumetric vasculature datasets is intended to encourage other researchers to explore vasculature datasets and enhance the network architectures used.",1
"A key problem in salient object detection is how to effectively model the semantic properties of salient objects in a data-driven manner. In this paper, we propose a multi-task deep saliency model based on a fully convolutional neural network (FCNN) with global input (whole raw images) and global output (whole saliency maps). In principle, the proposed saliency model takes a data-driven strategy for encoding the underlying saliency prior information, and then sets up a multi-task learning scheme for exploring the intrinsic correlations between saliency detection and semantic image segmentation. Through collaborative feature learning from such two correlated tasks, the shared fully convolutional layers produce effective features for object perception. Moreover, it is capable of capturing the semantic information on salient objects across different levels using the fully convolutional layers, which investigate the feature-sharing properties of salient object detection with great feature redundancy reduction. Finally, we present a graph Laplacian regularized nonlinear regression model for saliency refinement. Experimental results demonstrate the effectiveness of our approach in comparison with the state-of-the-art approaches.",0
"The semantic properties of salient objects in data-driven models are a significant issue in salient object detection. This study proposes a multi-task deep saliency model that uses a fully convolutional neural network (FCNN) with global input and output to address this problem. The proposed model takes a data-driven approach to encode saliency prior information and establishes a multi-task learning scheme to explore the intrinsic correlations between saliency detection and semantic image segmentation. By learning features collaboratively from these two related tasks, the shared fully convolutional layers generate effective features for object perception. Additionally, the model captures the semantic information of salient objects across different levels, which reduces feature redundancy. Finally, a graph Laplacian regularized nonlinear regression model is presented for saliency refinement. Experimental results prove the effectiveness of this approach compared to current state-of-the-art methods.",1
"Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called ""semantic image segmentation""). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our ""DeepLab"" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.",0
"High level vision tasks, including image classification and object detection, have seen improved performance with the use of Deep Convolutional Neural Networks (DCNNs). However, these networks are not ideal for pixel-level classification, also known as ""semantic image segmentation"", due to poor localization properties resulting from their invariance properties. To overcome this limitation, we combine the final layer of DCNNs with a fully connected Conditional Random Field (CRF) to improve accuracy in object segmentation. Our ""DeepLab"" system sets a new state-of-the-art in the PASCAL VOC-2012 semantic image segmentation task, achieving 71.6% IOU accuracy in the test set. We achieve these results efficiently with network re-purposing and utilizing the 'hole' algorithm from the wavelet community, allowing for dense computation of neural net responses at 8 frames per second on a modern GPU while localizing segment boundaries with greater accuracy than previous methods.",1
"Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic segmentation through the use of contextual information; specifically, we explore `patch-patch' context between image regions, and `patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012 dataset.",0
"The progress made in semantic image segmentation has been mainly due to the development of deep convolutional neural networks (CNNs). Our research focuses on enhancing semantic segmentation through the utilization of contextual information, specifically by examining the context between image regions (patch-patch) and between patches and their background (patch-background). We have developed Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches, which can be efficiently trained using a deep structured model. To capture patch-background context, we have found that a network design with traditional multi-scale image input and sliding pyramid pooling is effective. Our experiments demonstrate that our approach achieves state-of-the-art performance on popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow, with an intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012 dataset.",1
"It is well accepted that image segmentation can benefit from utilizing multilevel cues. The paper focuses on utilizing the FCNN-based dense semantic predictions in the bottom-up image segmentation, arguing to take semantic cues into account from the very beginning. By this we can avoid merging regions of similar appearance but distinct semantic categories as possible. The semantic inefficiency problem is handled. We also propose a straightforward way to use the contour cues to suppress the noise in multilevel cues, thus to improve the segmentation robustness. The evaluation on the BSDS500 shows that we obtain the competitive region and boundary performance. Furthermore, since all individual regions can be assigned with appropriate semantic labels during the computation, we are capable of extracting the adjusted semantic segmentations. The experiment on Pascal VOC 2012 shows our improvement to the original semantic segmentations which derives directly from the dense predictions.",0
"The utilization of multilevel cues is widely accepted to improve image segmentation. This paper focuses on using FCNN-based dense semantic predictions to enhance bottom-up image segmentation by incorporating semantic cues from the outset. This approach prevents the merging of regions that have similar appearances but belong to distinct semantic categories. Moreover, we have resolved the semantic inefficiency problem. We also suggest a simple method to suppress noise in multilevel cues by using contour cues, which ensures better segmentation robustness. On the BSDS500, our approach demonstrates competitive region and boundary performance. Additionally, we can assign appropriate semantic labels to all individual regions during the computation, facilitating the extraction of adjusted semantic segmentations. Our experiment on Pascal VOC 2012 shows that our approach enhances the original semantic segmentations directly obtained from the dense predictions.",1
"Image segmentation is often performed on medical images for identifying diseases in clinical evaluation. Hence it has become one of the major research areas. Conventional image segmentation techniques are unable to provide satisfactory segmentation results for medical images as they contain irregularities. They need to be pre-processed before segmentation. In order to obtain the most suitable method for medical image segmentation, we propose a two stage algorithm. The first stage automatically generates a binary marker image of the region of interest using mathematical morphology. This marker serves as the mask image for the second stage which uses GrabCut on the input image thus resulting in an efficient segmented result. The obtained result can be further refined by user interaction which can be done using the Graphical User Interface (GUI). Experimental results show that the proposed method is accurate and provides satisfactory segmentation results with minimum user interaction on medical as well as natural images.",0
"Medical image segmentation is a vital process in clinical evaluation to detect diseases, and it has become a major area of research. However, conventional segmentation techniques are inadequate for medical images due to their irregularities, and they require pre-processing. To achieve optimal segmentation for medical images, we propose a two-stage algorithm. The first stage generates a binary marker image using mathematical morphology, serving as a mask for the second stage, which employs GrabCut on the input image, resulting in efficient segmentation. The Graphical User Interface (GUI) allows for user interaction to refine the result. The proposed method is accurate and requires minimal user interaction, providing satisfactory segmentation for medical and natural images, as demonstrated by experimental results.",1
"Unsupervised image segmentation algorithms aim at identifying disjoint homogeneous regions in an image, and have been subject to considerable attention in the machine vision community. In this paper, a popular theoretical model with it's origins in statistical physics and social dynamics, known as the Deffuant-Weisbuch model, is applied to the image segmentation problem. The Deffuant-Weisbuch model has been found to be useful in modelling the evolution of a closed system of interacting agents characterised by their opinions or beliefs, leading to the formation of clusters of agents who share a similar opinion or belief at steady state. In the context of image segmentation, this paper considers a pixel as an agent and it's colour property as it's opinion, with opinion updates as per the Deffuant-Weisbuch model. Apart from applying the basic model to image segmentation, this paper incorporates adjacency and neighbourhood information in the model, which factors in the local similarity and smoothness properties of images. Convergence is reached when the number of unique pixel opinions, i.e., the number of colour centres, matches the pre-specified number of clusters. Experiments are performed on a set of images from the Berkeley Image Segmentation Dataset and the results are analysed both qualitatively and quantitatively, which indicate that this simple and intuitive method is promising for image segmentation. To the best of the knowledge of the author, this is the first work where a theoretical model from statistical physics and social dynamics has been successfully applied to image processing.",0
"Unsupervised algorithms for image segmentation are focused on detecting separate and uniform regions in an image, and have received significant attention from the field of machine vision. This study employs the Deffuant-Weisbuch model, a popular theoretical framework originating from statistical physics and social dynamics, to address the image segmentation problem. The Deffuant-Weisbuch model is commonly utilized for modeling the behavior of interacting agents in a closed system, where agents' beliefs or opinions lead to the formation of clusters of like-minded individuals at a stable state. In the context of image segmentation, this paper treats each pixel as an agent with a color attribute as its opinion, and updates opinions following the Deffuant-Weisbuch model. Additionally, the study incorporates the neighboring and adjacency information in the model to consider the image's local similarity and smoothness properties. Convergence is achieved when the number of unique pixel opinions, or color centers, matches the predetermined number of clusters. The study conducts experiments using images from the Berkeley Image Segmentation Dataset, and both qualitative and quantitative analyses indicate that this simple and intuitive method holds promise for image segmentation. This study marks the first successful application of a theoretical model from statistical physics and social dynamics to image processing, to the best of the author's knowledge.",1
"Deep convolutional neural networks (CNNs) are the backbone of state-of-art semantic image segmentation systems. Recent work has shown that complementing CNNs with fully-connected conditional random fields (CRFs) can significantly enhance their object localization accuracy, yet dense CRF inference is computationally expensive. We propose replacing the fully-connected CRF with domain transform (DT), a modern edge-preserving filtering method in which the amount of smoothing is controlled by a reference edge map. Domain transform filtering is several times faster than dense CRF inference and we show that it yields comparable semantic segmentation results, accurately capturing object boundaries. Importantly, our formulation allows learning the reference edge map from intermediate CNN features instead of using the image gradient magnitude as in standard DT filtering. This produces task-specific edges in an end-to-end trainable system optimizing the target semantic segmentation quality.",0
"Cutting-edge semantic image segmentation systems rely heavily on deep convolutional neural networks (CNNs). However, to enhance object localization accuracy, recent research has introduced fully-connected conditional random fields (CRFs) as a complementary tool to CNNs. While dense CRF inference is effective, it is also computationally expensive. Our solution is to replace fully-connected CRFs with domain transform (DT), which is a contemporary edge-preserving filtering technique. DT offers faster processing times compared to dense CRF inference, while also producing comparable semantic segmentation results that accurately capture object boundaries. Our approach is unique because it allows for learning the reference edge map from intermediate CNN features, resulting in task-specific edges in an end-to-end trainable system that optimizes the target semantic segmentation quality.",1
"Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation. One common way to extract multi-scale features is to feed multiple resized input images to a shared deep network and then merge the resulting features for pixelwise classification. In this work, we propose an attention mechanism that learns to softly weight the multi-scale features at each pixel location. We adapt a state-of-the-art semantic image segmentation model, which we jointly train with multi-scale input images and the attention model. The proposed attention model not only outperforms average- and max-pooling, but allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.",0
"To achieve state-of-the-art performance on semantic image segmentation, it is essential to integrate multi-scale features into fully convolutional neural networks (FCNs). One way to extract these features is by feeding multiple input images of different sizes to a shared deep network and then merging the resultant features for pixelwise classification. In this study, we propose an attention mechanism that softly weights the multi-scale features at each pixel location. We train a state-of-the-art semantic image segmentation model with multi-scale input images and the attention model. Our proposed attention model outperforms average- and max-pooling and enables us to visualize the significance of features at different positions and scales. Additionally, we found that adding extra supervision to the output at each scale is crucial for excellent performance when merging multi-scale features. Our model's effectiveness is demonstrated through extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012, and a subset of MS-COCO 2014.",1
"Recent years have shown that deep learned neural networks are a valuable tool in the field of computer vision. This paper addresses the use of two different kinds of network architectures, namely LeNet and Network in Network (NiN). They will be compared in terms of both performance and computational efficiency by addressing the classification and detection problems. In this paper, multiple databases will be used to test the networks. One of them contains images depicting burn wounds from pediatric cases, another one contains an extensive number of art images and other facial databases were used for facial keypoints detection.",0
"The past few years have demonstrated the usefulness of deep learned neural networks for computer vision. This article explores the utilization of two distinct network architectures, LeNet and Network in Network (NiN), in addressing classification and detection problems. The evaluation will compare their performance and computational efficiency using various databases. These include images of burn wounds from pediatric cases, an extensive collection of art images, and facial databases for detecting facial keypoints.",1
"Commonly used human motion capture systems require intrusive attachment of markers that are visually tracked with multiple cameras. In this work we present an efficient and inexpensive solution to markerless motion capture using only a few Kinect sensors. Unlike the previous work on 3d pose estimation using a single depth camera, we relax constraints on the camera location and do not assume a co-operative user. We apply recent image segmentation techniques to depth images and use curriculum learning to train our system on purely synthetic data. Our method accurately localizes body parts without requiring an explicit shape model. The body joint locations are then recovered by combining evidence from multiple views in real-time. We also introduce a dataset of ~6 million synthetic depth frames for pose estimation from multiple cameras and exceed state-of-the-art results on the Berkeley MHAD dataset.",0
"Conventional human motion capture methods necessitate the use of markers that are affixed to the body and tracked by several cameras. We propose a cost-effective and efficient alternative that does not require markers and uses only a few Kinect sensors. Unlike previous research that relied on a single depth camera and assumed a cooperative user, our approach does not impose constraints on camera placement and user behavior. We utilize contemporary image segmentation techniques on depth images and employ curriculum learning to train our system on synthetic data alone. Our technique detects body parts with high accuracy without the need for a specific shape model. By combining information from multiple views in real-time, we derive the location of body joints. Furthermore, we introduce a dataset comprising approximately six million synthetic depth frames for pose estimation from various cameras and surpass state-of-the-art results on the Berkeley MHAD dataset.",1
"Image segmentation, the process of separating the elements within an image, is frequently used for obtaining information from photomicrographs. However, segmentation methods should be used with reservations: incorrect segmentation can mislead when interpreting regions of interest (ROI), thus decreasing the success rate of additional procedures. Multi-Level Starlet Segmentation (MLSS) and Multi-Level Starlet Optimal Segmentation (MLSOS) were developed to address the photomicrograph segmentation deficiency on general tools. These methods gave rise to Jansen-MIDAS, an open-source software which a scientist can use to obtain a multi-level threshold segmentation of his/hers photomicrographs. This software is presented in two versions: a text-based version, for GNU Octave, and a graphical user interface (GUI) version, for MathWorks MATLAB. It can be used to process several types of images, becoming a reliable alternative to the scientist.",0
"Image segmentation is a common practice used to extract information from photomicrographs by separating different elements in the image. However, it should be used cautiously as incorrect segmentation can lead to misinterpretation of regions of interest and decrease the overall success rate of additional procedures. To overcome the limitations of general tools, Multi-Level Starlet Segmentation (MLSS) and Multi-Level Starlet Optimal Segmentation (MLSOS) were developed for photomicrograph segmentation. These methods have led to the creation of Jansen-MIDAS, an open-source software that provides reliable multi-level threshold segmentation for photomicrographs. The software is available in two versions: a text-based version for GNU Octave and a graphical user interface (GUI) version for MathWorks MATLAB, making it a dependable alternative for scientists to process various types of images.",1
"Quality evaluation of image segmentation algorithms are still subject of debate and research. Currently, there is no generic metric that could be applied to any algorithm reliably. This article contains an evaluation for the PSRN (Peak Signal-To-Noise Ratio) as a metric which has been used to evaluate threshold level selection as well as the number of thresholds in the case of multi-level segmentation. The results obtained in this study suggest that the PSNR is not an adequate quality measurement for segmentation algorithms.",0
"The assessment of image segmentation algorithms' quality is still under discussion and investigation. Presently, no universal standard exists that can be consistently applied to any algorithm. This piece presents an evaluation of the PSRN (Peak Signal-To-Noise Ratio) as a metric that has been utilized to assess threshold level selection and the number of thresholds in multi-level segmentation situations. Based on the findings of this study, the PSNR is an insufficient quality measurement for segmentation algorithms.",1
"We propose an approach to instance-level image segmentation that is built on top of category-level segmentation. Specifically, for each pixel in a semantic category mask, its corresponding instance bounding box is predicted using a deep fully convolutional regression network. Thus it follows a different pipeline to the popular detect-then-segment approaches that first predict instances' bounding boxes, which are the current state-of-the-art in instance segmentation. We show that, by leveraging the strength of our state-of-the-art semantic segmentation models, the proposed method can achieve comparable or even better results to detect-then-segment approaches. We make the following contributions. (i) First, we propose a simple yet effective approach to semantic instance segmentation. (ii) Second, we propose an online bootstrapping method during training, which is critically important for achieving good performance for both semantic category segmentation and instance-level segmentation. (iii) As the performance of semantic category segmentation has a significant impact on the instance-level segmentation, which is the second step of our approach, we train fully convolutional residual networks to achieve the best semantic category segmentation accuracy. On the PASCAL VOC 2012 dataset, we obtain the currently best mean intersection-over-union score of 79.1%. (iv) We also achieve state-of-the-art results for instance-level segmentation.",0
"Our proposed approach to instance-level image segmentation builds upon category-level segmentation. We utilize a deep fully convolutional regression network to predict the corresponding instance bounding box for each pixel in the semantic category mask. Unlike the detect-then-segment approach, which predicts instances' bounding boxes first, our method follows a different pipeline. By leveraging the strength of our state-of-the-art semantic segmentation models, we demonstrate that our approach achieves comparable or even superior results. We contribute three key components: (i) a simple yet effective approach to semantic instance segmentation, (ii) an online bootstrapping method during training for improved performance in both semantic category segmentation and instance-level segmentation, and (iii) the use of fully convolutional residual networks for achieving the best semantic category segmentation accuracy. Our approach achieves the currently best mean intersection-over-union score of 79.1% on the PASCAL VOC 2012 dataset and state-of-the-art results for instance-level segmentation.",1
"It is well-known in image processing that computational cost increases rapidly with the number and dimensions of the images to be processed. Several fields, such as medical imaging, routinely use numerous very large images, which might also be 3D and/or captured at several frequency bands, all adding to the computational expense. Multiresolution analysis is a method of increasing the efficiency of the segmentation process. One multiresolution approach is the coarse-to-fine segmentation strategy, whereby the segmentation starts at a coarse resolution and is then fine-tuned during subsequent steps. The starting resolution for segmentation is generally selected arbitrarily with no clear selection criteria. The research reported in this paper showed that starting from different resolutions for image segmentation results in different accuracies and computational times, even for images of the same category (depicting similar scenes or objects). An automated method for resolution selection for an input image would thus be beneficial. This paper introduces a framework for the automated selection of the best resolution for image segmentation. We propose a measure for defining the best resolution based on user/system criteria, offering a trade-off between accuracy and computation time. A learning approach is then introduced for the selection of the resolution, whereby extracted image features are mapped to the previously determined best resolution. In the learning process, class (i.e., resolution) distribution is generally imbalanced, making effective learning from the data difficult. Experiments conducted with three datasets using two different segmentation algorithms show that the resolutions selected through learning enable much faster segmentation than the original ones, while retaining at least the original accuracy.",0
"Image processing is known to become increasingly computationally expensive as the number and dimensions of images to be processed increase. Certain fields, such as medical imaging, often require processing of large, multidimensional images captured at various frequencies, further increasing computational costs. Multiresolution analysis is a method of enhancing segmentation efficiency, with coarse-to-fine segmentation being one such approach. However, the starting resolution for segmentation is generally selected arbitrarily, resulting in varying accuracies and computational times. This paper presents a framework for the automated selection of the optimal resolution for image segmentation, offering a balance between accuracy and computation time. A learning approach is employed, where image features are mapped to the best resolution and used to select the ideal resolution for an input image. Class distribution in the learning process is often imbalanced, making effective learning difficult. Experiments with three datasets using two segmentation algorithms demonstrate that the resolutions selected through learning enable faster segmentation while maintaining original accuracy.",1
"Superpixel segmentation is becoming ubiquitous in computer vision. In practice, an object can either be represented by a number of segments in finer levels of detail or included in a surrounding region at coarser levels of detail, and thus a superpixel segmentation hierarchy is useful for applications that require different levels of image segmentation detail depending on the particular image objects segmented. Unfortunately, there is no method that can generate all scales of superpixels accurately in real-time. As a result, a simple yet effective algorithm named Super Hierarchy (SH) is proposed in this paper. It is as accurate as the state-of-the-art but 1-2 orders of magnitude faster. The proposed method can be directly integrated with recent efficient edge detectors like the structured forest edges to significantly outperforms the state-of-the-art in terms of segmentation accuracy. Quantitative and qualitative evaluation on a number of computer vision applications was conducted, demonstrating that the proposed method is the top performer.",0
"Superpixel segmentation is becoming increasingly common in the field of computer vision. It is practical to represent an object either with many segments for fine details or to group it with other objects for coarser details. Therefore, a hierarchy of superpixel segmentation is useful for applications that require different levels of segmentation detail depending on the image objects. Unfortunately, no real-time method can generate all scales of superpixels accurately. To address this issue, this paper proposes a simple yet efficient algorithm called Super Hierarchy (SH), which is as precise as the current state-of-the-art methods but much faster (1-2 orders of magnitude). The proposed method can be directly integrated with advanced edge detectors such as the structured forest edges, leading to superior segmentation accuracy compared to previous methods. The proposed method was evaluated quantitatively and qualitatively in multiple computer vision applications, indicating its superior performance.",1
"Recent applications in computer vision have come to heavily rely on superpixel over-segmentation as a pre-processing step for higher level vision tasks, such as object recognition, image labelling or image segmentation. Here we present a new superpixel algorithm called Hierarchical Piecewise-Constant Super-regions (HPCS), which not only obtains superpixels comparable to the state-of-the-art, but can also be applied hierarchically to form what we call n-th order super-regions. In essence, a Markov Random Field (MRF)-based anisotropic denoising formulation over the quantized feature space is adopted to form piecewise-constant image regions, which are then combined with a graph-based split & merge post-processing step to form superpixels. The graph and quantized feature based formulation of the problem allows us to generalize it hierarchically to preserve boundary adherence with fewer superpixels. Experimental results show that, despite the simplicity of our framework, it is able to provide high quality superpixels, and to hierarchically apply them to form layers of over-segmentation, each with a decreasing number of superpixels, while maintaining the same desired properties (such as adherence to strong image edges). The algorithm is also memory efficient and has a low computational cost.",0
"Superpixel over-segmentation has become a crucial pre-processing step in computer vision applications for advanced vision tasks, including image segmentation, image labeling, and object recognition. Our study introduces a new superpixel algorithm, called Hierarchical Piecewise-Constant Super-regions (HPCS), that produces high-quality superpixels comparable to the latest technology while enabling hierarchical application to create n-th order super-regions. By utilizing a Markov Random Field (MRF)-based anisotropic denoising formulation over the quantized feature space to generate piecewise-constant image regions, we can combine a graph-based split & merge post-processing step to create superpixels. This approach allows for hierarchical generalization while maintaining boundary adherence with fewer superpixels. Our experimental results demonstrate that our simple methodology can deliver superior quality superpixels and hierarchical layering of over-segmentation, with decreasing numbers of superpixels while preserving key features such as adherence to robust image edges. Furthermore, our algorithm is memory-efficient and has a low computational cost.",1
"Segmentation partitions an image into its constituent parts. It is essentially the pre-processing stage of image analysis and computer vision. In this work, T1 and T2 weighted brain magnetic resonance images are segmented using multilevel thresholding and bacterial foraging optimization (BFO) algorithm. The thresholds are obtained by maximizing the between class variance (multilevel Otsu method) of the image. The BFO algorithm is used to optimize the threshold searching process. The edges are then obtained from the thresholded image by comparing the intensity of each pixel with its eight connected neighbourhood. Post processing is performed to remove spurious responses in the segmented image. The proposed segmentation technique is evaluated using edge detector evaluation parameters such as figure of merit, Rand Index and variation of information. The proposed brain MR image segmentation technique outperforms the traditional edge detectors such as canny and sobel.",0
"The act of segmentation involves dividing an image into its individual components, and it serves as a necessary step in image analysis and computer vision. In this particular study, T1 and T2 weighted brain magnetic resonance images underwent segmentation via multilevel thresholding and the bacterial foraging optimization (BFO) algorithm. The thresholds were established through the multilevel Otsu method, which maximizes the between-class variance of the image. The BFO algorithm was utilized to optimize the threshold search process. The resulting thresholded image produced edges through a comparison of pixel intensity with its eight connected neighborhood. To eliminate any false responses in the segmented image, post-processing was performed. The efficacy of the segmentation technique was assessed through edge detector evaluation parameters, including the figure of merit, Rand Index, and variation of information. Compared to traditional edge detectors like Canny and Sobel, the proposed brain MR image segmentation technique exhibited superior performance.",1
"This paper investigates the problem of image segmentation using superpixels. We propose two approaches to enhance the discriminative ability of the superpixel's covariance descriptors. In the first one, we employ the Log-Euclidean distance as the metric on the covariance manifolds, and then use the RBF kernel to measure the similarities between covariance descriptors. The second method is focused on extracting the subspace structure of the set of covariance descriptors by extending a low rank representation algorithm on to the covariance manifolds. Experiments are carried out with the Berkly Segmentation Dataset, and compared with the state-of-the-art segmentation algorithms, both methods are competitive.",0
"The aim of this study is to explore image segmentation through the use of superpixels. To improve the superpixel's ability to differentiate, we have developed two methods. The first involves utilizing the Log-Euclidean distance to measure the covariance manifolds, followed by using the RBF kernel to evaluate similarity between descriptors. The second approach focuses on extending a low rank representation algorithm onto the covariance manifolds to extract the subspace structure of the set of covariance descriptors. Using the Berkly Segmentation Dataset, we conducted experiments and found that both methods are comparable to the top segmentation algorithms.",1
"This paper describes a fast and accurate semantic image segmentation approach that encodes not only the discriminative features from deep neural networks, but also the high-order context compatibility among adjacent objects as well as low level image features. We formulate the underlying problem as the conditional random field that embeds local feature extraction, clique potential construction, and guided filtering within the same framework, and provide an efficient coarse-to-fine solver. At the coarse level, we combine local feature representation and context interaction using a deep convolutional network, and directly learn the interaction from high order cliques with a message passing routine, avoiding time-consuming explicit graph inference for joint probability distribution. At the fine level, we introduce a guided filtering interpretation for the mean field algorithm, and achieve accurate object boundaries with 100+ faster than classic learning methods. The two parts are connected and jointly trained in an end-to-end fashion. Experimental results on Pascal VOC 2012 dataset have shown that the proposed algorithm outperforms the state-of-the-art, and that it achieves the rank 1 performance at the time of submission, both of which prove the effectiveness of this unified framework for semantic image segmentation.",0
"In this paper, a semantic image segmentation approach is presented that is both fast and accurate. It incorporates discriminative features from deep neural networks, high-order context compatibility among adjacent objects, and low-level image features. The underlying problem is formulated as a conditional random field that combines local feature extraction, clique potential construction, and guided filtering within the same framework. An efficient coarse-to-fine solver is provided that utilizes a deep convolutional network to combine local feature representation and context interaction at the coarse level. At the fine level, a guided filtering interpretation is introduced for the mean field algorithm to achieve accurate object boundaries. The two parts are connected and jointly trained in an end-to-end fashion. Experimental results on the Pascal VOC 2012 dataset demonstrate that this unified framework outperforms state-of-the-art methods and achieves rank 1 performance.",1
"Image segmentation in the medical domain is a challenging field owing to poor resolution and limited contrast. The predominantly used conventional segmentation techniques and the thresholding methods suffer from limitations because of heavy dependence on user interactions. Uncertainties prevalent in an image cannot be captured by these techniques. The performance further deteriorates when the images are corrupted by noise, outliers and other artifacts. The objective of this paper is to develop an effective robust fuzzy C- means clustering for segmenting vertebral body from magnetic resonance image owing to its unsupervised form of learning. The motivation for this work is detection of spine geometry and proper localisation and labelling will enhance the diagnostic output of a physician. The method is compared with Otsu thresholding and K-means clustering to illustrate the robustness.The reference standard for validation was the annotated images from the radiologist, and the Dice coefficient and Hausdorff distance measures were used to evaluate the segmentation.",0
"Due to low resolution and limited contrast, medical image segmentation presents a significant challenge. Traditional segmentation techniques and thresholding methods heavily rely on user interaction and are unable to capture uncertainties in an image. These methods perform poorly when images are affected by noise, outliers, and other artifacts. This paper aims to create a robust and effective fuzzy C-means clustering algorithm for segmenting vertebral bodies in magnetic resonance images. The unsupervised learning approach is motivated by the desire to detect spine geometry and improve diagnostic outcomes for physicians. The proposed method is compared to Otsu thresholding and K-means clustering to demonstrate its robustness. The accuracy of the segmentation is evaluated using annotated images from a radiologist and measured using the Dice coefficient and Hausdorff distance.",1
"Estimates of image gradients play a ubiquitous role in image segmentation and classification problems since gradients directly relate to the boundaries or the edges of a scene. This paper proposes an unified approach to gradient estimation based on fractional calculus that is computationally cheap and readily applicable to any existing algorithm that relies on image gradients. We show experiments on edge detection and image segmentation on the Stanford Backgrounds Dataset where these improved local gradients outperforms state of the art, achieving a performance of 79.2% average accuracy.",0
"The calculation of image gradients is essential for image segmentation and classification tasks as they are closely related to the edges and boundaries of a scene. In this study, a cost-effective and universally applicable technique for gradient estimation is presented, employing fractional calculus. The proposed method can be easily integrated with any algorithm depending on image gradients. The effectiveness of the approach is demonstrated through experiments on edge detection and image segmentation using the Stanford Backgrounds Dataset, where the enhanced local gradients surpass the current leading methods, yielding an average accuracy of 79.2%.",1
"With the recent popularity of graphical clustering methods, there has been an increased focus on the information between samples. We show how learning cluster structure using edge features naturally and simultaneously determines the most likely number of clusters and addresses data scale issues. These results are particularly useful in instances where (a) there are a large number of clusters and (b) we have some labeled edges. Applications in this domain include image segmentation, community discovery and entity resolution. Our model is an extension of the planted partition model and our solution uses results of correlation clustering, which achieves a partition O(log(n))-close to the log-likelihood of the true clustering.",0
"Due to the rise in popularity of graphical clustering methods, there is now a greater emphasis on the information between samples. Our study demonstrates that learning cluster structure through edge features can naturally and simultaneously determine the most probable number of clusters and tackle data scale problems. This is particularly advantageous when dealing with a sizable number of clusters and labeled edges. The model we propose is an extension of the planted partition model and our solution employs correlation clustering results to achieve a partition that is O(log(n))-close to the log-likelihood of the true clustering. This approach has practical applications in image segmentation, community discovery, and entity resolution.",1
"This paper proposes a joint segmentation and deconvolution Bayesian method for medical ultrasound (US) images. Contrary to piecewise homogeneous images, US images exhibit heavy characteristic speckle patterns correlated with the tissue structures. The generalized Gaussian distribution (GGD) has been shown to be one of the most relevant distributions for characterizing the speckle in US images. Thus, we propose a GGD-Potts model defined by a label map coupling US image segmentation and deconvolution. The Bayesian estimators of the unknown model parameters, including the US image, the label map and all the hyperparameters are difficult to be expressed in closed form. Thus, we investigate a Gibbs sampler to generate samples distributed according to the posterior of interest. These generated samples are finally used to compute the Bayesian estimators of the unknown parameters. The performance of the proposed Bayesian model is compared with existing approaches via several experiments conducted on realistic synthetic data and in vivo US images.",0
"In this paper, a novel approach for segmenting and deconvolving medical ultrasound (US) images is proposed using a joint Bayesian method. Unlike uniform images, US images have distinct speckle patterns that are closely related to the tissue structures. The speckle patterns in US images can be accurately described by using the generalized Gaussian distribution (GGD). To address this, a GGD-Potts model is introduced, which combines US image segmentation and deconvolution with a label map. The parameters of the model, including the US image, label map, and hyperparameters, are challenging to estimate using closed-form expressions. Therefore, a Gibbs sampler is employed to generate samples that follow the desired posterior distribution. The generated samples are then used to compute the Bayesian estimators of the unknown parameters. The proposed Bayesian model is evaluated against existing methods using realistic synthetic data and in vivo US images in multiple experiments.",1
"In computer vision, many problems such as image segmentation, pixel labelling, and scene parsing can be formulated as binary quadratic programs (BQPs). For submodular problems, cuts based methods can be employed to efficiently solve large-scale problems. However, general nonsubmodular problems are significantly more challenging to solve. Finding a solution when the problem is of large size to be of practical interest, however, typically requires relaxation. Two standard relaxation methods are widely used for solving general BQPs--spectral methods and semidefinite programming (SDP), each with their own advantages and disadvantages. Spectral relaxation is simple and easy to implement, but its bound is loose. Semidefinite relaxation has a tighter bound, but its computational complexity is high, especially for large scale problems. In this work, we present a new SDP formulation for BQPs, with two desirable properties. First, it has a similar relaxation bound to conventional SDP formulations. Second, compared with conventional SDP methods, the new SDP formulation leads to a significantly more efficient and scalable dual optimization approach, which has the same degree of complexity as spectral methods. We then propose two solvers, namely, quasi-Newton and smoothing Newton methods, for the dual problem. Both of them are significantly more efficiently than standard interior-point methods. In practice, the smoothing Newton solver is faster than the quasi-Newton solver for dense or medium-sized problems, while the quasi-Newton solver is preferable for large sparse/structured problems. Our experiments on a few computer vision applications including clustering, image segmentation, co-segmentation and registration show the potential of our SDP formulation for solving large-scale BQPs.",0
"Binary quadratic programs (BQPs) are commonly used to address computer vision problems such as image segmentation, pixel labeling, and scene parsing. For submodular problems, cuts-based methods can solve large-scale issues efficiently. However, non-submodular problems are more challenging to solve. Relaxation is necessary to find solutions for practical problems of large size. Spectral methods and semidefinite programming (SDP) are two commonly used relaxation techniques for solving general BQPs, each with its own advantages and disadvantages. Spectral relaxation is simple but has a loose bound, whereas SDP relaxation has a tighter bound but high computational complexity for large-scale problems. In this study, we present a new SDP formulation for BQPs that has a similar relaxation bound to conventional SDP formulations. Compared with conventional SDP methods, the new formulation has a more efficient and scalable dual optimization approach, similar to spectral methods. We propose two solvers, the quasi-Newton and smoothing Newton methods, for the dual problem. Both solvers are significantly more efficient than standard interior-point methods. The smoothing Newton solver is faster than the quasi-Newton solver for dense or medium-sized problems, while the quasi-Newton solver is preferable for large sparse/structured problems. Our experiments on computer vision applications show the potential of our SDP formulation for solving large-scale BQPs.",1
"We propose a framework for top-down salient object detection that incorporates a tightly coupled image classification module. The classifier is trained on novel category-aware sparse codes computed on object dictionaries used for saliency modeling. A misclassification indicates that the corresponding saliency model is inaccurate. Hence, the classifier selects images for which the saliency models need to be updated. The category-aware sparse coding produces better image classification accuracy as compared to conventional sparse coding with a reduced computational complexity. A saliency-weighted max-pooling is proposed to improve image classification, which is further used to refine the saliency maps. Experimental results on Graz-02 and PASCAL VOC-07 datasets demonstrate the effectiveness of salient object detection. Although the role of the classifier is to support salient object detection, we evaluate its performance in image classification and also illustrate the utility of thresholded saliency maps for image segmentation.",0
"Our proposed framework for detecting top-down salient objects includes an image classification module that is closely integrated. The module is trained on novel category-aware sparse codes that are computed using object dictionaries for saliency modeling. If there is a misclassification, it indicates an inaccurate saliency model. As a result, the classifier chooses images that require updated saliency models. The category-aware sparse coding results in better image classification accuracy with less computational complexity than conventional sparse coding. We suggest a saliency-weighted max-pooling approach to improve image classification and to refine the saliency maps. Our experimental results, using Graz-02 and PASCAL VOC-07 datasets, show the effectiveness of salient object detection. Although the classifier's purpose is to assist in salient object detection, we also assess its performance in image classification and illustrate the usefulness of thresholded saliency maps for image segmentation.",1
"Large-scale data is of crucial importance for learning semantic segmentation models, but annotating per-pixel masks is a tedious and inefficient procedure. We note that for the topic of interactive image segmentation, scribbles are very widely used in academic research and commercial software, and are recognized as one of the most user-friendly ways of interacting. In this paper, we propose to use scribbles to annotate images, and develop an algorithm to train convolutional networks for semantic segmentation supervised by scribbles. Our algorithm is based on a graphical model that jointly propagates information from scribbles to unmarked pixels and learns network parameters. We present competitive object semantic segmentation results on the PASCAL VOC dataset by using scribbles as annotations. Scribbles are also favored for annotating stuff (e.g., water, sky, grass) that has no well-defined shape, and our method shows excellent results on the PASCAL-CONTEXT dataset thanks to extra inexpensive scribble annotations. Our scribble annotations on PASCAL VOC are available at http://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble_sup",0
"Learning semantic segmentation models requires large amounts of data, but manually annotating per-pixel masks can be time-consuming and inefficient. Scribbles, which are widely used in academia and commercial software for interactive image segmentation, are considered a user-friendly way of interacting. Our paper proposes the use of scribbles to annotate images and presents an algorithm to train convolutional networks for semantic segmentation supervised by scribbles. Our algorithm is based on a graphical model that propagates information from scribbles to unmarked pixels and learns network parameters. We achieved competitive object semantic segmentation results on the PASCAL VOC dataset using scribbles as annotations. Scribbles are also useful for annotating stuff, and our method showed excellent results on the PASCAL-CONTEXT dataset with inexpensive scribble annotations. Our scribble annotations for PASCAL VOC can be found at http://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble_sup.",1
"We propose a method for high-performance semantic image segmentation (or semantic pixel labelling) based on very deep residual networks, which achieves the state-of-the-art performance. A few design factors are carefully considered to this end.   We make the following contributions. (i) First, we evaluate different variations of a fully convolutional residual network so as to find the best configuration, including the number of layers, the resolution of feature maps, and the size of field-of-view. Our experiments show that further enlarging the field-of-view and increasing the resolution of feature maps are typically beneficial, which however inevitably leads to a higher demand for GPU memories. To walk around the limitation, we propose a new method to simulate a high resolution network with a low resolution network, which can be applied during training and/or testing. (ii) Second, we propose an online bootstrapping method for training. We demonstrate that online bootstrapping is critically important for achieving good accuracy. (iii) Third we apply the traditional dropout to some of the residual blocks, which further improves the performance. (iv) Finally, our method achieves the currently best mean intersection-over-union 78.3\% on the PASCAL VOC 2012 dataset, as well as on the recent dataset Cityscapes.",0
"Our proposed method for semantic image segmentation (or semantic pixel labelling) achieves state-of-the-art performance by utilizing very deep residual networks, while taking into account several design factors. We conducted experiments to evaluate various configurations of a fully convolutional residual network, including layer number, feature map resolution, and field-of-view size, and found that increasing the field-of-view and feature map resolution is beneficial but requires more GPU memory. To overcome this limitation, we introduce a method that simulates a high-resolution network with a low-resolution one during training and/or testing. We also propose an online bootstrapping method for training, which is critical for achieving good accuracy. Additionally, applying traditional dropout to some residual blocks further improves performance. Our method achieves the best mean intersection-over-union of 78.3% on both the PASCAL VOC 2012 and Cityscapes datasets.",1
"Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.",0
"Image understanding heavily relies on tasks that involve pixel-level labelling, such as semantic segmentation. Recent efforts have aimed to utilize deep learning techniques for image recognition to address these tasks. However, deep learning is limited in its ability to accurately distinguish visual objects. To overcome this issue, we introduce a new type of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. We use mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks. This network, known as CRF-RNN, is integrated into a CNN, creating a deep network that incorporates desirable properties of both CNNs and CRFs. The integration of CRF modelling with CNNs allows the entire deep network to be trained end-to-end with the standard back-propagation algorithm, eliminating the need for offline post-processing methods for object delineation. Our approach is applied to semantic image segmentation and achieves top results on the challenging Pascal VOC 2012 segmentation benchmark.",1
"In this paper a hierarchical model for pixel clustering and image segmentation is developed. In the model an image is hierarchically structured. The original image is treated as a set of nested images, which are capable to reversibly merge with each other. An object is defined as a structural element of an image, so that, an image is regarded as a maximal object. The simulating of none-hierarchical optimal pixel clustering by hierarchical clustering is studied. To generate a hierarchy of optimized piecewise constant image approximations, estimated by the standard deviation of approximation from the image, the conversion of any hierarchy of approximations into the hierarchy described in relation to the number of intensity levels by convex sequence of total squared errors is proposed.",0
"This paper presents a hierarchical model that can be used for image segmentation and pixel clustering. The model works by structuring the original image hierarchically, treating it as a set of nested images that can be merged together in a reversible manner. The model defines an object as a structural element of an image, with the image itself being considered as a maximal object. The study also looks at simulating non-hierarchical optimal pixel clustering through the use of hierarchical clustering. To create a hierarchy of optimized piecewise constant image approximations, the paper proposes converting any hierarchy of approximations into a hierarchy based on the number of intensity levels, using a convex sequence of total squared errors.",1
"Spectral embedding provides a framework for solving perceptual organization problems, including image segmentation and figure/ground organization. From an affinity matrix describing pairwise relationships between pixels, it clusters pixels into regions, and, using a complex-valued extension, orders pixels according to layer. We train a convolutional neural network (CNN) to directly predict the pairwise relationships that define this affinity matrix. Spectral embedding then resolves these predictions into a globally-consistent segmentation and figure/ground organization of the scene. Experiments demonstrate significant benefit to this direct coupling compared to prior works which use explicit intermediate stages, such as edge detection, on the pathway from image to affinities. Our results suggest spectral embedding as a powerful alternative to the conditional random field (CRF)-based globalization schemes typically coupled to deep neural networks.",0
"The use of spectral embedding offers a solution to various perceptual organization issues, including image segmentation and figure/ground organization. By clustering pixels into regions based on their pairwise relationships, as described in an affinity matrix, and utilizing a complex-valued extension to order pixels by layer, this method is effective. We have trained a convolutional neural network (CNN) to directly predict these pairwise relationships, resulting in a globally-consistent segmentation and figure/ground organization of the scene. Compared to previous methods which use edge detection as an intermediate step, our direct approach proves to be advantageous. These findings suggest that spectral embedding is a potent alternative to the typical conditional random field (CRF)-based globalization schemes associated with deep neural networks.",1
"Extracting a connectome from an electron microscopy (EM) data set requires identification of neurons and determination of synapses between neurons. As manual extraction of this information is very time-consuming, there has been extensive research effort to automatically segment the neurons to help guide and eventually replace manual tracing. Until recently, there has been comparatively less research on automatically detecting the actual synapses between neurons. This discrepancy can, in part, be attributed to several factors: obtaining neuronal shapes is a prerequisite first step in extracting a connectome, manual tracing is much more time-consuming than annotating synapses, and neuronal contact area can be used as a proxy for synapses in determining connections.   However, recent research has demonstrated that contact area alone is not a sufficient predictor of synaptic connection. Moreover, as segmentation has improved, we have observed that synapse annotation is consuming a more significant fraction of overall reconstruction time. This ratio will only get worse as segmentation improves, gating overall possible speed-up. Therefore, we address this problem by developing algorithms that automatically detect pre-synaptic neurons and their post-synaptic partners. In particular, pre-synaptic structures are detected using a Deep and Wide Multiscale Recursive Network, and post-synaptic partners are detected using a MLP with features conditioned on the local segmentation.   This work is novel because it requires minimal amount of training, leverages advances in image segmentation directly, and provides a complete solution for polyadic synapse detection. We further introduce novel metrics to evaluate our algorithm on connectomes of meaningful size. These metrics demonstrate that complete automatic prediction can be used to effectively characterize most connectivity correctly.",0
"To obtain a connectome from EM data, it is necessary to identify neurons and their synapses. Manual extraction of this information is time-consuming, so automatic segmentation techniques have been developed to replace manual tracing. However, there has been less research on detecting synapses automatically compared to segmenting neurons. This is because tracing neurons is a prerequisite for extracting a connectome, and neuronal contact area can be used as a proxy for synapses. Recent research has shown that contact area alone is not sufficient for predicting synaptic connection. As segmentation improves, synapse annotation consumes more reconstruction time, and this problem can be addressed by developing algorithms that automatically detect pre-synaptic neurons and their post-synaptic partners. Our work introduces novel metrics to evaluate this algorithm, and we demonstrate that complete automatic prediction can effectively characterize most connectivity correctly.",1
"Computer vision has become a major source of information for autonomous navigation of robots of various types, self-driving cars, military robots and mars/lunar rovers are some examples. Nevertheless, the majority of methods focus on analysing images captured in visible spectrum. In this manuscript we elaborate on the problem of segmenting cross-country scenes captured in IR spectrum. For this purpose we proposed employing salient features. Salient features are robust to variations in scale, brightness and view angle. We suggest the Speeded-Up Robust Features as a basis for our salient features for a number of reasons discussed in the paper. We also provide a comparison of two SURF implementations. The SURF features are extracted from images of different terrain types. For every feature we estimate a terrain class membership function. The membership values are obtained by means of either the multi-layer perceptron or nearest neighbours. The features' class membership values and their spatial positions are then applied to estimate class membership values for all pixels in the image. To decrease the effect of segmentation blinking that is caused by rapid switching between different terrain types and to speed up segmentation, we are tracking camera position and predict features' positions. The comparison of the multi-layer perception and the nearest neighbour classifiers is presented in the paper. The error rate of the terrain segmentation using the nearest neighbours obtained on the testing set is 16.6+-9.17%.",0
"Autonomous robots, self-driving cars, military robots, and rovers used for space exploration rely heavily on computer vision for navigation. However, most methods focus on analyzing visible spectrum images. This paper addresses the issue of segmenting cross-country scenes captured in the infrared spectrum by utilizing salient features that can accommodate scale, brightness, and view angle variations. The Speeded-Up Robust Features (SURF) are proposed as a basis for these features for various reasons outlined in the paper. Two implementations of SURF are compared by extracting features from images of different terrain types and estimating a terrain class membership function for each feature using either the multi-layer perceptron or nearest neighbors. The class membership values and spatial positions of the features are then used to estimate class membership values for all pixels in the image. To reduce segmentation blinking and speed up the process, the camera position is tracked to predict the features' positions. The paper presents a comparison of the multi-layer perceptron and nearest neighbor classifiers, with the nearest neighbor approach resulting in a terrain segmentation error rate of 16.6+-9.17% on the testing set.",1
"Topic models (e.g., pLSA, LDA, SLDA) have been widely used for segmenting imagery. These models are confined to crisp segmentation. Yet, there are many images in which some regions cannot be assigned a crisp label (e.g., transition regions between a foggy sky and the ground or between sand and water at a beach). In these cases, a visual word is best represented with partial memberships across multiple topics. To address this, we present a partial membership latent Dirichlet allocation (PM-LDA) model and associated parameter estimation algorithms. Experimental results on two natural image datasets and one SONAR image dataset show that PM-LDA can produce both crisp and soft semantic image segmentations; a capability existing methods do not have.",0
"Topic models such as pLSA, LDA, and SLDA have found extensive use in image segmentation. However, these models are limited to crisp segmentation and fail to assign a precise label to certain areas in images, such as the transitional regions between foggy skies and the ground or between sand and water at a beach. In such situations, it is more appropriate to represent a visual word with partial memberships across multiple topics. To overcome this limitation, we propose a partial membership latent Dirichlet allocation (PM-LDA) model and its associated parameter estimation algorithms. Our experimental results on two natural image datasets and one SONAR image dataset demonstrate that PM-LDA can generate both crisp and soft semantic image segmentations, a capability that existing methods lack.",1
"Color image segmentation is a crucial step in many computer vision and pattern recognition applications. This article introduces an adaptive and unsupervised clustering approach based on Voronoi regions, which can be applied to solve the color image segmentation problem. The proposed method performs region splitting and merging within Voronoi regions of the Dirichlet Tessellated image (also called a Voronoi diagram) , which improves the efficiency and the accuracy of the number of clusters and cluster centroids estimation process. Furthermore, the proposed method uses cluster centroid proximity to merge proximal clusters in order to find the final number of clusters and cluster centroids. In contrast to the existing adaptive unsupervised cluster-based image segmentation algorithms, the proposed method uses K-means clustering algorithm in place of the Fuzzy C-means algorithm to find the final segmented image. The proposed method was evaluated on three different unsupervised image segmentation evaluation benchmarks and its results were compared with two other adaptive unsupervised cluster-based image segmentation algorithms. The experimental results reported in this article confirm that the proposed method outperforms the existing algorithms in terms of the quality of image segmentation results. Also, the proposed method results in the lowest average execution time per image compared to the existing methods reported in this article.",0
"The segmentation of color images is a critical step for various applications related to computer vision and pattern recognition. This article proposes an adaptive and unsupervised clustering approach that leverages Voronoi regions to address this problem. By performing region splitting and merging within the Voronoi regions of the Dirichlet Tessellated image, this method achieves improved efficiency and accuracy in the estimation of the number of clusters and cluster centroids. Additionally, it uses proximity between cluster centroids to merge proximal clusters, thus determining the final number of clusters and centroids. Unlike other unsupervised cluster-based image segmentation algorithms, this approach replaces the Fuzzy C-means algorithm with the K-means clustering algorithm to obtain the final segmented image. The proposed method was evaluated on three unsupervised image segmentation benchmarks and compared to two other adaptive unsupervised cluster-based image segmentation algorithms. The results indicate that this method outperforms existing algorithms in terms of image segmentation quality and has the lowest average execution time per image.",1
"A novel global energy model for multi-class semantic image segmentation is proposed that admits very efficient exact inference and derivative calculations for learning. Inference in this model is equivalent to MAP inference in a particular kind of vector-valued Gaussian Markov random field, and ultimately reduces to solving a linear system of linear PDEs known as a reaction-diffusion system. Solving this system can be achieved in time scaling near-linearly in the number of image pixels by reducing it to sequential FFTs, after a linear change of basis. The efficiency and differentiability of the model make it especially well-suited for integration with convolutional neural networks, even allowing it to be used in interior, feature-generating layers and stacked multiple times. Experimental results are shown demonstrating that the model can be employed profitably in conjunction with different convolutional net architectures, and that doing so compares favorably to joint training of a fully-connected CRF with a convolutional net.",0
"The proposed energy model for multi-class semantic image segmentation offers a novel approach that enables highly efficient exact inference and derivative calculations for learning purposes. The model's inference process is equivalent to MAP inference in a vector-valued Gaussian Markov random field of a specific type, and it ultimately involves solving a linear system of linear PDEs referred to as a reaction-diffusion system. By reducing this system to sequential FFTs, after a linear change of basis, the system can be solved in a time that scales nearly-linearly with the number of image pixels. The model's efficiency and differentiability make it ideal for integration with convolutional neural networks, even in interior, feature-generating layers, and can be stacked multiple times. Experimental results demonstrate the model's usefulness when combined with various convolutional net architectures, which compares favorably to joint training a fully-connected CRF with a convolutional net.",1
"The paper proposes a novel Kernelized image segmentation scheme for noisy images that utilizes the concept of Smallest Univalue Segment Assimilating Nucleus (SUSAN) and incorporates spatial constraints by computing circular colour map induced weights. Fuzzy damping coefficients are obtained for each nucleus or center pixel on the basis of the corresponding weighted SUSAN area values, the weights being equal to the inverse of the number of horizontal and vertical moves required to reach a neighborhood pixel from the center pixel. These weights are used to vary the contributions of the different nuclei in the Kernel based framework. The paper also presents an edge quality metric obtained by fuzzy decision based edge candidate selection and final computation of the blurriness of the edges after their selection. The inability of existing algorithms to preserve edge information and structural details in their segmented maps necessitates the computation of the edge quality factor (EQF) for all the competing algorithms. Qualitative and quantitative analysis have been rendered with respect to state-of-the-art algorithms and for images ridden with varying types of noises. Speckle noise ridden SAR images and Rician noise ridden Magnetic Resonance Images have also been considered for evaluating the effectiveness of the proposed algorithm in extracting important segmentation information.",0
"A new approach to image segmentation for noisy images is proposed in this paper, which utilizes the concept of Smallest Univalue Segment Assimilating Nucleus (SUSAN) and incorporates spatial constraints by computing circular colour map induced weights. To determine the contributions of different nuclei in the Kernel based framework, fuzzy damping coefficients are obtained for each center pixel based on the corresponding weighted SUSAN area values. The weights are determined by the number of horizontal and vertical moves required to reach a neighborhood pixel from the center pixel. An edge quality metric is also presented in the paper, which involves fuzzy decision based edge candidate selection and computation of the blurriness of the edges after their selection. The proposed algorithm is evaluated for its effectiveness in extracting important segmentation information from images ridden with varying types of noises, including speckle noise ridden SAR images and Rician noise ridden Magnetic Resonance Images. Qualitative and quantitative analysis is performed with respect to state-of-the-art algorithms. The need for an edge quality factor (EQF) for all the competing algorithms is emphasized due to the inability of existing algorithms to preserve edge information and structural details in their segmented maps.",1
"One of the most challenging tasks in microarray image analysis is spot segmentation. A solution to this problem is to provide an algorithm than can be used to find any spot within the microarray image. Circular Hough Transformation (CHT) is a powerful feature extraction technique used in image analysis, computer vision, and digital image processing. CHT algorithm is applied on the cDNA microarray images to develop the accuracy and the efficiency of the spots localization, addressing and segmentation process. The purpose of the applied technique is to find imperfect instances of spots within a certain class of circles by applying a voting procedure on the cDNA microarray images for spots localization, addressing and characterizing the pixels of each spot into foreground pixels and background simultaneously. Intensive experiments on the University of North Carolina (UNC) microarray database indicate that the proposed method is superior to the K-means method and the Support vector machine (SVM). Keywords: Hough circle transformation, cDNA microarray image analysis, cDNA microarray image segmentation, spots localization and addressing, spots segmentation",0
"Spot segmentation is a challenging task in microarray image analysis, but an algorithm can be employed to locate any spot within the image. One such tool is Circular Hough Transformation (CHT), a feature extraction technique used in image analysis, computer vision, and digital image processing. By applying the CHT algorithm on cDNA microarray images, spot localization, addressing, and segmentation can be performed with greater accuracy and efficiency. The technique works by identifying imperfect instances of spots within a class of circles and characterizing each spot's foreground and background pixels simultaneously through a voting procedure. Extensive experimentation on the UNC microarray database has shown that this method outperforms the K-means method and Support Vector Machine (SVM). Keywords: Hough circle transformation, cDNA microarray image analysis, cDNA microarray image segmentation, spot localization and addressing, spot segmentation.",1
"Generating natural language descriptions for in-the-wild videos is a challenging task. Most state-of-the-art methods for solving this problem borrow existing deep convolutional neural network (CNN) architectures (AlexNet, GoogLeNet) to extract a visual representation of the input video. However, these deep CNN architectures are designed for single-label centered-positioned object classification. While they generate strong semantic features, they have no inherent structure allowing them to detect multiple objects of different sizes and locations in the frame. Our paper tries to solve this problem by integrating the base CNN into several fully convolutional neural networks (FCNs) to form a multi-scale network that handles multiple receptive field sizes in the original image. FCNs, previously applied to image segmentation, can generate class heat-maps efficiently compared to sliding window mechanisms, and can easily handle multiple scales. To further handle the ambiguity over multiple objects and locations, we incorporate the Multiple Instance Learning mechanism (MIL) to consider objects in different positions and at different scales simultaneously. We integrate our multi-scale multi-instance architecture with a sequence-to-sequence recurrent neural network to generate sentence descriptions based on the visual representation. Ours is the first end-to-end trainable architecture that is capable of multi-scale region processing. Evaluation on a Youtube video dataset shows the advantage of our approach compared to the original single-scale whole frame CNN model. Our flexible and efficient architecture can potentially be extended to support other video processing tasks.",0
"Describing natural language for videos captured in real-life scenarios can be a daunting task. To tackle this challenge, current state-of-the-art methods utilize deep convolutional neural network (CNN) architectures such as AlexNet and GoogLeNet to extract a visual representation of the input video. However, these CNN architectures are designed for single-label centered-positioned object classification and are not equipped to detect multiple objects of varying sizes and positions in a frame, despite their strong semantic features. Our paper proposes a solution to this problem by integrating the base CNN into multiple fully convolutional neural networks (FCNs) that form a multi-scale network capable of handling multiple receptive field sizes. This enables FCNs to generate class heat-maps more efficiently than sliding window mechanisms and easily handle multiple scales. To address the ambiguity surrounding multiple objects and positions, we incorporate the Multiple Instance Learning mechanism (MIL) to simultaneously consider objects in different positions and sizes. Our multi-scale multi-instance architecture is then integrated with a sequence-to-sequence recurrent neural network to generate sentence descriptions based on the visual representation. Our architecture is the first end-to-end trainable architecture capable of multi-scale region processing. Evaluation on a Youtube video dataset demonstrates the superiority of our approach compared to the original single-scale whole frame CNN model. Our flexible and efficient architecture can be potentially extended to support other video processing tasks.",1
"We propose a novel unsupervised image segmentation algorithm, which aims to segment an image into several coherent parts. It requires no user input, no supervised learning phase and assumes an unknown number of segments. It achieves this by first over-segmenting the image into several hundred superpixels. These are iteratively joined on the basis of a discriminative classifier trained on color and texture information obtained from each superpixel. The output of the classifier is regularized by a Markov random field that lends more influence to neighbouring superpixels that are more similar. In each iteration, similar superpixels fall under the same label, until only a few coherent regions remain in the image. The algorithm was tested on a standard evaluation data set, where it performs on par with state-of-the-art algorithms in term of precision and greatly outperforms the state of the art by reducing the oversegmentation of the object of interest.",0
"A new image segmentation algorithm is proposed, which is unsupervised and aims to segment an image into coherent parts without any user input or supervised learning phase, and with an unknown number of segments. The algorithm begins by dividing the image into several hundred superpixels, which are iteratively combined based on a discriminative classifier trained on color and texture information from each superpixel. A Markov random field regularizes the output of the classifier, giving more influence to neighboring superpixels that are more similar. Similar superpixels are grouped under the same label in each iteration until only a few coherent regions remain in the image. The algorithm performs as well as state-of-the-art algorithms in terms of precision and outperforms them by reducing the oversegmentation of the object of interest, as demonstrated by testing on a standard evaluation data set.",1
"Automatic image segmentation becomes very crucial for tumor detection in medical image processing.In general, manual and semi automatic segmentation techniques require more time and knowledge. However these drawbacks had overcome by automatic segmentation still there needs to develop more appropriate techniques for medical image segmentation. Therefore, we proposed hybrid approach based image segmentation using the combined features of region growing and threshold based segmentation techniques. It is followed by pre-processing stage to provide an accurate brain tumor extraction by the help of Magnetic Resonance Imaging (MRI). If the tumor has holes, the region growing segmentation algorithm cannot reveal but the proposed hybrid segmentation technique can be achieved and the result as well improved. Hence the result used to made assessment with the various performance measures as DICE, JACCARD similarity, accuracy, sensitivity and specificity. These similarity measures have been extensively used for evaluation with the ground truth of each processed image and its results are compared and analyzed.",0
"The importance of automatic image segmentation cannot be overstated in the detection of tumors in medical image processing. While manual and semi-automatic segmentation techniques require significant time and knowledge, automatic segmentation has helped to overcome these drawbacks. However, there is still a need to develop more appropriate techniques for medical image segmentation. To address this, we have proposed a hybrid approach to image segmentation that combines region growing and threshold-based segmentation techniques. This is followed by a pre-processing stage that uses Magnetic Resonance Imaging (MRI) to accurately extract brain tumors. If the tumor has holes, the region growing segmentation algorithm may not be able to reveal it, but the proposed hybrid segmentation technique can be achieved and the result improved. To assess the performance of this technique, we used various measures such as DICE, JACCARD similarity, accuracy, sensitivity, and specificity, which were extensively evaluated against the ground truth of each processed image. The results were then compared and analyzed.",1
The focus of this article is on the detection and classification of patterns based on groupoids. The approach hinges on descriptive proximity of points in a set based on the neighborliness property. This approach lends support to image analysis and understanding and in studying nearness of image segments. A practical application of the approach is in terms of the analysis of natural images for pattern identification and classification.,0
"This article centers on identifying and categorizing patterns utilizing groupoids. The method relies on the neighborliness property to measure the descriptive proximity of points within a set. This technique can aid in image analysis and comprehension, specifically in examining the closeness of image segments. A concrete use of this method is analyzing natural images to identify and classify patterns.",1
"Tackling pattern recognition problems in areas such as computer vision, bioinformatics, speech or text recognition is often done best by taking into account task-specific statistical relations between output variables. In structured prediction, this internal structure is used to predict multiple outputs simultaneously, leading to more accurate and coherent predictions. Structural support vector machines (SSVMs) are nonprobabilistic models that optimize a joint input-output function through margin-based learning. Because SSVMs generally disregard the interplay between unary and interaction factors during the training phase, final parameters are suboptimal. Moreover, its factors are often restricted to linear combinations of input features, limiting its generalization power. To improve prediction accuracy, this paper proposes: (i) Joint inference and learning by integration of back-propagation and loss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM factors to neural networks that form highly nonlinear functions of input features. Image segmentation benchmark results demonstrate improvements over conventional SSVM training methods in terms of accuracy, highlighting the feasibility of end-to-end SSVM training with neural factors.",0
"When it comes to solving pattern recognition problems in areas such as computer vision, bioinformatics, speech, or text recognition, it is essential to consider task-specific statistical relations between output variables. To achieve more precise and coherent predictions, structured prediction leverages the internal structure by predicting multiple outputs simultaneously. Structural support vector machines (SSVMs) are models that optimize a joint input-output function through margin-based learning, but they overlook the interaction between unary and interaction factors during training, leading to suboptimal final parameters. Additionally, the factors are often limited to linear combinations of input features, which decreases their generalization power. This paper suggests improving prediction accuracy by integrating back-propagation and loss-augmented inference in SSVM subgradient descent to enable joint inference and learning. Furthermore, it proposes extending SSVM factors to neural networks, which form highly nonlinear functions of input features. In image segmentation benchmark results, this approach improves accuracy compared to traditional SSVM training methods, demonstrating the feasibility of end-to-end SSVM training with neural factors.",1
"We propose a unified approach for bottom-up hierarchical image segmentation and object proposal generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object proposals by exploring efficiently their combinatorial space. We also present Single-scale Combinatorial Grouping (SCG), a faster version of MCG that produces competitive proposals in under five second per image. We conduct an extensive and comprehensive empirical validation on the BSDS500, SegVOC12, SBD, and COCO datasets, showing that MCG produces state-of-the-art contours, hierarchical regions, and object proposals.",0
"Our approach, which we call Multiscale Combinatorial Grouping (MCG), offers a comprehensive solution for generating object proposals and performing bottom-up hierarchical image segmentation for recognition purposes. To achieve this, we first introduce a rapid normalized cuts algorithm. We then present a high-performance hierarchical segmenter that efficiently utilizes multiscale data. Finally, we propose a grouping strategy that combines our multiscale regions to create highly precise object proposals by efficiently exploring their combinatorial space. Additionally, we present Single-scale Combinatorial Grouping (SCG), a faster variant of MCG that delivers competitive proposals in under five seconds per image. We conducted a thorough and extensive empirical validation on the BSDS500, SegVOC12, SBD, and COCO datasets, and our results demonstrate that MCG produces state-of-the-art contours, hierarchical regions, and object proposals.",1
"The progress in imaging techniques have allowed the study of various aspect of cellular mechanisms. To isolate individual cells in live imaging data, we introduce an elegant image segmentation framework that effectively extracts cell boundaries, even in the presence of poor edge details. Our approach works in two stages. First, we estimate pixel interior/border/exterior class probabilities using random ferns. Then, we use an energy minimization framework to compute boundaries whose localization is compliant with the pixel class probabilities. We validate our approach on a manually annotated dataset.",0
"Various aspects of cellular mechanisms can now be studied due to advancements in imaging techniques. Our elegant image segmentation framework effectively extracts cell boundaries from live imaging data, even in the presence of poor edge details, allowing for the isolation of individual cells. Our approach operates in two stages. Initially, we estimate pixel interior/border/exterior class probabilities using random ferns. We then utilize an energy minimization framework to determine boundaries that are in agreement with the pixel class probabilities. To confirm the efficacy of our approach, we tested it on a manually annotated dataset.",1
"A multi-scale greedy-based object proposal generation approach is presented. Based on the multi-scale nature of objects in images, our approach is built on top of a hierarchical segmentation. We first identify the representative and diverse exemplar clusters within each scale by using a diversity ranking algorithm. Object proposals are obtained by selecting a subset from the multi-scale segment pool via maximizing a submodular objective function, which consists of a weighted coverage term, a single-scale diversity term and a multi-scale reward term. The weighted coverage term forces the selected set of object proposals to be representative and compact; the single-scale diversity term encourages choosing segments from different exemplar clusters so that they will cover as many object patterns as possible; the multi-scale reward term encourages the selected proposals to be discriminative and selected from multiple layers generated by the hierarchical image segmentation. The experimental results on the Berkeley Segmentation Dataset and PASCAL VOC2012 segmentation dataset demonstrate the accuracy and efficiency of our object proposal model. Additionally, we validate our object proposals in simultaneous segmentation and detection and outperform the state-of-art performance.",0
"Our proposed approach for generating object proposals is based on a multi-scale greedy algorithm. We leverage the hierarchical segmentation of images to identify diverse and representative exemplar clusters at each scale using a diversity ranking algorithm. By maximizing a submodular objective function, we select a subset of proposals from the multi-scale segment pool. This objective function includes a weighted coverage term to ensure compactness and representativeness, a single-scale diversity term to encourage a variety of segments from different exemplar clusters, and a multi-scale reward term to promote discriminative proposals from multiple layers of the hierarchical segmentation. We demonstrate the effectiveness and efficiency of our approach on two segmentation datasets, Berkeley Segmentation and PASCAL VOC2012, and show superior performance in simultaneous segmentation and detection.",1
"In this study, a machine learning algorithm is used for disaggregation of SMAP brightness temperatures (T$_{\textrm{B}}$) from 36km to 9km. It uses image segmentation to cluster the study region based on meteorological and land cover similarity, followed by a support vector machine based regression that computes the value of the disaggregated T$_{\textrm{B}}$ at all pixels. High resolution remote sensing products such as land surface temperature, normalized difference vegetation index, enhanced vegetation index, precipitation, soil texture, and land-cover were used for disaggregation. The algorithm was implemented in Iowa, United States, from April to July 2015, and compared with the SMAP L3_SM_AP T$_{\textrm{B}}$ product at 9km. It was found that the disaggregated T$_{\textrm{B}}$ were very similar to the SMAP-T$_{\textrm{B}}$ product, even for vegetated areas with a mean difference $\leq$ 5K. However, the standard deviation of the disaggregation was lower by 7K than that of the AP product. The probability density functions of the disaggregated T$_{\textrm{B}}$ were similar to the SMAP-T$_{\textrm{B}}$. The results indicate that this algorithm may be used for disaggregating T$_{\textrm{B}}$ using complex non-linear correlations on a grid.",0
"This study employs a machine learning algorithm to disaggregate SMAP brightness temperatures (T$_{\textrm{B}}$) from 36km to 9km. The algorithm utilizes image segmentation to cluster the study area based on land cover and meteorological similarity, followed by support vector machine-based regression to calculate the disaggregated T$_{\textrm{B}}$ values for all pixels. High-resolution remote sensing products such as land surface temperature, precipitation, soil texture, normalized difference vegetation index, and enhanced vegetation index are utilized for disaggregation. The algorithm is tested in Iowa, United States, during the period April-July 2015, and compared against the SMAP L3_SM_AP T$_{\textrm{B}}$ product at 9km. The results show that the disaggregated T$_{\textrm{B}}$ values are comparable to the SMAP-T$_{\textrm{B}}$ product, with a mean difference of $\leq$ 5K even for vegetated areas. However, the standard deviation of the disaggregation is lower by 7K than that of the AP product. The probability density functions of the disaggregated T$_{\textrm{B}}$ are similar to the SMAP-T$_{\textrm{B}}$, indicating that this algorithm is capable of disaggregating T$_{\textrm{B}}$ using complex non-linear correlations on a grid.",1
"Unsupervised image segmentation aims at clustering the set of pixels of an image into spatially homogeneous regions. We introduce here a class of Bayesian nonparametric models to address this problem. These models are based on a combination of a Potts-like spatial smoothness component and a prior on partitions which is used to control both the number and size of clusters. This class of models is flexible enough to include the standard Potts model and the more recent Potts-Dirichlet Process model \cite{Orbanz2008}. More importantly, any prior on partitions can be introduced to control the global clustering structure so that it is possible to penalize small or large clusters if necessary. Bayesian computation is carried out using an original generalized Swendsen-Wang algorithm. Experiments demonstrate that our method is competitive in terms of RAND\ index compared to popular image segmentation methods, such as mean-shift, and recent alternative Bayesian nonparametric models.",0
"The objective of unsupervised image segmentation is to group pixels in an image into regions that are spatially similar. To address this issue, we present a group of Bayesian nonparametric models. These models combine a Potts-style spatial smoothness component with a partition prior, which controls both the number and size of clusters. This category of models is versatile and can incorporate both the standard Potts model and the more recent Potts-Dirichlet Process model \cite{Orbanz2008}. Additionally, any partition prior can be used to regulate the overall clustering structure, allowing small or large clusters to be penalized if required. Bayesian computation is done using an innovative generalized Swendsen-Wang algorithm. Our experiments demonstrate that our approach is competitive with well-known image segmentation techniques like mean-shift and recent alternative Bayesian nonparametric models, as measured by the RAND index.",1
"The present paper introduces the $\eta$ and {\eta} connections in order to add regional information on $\lambda$-flat zones, which only take into account a local information. A top-down approach is considered. First $\lambda$-flat zones are built in a way leading to a sub-segmentation. Then a finer segmentation is obtained by computing $\eta$-bounded regions and $\mu$-geodesic balls inside the $\lambda$-flat zones. The proposed algorithms for the construction of new partitions are based on queues with an ordered selection of seeds using the cumulative distance. $\eta$-bounded regions offers a control on the variations of amplitude in the class from a point, called center, and $\mu$-geodesic balls controls the ""size"" of the class. These results are applied to hyperspectral images.",0
"In this paper, the $\eta$ and {\eta} connections are introduced as a means of incorporating regional data into $\lambda$-flat zones, which currently rely solely on local information. The approach taken is top-down, beginning with the creation of $\lambda$-flat zones that facilitate sub-segmentation. The next step involves producing a more detailed segmentation using $\eta$-bounded regions and $\mu$-geodesic balls within the $\lambda$-flat zones. The algorithms used to generate these new partitions are queue-based and prioritize seeds through cumulative distance. $\eta$-bounded regions control amplitude variation within a class from a center point, while $\mu$-geodesic balls regulate class size. The efficacy of these techniques is demonstrated with hyperspectral imagery.",1
"Despite the great advances made in the field of image super-resolution (ISR) during the last years, the performance has merely been evaluated perceptually. Thus, it is still unclear whether ISR is helpful for other vision tasks. In this paper, we present the first comprehensive study and analysis of the usefulness of ISR for other vision applications. In particular, six ISR methods are evaluated on four popular vision tasks, namely edge detection, semantic image segmentation, digit recognition, and scene recognition. We show that applying ISR to input images of other vision systems does improve their performance when the input images are of low-resolution. We also study the correlation between four standard perceptual evaluation criteria (namely PSNR, SSIM, IFC, and NQM) and the usefulness of ISR to the vision tasks. Experiments show that they correlate well with each other in general, but perceptual criteria are still not accurate enough to be used as full proxies for the usefulness. We hope this work will inspire the community to evaluate ISR methods also in real vision applications, and to adopt ISR as a pre-processing step of other vision tasks if the resolution of their input images is low.",0
"Although significant progress has been made in image super-resolution (ISR) in recent years, its effectiveness has only been evaluated subjectively. Therefore, it remains unclear whether ISR has any benefits for other visual tasks. This study presents a thorough examination of the usefulness of ISR for various vision applications, including edge detection, semantic image segmentation, digit recognition, and scene recognition. Six ISR methods were tested, and it was found that applying ISR to low-resolution input images improved the performance of other vision systems. Additionally, the correlation between four standard perceptual evaluation criteria and the usefulness of ISR was analyzed. While the criteria generally aligned with each other, they were not sufficiently accurate to act as a complete substitute for evaluating ISR's usefulness. The study aims to encourage the use of ISR as a pre-processing step for other vision tasks when input images have low resolution.",1
"We present a probabilistic graphical model formulation for the graph clustering problem. This enables to locally represent uncertainty of image partitions by approximate marginal distributions in a mathematically substantiated way, and to rectify local data term cues so as to close contours and to obtain valid partitions.   We exploit recent progress on globally optimal MAP inference by integer programming and on perturbation-based approximations of the log-partition function, in order to sample clusterings and to estimate marginal distributions of node-pairs both more accurately and more efficiently than state-of-the-art methods. Our approach works for any graphically represented problem instance. This is demonstrated for image segmentation and social network cluster analysis. Our mathematical ansatz should be relevant also for other combinatorial problems.",0
"A model using probabilistic graphical formulation has been developed for graph clustering. This model helps in locally representing uncertainty in image partitions using approximate marginal distributions in a mathematically sound way and in rectifying local data term cues for obtaining valid partitions and closed contours. The approach utilizes recent advancements in global optimal MAP inference through integer programming and perturbation-based approximations of the log-partition function for more accurate and efficient sampling of clusterings and estimation of marginal distributions of node-pairs compared to current methods. The model is suitable for any graphically represented problem instance, as demonstrated in image segmentation and social network cluster analysis, and could be applied to other combinatorial problems.",1
"We study the applicability of a set of texture descriptors introduced in recent work by the author to texture-based segmentation of images. The texture descriptors under investigation result from applying graph indices from quantitative graph theory to graphs encoding the local structure of images. The underlying graphs arise from the computation of morphological amoebas as structuring elements for adaptive morphology, either as weighted or unweighted Dijkstra search trees or as edge-weighted pixel graphs within structuring elements. In the present paper we focus on texture descriptors in which the graph indices are entropy-based, and use them in a geodesic active contour framework for image segmentation. Experiments on several synthetic and one real-world image are shown to demonstrate texture segmentation by this approach. Forthermore, we undertake an attempt to analyse selected entropy-based texture descriptors with regard to what information about texture they actually encode. Whereas this analysis uses some heuristic assumptions, it indicates that the graph-based texture descriptors are related to fractal dimension measures that have been proven useful in texture analysis.",0
"The author's recent work introduced a set of texture descriptors that we are investigating for texture-based image segmentation. These descriptors are created by applying graph indices from quantitative graph theory to graphs that represent the local structure of images. The graphs are generated through the use of morphological amoebas as structuring elements for adaptive morphology. We focus on entropy-based texture descriptors and use them in a geodesic active contour framework to segment images. Our experiments, including both synthetic and real-world images, demonstrate the effectiveness of this approach. Additionally, we analyze the selected entropy-based texture descriptors to understand the texture information they encode. While this analysis relies on heuristic assumptions, it suggests that the graph-based texture descriptors are linked to fractal dimension measures that are valuable in texture analysis.",1
"Quantification of adipose tissue (fat) from computed tomography (CT) scans is conducted mostly through manual or semi-automated image segmentation algorithms with limited efficacy. In this work, we propose a completely unsupervised and automatic method to identify adipose tissue, and then separate Subcutaneous Adipose Tissue (SAT) from Visceral Adipose Tissue (VAT) at the abdominal region. We offer a three-phase pipeline consisting of (1) Initial boundary estimation using gradient points, (2) boundary refinement using Geometric Median Absolute Deviation and Appearance based Local Outlier Scores (3) Context driven label fusion using Conditional Random Fields (CRF) to obtain the final boundary between SAT and VAT. We evaluate the proposed method on 151 abdominal CT scans and obtain state-of-the-art 94% and 91% dice similarity scores for SAT and VAT segmentation, as well as significant reduction in fat quantification error measure.",0
"Currently, manual or semi-automated image segmentation algorithms are primarily used to quantify adipose tissue from computed tomography (CT) scans, but their effectiveness is limited. This study presents a novel, fully unsupervised and automatic method for identifying adipose tissue and separating Subcutaneous Adipose Tissue (SAT) from Visceral Adipose Tissue (VAT) in the abdomen. The method involves a three-phase pipeline, including (1) estimating the initial boundary using gradient points, (2) refining the boundary using Geometric Median Absolute Deviation and Appearance based Local Outlier Scores, and (3) using Conditional Random Fields (CRF) to drive the label fusion and obtain the final boundary between SAT and VAT. The proposed method was evaluated on 151 abdominal CT scans and achieved a state-of-the-art 94% and 91% dice similarity scores for SAT and VAT segmentation, respectively, as well as a significant reduction in fat quantification error measure.",1
"This paper presents a framework that supports the implementation of parallel solutions for the widespread parametric maximum flow computational routines used in image segmentation algorithms. The framework is based on supergraphs, a special construction combining several image graphs into a larger one, and works on various architectures (multi-core or GPU), either locally or remotely in a cluster of computing nodes. The framework can also be used for performance evaluation of parallel implementations of maximum flow algorithms. We present the case study of a state-of-the-art image segmentation algorithm based on graph cuts, Constrained Parametric Min-Cut (CPMC), that uses the parallel framework to solve parametric maximum flow problems, based on a GPU implementation of the well-known push-relabel algorithm. Our results indicate that real-time implementations based on the proposed techniques are possible.",0
"In this paper, a framework is introduced to facilitate the implementation of parallel solutions for parametric maximum flow computational routines utilized in image segmentation algorithms. The framework is based on supergraphs, which involve combining multiple image graphs into a larger one, and is compatible with various architectures (e.g. multi-core or GPU) for usage both locally and remotely in a cluster of computing nodes. Additionally, the framework can be utilized for evaluating the performance of parallel implementations of maximum flow algorithms. To demonstrate the effectiveness of this framework, a case study is presented using the state-of-the-art Constrained Parametric Min-Cut (CPMC) graph cuts image segmentation algorithm. The parallel framework is employed to solve parametric maximum flow problems utilizing a GPU implementation of the push-relabel algorithm. The results indicate that real-time implementations using the proposed techniques are feasible.",1
"This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.",0
"The objective of this study is to tackle the issue of image-based question-answering (QA) using novel models and datasets. Our approach involves the use of neural networks and visual semantic embeddings to predict answers to basic image-related queries, without the need for object detection or image segmentation. Our model's performance is 1.8 times better than the only published results on a pre-existing image QA dataset. Additionally, we introduce a question generation algorithm that transforms image descriptions into QA format. This algorithm generates a dataset that is ten times larger and has more balanced answers. We also present baseline results on this new dataset.",1
"Bilateral filters have wide spread use due to their edge-preserving properties. The common use case is to manually choose a parametric filter type, usually a Gaussian filter. In this paper, we will generalize the parametrization and in particular derive a gradient descent algorithm so the filter parameters can be learned from data. This derivation allows to learn high dimensional linear filters that operate in sparsely populated feature spaces. We build on the permutohedral lattice construction for efficient filtering. The ability to learn more general forms of high-dimensional filters can be used in several diverse applications. First, we demonstrate the use in applications where single filter applications are desired for runtime reasons. Further, we show how this algorithm can be used to learn the pairwise potentials in densely connected conditional random fields and apply these to different image segmentation tasks. Finally, we introduce layers of bilateral filters in CNNs and propose bilateral neural networks for the use of high-dimensional sparse data. This view provides new ways to encode model structure into network architectures. A diverse set of experiments empirically validates the usage of general forms of filters.",0
"Due to their ability to preserve edges, bilateral filters are widely used. Typically, a Gaussian filter is manually chosen as the parametric filter type. However, in this paper, we present a generalization of the parametrization and derive a gradient descent algorithm to learn the filter parameters from data. This allows for the learning of high-dimensional linear filters that operate in sparsely populated feature spaces, using the efficient permutohedral lattice construction for filtering. The ability to learn more general forms of high-dimensional filters has diverse applications. Firstly, we demonstrate their use in scenarios where single filter applications are needed for runtime purposes. Secondly, we show how the algorithm can be used to learn pairwise potentials in densely connected conditional random fields, applied to various image segmentation tasks. Finally, we introduce bilateral filters as layers in CNNs and propose bilateral neural networks for high-dimensional sparse data. This provides new ways to incorporate model structure into network architectures. Numerous experiments support the practicality of using general forms of filters.",1
"This paper proposes a convolutional neural network that can fuse high-level prior for semantic image segmentation. Motivated by humans' vision recognition system, our key design is a three-layer generative structure consisting of high-level coding, middle-level segmentation and low-level image to introduce global prior for semantic segmentation. Based on this structure, we proposed a generative model called conditional variational auto-encoder (CVAE) that can build up the links behind these three layers. These important links include an image encoder that extracts high level info from image, a segmentation encoder that extracts high level info from segmentation, and a hybrid decoder that outputs semantic segmentation from the high level prior and input image. We theoretically derive the semantic segmentation as an optimization problem parameterized by these links. Finally, the optimization problem enables us to take advantage of state-of-the-art fully convolutional network structure for the implementation of the above encoders and decoder. Experimental results on several representative datasets demonstrate our supreme performance for semantic segmentation.",0
"In this paper, we present a convolutional neural network that incorporates high-level prior for semantic image segmentation. Our approach is inspired by the human vision recognition system and features a three-layer generative structure comprising high-level coding, middle-level segmentation, and low-level image. This structure introduces global prior for semantic segmentation. To establish the connections between these layers, we propose a generative model called conditional variational auto-encoder (CVAE). The CVAE includes an image encoder that extracts high-level information from the image, a segmentation encoder that extracts high-level information from the segmentation, and a hybrid decoder that generates semantic segmentation from the high-level prior and input image. We derive the semantic segmentation as an optimization problem that is parameterized by these links. We leverage the state-of-the-art fully convolutional network structure to implement the encoders and decoder. Our experiments on several representative datasets demonstrate the superior performance of our approach for semantic segmentation.",1
"One popular approach to interactively segment the foreground object of interest from an image is to annotate a bounding box that covers the foreground object. Then, a binary labeling is performed to achieve a refined segmentation. One major issue of the existing algorithms for such interactive image segmentation is their preference of an input bounding box that tightly encloses the foreground object. This increases the annotation burden, and prevents these algorithms from utilizing automatically detected bounding boxes. In this paper, we develop a new LooseCut algorithm that can handle cases where the input bounding box only loosely covers the foreground object. We propose a new Markov Random Fields (MRF) model for segmentation with loosely bounded boxes, including a global similarity constraint to better distinguish the foreground and background, and an additional energy term to encourage consistent labeling of similar-appearance pixels. This MRF model is then solved by an iterated max-flow algorithm. In the experiments, we evaluate LooseCut in three publicly-available image datasets, and compare its performance against several state-of-the-art interactive image segmentation algorithms. We also show that LooseCut can be used for enhancing the performance of unsupervised video segmentation and image saliency detection.",0
"A common method of interactively segmenting the foreground object in an image involves annotating a bounding box that encompasses the object, followed by binary labeling to refine the segmentation. However, existing algorithms for this approach tend to prefer tightly enclosed bounding boxes, which increases the annotation workload and hinders the use of automatically detected bounding boxes. In this study, we introduce the LooseCut algorithm, designed to handle cases where the bounding box only loosely covers the foreground object. Our approach involves a Markov Random Fields (MRF) model for segmentation with loosely bounded boxes, incorporating a global similarity constraint to better distinguish foreground and background, and an additional energy term to encourage consistent labeling of similar-appearance pixels. The MRF model is solved through an iterated max-flow algorithm. We evaluate LooseCut using three publicly available image datasets, comparing its performance against several state-of-the-art interactive image segmentation algorithms. We also demonstrate that LooseCut can enhance the performance of unsupervised video segmentation and image saliency detection.",1
"Sparse decomposition has been extensively used for different applications including signal compression and denoising and document analysis. In this paper, sparse decomposition is used for image segmentation. The proposed algorithm separates the background and foreground using a sparse-smooth decomposition technique such that the smooth and sparse components correspond to the background and foreground respectively. This algorithm is tested on several test images from HEVC test sequences and is shown to have superior performance over other methods, such as the hierarchical k-means clustering in DjVu. This segmentation algorithm can also be used for text extraction, video compression and medical image segmentation.",0
"Sparse decomposition has found diverse applications, such as signal compression, denoising, and document analysis. In this study, we utilize sparse decomposition for image segmentation. Our proposed algorithm employs a sparse-smooth decomposition approach to separate the foreground and background, where the smooth and sparse elements correspond to the background and foreground, respectively. We evaluated our algorithm on various test images from HEVC test sequences and observed that it outperforms other methods, such as the hierarchical k-means clustering in DjVu. Furthermore, this segmentation algorithm can be extended to text extraction, medical image segmentation, and video compression.",1
"The segmentation of transparent objects can be very useful in computer vision applications. However, because they borrow texture from their background and have a similar appearance to their surroundings, transparent objects are not handled well by regular image segmentation methods. We propose a method that overcomes these problems using the consistency and distortion properties of a light-field image. Graph-cut optimization is applied for the pixel labeling problem. The light-field linearity is used to estimate the likelihood of a pixel belonging to the transparent object or Lambertian background, and the occlusion detector is used to find the occlusion boundary. We acquire a light field dataset for the transparent object, and use this dataset to evaluate our method. The results demonstrate that the proposed method successfully segments transparent objects from the background.",0
"Transparent objects can pose difficulties in image segmentation due to their ability to blend in with their surroundings and borrow texture from the background. However, a new method has been proposed that utilizes the consistency and distortion properties of light-field images to address these challenges. By applying graph-cut optimization for pixel labeling and leveraging light-field linearity to estimate pixel likelihood, the proposed method successfully separates transparent objects from Lambertian backgrounds. An occlusion detector is also employed to identify occlusion boundaries. To evaluate the effectiveness of this approach, a light field dataset for the transparent object is acquired and used to demonstrate accurate segmentation results.",1
"Multicuts enable to conveniently represent discrete graphical models for unsupervised and supervised image segmentation, in the case of local energy functions that exhibit symmetries. The basic Potts model and natural extensions thereof to higher-order models provide a prominent class of such objectives, that cover a broad range of segmentation problems relevant to image analysis and computer vision. We exhibit a way to systematically take into account such higher-order terms for computational inference. Furthermore, we present results of a comprehensive and competitive numerical evaluation of a variety of dedicated cutting-plane algorithms. Our approach enables the globally optimal evaluation of a significant subset of these models, without compromising runtime. Polynomially solvable relaxations are studied as well, along with advanced rounding schemes for post-processing.",0
"Multicuts are a useful tool for representing discrete graphical models in both unsupervised and supervised image segmentation, particularly for local energy functions that display symmetries. The Potts model and similar higher-order models are a prominent class of objectives that can be applied to many segmentation problems relevant to image analysis and computer vision. We describe a systematic approach to incorporating higher-order terms into computational inference, and present the results of a comprehensive numerical evaluation of various cutting-plane algorithms. Our approach allows for the globally optimal evaluation of many of these models without sacrificing runtime, and we also examine polynomially solvable relaxations and post-processing techniques.",1
"Recent works have highlighted scale invariance or symmetry that is present in the weight space of a typical deep network and the adverse effect that it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that these and other commonly used deep networks, such as those which use a max-pooling and sub-sampling layer, possess more complex forms of symmetry arising from scaling based reparameterization of the network weights. We then propose two symmetry-invariant gradient based weight updates for stochastic gradient descent based learning. Our empirical evidence based on the MNIST dataset shows that these updates improve the test performance without sacrificing the computational efficiency of the weight updates. We also show the results of training with one of the proposed weight updates on an image segmentation problem.",0
"New research has focused on the scale invariance or symmetry found in the weight space of deep networks, which negatively impacts the Euclidean gradient-based stochastic gradient descent optimization. In this study, we demonstrate that deep networks commonly employing max-pooling and sub-sampling layers exhibit more complex symmetry patterns resulting from scaling-based reparameterization of network weights. We introduce two gradient-based weight updates that are symmetry-invariant for stochastic gradient descent-based learning. Our experimental findings, utilizing the MNIST dataset, indicate that these updates enhance test performance without compromising weight update computational efficiency. Additionally, we present the outcomes of training with one of the proposed weight updates on an image segmentation problem.",1
We study the problem of multicut segmentation. We introduce modified versions of the Semi-PlanarCC based on bounding Lagrange multipliers. We apply our work to natural image segmentation.,0
"The focus of our research is on multicut segmentation, wherein we propose revised editions of Semi-PlanarCC that rely on bounding Lagrange multipliers. Our study finds practical application in the field of natural image segmentation.",1
"Image segmentation is the problem of partitioning an image into different subsets, where each subset may have a different characterization in terms of color, intensity, texture, and/or other features. Segmentation is a fundamental component of image processing, and plays a significant role in computer vision, object recognition, and object tracking. Active Contour Models (ACMs) constitute a powerful energy-based minimization framework for image segmentation, which relies on the concept of contour evolution. Starting from an initial guess, the contour is evolved with the aim of approximating better and better the actual object boundary. Handling complex images in an efficient, effective, and robust way is a real challenge, especially in the presence of intensity inhomogeneity, overlap between the foreground/background intensity distributions, objects characterized by many different intensities, and/or additive noise. In this thesis, to deal with these challenges, we propose a number of image segmentation models relying on variational level set methods and specific kinds of neural networks, to handle complex images in both supervised and unsupervised ways. Experimental results demonstrate the high accuracy of the segmentation results, obtained by the proposed models on various benchmark synthetic and real images compared with state-of-the-art active contour models.",0
"The task of image segmentation involves dividing an image into distinct subsets based on characteristics such as color, texture, and intensity. This is a crucial aspect of image processing and is integral to computer vision, object recognition, and object tracking. Active Contour Models (ACMs) are a powerful method for achieving image segmentation through contour evolution, where the initial guess is refined to better approximate the object boundary. However, handling complex images with intensity inhomogeneity, overlapping foreground/background intensity distributions, and objects with various intensities or additive noise poses a significant challenge. To address these challenges, we propose several image segmentation models utilizing variational level set methods and specific neural networks for both supervised and unsupervised approaches. Our experiments demonstrate that these models achieve highly accurate segmentation results on various benchmark synthetic and real images when compared to state-of-the-art active contour models.",1
"Non-intrusive biometrics of animals using images allows to analyze phenotypic populations and individuals with patterns like stripes and spots without affecting the studied subjects. However, non-intrusive biometrics demand a well trained subject or the development of computer vision algorithms that ease the identification task. In this work, an analysis of classic segmentation approaches that require a supervised tuning of their parameters such as threshold, adaptive threshold, histogram equalization, and saturation correction is presented. In contrast, a general unsupervised algorithm using Markov Random Fields (MRF) for segmentation of spots patterns is proposed. Active contours are used to boost results using MRF output as seeds. As study subject the Diploglossus millepunctatus lizard is used. The proposed method achieved a maximum efficiency of $91.11\%$.",0
"The use of non-intrusive biometrics through animal images enables the analysis of phenotypic individuals and populations with patterns such as stripes and spots, without causing any harm to the subjects. However, this method requires a well-trained subject or the development of computer vision algorithms to aid in identification. This study presents an analysis of classic segmentation techniques, such as threshold, adaptive threshold, histogram equalization, and saturation correction, which require supervised tuning of their parameters. In contrast, an unsupervised algorithm utilizing Markov Random Fields (MRF) for spot pattern segmentation is proposed. The MRF output serves as seeds for active contours to enhance the results. The Diploglossus millepunctatus lizard is used as the subject of study, and the proposed method achieved a maximum efficiency of $91.11\%$.",1
"We present an approach to learn a dense pixel-wise labeling from image-level tags. Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network (CNN) classifier. We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a CNN. Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization. The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks. Extensive experiments demonstrate the generality of our new learning framework. The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation. We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm.",0
"Our proposed method involves using image-level tags to teach a Convolutional Neural Network (CNN) to accurately label pixels. Each tag places restrictions on the CNN's output labeling, allowing us to optimize the network using linear constraints. We call this approach Constrained CNN (CCNN), and it utilizes a unique loss function that can be easily incorporated into standard stochastic gradient descent optimization. Our training objective is a biconvex optimization for linear models, which we then adapt to nonlinear deep networks. Through extensive experimentation, we show that our approach yields state-of-the-art results in weakly supervised semantic image segmentation. Additionally, we demonstrate that adding a small amount of supervision significantly enhances the algorithm's overall performance.",1
"Although topological considerations amongst multiple labels have been previously investigated in the context of continuous max-flow image segmentation, similar investigations have yet to be made about shape considerations in a general and extendable manner. This paper presents shape complexes for segmentation, which capture more complex shapes by combining multiple labels and super-labels constrained by geodesic star convexity. Shape complexes combine geodesic star convexity constraints with hierarchical label organization, which together allow for more complex shapes to be represented. This framework avoids the use of co-ordinate system warping techniques to convert shape constraints into topological constraints, which may be ambiguous or ill-defined for certain segmentation problems.",0
"Previous studies have examined the topological aspects of multiple labels in continuous max-flow image segmentation. However, there is a lack of research on shape considerations in a comprehensive and adaptable manner. This study introduces shape complexes for segmentation, which employ geodesic star convexity to combine multiple labels and super-labels, thereby capturing more intricate shapes. The hierarchical organization of labels and the use of geodesic star convexity constraints enable representation of complex shapes. The proposed framework avoids the use of coordinate system warping techniques to convert shape constraints to topological constraints, which may be uncertain or poorly defined for certain segmentation issues.",1
"Validation of image segmentation methods is of critical importance. Probabilistic image segmentation is increasingly popular as it captures uncertainty in the results. Image segmentation methods that support multi-region (as opposed to binary) delineation are more favourable as they capture interactions between the different objects in the image. The Dice similarity coefficient (DSC) has been a popular metric for evaluating the accuracy of automated or semi-automated segmentation methods by comparing their results to the ground truth. In this work, we develop an extension of the DSC to multi-region probabilistic segmentations (with unordered labels). We use bipartite graph matching to establish label correspondences and propose two functions that extend the DSC, one based on absolute probability differences and one based on the Aitchison distance. These provide a robust and accurate measure of multi-region probabilistic segmentation accuracy.",0
"The validation of image segmentation techniques is crucial, and probabilistic methods are gaining popularity due to their ability to capture uncertainty in results. Image segmentation methods that allow for multi-region delineation are preferred as they record interactions between different objects in the image. The Dice similarity coefficient (DSC) is a widely used metric for assessing the accuracy of automated or semi-automated segmentation techniques by comparing their results with the ground truth. This study proposes an extension of the DSC for multi-region probabilistic segmentations (with unordered labels) using bipartite graph matching to establish label correspondences. Two functions are introduced that extend the DSC: one based on absolute probability differences and the other on the Aitchison distance. These functions provide an accurate and reliable measure of the accuracy of multi-region probabilistic segmentation.",1
"Quorum sensing is a decentralized biological process, through which a community of cells with no global awareness coordinate their functional behaviors based solely on cell-medium interactions and local decisions. This paper draws inspirations from quorum sensing and colony competition to derive a new algorithm for data clustering. The algorithm treats each data as a single cell, and uses knowledge of local connectivity to cluster cells into multiple colonies simultaneously. It simulates auto-inducers secretion in quorum sensing to tune the influence radius for each cell. At the same time, sparsely distributed core cells spread their influences to form colonies, and interactions between colonies eventually determine each cell's identity. The algorithm has the flexibility to analyze not only static but also time-varying data, which surpasses the capacity of many existing algorithms. Its stability and convergence properties are established. The algorithm is tested on several applications, including both synthetic and real benchmarks data sets, alleles clustering, community detection, image segmentation. In particular, the algorithm's distinctive capability to deal with time-varying data allows us to experiment it on novel applications such as robotic swarms grouping and switching model identification. We believe that the algorithm's promising performance would stimulate many more exciting applications.",0
"Drawing inspiration from quorum sensing and colony competition, this paper presents a novel algorithm for data clustering. Quorum sensing is a decentralized biological process that enables a community of cells to coordinate their behaviors based on local interactions, and this algorithm treats each data point as a single cell, clustering them into multiple colonies simultaneously. The algorithm simulates auto-inducers secretion to adjust the influence radius for each cell and allows sparsely distributed core cells to spread their influence, which eventually determines each cell's identity. The algorithm is capable of analyzing both static and time-varying data and has been evaluated on various applications such as alleles clustering, community detection, image segmentation, and even robotic swarms grouping and switching model identification. The algorithm's promising performance suggests its potential for exciting future applications.",1
"Despite the popularity and empirical success of patch-based nearest-neighbor and weighted majority voting approaches to medical image segmentation, there has been no theoretical development on when, why, and how well these nonparametric methods work. We bridge this gap by providing a theoretical performance guarantee for nearest-neighbor and weighted majority voting segmentation under a new probabilistic model for patch-based image segmentation. Our analysis relies on a new local property for how similar nearby patches are, and fuses existing lines of work on modeling natural imagery patches and theory for nonparametric classification. We use the model to derive a new patch-based segmentation algorithm that iterates between inferring local label patches and merging these local segmentations to produce a globally consistent image segmentation. Many existing patch-based algorithms arise as special cases of the new algorithm.",0
"Patch-based nearest-neighbor and weighted majority voting approaches have been popular and successful in medical image segmentation, yet there is a lack of theoretical understanding regarding their efficacy. To address this issue, we present a new probabilistic model for patch-based image segmentation and provide a theoretical performance guarantee for these nonparametric methods. Our analysis introduces a new local property that measures the similarity of nearby patches, building upon previous work in natural imagery patch modeling and nonparametric classification theory. Based on this model, we propose a new patch-based segmentation algorithm that involves inferring local label patches and merging them to create a globally consistent image segmentation. This algorithm encompasses existing patch-based approaches as special cases.",1
"Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at https://bitbucket.org/deeplab/deeplab-public.",0
"Recently, semantic image segmentation has advanced significantly due to the use of deep convolutional neural networks (DCNNs) trained on numerous images with strong pixel-level annotations. However, we focus on the more difficult task of training DCNNs for semantic image segmentation using weakly annotated training data, such as bounding boxes or image-level labels, or a combination of both weakly and strongly labeled images from one or multiple datasets. To tackle this challenge, we have developed Expectation-Maximization (EM) methods for model training under weakly supervised and semi-supervised settings. Our experimental evaluation demonstrates that our proposed techniques can produce competitive results on the demanding PASCAL VOC 2012 image segmentation benchmark while requiring significantly less annotation effort. The source code for our system is available at https://bitbucket.org/deeplab/deeplab-public.",1
"We introduce Segment-Phrase Table (SPT), a large collection of bijective associations between textual phrases and their corresponding segmentations. Leveraging recent progress in object recognition and natural language semantics, we show how we can successfully build a high-quality segment-phrase table using minimal human supervision. More importantly, we demonstrate the unique value unleashed by this rich bimodal resource, for both vision as well as natural language understanding. First, we show that fine-grained textual labels facilitate contextual reasoning that helps in satisfying semantic constraints across image segments. This feature enables us to achieve state-of-the-art segmentation results on benchmark datasets. Next, we show that the association of high-quality segmentations to textual phrases aids in richer semantic understanding and reasoning of these textual phrases. Leveraging this feature, we motivate the problem of visual entailment and visual paraphrasing, and demonstrate its utility on a large dataset.",0
"The Segment-Phrase Table (SPT) is a comprehensive assortment of textual phrases and their corresponding segmentations that we have introduced. By utilizing advancements in natural language semantics and object recognition, we have successfully constructed a high-quality segment-phrase table with minimal human involvement. The advantages of this bimodal resource are apparent in both vision and natural language comprehension. We demonstrate that fine-grained textual labels enable contextual reasoning, which assists in fulfilling semantic constraints across image segments, resulting in state-of-the-art segmentation results on benchmark datasets. Additionally, associating high-quality segmentations with textual phrases aids in a more profound understanding and reasoning of these phrases. By leveraging this feature, we introduce the problem of visual entailment and visual paraphrasing and demonstrate its usefulness on a large dataset.",1
"This paper addresses semantic image segmentation by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. It has several appealing properties. First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing works as its special cases. Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where a single DPN model yields a new state-of-the-art segmentation accuracy.",0
"The aim of this study is to improve semantic image segmentation by enhancing the Markov Random Field (MRF) with high-order relations and mixtures of label contexts. Unlike previous MRF optimization methods that used iterative algorithms, a Convolutional Neural Network (CNN) called the Deep Parsing Network (DPN) is proposed to solve the MRF. The DPN provides a deterministic end-to-end computation in a single forward pass. The DPN includes additional layers to approximate the mean field algorithm (MF) for pairwise terms while extending a contemporary CNN architecture to model unary terms. The DPN has numerous advantages, including the ability to achieve high performance by approximating one iteration of MF, the representation of various types of pairwise terms, and the ability to parallelize and speed up MF in Graphical Processing Unit (GPU). The DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where it achieves a new state-of-the-art segmentation accuracy with a single model.",1
"Every segmentation algorithm has parameters that need to be adjusted in order to achieve good results. Evolving fuzzy systems for adjustment of segmentation parameters have been proposed recently (Evolving fuzzy image segmentation -- EFIS [1]. However, similar to any other algorithm, EFIS too suffers from a few limitations when used in practice. As a major drawback, EFIS depends on detection of the object of interest for feature calculation, a task that is highly application-dependent. In this paper, a new version of EFIS is proposed to overcome these limitations. The new EFIS, called self-configuring EFIS (SC-EFIS), uses available training data to auto-configure the parameters that are fixed in EFIS. As well, the proposed SC-EFIS relies on a feature selection process that does not require the detection of a region of interest (ROI).",0
"In order to achieve satisfactory outcomes, parameters of every segmentation algorithm need to be adjusted. Recently, Evolving Fuzzy Image Segmentation (EFIS) has been introduced as a means of adjusting segmentation parameters. However, like any other algorithm, EFIS has its drawbacks when utilized in real-life applications. One significant disadvantage is that EFIS relies on the detection of the object of interest for feature calculation, a task that varies depending on the application. This paper proposes a new variation of EFIS, known as Self-Configuring EFIS (SC-EFIS), which addresses these limitations. SC-EFIS utilizes available training data to automatically configure parameters that are fixed in EFIS. Furthermore, the proposed SC-EFIS incorporates a feature selection process that does not necessitate the detection of a particular region of interest (ROI).",1
"Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension for message estimation is the same as the number of classes, in contrast to the network output for general CNN potential functions in CRFs, which is exponential in the order of the potentials. Hence CNN message learning has fewer network parameters and is more scalable for cases that a large number of classes are involved. We apply our method to semantic image segmentation on the PASCAL VOC 2012 dataset. We achieve an intersection-over-union score of 73.4 on its test set, which is the best reported result for methods using the VOC training images alone. This impressive performance demonstrates the effectiveness and usefulness of our CNN message learning method.",0
"In the field of semantic image segmentation, deep structured output learning has shown potential. A new and efficient deep structured model learning scheme is offered, which utilizes deep Convolutional Neural Networks (CNNs) to estimate messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). This eliminates the need to learn or evaluate potential functions for message calculation, resulting in significant efficiency for learning. Compared to the network output for general CNN potential functions in CRFs, the network output dimension for message estimation is the same as the number of classes, making CNN message learning more scalable for cases involving a large number of classes. This method was applied to semantic image segmentation on the PASCAL VOC 2012 dataset, achieving an intersection-over-union score of 73.4 on its test set, the best reported result for methods using the VOC training images alone. This demonstrates the effectiveness and usefulness of the CNN message learning method.",1
"Segmenting an image into multiple components is a central task in computer vision. In many practical scenarios, prior knowledge about plausible components is available. Incorporating such prior knowledge into models and algorithms for image segmentation is highly desirable, yet can be non-trivial. In this work, we introduce a new approach that allows, for the first time, to constrain some or all components of a segmentation to have convex shapes. Specifically, we extend the Minimum Cost Multicut Problem by a class of constraints that enforce convexity. To solve instances of this APX-hard integer linear program to optimality, we separate the proposed constraints in the branch-and-cut loop of a state-of-the-art ILP solver. Results on natural and biological images demonstrate the effectiveness of the approach as well as its advantage over the state-of-the-art heuristic.",0
"The primary objective of computer vision is to divide an image into various segments. In most cases, there is prior knowledge available about the plausible components. However, integrating this knowledge into the image segmentation algorithms can be challenging. This research proposes a new technique that enables the segmentation components to have convex forms. To achieve this, we expand the Minimum Cost Multicut Problem with a set of constraints that enforce convexity. To solve the APX-hard integer linear program efficiently, we use a branch-and-cut loop of a state-of-the-art ILP solver to separate the proposed constraints. The results obtained from this approach on natural and biological images indicate its efficacy and superiority over the conventional heuristic.",1
"Recent advances in depth imaging sensors provide easy access to the synchronized depth with color, called RGB-D image. In this paper, we propose an unsupervised method for indoor RGB-D image segmentation and analysis. We consider a statistical image generation model based on the color and geometry of the scene. Our method consists of a joint color-spatial-directional clustering method followed by a statistical planar region merging method. We evaluate our method on the NYU depth database and compare it with existing unsupervised RGB-D segmentation methods. Results show that, it is comparable with the state of the art methods and it needs less computation time. Moreover, it opens interesting perspectives to fuse color and geometry in an unsupervised manner.",0
"Advancements in depth imaging sensors have made it convenient to obtain RGB-D images, which are synchronized depth with color. This article presents an unsupervised approach for analyzing and segmenting indoor RGB-D images. Our method is founded on a statistical image generation model that takes into account the scene's color and geometry. We employ a joint color-spatial-directional clustering approach, followed by a statistical planar region merging technique. We assessed our method using the NYU depth database and compared it with existing unsupervised RGB-D segmentation methods. Our method's results indicate that it is equivalent to state-of-the-art methods while requiring less computation time. Additionally, it offers promising possibilities for integrating color and geometry in an unsupervised manner.",1
"We explore the efficiency of the CRF inference beyond image level semantic segmentation and perform joint inference in video frames. The key idea is to combine best of two worlds: semantic co-labeling and more expressive models. Our formulation enables us to perform inference over ten thousand images within seconds and makes the system amenable to perform video semantic segmentation most effectively. On CamVid dataset, with TextonBoost unaries, our proposed method achieves up to 8% improvement in accuracy over individual semantic image segmentation without additional time overhead. The source code is available at https://github.com/subtri/video_inference",0
"Our research delves into the effectiveness of CRF inference beyond semantic segmentation at the image level, and instead utilizes joint inference in video frames. By merging the advantages of semantic co-labeling and expressive models, we have developed a formulation that allows for rapid inference over ten thousand images, and optimizes the system for video semantic segmentation. Using the CamVid dataset and TextonBoost unaries, our approach yields up to an 8% increase in accuracy over individual semantic image segmentation without any additional time demands. The source code can be accessed at https://github.com/subtri/video_inference.",1
"We propose an Active Learning approach to training a segmentation classifier that exploits geometric priors to streamline the annotation process in 3D image volumes. To this end, we use these priors not only to select voxels most in need of annotation but to guarantee that they lie on 2D planar patch, which makes it much easier to annotate than if they were randomly distributed in the volume. A simplified version of this approach is effective in natural 2D images. We evaluated our approach on Electron Microscopy and Magnetic Resonance image volumes, as well as on natural images. Comparing our approach against several accepted baselines demonstrates a marked performance increase.",0
"Our proposed method for training a segmentation classifier involves Active Learning, which takes advantage of geometric priors to simplify the annotation process in 3D image volumes. By using these priors, we not only select the most important voxels for annotation, but also ensure that they are situated on a 2D planar patch, making annotation much easier than if they were randomly dispersed throughout the volume. We found that this approach, which has already proven effective in natural 2D images, works well on Electron Microscopy and Magnetic Resonance image volumes. When compared to several established baselines, our method demonstrated a significant improvement in performance.",1
"In this paper we propose two saliency models for salient object segmentation based on a hierarchical image segmentation, a tree-like structure that represents regions at different scales from the details to the whole image (e.g. gPb-UCM, BPT). The first model is based on a hierarchy of image partitions. The saliency at each level is computed on a region basis, taking into account the contrast between regions. The maps obtained for the different partitions are then integrated into a final saliency map. The second model directly works on the structure created by the segmentation algorithm, computing saliency at each node and integrating these cues in a straightforward manner into a single saliency map. We show that the proposed models produce high quality saliency maps. Objective evaluation demonstrates that the two methods achieve state-of-the-art performance in several benchmark datasets.",0
"In this article, we introduce two saliency models that use a hierarchical image segmentation, which is represented as a tree-like structure that includes regions at various scales, from the details to the whole image (such as gPb-UCM, BPT). The first model uses a hierarchy of image partitions to compute the saliency on a region basis, taking into account the contrast between regions. The saliency maps generated for the different partitions are then combined to create a final saliency map. The second model computes saliency at each node of the segmentation algorithm's structure and integrates these cues into a single saliency map. Our research demonstrates that both models produce high-quality saliency maps and achieve state-of-the-art performance in several benchmark datasets, as shown by objective evaluation.",1
"Action recognition from still images is an important task of computer vision applications such as image annotation, robotic navigation, video surveillance and several others. Existing approaches mainly rely on either bag-of-feature representations or articulated body-part models. However, the relationship between the action and the image segments is still substantially unexplored. For this reason, in this paper we propose to approach action recognition by leveraging an intermediate layer of ""superpixels"" whose latent classes can act as attributes of the action. In the proposed approach, the action class is predicted by a structural model(learnt by Latent Structural SVM) based on measurements from the image superpixels and their latent classes. Experimental results over the challenging Stanford 40 Actions dataset report a significant average accuracy of 74.06% for the positive class and 88.50% for the negative class, giving evidence to the performance of the proposed approach.",0
"Recognizing actions in still images is a crucial task for various computer vision applications, including image annotation, robotic navigation, and video surveillance. Current methods for action recognition primarily depend on bag-of-feature representations or articulated body-part models. However, the correlation between the action and the image segments remains largely unexplored. Therefore, we propose a novel approach in this paper that employs an intermediate layer of ""superpixels"". These superpixels' latent classes can serve as action attributes, and a structural model learned by Latent Structural SVM predicts the action class based on measurements from the image superpixels and their latent classes. Our approach shows promising results, with an average accuracy of 74.06% for the positive class and 88.50% for the negative class, as demonstrated on the challenging Stanford 40 Actions dataset.",1
"We explore the efficiency of the CRF inference module beyond image level semantic segmentation. The key idea is to combine the best of two worlds of semantic co-labeling and exploiting more expressive models. Similar to [Alvarez14] our formulation enables us perform inference over ten thousand images within seconds. On the other hand, it can handle higher-order clique potentials similar to [vineet2014] in terms of region-level label consistency and context in terms of co-occurrences. We follow the mean-field updates for higher order potentials similar to [vineet2014] and extend the spatial smoothness and appearance kernels [DenseCRF13] to address video data inspired by [Alvarez14]; thus making the system amenable to perform video semantic segmentation most effectively.",0
"Our aim is to enhance the effectiveness of the CRF inference module beyond the realm of image level semantic segmentation. Our approach involves merging the advantages of semantic co-labeling and more expressive models. This approach, similar to the one used in [Alvarez14], allows us to perform inference over ten thousand images in mere seconds. Furthermore, it can handle higher-order clique potentials like those suggested in [vineet2014] to ensure region-level label consistency and context in terms of co-occurrences. We utilize mean-field updates for higher order potentials, as in [vineet2014], and extend the spatial smoothness and appearance kernels [DenseCRF13] to address video data in a manner inspired by [Alvarez14]. As a result, our system is well-suited for performing video semantic segmentation with optimal efficiency.",1
"Random fields have remained a topic of great interest over past decades for the purpose of structured inference, especially for problems such as image segmentation. The local nodal interactions commonly used in such models often suffer the short-boundary bias problem, which are tackled primarily through the incorporation of long-range nodal interactions. However, the issue of computational tractability becomes a significant issue when incorporating such long-range nodal interactions, particularly when a large number of long-range nodal interactions (e.g., fully-connected random fields) are modeled.   In this work, we introduce a generalized random field framework based around the concept of stochastic cliques, which addresses the issue of computational tractability when using fully-connected random fields by stochastically forming a sparse representation of the random field. The proposed framework allows for efficient structured inference using fully-connected random fields without any restrictions on the potential functions that can be utilized. Several realizations of the proposed framework using graph cuts are presented and evaluated, and experimental results demonstrate that the proposed framework can provide competitive performance for the purpose of image segmentation when compared to existing fully-connected and principled deep random field frameworks.",0
"Over the past few decades, random fields have remained a significant area of interest for structured inference, particularly in image segmentation. However, local nodal interactions used in such models often suffer from the short-boundary bias problem, which is mainly addressed by incorporating long-range nodal interactions. However, the incorporation of such long-range nodal interactions poses a significant challenge in terms of computational tractability, especially when modeling a large number of long-range nodal interactions such as fully-connected random fields. In this study, we propose a generalized random field framework based on stochastic cliques to tackle this issue of computational tractability. By stochastically forming a sparse representation of the random field, our proposed framework offers efficient structured inference using fully-connected random fields without any limitations on the potential functions that can be employed. We present and evaluate several realizations of our proposed framework using graph cuts, and our experimental results demonstrate that our framework can provide competitive performance for image segmentation when compared to existing fully-connected and principled deep random field frameworks.",1
"Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D videos to segment them. They have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants were hard to parallelize on GPUs. Here we re-arrange the traditional cuboid order of computations in MD-LSTM in pyramidal fashion. The resulting PyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks of brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).",0
"Segmentation of 2D images or 3D videos can be accomplished by shifting Convolutional Neural Networks (CNNs), which only perceive small local contexts of pixels in order to classify them as foreground or background due to their fixed input size. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, particularly when the RNN is a Long Short-Term Memory (LSTM). However, previous MD-LSTM variants were difficult to parallelize on GPUs, unlike CNNs. By rearranging the traditional cuboid order of computations in MD-LSTM in a pyramidal fashion, PyraMiD-LSTM was developed, which is highly parallelizable, particularly for 3D data like stacks of brain slice images. PyraMiD-LSTM achieved the best-known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).",1
"Since their introduction in the shape analysis community, functional maps have met with considerable success due to their ability to compactly represent dense correspondences between deformable shapes, with applications ranging from shape matching and image segmentation, to exploration of large shape collections. Despite the numerous advantages of such representation, however, the problem of converting a given functional map back to a point-to-point map has received a surprisingly limited interest. In this paper we analyze the general problem of point-wise map recovery from arbitrary functional maps. In doing so, we rule out many of the assumptions required by the currently established approach -- most notably, the limiting requirement of the input shapes being nearly-isometric. We devise an efficient recovery process based on a simple probabilistic model. Experiments confirm that this approach achieves remarkable accuracy improvements in very challenging cases.",0
"Functional maps have proven to be successful in the shape analysis community for their ability to represent dense correspondences between deformable shapes in a compact manner, with applications ranging from shape matching to image segmentation and large shape collection exploration. However, despite their benefits, the issue of converting a functional map to a point-to-point map has not received much attention. This paper aims to address this problem by analyzing the general task of point-wise map recovery from arbitrary functional maps. We eliminate many of the assumptions required by current methods, such as the need for nearly-isometric input shapes, and propose a more efficient recovery process based on a probabilistic model. Experimentation shows that our approach achieves impressive accuracy improvements even in challenging cases.",1
"Multi-atlas segmentation (MAS), first introduced and popularized by the pioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh, Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckert and Hammers (2006), is becoming one of the most widely-used and successful image segmentation techniques in biomedical applications. By manipulating and utilizing the entire dataset of ""atlases"" (training images that have been previously labeled, e.g., manually by an expert), rather than some model-based average representation, MAS has the flexibility to better capture anatomical variation, thus offering superior segmentation accuracy. This benefit, however, typically comes at a high computational cost. Recent advancements in computer hardware and image processing software have been instrumental in addressing this challenge and facilitated the wide adoption of MAS. Today, MAS has come a long way and the approach includes a wide array of sophisticated algorithms that employ ideas from machine learning, probabilistic modeling, optimization, and computer vision, among other fields. This paper presents a survey of published MAS algorithms and studies that have applied these methods to various biomedical problems. In writing this survey, we have three distinct aims. Our primary goal is to document how MAS was originally conceived, later evolved, and now relates to alternative methods. Second, this paper is intended to be a detailed reference of past research activity in MAS, which now spans over a decade (2003 - 2014) and entails novel methodological developments and application-specific solutions. Finally, our goal is to also present a perspective on the future of MAS, which, we believe, will be one of the dominant approaches in biomedical image segmentation.",0
"The technique of Multi-atlas Segmentation (MAS) has gained popularity in biomedical applications after being introduced and promoted by the works of Rohlfing, Brandt, Menzel, Maurer Jr, Klein, Mensh, Ghosh, Tourville, Hirsch, Heckemann, Hajnal, Aljabar, Rueckert, and Hammers. MAS employs entire sets of ""atlases"" - pre-labelled training images, instead of an average model-based representation, to capture anatomical variation and provide more accurate segmentation. However, this comes at a high computational cost, which recent advancements in hardware and software have helped to address. Today, MAS uses algorithms from machine learning, probabilistic modeling, optimization, and computer vision. This paper presents a survey of MAS algorithms and their applications in biomedical problems over the past decade. The aims of this survey are to document the evolution of MAS, provide a detailed reference of past research, and present a perspective on its future as a dominant approach in biomedical image segmentation.",1
"This paper presents a new probabilistic generative model for image segmentation, i.e. the task of partitioning an image into homogeneous regions. Our model is grounded on a mid-level image representation, called a region tree, in which regions are recursively split into subregions until superpixels are reached. Given the region tree, image segmentation is formalized as sampling cuts in the tree from the model. Inference for the cuts is exact, and formulated using dynamic programming. Our tree-cut model can be tuned to sample segmentations at a particular scale of interest out of many possible multiscale image segmentations. This generalizes the common notion that there should be only one correct segmentation per image. Also, it allows moving beyond the standard single-scale evaluation, where the segmentation result for an image is averaged against the corresponding set of coarse and fine human annotations, to conduct a scale-specific evaluation. Our quantitative results are comparable to those of the leading gPb-owt-ucm method, with the notable advantage that we additionally produce a distribution over all possible tree-consistent segmentations of the image.",0
"A novel probabilistic generative model for image segmentation is introduced in this paper, which involves dividing an image into homogeneous regions. The model is based on a region tree, a mid-level image representation where regions are recursively divided into subregions until superpixels are reached. Using dynamic programming, the model formalizes image segmentation as a process of sampling cuts in the tree. This tree-cut model can be adjusted to sample segmentations at a specific scale from multiple multiscale image segmentations, offering a more versatile approach than the traditional single-scale evaluation. Additionally, the model produces a distribution over all possible tree-consistent segmentations of the image, giving an advantage over the leading gPb-owt-ucm method. Our quantitative results demonstrate that the proposed model performs comparably to the gPb-owt-ucm method.",1
"In computer vision, image segmentation is always selected as a major research topic by researchers. Due to its vital rule in image processing, there always arises the need of a better image segmentation method. Clustering is an unsupervised study with its application in almost every field of science and engineering. Many researchers used clustering in image segmentation process. But still there requires improvement of such approaches. In this paper, a novel approach for clustering based image segmentation is proposed. Here, we give importance on color space and choose lab for this task. The famous hard clustering algorithm K-means is used, but as its performance is dependent on choosing a proper distance measure, so, we go for cosine distance measure. Then the segmented image is filtered with sobel filter. The filtered image is analyzed with marker watershed algorithm to have the final segmented result of our original image. The MSE and PSNR values are evaluated to observe the performance.",0
"Researchers in the field of computer vision consistently prioritize image segmentation as a major area of study. Given its crucial role in image processing, there is a persistent need for improved image segmentation methods. Clustering, an unsupervised technique used across various scientific and engineering fields, has been employed by many researchers in the image segmentation process. However, there is still room for improvement in these approaches. This paper presents a new clustering-based image segmentation approach with a focus on the color space, specifically lab. The well-known K-means algorithm is utilized, but with the cosine distance measure, which performs better with proper selection. The segmented image is then processed with a sobel filter and analyzed with a marker watershed algorithm to produce the final segmented result of the original image. Performance is evaluated via the MSE and PSNR values.",1
"Color image segmentation is a very emerging topic for image processing research. Since it has the ability to present the result in a way that is much more close to the human yes perceive, so todays more research is going on this area. Choosing a proper color space is a very important issue for color image segmentation process. Generally LAB and HSV are the two frequently chosen color spaces. In this paper a comparative analysis is performed between these two color spaces with respect to color image segmentation. For measuring their performance, we consider the parameters: mse and psnr . It is found that HSV color space is performing better than LAB.",0
"The topic of color image segmentation is gaining prominence in the field of image processing research due to its ability to create results that closely resemble human perception. As a result, significant research efforts are being directed towards this area. One crucial aspect of the color image segmentation process is selecting an appropriate color space, with LAB and HSV being the most commonly used. This study conducts a comparative analysis of these two color spaces in the context of color image segmentation, using the parameters mse and psnr to measure performance. The results indicate that HSV color space outperforms LAB.",1
"There has been significant interest in the use of fully-connected graphical models and deep-structured graphical models for the purpose of structured inference. However, fully-connected and deep-structured graphical models have been largely explored independently, leaving the unification of these two concepts ripe for exploration. A fundamental challenge with unifying these two types of models is in dealing with computational complexity. In this study, we investigate the feasibility of unifying fully-connected and deep-structured models in a computationally tractable manner for the purpose of structured inference. To accomplish this, we introduce a deep-structured fully-connected random field (DFRF) model that integrates a series of intermediate sparse auto-encoding layers placed between state layers to significantly reduce computational complexity. The problem of image segmentation was used to illustrate the feasibility of using the DFRF for structured inference in a computationally tractable manner. Results in this study show that it is feasible to unify fully-connected and deep-structured models in a computationally tractable manner for solving structured inference problems such as image segmentation.",0
"The use of fully-connected graphical models and deep-structured graphical models has generated considerable interest in structured inference. However, these models have been studied separately, presenting an opportunity for their unification. The main challenge in combining these models is managing computational complexity. This study aims to explore the possibility of merging fully-connected and deep-structured models in a computationally feasible way for structured inference. To achieve this, we propose a deep-structured fully-connected random field (DFRF) model that incorporates sparse auto-encoding layers between state layers to reduce computational complexity. We demonstrate the DFRF's utility for image segmentation, highlighting the feasibility of integrating fully-connected and deep-structured models for solving structured inference problems.",1
"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",0
"The consensus is that to effectively train deep networks, a vast number of annotated training samples is necessary. This paper proposes a network and training approach that maximizes the use of data augmentation, making better use of the available annotated samples. The network is structured with a contracting path for context and an expanding path for precise localization. Results show that the proposed network can be trained with minimal images and outperforms the previous best method for segmentation of neuronal structures in electron microscopic stacks. Additionally, the network's speed is noteworthy, able to segment a 512x512 image in less than a second on a recent GPU. The full implementation and trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net. The network was also successful in winning the ISBI cell tracking challenge 2015 in categories like phase contrast and DIC.",1
"Driven by recent vision and graphics applications such as image segmentation and object recognition, computing pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly important. In this paper, we propose a unified framework called PISA, which stands for Pixelwise Image Saliency Aggregating various bottom-up cues and priors. It generates spatially coherent yet detail-preserving, pixel-accurate and fine-grained saliency, and overcomes the limitations of previous methods which use homogeneous superpixel-based and color only treatment. PISA aggregates multiple saliency cues in a global context such as complementary color and structure contrast measures with their spatial priors in the image domain. The saliency confidence is further jointly modeled with a neighborhood consistence constraint into an energy minimization formulation, in which each pixel will be evaluated with multiple hypothetical saliency levels. Instead of using global discrete optimization methods, we employ the cost-volume filtering technique to solve our formulation, assigning the saliency levels smoothly while preserving the edge-aware structure details. In addition, a faster version of PISA is developed using a gradient-driven image sub-sampling strategy to greatly improve the runtime efficiency while keeping comparable detection accuracy. Extensive experiments on a number of public datasets suggest that PISA convincingly outperforms other state-of-the-art approaches. In addition, with this work we also create a new dataset containing $800$ commodity images for evaluating saliency detection. The dataset and source code of PISA can be downloaded at http://vision.sysu.edu.cn/project/PISA/",0
"The importance of computing accurate saliency values for highlighting foreground objects has increased due to recent vision and graphics applications such as object recognition and image segmentation. This paper proposes a unified framework called PISA, which aggregates various bottom-up cues and priors to generate pixel-accurate and fine-grained saliency that is spatially coherent yet preserves details. PISA overcomes the limitations of previous methods that use homogeneous superpixel-based and color-only treatment. It aggregates multiple saliency cues in a global context and models the saliency confidence with a neighborhood consistence constraint into an energy minimization formulation. The cost-volume filtering technique is used to solve the formulation, assigning the saliency levels smoothly while preserving the edge-aware structure details. A faster version of PISA is also developed using a gradient-driven image sub-sampling strategy to improve runtime efficiency. Extensive experiments on several public datasets show that PISA outperforms other state-of-the-art approaches. Additionally, a new dataset containing 800 commodity images for evaluating saliency detection is created, and the dataset and source code of PISA can be downloaded at http://vision.sysu.edu.cn/project/PISA/.",1
"This report provides an overview of the current state of the art deep learning architectures and optimisation techniques, and uses the ADNI hippocampus MRI dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3-dimensional hippocampal segmentation, which is important in the diagnosis of Alzheimer's Disease. We found that a slightly unconventional ""stacked 2D"" approach provides much better classification performance than simple 2D patches without requiring significantly more computational power. We also examined the popular ""tri-planar"" approach used in some recently published studies, and found that it provides much better results than the 2D approaches, but also with a moderate increase in computational power requirement. Finally, we evaluated a full 3D convolutional architecture, and found that it provides marginally better results than the tri-planar approach, but at the cost of a very significant increase in computational power requirement.",0
"This report offers an outline of the latest deep learning architectures and optimization techniques, using the ADNI hippocampus MRI dataset as an example to compare different convolutional architectures' effectiveness and efficiency in patch-based 3-dimensional hippocampal segmentation. This segmentation is crucial in diagnosing Alzheimer's Disease. Through our analysis, we discovered that the ""stacked 2D"" approach, which is slightly unconventional, offers much better classification performance than simple 2D patches but does not require significantly more computational power. We also examined the ""tri-planar"" approach used in recent studies, which provides better results than 2D approaches but requires a moderate increase in computational power. Finally, we evaluated a full 3D convolutional architecture, which marginally improves results compared to the tri-planar approach but requires a significant increase in computational power.",1
"In this paper, we propose several novel deep learning methods for object saliency detection based on the powerful convolutional neural networks. In our approach, we use a gradient descent method to iteratively modify an input image based on the pixel-wise gradients to reduce a cost function measuring the class-specific objectness of the image. The pixel-wise gradients can be efficiently computed using the back-propagation algorithm. The discrepancy between the modified image and the original one may be used as a saliency map for the image. Moreover, we have further proposed several new training methods to learn saliency-specific convolutional nets for object saliency detection, in order to leverage the available pixel-wise segmentation information. Our methods are extremely computationally efficient (processing 20-40 images per second in one GPU). In this work, we use the computed saliency maps for image segmentation. Experimental results on two benchmark tasks, namely Microsoft COCO and Pascal VOC 2012, have shown that our proposed methods can generate high-quality salience maps, clearly outperforming many existing methods. In particular, our approaches excel in handling many difficult images, which contain complex background, highly-variable salient objects, multiple objects, and/or very small salient objects.",0
"This paper introduces new approaches to object saliency detection using convolutional neural networks. Our method involves iteratively modifying an input image based on pixel-wise gradients to reduce a cost function measuring the objectness of the image. We then use the discrepancy between the modified and original image as a saliency map, and have proposed new training methods to learn saliency-specific convolutional nets. Our methods are highly computationally efficient and have been tested on Microsoft COCO and Pascal VOC 2012, outperforming many existing methods in handling difficult images with complex backgrounds, highly-variable salient objects, multiple objects, and small salient objects. Additionally, we use the computed saliency maps for image segmentation.",1
"In this chapter we review the main literature related to kernel spectral clustering (KSC), an approach to clustering cast within a kernel-based optimization setting. KSC represents a least-squares support vector machine based formulation of spectral clustering described by a weighted kernel PCA objective. Just as in the classifier case, the binary clustering model is expressed by a hyperplane in a high dimensional space induced by a kernel. In addition, the multi-way clustering can be obtained by combining a set of binary decision functions via an Error Correcting Output Codes (ECOC) encoding scheme. Because of its model-based nature, the KSC method encompasses three main steps: training, validation, testing. In the validation stage model selection is performed to obtain tuning parameters, like the number of clusters present in the data. This is a major advantage compared to classical spectral clustering where the determination of the clustering parameters is unclear and relies on heuristics. Once a KSC model is trained on a small subset of the entire data, it is able to generalize well to unseen test points. Beyond the basic formulation, sparse KSC algorithms based on the Incomplete Cholesky Decomposition (ICD) and $L_0$, $L_1, L_0 + L_1$, Group Lasso regularization are reviewed. In that respect, we show how it is possible to handle large scale data. Also, two possible ways to perform hierarchical clustering and a soft clustering method are presented. Finally, real-world applications such as image segmentation, power load time-series clustering, document clustering and big data learning are considered.",0
"This chapter provides an overview of kernel spectral clustering (KSC), which is a technique used for clustering and is set within a kernel-based optimization framework. KSC is based on a weighted kernel PCA objective and is a least-squares support vector machine-based formulation of spectral clustering. Similar to the classifier case, KSC uses a hyperplane in a high-dimensional space induced by a kernel to represent a binary clustering model. Multi-way clustering can be achieved by combining binary decision functions using an Error Correcting Output Codes (ECOC) encoding scheme. The KSC method has three main steps: training, validation, and testing. During validation, model selection is performed to determine tuning parameters, such as the number of clusters in the data. This is advantageous compared to classical spectral clustering, which relies on heuristics to determine clustering parameters. Once a KSC model is trained on a small subset of data, it can generalize well to unseen test points. The chapter also covers sparse KSC algorithms based on Incomplete Cholesky Decomposition (ICD) and $L_0$, $L_1, L_0 + L_1$, Group Lasso regularization, which can handle large-scale data. Additionally, two hierarchical clustering approaches and a soft clustering method are presented. Real-world applications, such as image segmentation, power load time-series clustering, document clustering, and big data learning, are also discussed.",1
"We present a method for hierarchical image segmentation that defines a disaffinity graph on the image, over-segments it into watershed basins, defines a new graph on the basins, and then merges basins with a modified, size-dependent version of single linkage clustering. The quasilinear runtime of the method makes it suitable for segmenting large images. We illustrate the method on the challenging problem of segmenting 3D electron microscopic brain images.",0
"Our approach to hierarchical image segmentation involves creating a disaffinity graph on the image, dividing it into watershed basins, constructing a new graph based on these basins, and then merging the basins using a modified form of single linkage clustering that takes size into account. Thanks to its quasilinear runtime, this method is ideal for segmenting large images. We demonstrate its effectiveness by applying it to the difficult task of segmenting 3D electron microscopic brain images.",1
"In this article, a new method for segmentation and restoration of images on two-dimensional surfaces is given. Active contour models for image segmentation are extended to images on surfaces. The evolving curves on the surfaces are mathematically described using a parametric approach. For image restoration, a diffusion equation with Neumann boundary conditions is solved in a postprocessing step in the individual regions. Numerical schemes are presented which allow to efficiently compute segmentations and denoised versions of images on surfaces. Also topology changes of the evolving curves are detected and performed using a fast sub-routine. Finally, several experiments are presented where the developed methods are applied on different artificial and real images defined on different surfaces.",0
This article presents a novel technique to segment and restore images on two-dimensional surfaces. The method involves extending active contour models to surfaces and using a parametric approach to describe the evolving curves. A diffusion equation with Neumann boundary conditions is solved in a postprocessing step for image restoration in individual regions. The article also provides numerical schemes for efficient computation of segmentations and denoised images on surfaces. The technique detects and performs topology changes of the evolving curves through a fast sub-routine. The article concludes with several experiments that demonstrate the effectiveness of the technique on both artificial and real images on various surfaces.,1
"Challenging computer vision tasks, in particular semantic image segmentation, require large training sets of annotated images. While obtaining the actual images is often unproblematic, creating the necessary annotation is a tedious and costly process. Therefore, one often has to work with unreliable annotation sources, such as Amazon Mechanical Turk or (semi-)automatic algorithmic techniques. In this work, we present a Gaussian process (GP) based technique for simultaneously identifying which images of a training set have unreliable annotation and learning a segmentation model in which the negative effect of these images is suppressed. Alternatively, the model can also just be used to identify the most reliably annotated images from the training set, which can then be used for training any other segmentation method. By relying on ""deep features"" in combination with a linear covariance function, our GP can be learned and its hyperparameter determined efficiently using only matrix operations and gradient-based optimization. This makes our method scalable even to large datasets with several million training instances.",0
"To tackle difficult computer vision tasks, like semantic image segmentation, substantial training sets of annotated images are essential. Although acquiring images is generally straightforward, annotating them is often tedious and costly. Consequently, researchers frequently resort to unreliable annotation sources, such as Amazon Mechanical Turk or (semi-)automatic algorithmic techniques. In this study, we present a Gaussian process (GP) technique that can recognize which images in a training set have unreliable annotation while simultaneously suppressing the negative impact of these images in the segmentation model. Alternatively, the model can identify the most reliably annotated images from the training set, which can be used to train other segmentation methods. By employing ""deep features"" and a linear covariance function, our GP can be trained and optimized using matrix operations and gradient-based techniques. As a result, our method can scale to large datasets with millions of training examples.",1
"In this paper, we introduce a novel approach for active contours with free endpoints. A scheme is presented for image segmentation and restoration based on a discrete version of the Mumford-Shah functional where the contours can be both closed and open curves. Additional to a flow of the curves in normal direction, evolution laws for the tangential flow of the endpoints are derived. Using a parametric approach to describe the evolving contours together with an edge-preserving denoising, we obtain a fast method for image segmentation and restoration. The analytical and numerical schemes are presented followed by numerical experiments with artificial test images and with a real medical image.",0
"A new method for active contours with free endpoints is introduced in this paper, which offers a scheme for image segmentation and restoration. The approach is based on a discrete version of the Mumford-Shah functional and allows for both closed and open curves. The evolution laws for the tangential flow of the endpoints are also derived, in addition to the normal direction flow of the curves. By utilizing a parametric approach to describe the evolving contours and edge-preserving denoising, a quick method for image segmentation and restoration is obtained. The paper presents analytical and numerical schemes, followed by experiments conducted on artificial test images and a medical image.",1
"We present a supervised hyperspectral image segmentation algorithm based on a convex formulation of a marginal maximum a posteriori segmentation with hidden fields and structure tensor regularization: Segmentation via the Constraint Split Augmented Lagrangian Shrinkage by Structure Tensor Regularization (SegSALSA-STR). This formulation avoids the generally discrete nature of segmentation problems and the inherent NP-hardness of the integer optimization associated.   We extend the Segmentation via the Constraint Split Augmented Lagrangian Shrinkage (SegSALSA) algorithm by generalizing the vectorial total variation prior using a structure tensor prior constructed from a patch-based Jacobian. The resulting algorithm is convex, time-efficient and highly parallelizable. This shows the potential of combining hidden fields with convex optimization through the inclusion of different regularizers. The SegSALSA-STR algorithm is validated in the segmentation of real hyperspectral images.",0
"Our paper introduces the Segmentation via the Constraint Split Augmented Lagrangian Shrinkage by Structure Tensor Regularization (SegSALSA-STR) algorithm, which is a supervised method for hyperspectral image segmentation. This algorithm is based on a convex formulation of a marginal maximum a posteriori segmentation, which utilizes hidden fields and structure tensor regularization. This approach avoids the typical discrete nature and NP-hardness of segmentation problems. We have extended the SegSALSA algorithm by implementing a structure tensor prior, which is constructed from a patch-based Jacobian. This modification results in a convex, time-efficient, and highly parallelizable algorithm. By combining hidden fields with convex optimization, the potential for including different regularizers is demonstrated. We have validated the SegSALSA-STR algorithm by applying it to real hyperspectral images.",1
"Current image segmentation techniques usually require that the user tune several parameters in order to obtain maximum segmentation accuracy, a computationally inefficient approach, especially when a large number of images must be processed sequentially in daily practice. The use of evolving fuzzy systems for designing a method that automatically adjusts parameters to segment medical images according to the quality expectation of expert users has been proposed recently (Evolving fuzzy image segmentation EFIS). However, EFIS suffers from a few limitations when used in practice mainly due to some fixed parameters. For instance, EFIS depends on auto-detection of the object of interest for feature calculation, a task that is highly application-dependent. This shortcoming limits the applicability of EFIS, which was proposed with the ultimate goal of offering a generic but adjustable segmentation scheme. In this paper, a new version of EFIS is proposed to overcome these limitations. The new EFIS, called self-configuring EFIS (SC-EFIS), uses available training data to self-estimate the parameters that are fixed in EFIS. As well, the proposed SC-EFIS relies on a feature selection process that does not require auto-detection of an ROI. The proposed SC-EFIS was evaluated using the same segmentation algorithms and the same dataset as for EFIS. The results show that SC-EFIS can provide the same results as EFIS but with a higher level of automation.",0
"Typically, current techniques for image segmentation require users to adjust multiple parameters in order to achieve optimal accuracy, which can be time-consuming and impractical when dealing with large quantities of images. Recently, a method called Evolving Fuzzy Image Segmentation (EFIS) has been proposed to automatically adjust parameters based on user expectations. However, EFIS has limitations that restrict its usability, such as a dependence on auto-detection for object of interest and fixed parameters. To address these issues, a new version of EFIS called Self-Configuring EFIS (SC-EFIS) has been developed. SC-EFIS uses available training data to estimate previously fixed parameters and employs a feature selection process that removes the need for auto-detection of a Region of Interest (ROI). Evaluation of SC-EFIS using the same dataset and segmentation algorithms as EFIS showed that it provides comparable results with higher automation.",1
"Contour detection has been a fundamental component in many image segmentation and object detection systems. Most previous work utilizes low-level features such as texture or saliency to detect contours and then use them as cues for a higher-level task such as object detection. However, we claim that recognizing objects and predicting contours are two mutually related tasks. Contrary to traditional approaches, we show that we can invert the commonly established pipeline: instead of detecting contours with low-level cues for a higher-level recognition task, we exploit object-related features as high-level cues for contour detection.   We achieve this goal by means of a multi-scale deep network that consists of five convolutional layers and a bifurcated fully-connected sub-network. The section from the input layer to the fifth convolutional layer is fixed and directly lifted from a pre-trained network optimized over a large-scale object classification task. This section of the network is applied to four different scales of the image input. These four parallel and identical streams are then attached to a bifurcated sub-network consisting of two independently-trained branches. One branch learns to predict the contour likelihood (with a classification objective) whereas the other branch is trained to learn the fraction of human labelers agreeing about the contour presence at a given point (with a regression criterion).   We show that without any feature engineering our multi-scale deep learning approach achieves state-of-the-art results in contour detection.",0
"Detecting contours is a crucial aspect of many image segmentation and object detection systems. Traditionally, this has been accomplished by utilizing low-level features, such as texture or saliency, to identify contours and then using them as cues for higher-level recognition tasks. However, we argue that recognizing objects and predicting contours are inherently linked tasks. In contrast to conventional methods, we have developed a new approach that uses object-related features as high-level cues for contour detection. Our method employs a multi-scale deep network with five convolutional layers and a bifurcated fully-connected sub-network. The first section of the network is pre-trained for object classification and applied to four different image scales. The four parallel streams are then connected to a bifurcated sub-network, with one branch predicting contour likelihood and the other predicting the fraction of labelers agreeing on contour presence. Remarkably, our approach achieves state-of-the-art results in contour detection without requiring any feature engineering.",1
"Visual saliency is a fundamental problem in both cognitive and computational sciences, including computer vision. In this CVPR 2015 paper, we discover that a high-quality visual saliency model can be trained with multiscale features extracted using a popular deep learning architecture, convolutional neural networks (CNNs), which have had many successes in visual recognition tasks. For learning such saliency models, we introduce a neural network architecture, which has fully connected layers on top of CNNs responsible for extracting features at three different scales. We then propose a refinement method to enhance the spatial coherence of our saliency results. Finally, aggregating multiple saliency maps computed for different levels of image segmentation can further boost the performance, yielding saliency maps better than those generated from a single segmentation. To promote further research and evaluation of visual saliency models, we also construct a new large database of 4447 challenging images and their pixelwise saliency annotation. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks, improving the F-Measure by 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset (HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively on these two datasets.",0
"Visual saliency is a significant issue in both cognitive and computational sciences, including computer vision. In this CVPR 2015 article, we discovered that it is possible to train a high-quality visual saliency model with multiscale features extracted using convolutional neural networks (CNNs), a popular deep learning architecture that has achieved many successes in visual recognition tasks. To learn such saliency models, we introduced a neural network architecture with fully connected layers on top of CNNs that are responsible for extracting features at three different scales. We also proposed a refinement method to improve the spatial coherence of our saliency results. Additionally, we found that aggregating multiple saliency maps computed for different levels of image segmentation can further enhance performance, producing better saliency maps than those generated from a single segmentation. To encourage further research and evaluation of visual saliency models, we created a new large database of 4447 challenging images and their pixelwise saliency annotation. Our experimental results demonstrate that our proposed method achieves state-of-the-art performance on all public benchmarks, improving the F-Measure by 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset (HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively on these two datasets.",1
"Conditional Random Rields (CRF) have been widely applied in image segmentations. While most studies rely on hand-crafted features, we here propose to exploit a pre-trained large convolutional neural network (CNN) to generate deep features for CRF learning. The deep CNN is trained on the ImageNet dataset and transferred to image segmentations here for constructing potentials of superpixels. Then the CRF parameters are learnt using a structured support vector machine (SSVM). To fully exploit context information in inference, we construct spatially related co-occurrence pairwise potentials and incorporate them into the energy function. This prefers labelling of object pairs that frequently co-occur in a certain spatial layout and at the same time avoids implausible labellings during the inference. Extensive experiments on binary and multi-class segmentation benchmarks demonstrate the promise of the proposed method. We thus provide new baselines for the segmentation performance on the Weizmann horse, Graz-02, MSRC-21, Stanford Background and PASCAL VOC 2011 datasets.",0
"Image segmentations have frequently utilized Conditional Random Fields (CRF), but typically rely on manually created features. Our proposal, however, utilizes a pre-trained convolutional neural network (CNN) trained on the ImageNet dataset to generate deep features for CRF learning. By constructing potentials of superpixels, we are able to learn the CRF parameters using a structured support vector machine (SSVM). We further incorporate spatially related co-occurrence pairwise potentials into the energy function to fully exploit context information in inference. This approach enhances the labeling of object pairs that frequently co-occur in a certain spatial layout, while preventing implausible labellings during inference. Our method has been thoroughly tested through experiments on binary and multi-class segmentation benchmarks, resulting in new baselines for segmentation performance on datasets such as Weizmann horse, Graz-02, MSRC-21, Stanford Background, and PASCAL VOC 2011.",1
"This work is about the use of regularized optimal-transport distances for convex, histogram-based image segmentation. In the considered framework, fixed exemplar histograms define a prior on the statistical features of the two regions in competition. In this paper, we investigate the use of various transport-based cost functions as discrepancy measures and rely on a primal-dual algorithm to solve the obtained convex optimization problem.",0
"The focus of this research is on applying regularized optimal-transport distances to achieve convex, histogram-based image segmentation. The approach involves utilizing pre-determined histograms as a reference for the statistical characteristics of two competing regions. The study explores different transport-based cost functions as a means of evaluating differences and employs a primal-dual algorithm to solve the convex optimization problem derived from the analysis.",1
"Five different threshold segmentation based approaches have been reviewed and compared over here to extract the tumor from set of brain images. This research focuses on the analysis of image segmentation methods, a comparison of five semi-automated methods have been undertaken for evaluating their relative performance in the segmentation of tumor. Consequently, results are compared on the basis of quantitative and qualitative analysis of respective methods. The purpose of this study was to analytically identify the methods, most suitable for application for a particular genre of problems. The results show that of the region growing segmentation performed better than rest in most cases.",0
"This study examines and compares five threshold segmentation methods for extracting tumors from a set of brain images. The research focuses on analyzing image segmentation techniques, with a particular emphasis on evaluating the performance of five semi-automated methods. The study compares the results based on quantitative and qualitative analysis of each method, aiming to identify the most suitable methods for specific problems. The findings indicate that region growing segmentation is the most effective method in most cases.",1
"Convolutional neural networks with many layers have recently been shown to achieve excellent results on many high-level tasks such as image classification, object detection and more recently also semantic segmentation. Particularly for semantic segmentation, a two-stage procedure is often employed. Hereby, convolutional networks are trained to provide good local pixel-wise features for the second step being traditionally a more global graphical model. In this work we unify this two-stage process into a single joint training algorithm. We demonstrate our method on the semantic image segmentation task and show encouraging results on the challenging PASCAL VOC 2012 dataset.",0
"Recently, it has been demonstrated that convolutional neural networks with multiple layers can produce exceptional outcomes for various high-level tasks, including image classification, object detection, and, more recently, semantic segmentation. For semantic segmentation, a typical two-stage approach is used, where convolutional networks are trained to create effective local pixel-wise features for the second stage, which is typically a more global graphical model. In this study, we have integrated this two-stage process into a single training algorithm. We showcase our approach on the challenging PASCAL VOC 2012 dataset for the task of semantic image segmentation and achieve promising results.",1
"Many machine learning tasks can be formulated in terms of predicting structured outputs. In frameworks such as the structured support vector machine (SVM-Struct) and the structured perceptron, discriminative functions are learned by iteratively applying efficient maximum a posteriori (MAP) decoding. However, maximum likelihood estimation (MLE) of probabilistic models over these same structured spaces requires computing partition functions, which is generally intractable. This paper presents a method for learning discrete exponential family models using the Bethe approximation to the MLE. Remarkably, this problem also reduces to iterative (MAP) decoding. This connection emerges by combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a convex dual objective which circumvents the intractable partition function. The result is a new single loop algorithm MLE-Struct, which is substantially more efficient than previous double-loop methods for approximate maximum likelihood estimation. Our algorithm outperforms existing methods in experiments involving image segmentation, matching problems from vision, and a new dataset of university roommate assignments.",0
"Structured outputs can be predicted through various machine learning tasks. Approaches such as SVM-Struct and structured perceptron use discriminative functions learned through iterative MAP decoding. However, MLE of probabilistic models for the same structured spaces is difficult due to the need for computing partition functions. A new method presented in this paper uses the Bethe approximation to MLE for learning discrete exponential family models. This approach also involves iterative MAP decoding and a FW algorithm on a convex dual objective to avoid the intractable partition function. The resulting algorithm, MLE-Struct, is more efficient than previous double-loop methods for approximate maximum likelihood estimation. Experimental results show that our algorithm outperforms existing methods in image segmentation, matching problems from vision, and a new dataset of university roommate assignments.",1
"A natural way to characterize the cluster structure of a dataset is by finding regions containing a high density of data. This can be done in a nonparametric way with a kernel density estimate, whose modes and hence clusters can be found using mean-shift algorithms. We describe the theory and practice behind clustering based on kernel density estimates and mean-shift algorithms. We discuss the blurring and non-blurring versions of mean-shift; theoretical results about mean-shift algorithms and Gaussian mixtures; relations with scale-space theory, spectral clustering and other algorithms; extensions to tracking, to manifold and graph data, and to manifold denoising; K-modes and Laplacian K-modes algorithms; acceleration strategies for large datasets; and applications to image segmentation, manifold denoising and multivalued regression.",0
"One way to identify clusters within a dataset is by detecting areas with a high concentration of data. To achieve this in a nonparametric manner, a kernel density estimate can be used, followed by mean-shift algorithms to locate the modes and resulting clusters. In this article, we delve into the principles and practicalities of clustering using kernel density estimates and mean-shift algorithms. Our discussion covers both the blurred and non-blurred versions of mean-shift, as well as theoretical findings about mean-shift algorithms and Gaussian mixtures. We explore connections with scale-space theory, spectral clustering, and other related algorithms. Additionally, we examine extensions to tracking, manifold and graph data, and manifold denoising, and introduce the K-modes and Laplacian K-modes algorithms. To address the challenges of large datasets, we explore strategies for acceleration. Finally, we provide examples of image segmentation, manifold denoising, and multivalued regression applications.",1
"The Liquid Argon Time Projection Chamber (LAr-TPC) detectors provide excellent imaging and particle identification ability for studying neutrinos. An efficient and automatic reconstruction procedures are required to exploit potential of this imaging technology. Herein, a novel method for segmentation of images from LAr-TPC detectors is presented. The proposed approach computes a feature descriptor for each pixel in the image, which characterizes amplitude distribution in pixel and its neighbourhood. The supervised classifier is employed to distinguish between pixels representing particle's track and noise. The classifier is trained and evaluated on the hand-labeled dataset. The proposed approach can be a preprocessing step for reconstructing algorithms working directly on detector images.",0
"To study neutrinos, the Liquid Argon Time Projection Chamber (LAr-TPC) detectors are highly effective for imaging and particle identification. However, to fully utilize this imaging technology, it is necessary to have efficient and automated reconstruction procedures. This article introduces a new method for segmenting images from LAr-TPC detectors. The method involves computing a feature descriptor for each pixel in the image, which describes the amplitude distribution in the pixel and its surroundings. A supervised classifier is used to differentiate between pixels that represent the particle's track and those that represent noise. This classifier is trained and assessed using a hand-labeled dataset. The proposed approach can serve as a preprocessing step for reconstructing algorithms that operate directly on detector images.",1
"We consider the problem of approximate Bayesian inference in log-supermodular models. These models encompass regular pairwise MRFs with binary variables, but allow to capture high-order interactions, which are intractable for existing approximate inference techniques such as belief propagation, mean field, and variants. We show that a recently proposed variational approach to inference in log-supermodular models -L-FIELD- reduces to the widely-studied minimum norm problem for submodular minimization. This insight allows to leverage powerful existing tools, and hence to solve the variational problem orders of magnitude more efficiently than previously possible. We then provide another natural interpretation of L-FIELD, demonstrating that it exactly minimizes a specific type of R\'enyi divergence measure. This insight sheds light on the nature of the variational approximations produced by L-FIELD. Furthermore, we show how to perform parallel inference as message passing in a suitable factor graph at a linear convergence rate, without having to sum up over all the configurations of the factor. Finally, we apply our approach to a challenging image segmentation task. Our experiments confirm scalability of our approach, high quality of the marginals, and the benefit of incorporating higher-order potentials.",0
"The focus of our study is on the issue of imprecise Bayesian inference in models that are log-supermodular. These models cover regular pairwise Markov Random Fields (MRFs) that have binary variables and can capture high-order interactions that are too complex for existing inference techniques such as belief propagation, mean field, and similar methods. We demonstrate that L-FIELD, a recently introduced variational approach to inference in log-supermodular models, can be reduced to the minimum norm problem for submodular minimization. This insight enables us to utilize effective existing tools to solve the variational problem more efficiently than before. We also provide an alternative interpretation of L-FIELD, showing that it precisely minimizes a specific type of R\'enyi divergence measure. This understanding offers valuable insight into the nature of the variational approximations generated by L-FIELD. Additionally, we explain how to conduct parallel inference, using message passing in a relevant factor graph, at a linear convergence rate, without the need to sum up all the factor configurations. Lastly, we apply our method to a challenging image segmentation problem, and our results confirm that our approach is scalable, produces high-quality marginals, and is effective in including higher-order potentials.",1
"The maximum entropy principle is often used for bi-level or multi-level thresholding of images. For this purpose, some methods are available based on Shannon and Tsallis entropies. In this paper, we discuss them and propose a method based on Kaniadakis entropy.",0
Bi-level or multi-level thresholding of images is frequently achieved through the utilization of the maximum entropy principle. There are several techniques available that rely on Shannon and Tsallis entropies. This article examines these techniques and introduces a new approach that employs Kaniadakis entropy.,1
"In this paper we are proposing the use of Kaniadakis entropy in the bi-level thresholding of images, in the framework of a maximum entropy principle. We discuss the role of its entropic index in determining the threshold and in driving an ""image transition"", that is, an abrupt transition in the appearance of the corresponding bi-level image. Some examples are proposed to illustrate the method and for comparing it to the approach which is using the Tsallis entropy.",0
"The utilization of Kaniadakis entropy in bi-level image thresholding within the confines of a maximum entropy principle is recommended in this manuscript. The significance of its entropic index in determining the threshold and inducing an ""image transition"" or an abrupt change in the appearance of the respective bi-level image is examined. Several illustrations are provided to demonstrate the technique and to contrast it with the utilization of Tsallis entropy.",1
"The maximum entropy principle is largely used in thresholding and segmentation of images. Among the several formulations of this principle, the most effectively applied is that based on Tsallis non-extensive entropy. Here, we discuss the role of its entropic index in determining the thresholds. When this index is spanning the interval (0,1), for some images, the values of thresholds can have large leaps. In this manner, we observe abrupt transitions in the appearance of corresponding bi-level or multi-level images. These gray-level image transitions are analogous to order or texture transitions observed in physical systems, transitions which are driven by the temperature or by other physical quantities.",0
"Thresholding and segmentation of images commonly utilize the maximum entropy principle, with the most commonly used formulation being based on Tsallis non-extensive entropy. The entropic index of this principle plays a crucial role in determining thresholds, with some images experiencing significant jumps in threshold values when the index ranges from 0 to 1. This results in abrupt transitions in the appearance of bi-level or multi-level images, similar to physical order or texture transitions driven by temperature or other physical quantities.",1
"We propose an algorithm for separating the foreground (mainly text and line graphics) from the smoothly varying background in screen content images. The proposed method is designed based on the assumption that the background part of the image is smoothly varying and can be represented by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity and cannot be modeled by this smooth representation. The algorithm separates the background and foreground using a least absolute deviation method to fit the smooth model to the image pixels. This algorithm has been tested on several images from HEVC standard test sequences for screen content coding, and is shown to have superior performance over other popular methods, such as k-means clustering based segmentation in DjVu and shape primitive extraction and coding (SPEC) algorithm. Such background/foreground segmentation are important pre-processing steps for text extraction and separate coding of background and foreground for compression of screen content images.",0
"Our proposed algorithm aims to separate foreground elements (mostly text and line graphics) from the smoothly varying background in screen content images. We assume that the background section of the image is smooth and can be represented by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create abrupt changes that cannot be modeled by this smooth representation. To separate these two components, we employ a least absolute deviation method to fit the smooth model to the image pixels. This algorithm has been extensively tested on various images from HEVC standard test sequences for screen content coding and has shown superior performance compared to other popular methods such as k-means clustering based segmentation in DjVu and shape primitive extraction and coding (SPEC) algorithm. This background/foreground segmentation is a crucial pre-processing step for text extraction and separate coding of background and foreground, which is essential for compressing screen content images.",1
"A new segmentation fusion method is proposed that ensembles the output of several segmentation algorithms applied on a remotely sensed image. The candidate segmentation sets are processed to achieve a consensus segmentation using a stochastic optimization algorithm based on the Filtered Stochastic BOEM (Best One Element Move) method. For this purpose, Filtered Stochastic BOEM is reformulated as a segmentation fusion problem by designing a new distance learning approach. The proposed algorithm also embeds the computation of the optimum number of clusters into the segmentation fusion problem.",0
"The proposed method suggests a fusion of different segmentation techniques to analyze a remotely sensed image. The resulting segmentation sets are then subjected to a stochastic optimization algorithm that utilizes the Filtered Stochastic BOEM method to achieve a consensus segmentation. To enable this process, a new distance learning approach is introduced by reformulating Filtered Stochastic BOEM as a segmentation fusion problem. The algorithm additionally incorporates the calculation of the optimal number of clusters within the segmentation fusion problem.",1
"Broadly speaking, the objective in cardiac image segmentation is to delineate the outer and inner walls of the heart to segment out either the entire or parts of the organ boundaries. This paper will focus on MR images as they are the most widely used in cardiac segmentation -- as a result of the accurate morphological information and better soft tissue contrast they provide. This cardiac segmentation information is very useful as it eases physical measurements that provides useful metrics for cardiac diagnosis such as infracted volumes, ventricular volumes, ejection fraction, myocardial mass, cardiac movement, and the like. But, this task is difficult due to the intensity and texture similarities amongst the different cardiac and background structures on top of some noisy artifacts present in MR images. Thus far, various researchers have proposed different techniques to solve some of the pressing issues. This seminar paper presents an overview of representative medical image segmentation techniques. The paper also highlights preferred approaches for segmentation of the four cardiac chambers: the left ventricle (LV), right ventricle (RV), left atrium (LA) and right atrium (RA), on short axis image planes.",0
"The main goal in cardiac image segmentation is to accurately distinguish the outer and inner walls of the heart, either for the entire organ or specific parts of it. This article focuses on MR images, which are widely used due to their precise morphological information and better soft tissue contrast. Accurate cardiac segmentation is crucial for physical measurements that aid in cardiac diagnosis, including infracted volumes, ventricular volumes, ejection fraction, myocardial mass, cardiac movement, and more. However, this task is challenging due to similarities in intensity and texture between various cardiac and background structures, as well as noisy artifacts in MR images. To address these issues, different techniques have been proposed by researchers. This seminar paper provides an overview of medical image segmentation techniques and highlights preferred approaches for segmenting the four cardiac chambers (LV, RV, LA, and RA) on short axis image planes.",1
"This paper presents a novel selective constraint propagation method for constrained image segmentation. In the literature, many pairwise constraint propagation methods have been developed to exploit pairwise constraints for cluster analysis. However, since most of these methods have a polynomial time complexity, they are not much suitable for segmentation of images even with a moderate size, which is actually equivalent to cluster analysis with a large data size. Considering the local homogeneousness of a natural image, we choose to perform pairwise constraint propagation only over a selected subset of pixels, but not over the whole image. Such a selective constraint propagation problem is then solved by an efficient graph-based learning algorithm. To further speed up our selective constraint propagation, we also discard those less important propagated constraints during graph-based learning. Finally, the selectively propagated constraints are exploited based on $L_1$-minimization for normalized cuts over the whole image. The experimental results demonstrate the promising performance of the proposed method for segmentation with selectively propagated constraints.",0
"A new method for constrained image segmentation using selective constraint propagation is presented in this paper. While pairwise constraint propagation methods have been developed for cluster analysis, they are not suitable for image segmentation due to their polynomial time complexity. To address this issue, the proposed method performs pairwise constraint propagation only over a selected subset of pixels based on the local homogeneousness of the image. The selective constraint propagation is solved using a graph-based learning algorithm that discards less important propagated constraints to speed up the process. The selectively propagated constraints are then used for normalized cuts over the entire image using $L_1$-minimization. The experimental results demonstrate the effectiveness of the proposed method for segmentation with selectively propagated constraints.",1
"This paper aims at developing an integrated system of clothing co-parsing, in order to jointly parse a set of clothing images (unsegmented but annotated with tags) into semantic configurations. We propose a data-driven framework consisting of two phases of inference. The first phase, referred as ""image co-segmentation"", iterates to extract consistent regions on images and jointly refines the regions over all images by employing the exemplar-SVM (E-SVM) technique [23]. In the second phase (i.e. ""region co-labeling""), we construct a multi-image graphical model by taking the segmented regions as vertices, and incorporate several contexts of clothing configuration (e.g., item location and mutual interactions). The joint label assignment can be solved using the efficient Graph Cuts algorithm. In addition to evaluate our framework on the Fashionista dataset [30], we construct a dataset called CCP consisting of 2098 high-resolution street fashion photos to demonstrate the performance of our system. We achieve 90.29% / 88.23% segmentation accuracy and 65.52% / 63.89% recognition rate on the Fashionista and the CCP datasets, respectively, which are superior compared with state-of-the-art methods.",0
"The objective of this paper is to create a clothing co-parsing system that can parse a group of clothing images with tags into semantic configurations. A data-driven framework with two phases of inference is proposed. The first phase, called ""image co-segmentation"", extracts consistent regions on images and refines them using the exemplar-SVM technique. The second phase, ""region co-labeling"", constructs a multi-image graphical model using the segmented regions as vertices and includes various contexts of clothing configuration. The joint label assignment can be solved using the Graph Cuts algorithm. The framework is evaluated on the Fashionista dataset, and a new dataset called CCP is created to showcase the system's performance. The system achieves high segmentation accuracy and recognition rate on both datasets, surpassing state-of-the-art methods.",1
"Image segmentation refers to the process to divide an image into nonoverlapping meaningful regions according to human perception, which has become a classic topic since the early ages of computer vision. A lot of research has been conducted and has resulted in many applications. However, while many segmentation algorithms exist, yet there are only a few sparse and outdated summarizations available, an overview of the recent achievements and issues is lacking. We aim to provide a comprehensive review of the recent progress in this field. Covering 180 publications, we give an overview of broad areas of segmentation topics including not only the classic bottom-up approaches, but also the recent development in superpixel, interactive methods, object proposals, semantic image parsing and image cosegmentation. In addition, we also review the existing influential datasets and evaluation metrics. Finally, we suggest some design flavors and research directions for future research in image segmentation.",0
"The division of an image into distinct and meaningful regions based on human perception is known as image segmentation, which has been a significant topic in computer vision since its inception. Despite extensive research and numerous applications, there are limited and outdated summaries available, and a comprehensive review of recent advancements and challenges is missing. Our aim is to provide a comprehensive analysis of recent progress in this field by examining 180 publications, covering a variety of segmentation topics, including traditional bottom-up techniques, as well as recent developments in superpixel, interactive methods, object proposals, semantic image parsing, and image cosegmentation. Additionally, we review existing datasets and evaluation metrics that have had a significant impact. Finally, we suggest potential areas for future research and design concepts to improve image segmentation.",1
"The figure-ground segmentation of humans in images captured in natural environments is an outstanding open problem due to the presence of complex backgrounds, articulation, varying body proportions, partial views and viewpoint changes. In this work we propose class-specific segmentation models that leverage parametric max-flow image segmentation and a large dataset of human shapes. Our contributions are as follows: (1) formulation of a sub-modular energy model that combines class-specific structural constraints and data-driven shape priors, within a parametric max-flow optimization methodology that systematically computes all breakpoints of the model in polynomial time; (2) design of a data-driven class-specific fusion methodology, based on matching against a large training set of exemplar human shapes (100,000 in our experiments), that allows the shape prior to be constructed on-the-fly, for arbitrary viewpoints and partial views. (3) demonstration of state of the art results, in two challenging datasets, H3D and MPII (where figure-ground segmentation annotations have been added by us), where we substantially improve on the first ranked hypothesis estimates of mid-level segmentation methods, by 20%, with hypothesis set sizes that are up to one order of magnitude smaller.",0
"The problem of separating humans from the background in images taken in natural settings is difficult due to various factors such as complex backgrounds, differing body shapes and viewpoints, and partial views. This study proposes segmentation models that are specific to human classes and use a large dataset of human shapes along with parametric max-flow image segmentation. The study's contributions include a sub-modular energy model that combines data-driven shape priors and class-specific structural constraints, a fusion methodology that matches against a training set of 100,000 exemplar human shapes to construct shape priors in real-time for arbitrary viewpoints and partial views, and improved results on two challenging datasets (H3D and MPII) by 20% compared to mid-level segmentation methods, with smaller hypothesis set sizes.",1
"Today Bayesian networks are more used in many areas of decision support and image processing. In this way, our proposed approach uses Bayesian Network to modelize the segmented image quality. This quality is calculated on a set of attributes that represent local evaluation measures. The idea is to have these local levels chosen in a way to be intersected into them to keep the overall appearance of segmentation. The approach operates in two phases: the first phase is to make an over-segmentation which gives superpixels card. In the second phase, we model the superpixels by a Bayesian Network. To find the segmented image with the best overall quality we used two approximate inference methods, the first using ICM algorithm which is widely used in Markov Models and a second is a recursive method called algorithm of model decomposition based on max-product algorithm which is very popular in the recent works of image segmentation. For our model, we have shown that the composition of these two algorithms leads to good segmentation performance.",0
"In contemporary times, Bayesian networks have gained widespread usage in various domains, including decision support and image processing. Our proposed methodology involves leveraging Bayesian Network to construct a model for the quality of a segmented image. This quality is computed using a collection of attributes that represent local evaluation measures, with the objective being to select these local levels in a manner that ensures the overall appearance of segmentation is preserved. The methodology is carried out in two phases, with the first phase entailing an over-segmentation that yields superpixels card. The second phase involves modeling the superpixels using a Bayesian Network. To obtain the segmented image with the most optimal overall quality, we employed two approximate inference methods. The first method was the ICM algorithm, which is commonly utilized in Markov Models. The second method was a recursive technique known as the algorithm of model decomposition, based on max-product algorithm, which has gained popularity in recent image segmentation works. Our study illustrates that combining both algorithms results in a high-performance segmentation.",1
"The paper concentrates on improvement of segmentation accuracy by addressing some of the key challenges of handwritten Devanagari word image segmentation technique. In the present work, we have developed a new feature based approach for identification of Matra pixels from a word image, design of a non-linear fuzzy membership functions for headline estimation and finally design of a non-linear fuzzy functions for identifying segmentation points on the Matra. The segmentation accuracy achieved by the current technique is 94.8%. This shows an improvement of performance by 1.8% over the previous technique [1] on a 300-word dataset, used for the current experiment.",0
"The focus of the article is to enhance the precision of handwritten Devanagari word image segmentation by addressing significant challenges. A novel feature-based approach has been devised to identify Matra pixels in a word image, accompanied by the development of non-linear fuzzy membership functions for headline estimation and identifying segmentation points on the Matra. The new technique has achieved a segmentation accuracy of 94.8%, an improvement of 1.8% over the prior technique [1], using a dataset of 300 words for the current experiment.",1
"The segmentation of synthetic aperture radar (SAR) images is a longstanding yet challenging task, not only because of the presence of speckle, but also due to the variations of surface backscattering properties in the images. Tremendous investigations have been made to eliminate the speckle effects for the segmentation of SAR images, while few work devotes to dealing with the variations of backscattering coefficients in the images. In order to overcome both the two difficulties, this paper presents a novel SAR image segmentation method by exploiting a multi-scale active contour model based on the non-local processing principle. More precisely, we first formulize the SAR segmentation problem with an active contour model by integrating the non-local interactions between pairs of patches inside and outside the segmented regions. Secondly, a multi-scale strategy is proposed to speed up the non-local active contour segmentation procedure and to avoid falling into local minimum for achieving more accurate segmentation results. Experimental results on simulated and real SAR images demonstrate the efficiency and feasibility of the proposed method: it can not only achieve precise segmentations for images with heavy speckles and non-local intensity variations, but also can be used for SAR images from different types of sensors.",0
"Segmenting synthetic aperture radar (SAR) images is a challenging task due to the presence of speckle and variations in surface backscattering properties. While many studies have focused on eliminating speckle effects for SAR segmentation, few have addressed the issue of backscattering coefficient variations. This paper proposes a novel SAR image segmentation method that utilizes a multi-scale active contour model based on the non-local processing principle to overcome both challenges. The approach formulates the SAR segmentation problem with an active contour model that integrates non-local interactions between pairs of patches within and outside segmented regions. A multi-scale strategy is also proposed to speed up the non-local active contour segmentation procedure and achieve more precise segmentation results. Experimental results demonstrate the method's effectiveness in achieving accurate segmentations for images with speckles and non-local intensity variations from different types of sensors.",1
"Fingerprint recognition plays an important role in many commercial applications and is used by millions of people every day, e.g. for unlocking mobile phones. Fingerprint image segmentation is typically the first processing step of most fingerprint algorithms and it divides an image into foreground, the region of interest, and background. Two types of error can occur during this step which both have a negative impact on the recognition performance: 'true' foreground can be labeled as background and features like minutiae can be lost, or conversely 'true' background can be misclassified as foreground and spurious features can be introduced. The contribution of this paper is threefold: firstly, we propose a novel factorized directional bandpass (FDB) segmentation method for texture extraction based on the directional Hilbert transform of a Butterworth bandpass (DHBB) filter interwoven with soft-thresholding. Secondly, we provide a manually marked ground truth segmentation for 10560 images as an evaluation benchmark. Thirdly, we conduct a systematic performance comparison between the FDB method and four of the most often cited fingerprint segmentation algorithms showing that the FDB segmentation method clearly outperforms these four widely used methods. The benchmark and the implementation of the FDB method are made publicly available.",0
"Millions of people use fingerprint recognition in commercial applications such as unlocking mobile phones. Fingerprint image segmentation is a crucial first step in most fingerprint algorithms, separating the foreground (region of interest) from the background. Errors can occur during this step, leading to a negative impact on recognition performance. This paper proposes a novel factorized directional bandpass (FDB) segmentation method for texture extraction, based on the directional Hilbert transform of a Butterworth bandpass (DHBB) filter interwoven with soft-thresholding. The paper also provides a manually marked ground truth segmentation for 10560 images as an evaluation benchmark and conducts a systematic performance comparison between the FDB method and four widely used fingerprint segmentation algorithms. The FDB method consistently outperforms the other methods, and the benchmark and implementation are publicly available.",1
"A new Bayesian image segmentation algorithm is proposed by combining a loopy belief propagation with an inverse real space renormalization group transformation to reduce the computational time. In results of our experiment, we observe that the proposed method can reduce the computational time to less than one-tenth of that taken by conventional Bayesian approaches.",0
We propose a novel Bayesian image segmentation algorithm that utilizes a combination of loopy belief propagation and inverse real space renormalization group transformation to minimize computational time. Our experimental findings indicate that this approach can decrease computational time to under 10% of that required by traditional Bayesian methodologies.,1
"A functional for joint variational object segmentation and shape matching is developed. The formulation is based on optimal transport w.r.t. geometric distance and local feature similarity. Geometric invariance and modelling of object-typical statistical variations is achieved by introducing degrees of freedom that describe transformations and deformations of the shape template. The shape model is mathematically equivalent to contour-based approaches but inference can be performed without conversion between the contour and region representations, allowing combination with other convex segmentation approaches and simplifying optimization. While the overall functional is non-convex, non-convexity is confined to a low-dimensional variable. We propose a locally optimal alternating optimization scheme and a globally optimal branch and bound scheme, based on adaptive convex relaxation. Combining both methods allows to eliminate the delicate initialization problem inherent to many contour based approaches while remaining computationally practical. The properties of the functional, its ability to adapt to a wide range of input data structures and the different optimization schemes are illustrated and compared by numerical experiments.",0
"The development of a functional for joint variational object segmentation and shape matching is achieved using optimal transport based on geometric distance and local feature similarity. To ensure geometric invariance and modeling of object-typical statistical variations, degrees of freedom are introduced to describe transformations and deformations of the shape template. The shape model is equivalent to contour-based methods, but inference can be performed without the need for conversion between contour and region representations, simplifying optimization and allowing for combination with other convex segmentation approaches. Although the overall functional is non-convex, the non-convexity is limited to a low-dimensional variable. We present a locally optimal alternating optimization scheme and a globally optimal branch and bound scheme, based on adaptive convex relaxation, which, when combined, eliminate the delicate initialization problem associated with many contour-based approaches and remain computationally practical. Numerical experiments illustrate and compare the properties of the functional, its ability to adapt to a wide range of input data structures, and the different optimization schemes.",1
"We present highly efficient algorithms for performing forward and backward propagation of Convolutional Neural Network (CNN) for pixelwise classification on images. For pixelwise classification tasks, such as image segmentation and object detection, surrounding image patches are fed into CNN for predicting the classes of centered pixels via forward propagation and for updating CNN parameters via backward propagation. However, forward and backward propagation was originally designed for whole-image classification. Directly applying it to pixelwise classification in a patch-by-patch scanning manner is extremely inefficient, because surrounding patches of pixels have large overlaps, which lead to a lot of redundant computation.   The proposed algorithms eliminate all the redundant computation in convolution and pooling on images by introducing novel d-regularly sparse kernels. It generates exactly the same results as those by patch-by-patch scanning. Convolution and pooling operations with such kernels are able to continuously access memory and can run efficiently on GPUs. A fraction of patches of interest can be chosen from each training image for backward propagation by applying a mask to the error map at the last CNN layer. Its computation complexity is constant with respect to the number of patches sampled from the image. Experiments have shown that our proposed algorithms speed up commonly used patch-by-patch scanning over 1500 times in both forward and backward propagation. The speedup increases with the sizes of images and patches.",0
"Efficient algorithms have been developed for Convolutional Neural Network (CNN) forward and backward propagation in pixelwise classification of images. The CNN is fed with surrounding image patches to predict the classes of centered pixels via forward propagation and to update its parameters via backward propagation for image segmentation and object detection. However, applying the original forward and backward propagation designed for whole-image classification in a patch-by-patch scanning manner is inefficient due to the large overlaps in surrounding patches resulting in redundant computation. The proposed algorithms utilize d-regularly sparse kernels to eliminate redundant computation in convolution and pooling operations on images, achieving the same results as patch-by-patch scanning with continuous memory access and GPU efficiency. A mask is applied to the error map at the last CNN layer to select a fraction of patches of interest for backward propagation, with a computation complexity that is constant regardless of the number of patches sampled. Experiments demonstrate that the proposed algorithms achieve over 1500 times speedup in both forward and backward propagation for commonly used patch-by-patch scanning, with increased speedup for larger images and patches.",1
"In this paper, an automatic seeded region growing algorithm is proposed for cellular image segmentation. First, the regions of interest (ROIs) extracted from the preprocessed image. Second, the initial seeds are automatically selected based on ROIs extracted from the image. Third, the most reprehensive seeds are selected using a machine learning algorithm. Finally, the cellular image is segmented into regions where each region corresponds to a seed. The aim of the proposed is to automatically extract the Region of Interests (ROI) from the cellular images in terms of overcoming the explosion, under segmentation and over segmentation problems. Experimental results show that the proposed algorithm can improve the segmented image and the segmented results are less noisy as compared to some existing algorithms.",0
"This paper presents an automated seeded region growing algorithm that is designed to segment cellular images. The algorithm comprises four main stages. Firstly, the regions of interest (ROIs) are extracted from the preprocessed image. Secondly, the initial seeds are automatically selected based on the ROIs that were extracted from the image. Thirdly, the most representative seeds are then chosen using a machine learning algorithm. Finally, the cellular image is segmented into regions, and each region corresponds to a seed. The primary objective of the proposed algorithm is to automatically extract the ROIs from cellular images while overcoming issues such as over-segmentation, under-segmentation, and explosion. Experimental results demonstrate that the proposed algorithm enhances the segmented image and produces less noisy segmented results compared to some of the existing algorithms.",1
"When evaluating computer vision systems, we are often concerned with performance on a task-specific evaluation measure such as the Intersection-Over-Union score used in the PASCAL VOC image segmentation challenge. Ideally, our systems would be tuned specifically to these evaluation measures. However, despite much work on loss-aware structured prediction, top performing systems do not use these techniques. In this work, we seek to address this problem, incorporating loss-aware prediction in a manner that is amenable to the approaches taken by top performing systems. Our main idea is to simultaneously leverage two systems: a highly tuned pipeline system as is found on top of leaderboards, and a traditional CRF. We show how to combine high quality candidate solutions from the pipeline with the probabilistic approach of the CRF that is amenable to loss-aware prediction. The result is that we can use loss-aware prediction methodology to improve performance of the highly tuned pipeline system.",0
"The evaluation of computer vision systems involves assessing their performance on specific tasks, such as the Intersection-Over-Union score used in the PASCAL VOC image segmentation challenge. Ideally, these systems would be optimized for these measures, but current top performing systems do not utilize loss-aware structured prediction techniques. This study aims to address this issue by integrating loss-aware prediction into the approaches used by leading systems. The proposed method combines a highly refined pipeline system with a traditional CRF, leveraging the strengths of both to improve performance through loss-aware prediction. By incorporating this methodology, the highly tuned pipeline system can achieve even better results.",1
"Advances in computing technology have allowed researchers across many fields of endeavor to collect and maintain vast amounts of observational statistical data such as clinical data,biological patient data,data regarding access of web sites,financial data,and the like.Brain Magnetic Resonance Imaging(MRI)segmentation is a complex problem in the field of medical imaging despite various presented methods.MR image of human brain can be divided into several sub regions especially soft tissues such as gray matter,white matter and cerebrospinal fluid.Although edge information is the main clue in image segmentation,it can not get a better result in analysis the content of images without combining other information.The segmentation of brain tissue in the magnetic resonance imaging(MRI)is very important for detecting the existence and outlines of tumors.In this paper,an algorithm about segmentation based on the symmetry character of brain MRI image is presented.Our goal is to detect the position and boundary of tumors automatically.Experiments were conducted on real pictures,and the results show that the algorithm is flexible and convenient.",0
"With advances in computing technology, researchers in various fields have been able to collect and maintain observational statistical data on a large scale. This includes data from clinical trials, biological patient data, website usage data, financial data, and more. Brain Magnetic Resonance Imaging (MRI) segmentation presents a complex problem in the field of medical imaging, despite various methods presented. Human brain MR images can be divided into sub-regions, such as gray matter, white matter, and cerebrospinal fluid. While edge information is crucial in image segmentation, combining other information is necessary for better analysis of image content. Segmentation of brain tissue in MRI is vital in detecting tumors' existence and outlines. This paper presents an algorithm based on the symmetry character of brain MRI images for segmentation. Our objective is to detect the position and boundary of tumors automatically. Experiments using real pictures demonstrate the algorithm's flexibility and convenience.",1
"Algorithms based on spectral graph cut objectives such as normalized cuts, ratio cuts and ratio association have become popular in recent years because they are widely applicable and simple to implement via standard eigenvector computations. Despite strong performance for a number of clustering tasks, spectral graph cut algorithms still suffer from several limitations: first, they require the number of clusters to be known in advance, but this information is often unknown a priori; second, they tend to produce clusters with uniform sizes. In some cases, the true clusters exhibit a known size distribution; in image segmentation, for instance, human-segmented images tend to yield segment sizes that follow a power-law distribution. In this paper, we propose a general framework of power-law graph cut algorithms that produce clusters whose sizes are power-law distributed, and also does not fix the number of clusters upfront. To achieve our goals, we treat the Pitman-Yor exchangeable partition probability function (EPPF) as a regularizer to graph cut objectives. Because the resulting objectives cannot be solved by relaxing via eigenvectors, we derive a simple iterative algorithm to locally optimize the objectives. Moreover, we show that our proposed algorithm can be viewed as performing MAP inference on a particular Pitman-Yor mixture model. Our experiments on various data sets show the effectiveness of our algorithms.",0
"Spectral graph cut algorithms, such as normalized cuts, ratio cuts, and ratio association, have gained popularity in recent years due to their versatility and ease of implementation using standard eigenvector computations. However, they still have limitations, including the requirement of prior knowledge of the number of clusters and the tendency to produce clusters of uniform size. In certain cases, such as in image segmentation, the true clusters have a known size distribution, such as a power-law distribution. To address these limitations, we present a framework for power-law graph cut algorithms that produce clusters with power-law distributed sizes and do not fix the number of clusters upfront. We use the Pitman-Yor exchangeable partition probability function as a regularizer to the graph cut objectives, which cannot be solved by relaxing via eigenvectors. Instead, we derive a simple iterative algorithm to optimize the objectives locally. Our proposed algorithm can be viewed as performing MAP inference on a particular Pitman-Yor mixture model. Our experiments on various data sets demonstrate the effectiveness of our algorithms.",1
"Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets.",0
"Many vision systems, such as image segmentation algorithms and object detectors, rely heavily on edge detection. Local image patches with edges often display certain forms of local structure, such as straight lines or T-junctions. This research paper takes advantage of this structure to develop an edge detector that is both accurate and computationally efficient. The authors apply a structured learning framework to random decision forests to predict local edge masks. The decision trees are learned using a novel approach that maps the structured labels to a discrete space, making it possible to evaluate standard information gain measures. The resulting approach achieves state-of-the-art edge detection results on both the BSDS500 Segmentation dataset and NYU Depth dataset, while also being orders of magnitude faster than many competing state-of-the-art approaches. Additionally, the authors show that their approach is a general purpose edge detector that performs well across multiple datasets.",1
"Ring theory is one of the branches of the abstract algebra that has been broadly used in images. However, ring theory has not been very related with image segmentation. In this paper, we propose a new index of similarity among images using Zn rings and the entropy function. This new index was applied as a new stopping criterion to the Mean Shift Iterative Algorithm with the goal to reach a better segmentation. An analysis on the performance of the algorithm with this new stopping criterion is carried out. The obtained results proved that the new index is a suitable tool to compare images.",0
"Abstract algebra encompasses various branches, including ring theory, which has been widely utilized in image processing. Despite this, it has not been closely associated with image segmentation. This article introduces a novel similarity index between images utilizing Zn rings and the entropy function. The index is implemented as a new stopping criterion in the Mean Shift Iterative Algorithm, leading to improved segmentation. A comprehensive analysis of the algorithm's performance with the new stopping criterion demonstrates that the index is an effective tool for comparing images.",1
"Structural support vector machines (SSVMs) are amongst the best performing models for structured computer vision tasks, such as semantic image segmentation or human pose estimation. Training SSVMs, however, is computationally costly, because it requires repeated calls to a structured prediction subroutine (called \emph{max-oracle}), which has to solve an optimization problem itself, e.g. a graph cut.   In this work, we introduce a new algorithm for SSVM training that is more efficient than earlier techniques when the max-oracle is computationally expensive, as it is frequently the case in computer vision tasks. The main idea is to (i) combine the recent stochastic Block-Coordinate Frank-Wolfe algorithm with efficient hyperplane caching, and (ii) use an automatic selection rule for deciding whether to call the exact max-oracle or to rely on an approximate one based on the cached hyperplanes.   We show experimentally that this strategy leads to faster convergence to the optimum with respect to the number of requires oracle calls, and that this translates into faster convergence with respect to the total runtime when the max-oracle is slow compared to the other steps of the algorithm.   A publicly available C++ implementation is provided at http://pub.ist.ac.at/~vnk/papers/SVM.html .",0
"Structural support vector machines (SSVMs) are highly effective models for computer vision tasks that involve structured data, such as human pose estimation or semantic image segmentation. However, training SSVMs can be computationally expensive because it involves calling a structured prediction subroutine called the ""max-oracle,"" which must solve an optimization problem such as a graph cut. In this study, we present a new algorithm for training SSVMs that is more efficient than previous methods when the max-oracle is computationally expensive, as is often the case in computer vision tasks. Our approach involves combining the stochastic Block-Coordinate Frank-Wolfe algorithm with efficient hyperplane caching and using an automatic selection rule to determine whether to call the exact max-oracle or rely on an approximate one based on cached hyperplanes. Our experimental results demonstrate that this strategy leads to faster convergence to the optimum and faster overall runtime when the max-oracle is slow compared to other algorithm steps. A publicly available C++ implementation is provided at http://pub.ist.ac.at/~vnk/papers/SVM.html.",1
"In this paper, we propose a fast fully convolutional neural network (FCNN) for crowd segmentation. By replacing the fully connected layers in CNN with 1 by 1 convolution kernels, FCNN takes whole images as inputs and directly outputs segmentation maps by one pass of forward propagation. It has the property of translation invariance like patch-by-patch scanning but with much lower computation cost. Once FCNN is learned, it can process input images of any sizes without warping them to a standard size. These attractive properties make it extendable to other general image segmentation problems. Based on FCNN, a multi-stage deep learning is proposed to integrate appearance and motion cues for crowd segmentation. Both appearance filters and motion filers are pretrained stage-by-stage and then jointly optimized. Different combination methods are investigated. The effectiveness of our approach and component-wise analysis are evaluated on two crowd segmentation datasets created by us, which include image frames from 235 and 11 scenes, respectively. They are currently the largest crowd segmentation datasets and will be released to the public.",0
"This paper presents a rapid FCNN for crowd segmentation that replaces fully connected layers in CNN with 1 by 1 convolution kernels. The FCNN takes whole images as inputs and directly produces segmentation maps through a single forward propagation pass. The FCNN has the translation invariance property, similar to patch-by-patch scanning, but with lower computation costs. Additionally, the FCNN can process input images of any size without warping them to a standard size, making it applicable to other general image segmentation problems. The paper also proposes a multi-stage deep learning approach to combine appearance and motion cues for crowd segmentation. Appearance and motion filters are pretrained in stages and then optimized jointly using various combination methods. The effectiveness of the approach and component-wise analysis are evaluated on two newly created crowd segmentation datasets, consisting of image frames from 235 and 11 scenes, respectively. These datasets are currently the largest crowd segmentation datasets and will be released to the public.",1
"In this paper, we propose an approach to the unsupervised segmentation of images using Markov Random Field. The proposed approach is based on the idea of Bit Plane Slicing. We use the planes as initial labellings for an ensemble of segmentations. With pixelwise voting, a robust segmentation approach can be achieved, which we demonstrate on microscope cell images. We tested our approach on a publicly available database, where it proven to be competitive with other methods and manual segmentation.",0
"Our paper suggests a Markov Random Field approach for unsupervised image segmentation. Our method employs Bit Plane Slicing to initialize labelling for a group of segmentations. By utilizing pixelwise voting, we establish a resilient segmentation approach, as evidenced in our demonstration of microscope cell images. We tested our approach on a public database, where it proved to be on par with other techniques and manual segmentation.",1
"Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we formulate saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, utilizes the supervised learning approach to map the regional feature vector to a saliency score. Saliency scores across multiple levels are finally fused to produce the saliency map. The contributions lie in two-fold. One is that we propose a discriminate regional feature integration approach for salient object detection. Compared with existing heuristic models, our proposed method is able to automatically integrate high-dimensional regional saliency features and choose discriminative ones. The other is that by investigating standard generic region properties as well as two widely studied concepts for salient object detection, i.e., regional contrast and backgroundness, our approach significantly outperforms state-of-the-art methods on six benchmark datasets. Meanwhile, we demonstrate that our method runs as fast as most existing algorithms.",0
"Numerous heuristic computational models have been developed for salient object detection, which has gained significant attention. In this paper, we present a new approach to calculate saliency maps by treating it as a regression problem. Our technique employs multi-level image segmentation and uses supervised learning to map regional feature vectors to saliency scores. We then combine the scores from different levels to generate a saliency map. Our method contributes in two ways. Firstly, we propose a regional feature integration approach that automatically selects discriminative high-dimensional regional saliency features, unlike existing heuristic models. Secondly, by exploring standard generic region properties and two widely studied concepts, regional contrast and backgroundness, our method outperforms current state-of-the-art techniques on six benchmark datasets. Additionally, our approach runs as quickly as most existing algorithms.",1
"Person re-identification is becoming a hot research for developing both machine learning algorithms and video surveillance applications. The task of person re-identification is to determine which person in a gallery has the same identity to a probe image. This task basically assumes that the subject of the probe image belongs to the gallery, that is, the gallery contains this person. However, in practical applications such as searching a suspect in a video, this assumption is usually not true. In this paper, we consider the open-set person re-identification problem, which includes two sub-tasks, detection and identification. The detection sub-task is to determine the presence of the probe subject in the gallery, and the identification sub-task is to determine which person in the gallery has the same identity as the accepted probe. We present a database collected from a video surveillance setting of 6 cameras, with 200 persons and 7,413 images segmented. Based on this database, we develop a benchmark protocol for evaluating the performance under the open-set person re-identification scenario. Several popular metric learning algorithms for person re-identification have been evaluated as baselines. From the baseline performance, we observe that the open-set person re-identification problem is still largely unresolved, thus further attention and effort is needed.",0
"Currently, person re-identification is a trending research topic for machine learning algorithms and video surveillance applications. The task aims to match a probe image with a person in a gallery who has the same identity. However, this task assumes that the person in the probe image is present in the gallery, which is not always the case in practical applications such as identifying a suspect in a video. This paper addresses the open-set person re-identification problem, which involves two sub-tasks: detection and identification. The detection sub-task confirms if the probe person is present in the gallery while the identification sub-task matches the probe person to the correct identity. The paper presents a database of 7,413 segmented images collected from 6 cameras, containing 200 persons, and a benchmark protocol for evaluating open-set person re-identification performance. The baseline results from evaluating several metric learning algorithms reveal that the problem is unresolved and requires further investigation.",1
"Automatic face recognition has received significant performance improvement by developing specialised facial image representations. On the other hand, generic object recognition has rarely been applied to the face recognition. Spatial pyramid pooling of features encoded by an over-complete dictionary has been the key component of many state-of-the-art image classification systems. Inspired by its success, in this work we develop a new face image representation method inspired by the second-order pooling in Carreira et al. [1], which was originally proposed for image segmentation. The proposed method differs from the previous methods in that, we encode the densely extracted local patches by a small-size dictionary; and the facial image signatures are obtained by pooling the second-order statistics of the encoded features. We show the importance of pooling on encoded features, which is bypassed by the original second-order pooling method to avoid the high computational cost. Equipped with a simple linear classifier, the proposed method outperforms the state-of-the-art face identification performance by large margins. For example, on the LFW databases, the proposed method performs better than the previous best by around 13% accuracy.",0
"The performance of automatic face recognition has seen a significant boost with the development of specialized facial image representations. However, generic object recognition has not been commonly utilized in face recognition. Many image classification systems have used spatial pyramid pooling of features from an over-complete dictionary, resulting in improved performance. Building on this success, the authors of this study have created a new face image representation method inspired by the second-order pooling technique originally designed for image segmentation by Carreira et al. Unlike previous methods, the authors encode densely extracted local patches with a small-size dictionary and obtain facial image signatures by pooling the second-order statistics of the encoded features. The importance of this pooling process is highlighted, as it was avoided in the original second-order pooling method due to high computational costs. The proposed method, equipped with a simple linear classifier, significantly outperforms the state-of-the-art face identification performance, with an accuracy increase of around 13% on the LFW databases.",1
"A quantum edge detector for image segmentation in optical environments is presented in this work. A Boolean version of the same detector is presented too. The quantum version of the new edge detector works with computational basis states, exclusively. This way, we can easily avoid the problem of quantum measurement retrieving the result of applying the new detector on the image. Besides, a new criterion and logic based on projections onto vertical axis of Bloch's Sphere exclusively are presented too. This approach will allow us: 1) a simpler development of logic quantum operations, where they will closer to those used in the classical logic operations, 2) building simple and robust classical-to-quantum and quantum-to-classical interfaces. Said so far is extended to quantum algorithms outside image processing too. In a special section on metric and simulations, a new metric based on the comparison between the classical and quantum versions algorithms for edge detection of images is presented. Notable differences between the results of classical and quantum versions of such algorithms (outside and inside of quantum computer, respectively) show the existence of implementation problems involved in the experiment, and that they have not been properly modeled for optical environments. However, although they are different, the quantum results are equally valid. The latter is clearly seen in the computer simulations",0
"This study introduces a quantum edge detector for image segmentation in optical settings and also presents a Boolean version of the detector. The quantum detector operates solely with computational basis states to avoid issues with quantum measurement and obtaining results from the image. Additionally, the study introduces a new criterion and logic based on projections onto the vertical axis of Bloch's Sphere. This approach simplifies the development of logic quantum operations and enables the creation of robust classical-to-quantum and quantum-to-classical interfaces. This methodology can also be applied to quantum algorithms beyond image processing. The study includes a section on metrics and simulations where a comparison is made between classical and quantum algorithms for edge detection of images. Although there are notable differences between the results of classical and quantum algorithms, the quantum results are equally valid. These differences highlight the challenges involved in properly modeling optical environments for implementation. The validity of the quantum results is confirmed through computer simulations.",1
"In this work, we propose a learning framework for identifying synapses using a deep and wide multi-scale recursive (DAWMR) network, previously considered in image segmentation applications. We apply this approach on electron microscopy data from invertebrate fly brain tissue. By learning features directly from the data, we are able to achieve considerable improvements over existing techniques that rely on a small set of hand-designed features. We show that this system can reduce the amount of manual annotation required, in both acquisition of training data as well as verification of inferred detections.",0
"Our study introduces a learning framework that utilizes a deep and wide multi-scale recursive network (DAWMR), which was previously utilized in image segmentation applications, to identify synapses. We implement this approach on electron microscopy data obtained from invertebrate fly brain tissue. Our system learns features directly from the data, resulting in significant improvements over current methods that depend on a limited number of hand-designed features. We demonstrate that our system minimizes the need for manual annotation, both in collecting training data and verifying inferred detections.",1
"The visual analysis of retina and of its vascular characteristics is important in the diagnosis and monitoring of diseases of visual perception. In the related medical diagnoses, the digital processing of the fundus images is used to obtain the segmentation of retinal vessels. However, an image segmentation is often requiring methods based on peculiar or complex algorithms: in this paper we will show some alternative approaches obtained by applying freely available tools to enhance, without a specific segmentation, the images of the fundus of the eye. We will see in particular, that combining the use of GIMP, the GNU Image Manipulation Program, with the wavelet filter of Iris, a program well-known for processing astronomical images, the result is giving images which can be alternative of those obtained from segmentation.",0
"The examination of the retina and its vascular characteristics is crucial for detecting and monitoring visual perception disorders. To diagnose such conditions, medical professionals typically utilize digital processing of fundus images to segment retinal vessels. However, this often requires complex algorithms. In this article, we present alternative techniques that enhance fundus images without specific segmentation, using freely available tools. By combining GIMP (GNU Image Manipulation Program) with the wavelet filter of Iris, a program commonly used for processing astronomical images, we demonstrate that our approach can produce comparable results to those achieved by segmentation.",1
"This paper presents a Bayesian image segmentation model based on Potts prior and loopy belief propagation. The proposed Bayesian model involves several terms, including the pairwise interactions of Potts models, and the average vectors and covariant matrices of Gauss distributions in color image modeling. These terms are often referred to as hyperparameters in statistical machine learning theory. In order to determine these hyperparameters, we propose a new scheme for hyperparameter estimation based on conditional maximization of entropy in the Potts prior. The algorithm is given based on loopy belief propagation. In addition, we compare our conditional maximum entropy framework with the conventional maximum likelihood framework, and also clarify how the first order phase transitions in LBP's for Potts models influence our hyperparameter estimation procedures.",0
"The focus of this paper is to introduce a Bayesian model for image segmentation that utilizes Potts prior and loopy belief propagation. The model includes a number of hyperparameters, such as the pairwise interactions of Potts models and the average vectors and covariant matrices of Gauss distributions in color image modeling. These hyperparameters are commonly known as statistical machine learning theory's hyperparameters. Our proposed method for determining these hyperparameters involves a new scheme for hyperparameter estimation, which is based on conditional maximization of entropy in the Potts prior. The algorithm is developed using loopy belief propagation. Furthermore, we compare our conditional maximum entropy framework with the traditional maximum likelihood framework and investigate how the first-order phase transitions in LBP's for Potts models affect our hyperparameter estimation procedures.",1
"Hand gesture recognition possesses extensive applications in virtual reality, sign language recognition, and computer games. The direct interface of hand gestures provides us a new way for communicating with the virtual environment. In this paper a novel and real-time approach for hand gesture recognition system is presented. In the suggested method, first, the hand gesture is extracted from the main image by the image segmentation and morphological operation and then is sent to feature extraction stage. In feature extraction stage the Cross-correlation coefficient is applied on the gesture to recognize it. In the result part, the proposed approach is applied on American Sign Language (ASL) database and the accuracy rate obtained 98.34%.",0
"Hand gesture recognition has wide-ranging applications in virtual reality, sign language recognition, and computer games, offering a new way to interact with virtual environments. This paper proposes a real-time approach to hand gesture recognition that involves extracting the gesture from the main image using image segmentation and morphological operation techniques, before sending it to the feature extraction stage. Here, the Cross-correlation coefficient is applied to recognize the gesture. The approach is tested on an American Sign Language (ASL) database and achieved an accuracy rate of 98.34%.",1
"Image processing is an important research area in computer vision. Image segmentation plays the vital rule in image processing research. There exist so many methods for image segmentation. Clustering is an unsupervised study. Clustering can also be used for image segmentation. In this paper, an in-depth study is done on different clustering techniques that can be used for image segmentation with their pros and cons. An experiment for color image segmentation based on clustering with K-Means algorithm is performed to observe the accuracy of clustering technique for the segmentation purpose.",0
"Computer vision research emphasizes the significance of image processing, with image segmentation being a crucial aspect. Numerous methods for image segmentation are available, one of which is clustering, a form of unsupervised study. This paper conducts a thorough examination of diverse clustering techniques used for image segmentation, analyzing their advantages and disadvantages. An experiment utilizing the K-Means algorithm for color image segmentation is conducted to determine the accuracy of clustering techniques in segmentation.",1
"A good segmentation result depends on a set of ""correct"" choice for the seeds. When the input images are noisy, the seeds may fall on atypical pixels that are not representative of the region statistics. This can lead to erroneous segmentation results. In this paper, an automatic seeded region growing algorithm is proposed for cellular image segmentation. First, the regions of interest (ROIs) extracted from the preprocessed image. Second, the initial seeds are automatically selected based on ROIs extracted from the image. Third, the most reprehensive seeds are selected using a machine learning algorithm. Finally, the cellular image is segmented into regions where each region corresponds to a seed. The aim of the proposed is to automatically extract the Region of Interests (ROI) from in the cellular images in terms of overcoming the explosion, under segmentation and over segmentation problems. Experimental results show that the proposed algorithm can improve the segmented image and the segmented results are less noisy as compared to some existing algorithms.",0
"The accuracy of a segmentation outcome is reliant on selecting the appropriate seeds. When images contain noise, the seeds may be placed on non-typical pixels that do not represent the statistics of the region, leading to incorrect segmentation outcomes. This paper presents an automated seeded region growing algorithm for segmenting cellular images. Firstly, regions of interest (ROIs) are extracted from the preprocessed image. Secondly, initial seeds are automatically chosen based on the ROIs extracted from the image. Thirdly, a machine learning algorithm chooses the most representative seeds. Finally, the cellular image is segmented into regions corresponding to each seed. The objective of this proposed algorithm is to automatically extract ROIs from cellular images to overcome problems of explosion, under-segmentation, and over-segmentation. Experimental results show that the proposed algorithm enhances the segmented image and produces less noisy segmentation results than some existing algorithms.",1
"Accurate delineation of pathological lungs from computed tomography (CT) images remains mostly unsolved because available methods fail to provide a reliable generic solution due to high variability of abnormality appearance. Local descriptor-based classification methods have shown to work well in annotating pathologies; however, these methods are usually computationally intensive which restricts their widespread use in real-time or near-real-time clinical applications. In this paper, we present a novel approach for fast, accurate, reliable segmentation of pathological lungs from CT scans by combining region-based segmentation method with local descriptor classification that is performed on an optimized sampling grid. Our method works in two stages; during stage one, we adapted the fuzzy connectedness (FC) image segmentation algorithm to perform initial lung parenchyma extraction. In the second stage, texture-based local descriptors are utilized to segment abnormal imaging patterns using a near optimal keypoint analysis by employing centroid of supervoxel as grid points. The quantitative results show that our pathological lung segmentation method is fast, robust, and improves on current standards and has potential to enhance the performance of routine clinical tasks.",0
"Computed tomography (CT) images pose a challenge in accurately identifying pathological lungs due to the variability in abnormality appearance, making available methods unreliable for generic solutions. While local descriptor-based classification methods work well in annotating pathologies, they are computationally intensive and not suitable for real-time clinical use. This paper proposes a new approach that combines a region-based segmentation method with a local descriptor classification on an optimized sampling grid to enable fast, reliable segmentation of pathological lungs from CT scans. The method works in two stages, with the first stage using the fuzzy connectedness (FC) algorithm for initial lung parenchyma extraction, followed by texture-based local descriptors in the second stage to segment abnormal patterns. The proposed method proves to be fast, robust, and improves current standards, with the potential to enhance routine clinical tasks.",1
"Accurate and fast extraction of lung volumes from computed tomography (CT) scans remains in a great demand in the clinical environment because the available methods fail to provide a generic solution due to wide anatomical variations of lungs and existence of pathologies. Manual annotation, current gold standard, is time consuming and often subject to human bias. On the other hand, current state-of-the-art fully automated lung segmentation methods fail to make their way into the clinical practice due to their inability to efficiently incorporate human input for handling misclassifications and praxis. This paper presents a lung annotation tool for CT images that is interactive, efficient, and robust. The proposed annotation tool produces an ""as accurate as possible"" initial annotation based on the fuzzy-connectedness image segmentation, followed by efficient manual fixation of the initial extraction if deemed necessary by the practitioner. To provide maximum flexibility to the users, our annotation tool is supported in three major operating systems (Windows, Linux, and the Mac OS X). The quantitative results comparing our free software with commercially available lung segmentation tools show higher degree of consistency and precision of our software with a considerable potential to enhance the performance of routine clinical tasks.",0
"There is a high demand for accurate and speedy extraction of lung volumes from CT scans in the clinical environment. The current methods available fail to provide a universal solution due to the wide range of anatomical variations of lungs and the presence of pathologies. Manual annotation is the current gold standard, but this process is time-consuming and subject to human bias. Although fully automated lung segmentation methods exist, they are not practical for clinical use due to their inability to efficiently incorporate human input. This paper introduces an interactive, efficient, and robust lung annotation tool for CT images. The proposed tool uses fuzzy-connectedness image segmentation to produce an initial annotation that is as accurate as possible. If necessary, the practitioner can then efficiently make manual adjustments to the initial extraction. The tool is available for use on Windows, Linux, and Mac OS X operating systems, providing maximum flexibility to users. Our free software has been compared with commercially available lung segmentation tools, and the results show higher consistency and precision, with considerable potential to improve routine clinical tasks.",1
"Biofilm is a formation of microbial material on tooth substrata. Several methods to quantify dental biofilm coverage have recently been reported in the literature, but at best they provide a semi-automated approach to quantification with significant input from a human grader that comes with the graders bias of what are foreground, background, biofilm, and tooth. Additionally, human assessment indices limit the resolution of the quantification scale; most commercial scales use five levels of quantification for biofilm coverage (0%, 25%, 50%, 75%, and 100%). On the other hand, current state-of-the-art techniques in automatic plaque quantification fail to make their way into practical applications owing to their inability to incorporate human input to handle misclassifications. This paper proposes a new interactive method for biofilm quantification in Quantitative light-induced fluorescence (QLF) images of canine teeth that is independent of the perceptual bias of the grader. The method partitions a QLF image into segments of uniform texture and intensity called superpixels; every superpixel is statistically modeled as a realization of a single 2D Gaussian Markov random field (GMRF) whose parameters are estimated; the superpixel is then assigned to one of three classes (background, biofilm, tooth substratum) based on the training set of data. The quantification results show a high degree of consistency and precision. At the same time, the proposed method gives pathologists full control to post-process the automatic quantification by flipping misclassified superpixels to a different state (background, tooth, biofilm) with a single click, providing greater usability than simply marking the boundaries of biofilm and tooth as done by current state-of-the-art methods.",0
"The accumulation of microbial material on tooth substrata is known as biofilm. Although various methods have been introduced in recent literature to measure dental biofilm coverage, they mostly rely on a semi-automated approach that involves significant input from a human grader. This input can be influenced by the grader's biases towards foreground, background, biofilm, and tooth. Furthermore, human assessment indices limit the resolution of the quantification scale, which usually has five levels of quantification for biofilm coverage (0%, 25%, 50%, 75%, and 100%). Although current state-of-the-art techniques in automatic plaque quantification exist, they are not practical due to their inability to incorporate human input to handle misclassifications. This paper presents a new interactive method for biofilm quantification in Quantitative light-induced fluorescence (QLF) images of canine teeth that eliminates the perceptual bias of the grader. The method partitions a QLF image into segments of uniform texture and intensity called superpixels, and each superpixel is statistically modeled as a realization of a single 2D Gaussian Markov random field (GMRF) whose parameters are estimated. The superpixel is then assigned to one of three classes (background, biofilm, tooth substratum) based on the training set of data. The quantification results show a high degree of consistency and precision. Additionally, the proposed method gives pathologists complete control to post-process the automatic quantification by flipping misclassified superpixels to a different state (background, tooth, biofilm) with a single click, making it more user-friendly than current state-of-the-art methods that only mark the boundaries of biofilm and tooth.",1
"This paper addresses the problem of holistic road scene understanding based on the integration of visual and range data. To achieve the grand goal, we propose an approach that jointly tackles object-level image segmentation and semantic region labeling within a conditional random field (CRF) framework. Specifically, we first generate semantic object hypotheses by clustering 3D points, learning their prior appearance models, and using a deep learning method for reasoning their semantic categories. The learned priors, together with spatial and geometric contexts, are incorporated in CRF. With this formulation, visual and range data are fused thoroughly, and moreover, the coupled segmentation and semantic labeling problem can be inferred via Graph Cuts. Our approach is validated on the challenging KITTI dataset that contains diverse complicated road scenarios. Both quantitative and qualitative evaluations demonstrate its effectiveness.",0
"In this article, we tackle the issue of comprehensive comprehension of road scenes by combining visual and range data. Our proposed solution involves a joint approach to object-level image segmentation and semantic region labeling using a conditional random field (CRF) framework. Initially, we generate semantic object hypotheses by clustering 3D points, learning their prior appearance models, and using a deep learning method for reasoning their semantic categories. The CRF incorporates learned priors, spatial and geometric contexts, which fuse the visual and range data. Moreover, the coupled segmentation and semantic labeling problem is inferred via Graph Cuts. We validate our approach on the KITTI dataset, which contains complex road scenarios. Our approach's effectiveness is demonstrated through both quantitative and qualitative evaluations.",1
"We classify very-mild to moderate dementia in patients (CDR ranging from 0 to 2) using a support vector machine classifier acting on dimensionally reduced feature set derived from MRI brain scans of the 416 subjects available in the OASIS-Brains dataset. We use image segmentation and principal component analysis to reduce the dimensionality of the data. Our resulting feature set contains 11 features for each subject. Performance of the classifiers is evaluated using 10-fold cross-validation. Using linear and (gaussian) kernels, we obtain a training classification accuracy of 86.4% (90.1%), test accuracy of 85.0% (85.7%), test precision of 68.7% (68.5%), test recall of 68.0% (74.0%), and test Matthews correlation coefficient of 0.594 (0.616).",0
"We utilize a support vector machine classifier to categorize patients with very-mild to moderate dementia, determined by a CDR score ranging from 0 to 2. The classifier is applied to a set of MRI brain scans from 416 subjects in the OASIS-Brains dataset, which have been dimensionally reduced through image segmentation and principal component analysis. Our resulting feature set consists of 11 features per subject, and we evaluate classifier performance through 10-fold cross-validation. By using linear and (gaussian) kernels, we achieve a training classification accuracy of 86.4% (90.1%) and a test accuracy of 85.0% (85.7%). Our test precision and recall are 68.7% (68.5%) and 68.0% (74.0%), respectively, with a test Matthews correlation coefficient of 0.594 (0.616).",1
"Segmentation is one of the most important tasks in image processing. It consist in classify the pixels into two or more groups depending on their intensity levels and a threshold value. The quality of the segmentation depends on the method applied to select the threshold. The use of the classical implementations for multilevel thresholding is computationally expensive since they exhaustively search the best values to optimize the objective function. Under such conditions, the use of optimization evolutionary approaches has been extended. The Electromagnetism Like algorithm (EMO) is an evolutionary method which mimics the attraction repulsion mechanism among charges to evolve the members of a population. Different to other algorithms, EMO exhibits interesting search capabilities whereas maintains a low computational overhead. In this paper, a multilevel thresholding (MT) algorithm based on the EMO is introduced. The approach combines the good search capabilities of EMO algorithm with objective functions proposed by the popular MT methods of Otsu and Kapur. The algorithm takes random samples from a feasible search space inside the image histogram. Such samples build each particle in the EMO context whereas its quality is evaluated considering the objective that is function employed by the Otsu or Kapur method. Guided by these objective values the set of candidate solutions are evolved through the EMO operators until an optimal solution is found. The approach generates a multilevel segmentation algorithm which can effectively identify the threshold values of a digital image in a reduced number of iterations. Experimental results show performance evidence of the implementation of EMO for digital image segmentation.",0
"Image processing involves several important tasks, one of which is segmentation. The process involves sorting pixels into groups based on their intensity levels and a specific threshold value. The segmentation quality depends on the chosen threshold selection method. Traditional multilevel thresholding implementations can be computationally expensive since they require exhaustive searches for optimal values to optimize the objective function. Evolutionary optimization approaches, such as the Electromagnetism Like algorithm (EMO), have been increasingly used to address this issue. EMO mimics the attraction-repulsion mechanism among charges to evolve a population's members, exhibiting excellent search capabilities while maintaining low computational overhead. In this paper, a new multilevel thresholding algorithm based on EMO is presented. The approach combines EMO's search capabilities with the objective functions proposed by popular multilevel thresholding methods, such as Otsu and Kapur. The algorithm randomly samples the search space within the image histogram, and each sample builds a particle within the EMO context. The particle's quality is evaluated using the objective function employed by the Otsu or Kapur method. Guided by these objective values, the set of candidate solutions evolves through EMO operators until an optimal solution is found. This approach generates a multilevel segmentation algorithm that can efficiently identify threshold values in a digital image with minimal iterations. Experimental results demonstrate the performance benefits of EMO for digital image segmentation.",1
"Recent progress in computational photography has shown that we can acquire near-infrared (NIR) information in addition to the normal visible (RGB) band, with only slight modifications to standard digital cameras. Due to the proximity of the NIR band to visible radiation, NIR images share many properties with visible images. However, as a result of the material dependent reflection in the NIR part of the spectrum, such images reveal different characteristics of the scene. We investigate how to effectively exploit these differences to improve performance on the semantic image segmentation task. Based on a state-of-the-art segmentation framework and a novel manually segmented image database (both indoor and outdoor scenes) that contain 4-channel images (RGB+NIR), we study how to best incorporate the specific characteristics of the NIR response. We show that adding NIR leads to improved performance for classes that correspond to a specific type of material in both outdoor and indoor scenes. We also discuss the results with respect to the physical properties of the NIR response.",0
"Recent advancements in computational photography have demonstrated that it is possible to obtain near-infrared (NIR) data alongside the standard visible (RGB) spectrum, using minimal adjustments to digital cameras. Despite the similarities between NIR and visible images, NIR images possess distinct features due to the reflection of materials in the NIR range. Our research aims to explore ways to capitalize on these differences and enhance the accuracy of semantic image segmentation. Through the use of a contemporary segmentation framework and a newly developed database of manually segmented images (comprising both indoor and outdoor scenes), each containing 4-channel images (RGB+NIR), we analyze the most effective methods to utilize the unique properties of NIR responses. Our findings reveal that incorporating NIR data results in superior performance for specific material-based categories in both indoor and outdoor environments, and we discuss the implications of these results in relation to the physical characteristics of NIR responses.",1
"Structured-output learning is a challenging problem; particularly so because of the difficulty in obtaining large datasets of fully labelled instances for training. In this paper we try to overcome this difficulty by presenting a multi-utility learning framework for structured prediction that can learn from training instances with different forms of supervision. We propose a unified technique for inferring the loss functions most suitable for quantifying the consistency of solutions with the given weak annotation. We demonstrate the effectiveness of our framework on the challenging semantic image segmentation problem for which a wide variety of annotations can be used. For instance, the popular training datasets for semantic segmentation are composed of images with hard-to-generate full pixel labellings, as well as images with easy-to-obtain weak annotations, such as bounding boxes around objects, or image-level labels that specify which object categories are present in an image. Experimental evaluation shows that the use of annotation-specific loss functions dramatically improves segmentation accuracy compared to the baseline system where only one type of weak annotation is used.",0
"The problem of structured-output learning is difficult due to the challenge of acquiring a sufficient amount of fully labelled datasets for training. To address this issue, we present a multi-utility learning framework that can learn from various forms of supervision. Our approach involves a unified technique for deducing the most appropriate loss functions for measuring the consistency of solutions with the given weak annotation. We showcase the effectiveness of our framework in the demanding semantic image segmentation problem, which can utilize a wide range of annotations. For instance, popular training datasets for this task consist of images with hard-to-generate full pixel labellings, as well as images with easier-to-obtain weak annotations like bounding boxes or image-level labels. Our experimental results indicate that using annotation-specific loss functions vastly enhances segmentation accuracy compared to a baseline system that only employs one type of weak annotation.",1
"Many image segmentation techniques have been developed over the past two decades for segmenting the images, which help for object recognition, occlusion boundary estimation within motion or stereo systems, image compression, image editing.   In this, there is a combined approach for segmenting the image. By using histogram equalization to the input image, from which it gives contrast enhancement output image .After that by applying median filtering,which will remove noise from contrast output image . At last I applied fuzzy c-mean clustering algorithm to denoising output image, which give segmented output image. In this way it produce better segmented image with less computation time.",0
"Numerous methods for segmenting images have been developed in the past twenty years to aid in object recognition, occlusion boundary estimation within motion or stereo systems, image compression, and image editing. One approach combines multiple techniques to achieve image segmentation. Initially, histogram equalization is applied to the input image to enhance contrast and produce an output image. Subsequently, median filtering is utilized to remove noise from the contrast output image. Finally, the fuzzy c-mean clustering algorithm is applied to the denoised output image, resulting in a segmented output image. This approach yields a superior segmented image while minimizing computational time.",1
"In the field of image analysis, segmentation is one of the most important preprocessing steps. One way to achieve segmentation is by mean of threshold selection, where each pixel that belongs to a determined class islabeled according to the selected threshold, giving as a result pixel groups that share visual characteristics in the image. Several methods have been proposed in order to solve threshold selectionproblems; in this work, it is used the method based on the mixture of Gaussian functions to approximate the 1D histogram of a gray level image and whose parameters are calculated using three nature inspired algorithms (Particle Swarm Optimization, Artificial Bee Colony Optimization and Differential Evolution). Each Gaussian function approximates thehistogram, representing a pixel class and therefore a threshold point. Experimental results are shown, comparing in quantitative and qualitative fashion as well as the main advantages and drawbacks of each algorithm, applied to multi-threshold problem.",0
"Segmentation is a crucial preprocessing step in image analysis. Threshold selection is a common approach for segmentation, where pixels belonging to a certain class are labeled based on the selected threshold. This creates groups of pixels that share similar visual characteristics in the image. Various methods have been proposed to address threshold selection issues. In this study, the mixture of Gaussian functions method is employed to approximate the 1D histogram of a gray level image. The parameters are determined using three nature-inspired algorithms- Particle Swarm Optimization, Artificial Bee Colony Optimization, and Differential Evolution. Each Gaussian function represents a pixel class and a threshold point. The study presents experimental results that quantitatively and qualitatively compare the advantages and disadvantages of each algorithm in multi-threshold problems.",1
"This paper explores the use of the Learning Automata (LA) algorithm to compute threshold selection for image segmentation as it is a critical preprocessing step for image analysis, pattern recognition and computer vision. LA is a heuristic method which is able to solve complex optimization problems with interesting results in parameter estimation. Despite other techniques commonly seek through the parameter map, LA explores in the probability space providing appropriate convergence properties and robustness. The segmentation task is therefore considered as an optimization problem and the LA is used to generate the image multi-threshold separation. In this approach, one 1D histogram of a given image is approximated through a Gaussian mixture model whose parameters are calculated using the LA algorithm. Each Gaussian function approximating the histogram represents a pixel class and therefore a threshold point. The method shows fast convergence avoiding the typical sensitivity to initial conditions such as the Expectation Maximization (EM) algorithm or the complex time-consuming computations commonly found in gradient methods. Experimental results demonstrate the algorithm ability to perform automatic multi-threshold selection and show interesting advantages as it is compared to other algorithms solving the same task.",0
"The purpose of this study is to investigate the effectiveness of the Learning Automata (LA) algorithm in determining threshold selection for image segmentation. This step is crucial for image analysis, pattern recognition, and computer vision. Unlike other methods that examine the parameter map, LA explores the probability space, resulting in improved convergence properties and robustness. By treating the segmentation task as an optimization problem, LA is utilized to produce the image's multi-threshold separation. The approach involves approximating a given image's 1D histogram using a Gaussian mixture model, where each Gaussian function represents a pixel class and a threshold point. The algorithm demonstrates rapid convergence and avoids the sensitivity to initial conditions found in methods such as the Expectation Maximization (EM) algorithm or gradient methods. Experimental results reveal the LA algorithm's ability to perform automatic multi-threshold selection and its superiority to other algorithms that solve the same task.",1
"This paper explores the use of the Artificial Bee Colony (ABC) algorithm to compute threshold selection for image segmentation. ABC is a heuristic algorithm motivated by the intelligent behavior of honey-bees which has been successfully employed to solve complex optimization problems. In this approach, an image 1D histogram is approximated through a Gaussian mixture model whose parameters are calculated by the ABC algorithm. For the approximation scheme, each Gaussian function represents a pixel class and therefore a threshold. Unlike the Expectation Maximization (EM) algorithm, the ABC based method shows fast convergence and low sensitivity to initial conditions. Remarkably, it also improves complex time consuming computations commonly required by gradient-based methods. Experimental results demonstrate the algorithms ability to perform automatic multi threshold selection yet showing interesting advantages by comparison to other well known algorithms.",0
"The objective of this paper is to examine how the Artificial Bee Colony (ABC) algorithm can be utilized for image segmentation threshold selection. ABC is a heuristic algorithm inspired by honey-bee behavior, which has been effectively utilized to solve intricate optimization issues. The proposed approach involves approximating an image's 1D histogram using a Gaussian mixture model whose parameters are calculated using the ABC algorithm. Each Gaussian function represents a pixel class and thus a threshold in the approximation scheme. Compared to the Expectation Maximization (EM) algorithm, the ABC-based method has a speedy convergence rate and is less sensitive to initial conditions. Furthermore, it enhances complex time-consuming computations that are typically required by gradient-based techniques. The experimental results indicate that the proposed algorithm can perform automatic multi threshold selection and offers significant advantages over other popular algorithms.",1
"Synchronization of coupled oscillators is observed at multiple levels of neural systems, and has been shown to play an important function in visual perception. We propose a computing system based on locally coupled oscillator networks for image segmentation. The system can serve as the preprocessing front-end of an image processing pipeline where the common frequencies of clusters of oscillators reflect the segmentation results. To demonstrate the feasibility of our design, the system is simulated and tested on a human face image dataset and its performance is compared with traditional intensity threshold based algorithms. Our system shows both better performance and higher noise tolerance than traditional methods.",0
"Coupled oscillators are observed to synchronize at various levels of neural systems and have a crucial role in visual perception. Our proposal is to use locally coupled oscillator networks as the computing system for image segmentation. The system can act as the front-end of an image processing pipeline where the segmentation results are reflected by the common frequencies of oscillator clusters. To prove the practicality of our design, we simulated and tested the system on a human face image dataset and compared its performance with traditional algorithms based on intensity threshold. Our system exhibits superior performance and greater noise tolerance than conventional methods.",1
"Image segmentation and image restoration are two important topics in image processing with great achievements. In this paper, we propose a new multiphase segmentation model by combining image restoration and image segmentation models. Utilizing image restoration aspects, the proposed segmentation model can effectively and robustly tackle high noisy images, blurry images, images with missing pixels, and vector-valued images. In particular, one of the most important segmentation models, the piecewise constant Mumford-Shah model, can be extended easily in this way to segment gray and vector-valued images corrupted for example by noise, blur or missing pixels after coupling a new data fidelity term which comes from image restoration topics. It can be solved efficiently using the alternating minimization algorithm, and we prove the convergence of this algorithm with three variables under mild condition. Experiments on many synthetic and real-world images demonstrate that our method gives better segmentation results in comparison to others state-of-the-art segmentation models especially for blurry images and images with missing pixels values.",0
"The fields of image segmentation and image restoration have both made significant strides in image processing. This study proposes a novel approach to multiphase segmentation by combining both image restoration and segmentation models. This technique leverages the restorative properties of image processing to effectively and reliably address common image issues, such as high levels of noise, blurring, missing pixels, and vector-valued images. By incorporating a new data fidelity term into the piecewise constant Mumford-Shah model, the proposed method extends this crucial segmentation model to address these common image problems. The alternating minimization algorithm efficiently solves this extended model, and we demonstrate its convergence under mild conditions with three variables. Our experiments on synthetic and real-world images show that this approach outperforms other state-of-the-art segmentation models, particularly in the case of blurry images and those with missing pixel values.",1
"Separating an image into cartoon and texture components comes useful in image processing applications, such as image compression, image segmentation, image inpainting. Yves Meyer's influential cartoon texture decomposition model involves deriving an energy functional by choosing appropriate spaces and functionals. Minimizers of the derived energy functional are cartoon and texture components of an image. In this study, cartoon part of an image is separated, by reconstructing it from pixels of multi scale Total-Variation filtered versions of the original image which is sought to be decomposed into cartoon and texture parts. An information theoretic pixel by pixel selection criteria is employed to choose the contributing pixels and their scales.",0
"The separation of an image into its cartoon and texture components is a useful technique in various image processing applications, including image compression, segmentation, and inpainting. Yves Meyer's cartoon texture decomposition model involves selecting appropriate spaces and functionals to derive an energy functional, with the minimizers of this functional representing the cartoon and texture components of the image. This study focuses on separating the cartoon component of an image by reconstructing it from multi-scale Total-Variation filtered versions of the original image. An information theoretic pixel selection criteria is applied to choose the relevant pixels and their scales.",1
"This paper represents an text extraction method from Google maps, GIS maps/images. Due to an unsupervised approach there is no requirement of any prior knowledge or training set about the textual and non-textual parts. Fuzzy CMeans clustering technique is used for image segmentation and Prewitt method is used to detect the edges. Connected component analysis and gridding technique enhance the correctness of the results. The proposed method reaches 98.5% accuracy level on the basis of experimental data sets.",0
"The aim of this paper is to describe a process for extracting text from Google maps and GIS maps/images. The method utilizes an unsupervised approach, eliminating the need for prior knowledge or training sets regarding the textual and non-textual elements. To achieve image segmentation, Fuzzy CMeans clustering technique is employed, while the Prewitt method is utilized for edge detection. The accuracy of the results is improved through connected component analysis and gridding technique. Experimental data sets indicate that the proposed method achieves an accuracy level of 98.5%.",1
"Texture segmentation is the process of partitioning an image into regions with different textures containing a similar group of pixels. Detecting the discontinuity of the filter's output and their statistical properties help in segmenting and classifying a given image with different texture regions. In this proposed paper, chili x-ray image texture segmentation is performed by using Gabor filter. The texture segmented result obtained from Gabor filter fed into three texture filters, namely Entropy, Standard Deviation and Range filter. After performing texture analysis, features can be extracted by using Statistical methods. In this paper Gray Level Co-occurrence Matrices and First order statistics are used as feature extraction methods. Features extracted from statistical methods are given to Support Vector Machine (SVM) classifier. Using this methodology, it is found that texture segmentation is followed by the Gray Level Co-occurrence Matrix feature extraction method gives a higher accuracy rate of 84% when compared with First order feature extraction method.   Key Words: Texture segmentation, Texture filter, Gabor filter, Feature extraction methods, SVM classifier.",0
"The process of texture segmentation involves dividing an image into different regions with varying textures, which share a common group of pixels. To segment and classify such images, it is helpful to identify discontinuities in filter output and analyze their statistical properties. This proposed paper explores the use of Gabor filter for texture segmentation in chili x-ray images. The texture-segmented output is then fed into three texture filters: Entropy, Standard Deviation, and Range filter. Statistical methods are used to extract features, with Gray Level Co-occurrence Matrices and First order statistics employed in this study. These extracted features are then inputted into a Support Vector Machine (SVM) classifier. Results show that using the Gray Level Co-occurrence Matrix feature extraction method after texture segmentation leads to a higher accuracy rate of 84% compared to the First order feature extraction method. Key words for this study include Texture segmentation, Texture filter, Gabor filter, Feature extraction methods, and SVM classifier.",1
"We present MBIS (Multivariate Bayesian Image Segmentation tool), a clustering tool based on the mixture of multivariate normal distributions model. MBIS supports multi-channel bias field correction based on a B-spline model. A second methodological novelty is the inclusion of graph-cuts optimization for the stationary anisotropic hidden Markov random field model. Along with MBIS, we release an evaluation framework that contains three different experiments on multi-site data. We first validate the accuracy of segmentation and the estimated bias field for each channel. MBIS outperforms a widely used segmentation tool in a cross-comparison evaluation. The second experiment demonstrates the robustness of results on atlas-free segmentation of two image sets from scan-rescan protocols on 21 healthy subjects. Multivariate segmentation is more replicable than the monospectral counterpart on T1-weighted images. Finally, we provide a third experiment to illustrate how MBIS can be used in a large-scale study of tissue volume change with increasing age in 584 healthy subjects. This last result is meaningful as multivariate segmentation performs robustly without the need for prior knowledge",0
"Introducing MBIS, a clustering tool that employs the mixture of multivariate normal distributions model and supports multi-channel bias field correction based on a B-spline model. Our tool also includes graph-cuts optimization for the stationary anisotropic hidden Markov random field model. Additionally, we provide an evaluation framework that includes three experiments on multi-site data. The first experiment validates the accuracy of segmentation and the estimated bias field for each channel. MBIS outperforms a widely used segmentation tool in a cross-comparison evaluation. The second experiment demonstrates the robustness of results on atlas-free segmentation of two image sets from scan-rescan protocols on 21 healthy subjects. Multivariate segmentation is more replicable than the monospectral counterpart on T1-weighted images. Lastly, we present a third experiment that showcases the scalability of MBIS in a study of tissue volume change with increasing age in 584 healthy subjects. This is significant as multivariate segmentation can perform reliably without prior knowledge.",1
"We present LS-CRF, a new method for very efficient large-scale training of Conditional Random Fields (CRFs). It is inspired by existing closed-form expressions for the maximum likelihood parameters of a generative graphical model with tree topology. LS-CRF training requires only solving a set of independent regression problems, for which closed-form expression as well as efficient iterative solvers are available. This makes it orders of magnitude faster than conventional maximum likelihood learning for CRFs that require repeated runs of probabilistic inference. At the same time, the models learned by our method still allow for joint inference at test time. We apply LS-CRF to the task of semantic image segmentation, showing that it is highly efficient, even for loopy models where probabilistic inference is problematic. It allows the training of image segmentation models from significantly larger training sets than had been used previously. We demonstrate this on two new datasets that form a second contribution of this paper. They consist of over 180,000 images with figure-ground segmentation annotations. Our large-scale experiments show that the possibilities of CRF-based image segmentation are far from exhausted, indicating, for example, that semi-supervised learning and the use of non-linear predictors are promising directions for achieving higher segmentation accuracy in the future.",0
"A new method, LS-CRF, has been developed for training Conditional Random Fields (CRFs) on a large scale. This method is based on existing closed-form expressions for the maximum likelihood parameters of a generative graphical model with tree topology. The LS-CRF method requires solving a set of independent regression problems, for which closed-form expressions and efficient iterative solvers are available, making it much faster than conventional maximum likelihood learning for CRFs. Despite its efficiency, the models learned by LS-CRF still allow for joint inference at test time. LS-CRF has been applied to the task of semantic image segmentation, demonstrating its effectiveness even for loopy models where probabilistic inference is problematic. The method allows for the training of image segmentation models from significantly larger training sets than previously used, as demonstrated by two new datasets consisting of over 180,000 images with figure-ground segmentation annotations. The results of the large-scale experiments suggest that CRF-based image segmentation has promising potential for future research, including semi-supervised learning and the use of non-linear predictors to achieve higher segmentation accuracy.",1
"A large number of problems in computer vision can be modelled as energy minimization problems in a Markov Random Field (MRF) or Conditional Random Field (CRF) framework. Graph-cuts based $\alpha$-expansion is a standard move-making method to minimize the energy functions with sub-modular pairwise terms. However, certain problems require more complex pairwise terms where the $\alpha$-expansion method is generally not applicable.   In this paper, we propose an iterative {\em tiered move making algorithm} which is able to handle general pairwise terms. Each move to the next configuration is based on the current labeling and an optimal tiered move, where each tiered move requires one application of the dynamic programming based tiered labeling method introduced in Felzenszwalb et. al. \cite{tiered_cvpr_felzenszwalbV10}. The algorithm converges to a local minimum for any general pairwise potential, and we give a theoretical analysis of the properties of the algorithm, characterizing the situations in which we can expect good performance. We first evaluate our method on an object-class segmentation problem using the Pascal VOC-11 segmentation dataset where we learn general pairwise terms. Further we evaluate the algorithm on many other benchmark labeling problems such as stereo, image segmentation, image stitching and image denoising. Our method consistently gets better accuracy and energy values than alpha-expansion, loopy belief propagation (LBP), quadratic pseudo-boolean optimization (QPBO), and is competitive with TRWS.",0
"In computer vision, many problems can be represented as energy minimization problems within a Markov Random Field (MRF) or Conditional Random Field (CRF) framework. The standard method for minimizing energy functions with sub-modular pairwise terms is the graph-cuts based alpha-expansion move-making method. However, more complex pairwise terms require an alternative method as the alpha-expansion method is generally not applicable. This paper introduces an iterative tiered move-making algorithm that is capable of handling general pairwise terms. The algorithm uses an optimal tiered move based on the current labeling and requires one application of the dynamic programming based tiered labeling method introduced in Felzenszwalb et. al. \cite{tiered_cvpr_felzenszwalbV10}. It converges to a local minimum for any general pairwise potential, and we provide a theoretical analysis of the algorithm's properties characterizing the situations where we can expect good performance. The method is evaluated using various benchmark labeling problems such as stereo, image segmentation, image stitching, and image denoising, and consistently achieves better accuracy and energy values than alpha-expansion, loopy belief propagation (LBP), quadratic pseudo-boolean optimization (QPBO), and is competitive with TRWS. We first demonstrate our method on an object-class segmentation problem using the Pascal VOC-11 segmentation dataset where we learn general pairwise terms.",1
"Image segmentation some of the challenging issues on brain magnetic resonance image tumor segmentation caused by the weak correlation between magnetic resonance imaging intensity and anatomical meaning.With the objective of utilizing more meaningful information to improve brain tumor segmentation,an approach which employs bilateral symmetry information as an additional feature for segmentation is proposed.This is motivated by potential performance improvement in the general automatic brain tumor segmentation systems which are important for many medical and scientific applications.Brain Magnetic Resonance Imaging segmentation is a complex problem in the field of medical imaging despite various presented methods.MR image of human brain can be divided into several sub-regions especially soft tissues such as gray matter,white matter and cerebra spinal fluid.Although edge information is the main clue in image segmentation,it cannot get a better result in analysis the content of images without combining other information.Our goal is to detect the position and boundary of tumors automatically.Experiments were conducted on real pictures,and the results show that the algorithm is flexible and convenient.",0
"The weak correlation between magnetic resonance imaging intensity and anatomical meaning poses a challenge in brain tumor segmentation, making it a complex issue. To address this challenge, a new approach that utilizes bilateral symmetry information as an additional feature for segmentation is proposed. This approach aims to improve the performance of automatic brain tumor segmentation systems and is crucial for medical and scientific applications. Despite the various methods presented, brain magnetic resonance imaging segmentation remains a complex problem due to the presence of soft tissues such as gray matter, white matter, and cerebra spinal fluid. While edge information is a crucial clue in image segmentation, it is insufficient to analyze image content without combining other information. Our goal is to automatically detect the position and boundary of tumors. We conducted experiments on real pictures, and the results demonstrate that the algorithm is both flexible and convenient.",1
"Motivated by multi-distribution divergences, which originate in information theory, we propose a notion of `multi-point' kernels, and study their applications. We study a class of kernels based on Jensen type divergences and show that these can be extended to measure similarity among multiple points. We study tensor flattening methods and develop a multi-point (kernel) spectral clustering (MSC) method. We further emphasize on a special case of the proposed kernels, which is a multi-point extension of the linear (dot-product) kernel and show the existence of cubic time tensor flattening algorithm in this case. Finally, we illustrate the usefulness of our contributions using standard data sets and image segmentation tasks.",0
"Our focus on multi-distribution divergences, originating from information theory, has led us to introduce the concept of `multi-point' kernels. Through our research, we have explored a range of kernels based on Jensen type divergences, which can effectively measure similarity among multiple points. Additionally, we have investigated tensor flattening methods and developed a multi-point spectral clustering (MSC) technique. We have also placed special emphasis on a specific type of kernel, which extends the linear (dot-product) kernel to multiple points, and have demonstrated the existence of a cubic time tensor flattening algorithm in this scenario. Finally, we have applied our findings to standard data sets and image segmentation tasks, showcasing their practical value.",1
"This paper proposes the Parallel WiSARD Object Tracker (PWOT), a new object tracker based on the WiSARD weightless neural network that is robust against quantization errors. Object tracking in video is an important and challenging task in many applications. Difficulties can arise due to weather conditions, target trajectory and appearance, occlusions, lighting conditions and noise. Tracking is a high-level application and requires the object location frame by frame in real time. This paper proposes a fast hybrid image segmentation (threshold and edge detection) in YcbCr color model and a parallel RAM based discriminator that improves efficiency when quantization errors occur. The original WiSARD training algorithm was changed to allow the tracking.",0
"The Parallel WiSARD Object Tracker (PWOT) is a novel object tracking system that utilizes the WiSARD weightless neural network. Its design enables it to be resistant to quantization errors. Object tracking is a complex task faced with many challenges such as changing weather conditions, target appearance, trajectory, occlusions, lighting conditions and noise. The tracking process requires the real-time monitoring of object location in each frame. This paper proposes a rapid hybrid image segmentation approach using threshold and edge detection in the YcbCr color model. Additionally, it uses a parallel RAM-based discriminator to enhance efficacy when quantization errors occur. The training approach of the original WiSARD algorithm was modified to allow for object tracking.",1
"Bio-image analysis is challenging due to inhomogeneous intensity distributions and high levels of noise in the images. Bayesian inference provides a principled way for regularizing the problem using prior knowledge. A fundamental choice is how one measures ""distances"" between shapes in an image. It has been shown that the straightforward geometric L2 distance is degenerate and leads to pathological situations. This is avoided when using Sobolev gradients, rendering the segmentation problem less ill-posed. The high computational cost and implementation overhead of Sobolev gradients, however, have hampered practical applications. We show how particle methods as applied to image segmentation allow for a simple and computationally efficient implementation of Sobolev gradients. We show that the evaluation of Sobolev gradients amounts to particle-particle interactions along the contour in an image. We extend an existing particle-based segmentation algorithm to using Sobolev gradients. Using synthetic and real-world images, we benchmark the results for both 2D and 3D images using piecewise smooth and piecewise constant region models. The present particle approximation of Sobolev gradients is 2.8 to 10 times faster than the previous reference implementation, but retains the known favorable properties of Sobolev gradients. This speedup is achieved by using local particle-particle interactions instead of solving a global Poisson equation at each iteration. The computational time per iteration is higher for Sobolev gradients than for L2 gradients. Since Sobolev gradients precondition the optimization problem, however, a smaller number of overall iterations may be necessary for the algorithm to converge, which can in some cases amortize the higher per-iteration cost.",0
"The analysis of bio-images is difficult due to non-uniform intensity distributions and significant amounts of noise in the images. Bayesian inference can be used to regularize the problem with prior knowledge. However, the ""distance"" between shapes in an image must be carefully selected to avoid degenerate and problematic situations. Sobolev gradients have been found to be more effective than geometric L2 distance for image segmentation, but their high computational cost has limited their practical use. We propose a particle-based approach that simplifies and speeds up the implementation of Sobolev gradients. Our method involves particle-particle interactions along the contour of an image, and we have extended an existing algorithm to incorporate Sobolev gradients. We have evaluated the performance of our method using 2D and 3D images with piecewise smooth and piecewise constant region models. Our approach is faster than previous implementations of Sobolev gradients, but still retains their desirable properties. The computational time per iteration is higher than L2 gradients, but the preconditioning effect of Sobolev gradients may reduce the overall number of iterations needed for convergence.",1
"Image segmentation is the most challenging issue in computer vision applications. And most difficulties for crops management in agriculture are the lack of appropriate methods for detecting the leaf damage for pests treatment. In this paper we proposed an automatic method for leaf damage detection and severity estimation of coffee leaf by avoiding defoliation. After enhancing the contrast of the original image using LUT based gamma correction, the image is processed to remove the background, and the output leaf is clustered using Fuzzy c-means segmentation in V channel of YUV color space to maximize all leaf damage detection, and finally, the severity of leaf is estimated in terms of ratio for leaf pixel distribution between the normal and the detected leaf damage. The results in each proposed method was compared to the current researches and the accuracy is obvious either in the background removal or damage detection.",0
"Computer vision applications face their biggest challenge in image segmentation, while agriculture struggles with the lack of suitable methods for detecting leaf damage caused by pests. In this study, we present an automatic technique for accurately detecting and estimating the severity of damage to coffee leaves without causing defoliation. Our method involves enhancing the image contrast by using LUT based gamma correction, removing the background, and clustering the output leaf using Fuzzy c-means segmentation in the V channel of YUV color space. We then estimate the severity of the leaf damage by calculating the ratio of pixel distribution between normal and damaged areas. Our proposed method outperforms current research in both background removal and damage detection, as demonstrated through rigorous comparisons.",1
"Nowadays digital image compression and decompression techniques are very much important. So our aim is to calculate the quality of face and other regions of the compressed image with respect to the original image. Image segmentation is typically used to locate objects and boundaries (lines, curves etc.)in images. After segmentation the image is changed into something which is more meaningful to analyze. Using Universal Image Quality Index(Q),Structural Similarity Index(SSIM) and Gradient-based Structural Similarity Index(G-SSIM) it can be shown that face region is less compressed than any other region of the image.",0
"In the present times, digital image compression and decompression techniques hold great significance. Thus, our objective is to determine the compressed image's face and other areas' quality in comparison to the original image. Image segmentation is commonly employed to identify objects and boundaries such as lines and curves in images. Following segmentation, the image is transformed into a more significant form for analysis. Through the utilization of Universal Image Quality Index (Q), Structural Similarity Index (SSIM), and Gradient-based Structural Similarity Index (G-SSIM), it can be demonstrated that the face region of the image undergoes less compression than any other area.",1
"Scene labeling is the problem of assigning an object label to each pixel. It unifies the image segmentation and object recognition problems. The importance of using contextual information in scene labeling frameworks has been widely realized in the field. We propose a contextual framework, called contextual hierarchical model (CHM), which learns contextual information in a hierarchical framework for scene labeling. At each level of the hierarchy, a classifier is trained based on downsampled input images and outputs of previous levels. Our model then incorporates the resulting multi-resolution contextual information into a classifier to segment the input image at original resolution. This training strategy allows for optimization of a joint posterior probability at multiple resolutions through the hierarchy. Contextual hierarchical model is purely based on the input image patches and does not make use of any fragments or shape examples. Hence, it is applicable to a variety of problems such as object segmentation and edge detection. We demonstrate that CHM outperforms state-of-the-art on Stanford background and Weizmann horse datasets. It also outperforms state-of-the-art edge detection methods on NYU depth dataset and achieves state-of-the-art on Berkeley segmentation dataset (BSDS 500).",0
"The task of assigning an object label to each pixel is known as scene labeling, which combines the challenges of image segmentation and object recognition. The significance of incorporating contextual information into scene labeling systems has been widely acknowledged. Our proposal is a contextual hierarchical model (CHM), which learns contextual data in a hierarchical structure to enable scene labeling. At each level of the hierarchy, a classifier is trained using downsampled input images and previous level outputs. This enables our model to incorporate the resulting multi-resolution contextual information into a classifier to segment the input image at its original resolution. Our training strategy optimizes a joint posterior probability at different resolutions throughout the hierarchy. CHM is based solely on input image patches and does not require fragments or shape examples, making it applicable to various problems such as object segmentation and edge detection. Our experiments demonstrate that CHM outperforms existing methods on the Stanford background and Weizmann horse datasets. Furthermore, it surpasses state-of-the-art edge detection techniques on the NYU depth dataset and achieves the highest performance on the Berkeley segmentation dataset (BSDS 500).",1
In the paper a piecewise constant image approximations of sequential number of pixel clusters or segments are treated. A majorizing of optimal approximation sequence by hierarchical sequence of image approximations is studied. Transition from pixel clustering to image segmentation by reducing of segment numbers in clusters is provided. Algorithms are proved by elementary formulas.,0
"The paper discusses approximations of images using a piecewise constant approach that involves analyzing sequential clusters or segments of pixels. The study explores a hierarchical sequence of image approximations that offers a better approximation than the optimal sequence. Additionally, the paper examines the transition from pixel clustering to image segmentation by reducing the number of segments in each cluster. The algorithms used in the study are supported by elementary formulas.",1
"Thresholding is an important task in image processing. It is a main tool in pattern recognition, image segmentation, edge detection and scene analysis. In this paper, we present a new thresholding technique based on two-dimensional Tsallis entropy. The two-dimensional Tsallis entropy was obtained from the twodimensional histogram which was determined by using the gray value of the pixels and the local average gray value of the pixels, the work it was applied a generalized entropy formalism that represents a recent development in statistical mechanics. The effectiveness of the proposed method is demonstrated by using examples from the real-world and synthetic images. The performance evaluation of the proposed technique in terms of the quality of the thresholded images are presented. Experimental results demonstrate that the proposed method achieve better result than the Shannon method.",0
"Image processing relies heavily on thresholding, as it plays a crucial role in pattern recognition, edge detection, image segmentation, and scene analysis. In this study, we introduce a novel thresholding approach that utilizes two-dimensional Tsallis entropy. To obtain the two-dimensional Tsallis entropy, we utilized a generalized entropy formalism based on the gray value of the pixels and the local average gray value of the pixels, which is a recent development in statistical mechanics. We demonstrate the effectiveness of our method by presenting examples of real-world and synthetic images and evaluating the quality of the thresholded images. The experimental results reveal that our approach outperforms the Shannon method.",1
"In this paper, we present an algorithm to automatically detect meaningful modes in a histogram. The proposed method is based on the behavior of local minima in a scale-space representation. We show that the detection of such meaningful modes is equivalent in a two classes clustering problem on the length of minima scale-space curves. The algorithm is easy to implement, fast, and does not require any parameters. We present several results on histogram and spectrum segmentation, grayscale image segmentation and color image reduction.",0
"The paper introduces an automated algorithm for identifying significant modes in a histogram by utilizing the characteristics of local minima in a scale-space display. It is demonstrated that detecting these meaningful modes is equivalent to solving a two-class clustering issue regarding the length of minima scale-space curves. The algorithm is straightforward to execute, efficient, and doesn't necessitate any parameters. The study includes various findings on histogram and spectrum segmentation, grayscale image segmentation, and color image reduction.",1
"We propose an algorithm for segmenting natural images based on texture and color information, which leverages the co-sparse analysis model for image segmentation within a convex multilabel optimization framework. As a key ingredient of this method, we introduce a novel textural similarity measure, which builds upon the co-sparse representation of image patches. We propose a Bayesian approach to merge textural similarity with information about color and location. Combined with recently developed convex multilabel optimization methods this leads to an efficient algorithm for both supervised and unsupervised segmentation, which is easily parallelized on graphics hardware. The approach provides competitive results in unsupervised segmentation and outperforms state-of-the-art interactive segmentation methods on the Graz Benchmark.",0
"Our proposed algorithm utilizes texture and color information to segment natural images. It employs the co-sparse analysis model for image segmentation in a convex multilabel optimization framework. A novel textural similarity measure is introduced as a key component, which is based on the co-sparse representation of image patches. We merge the textural similarity with information about color and location through a Bayesian approach. By combining these techniques with recently developed convex multilabel optimization methods, we are able to create an efficient algorithm for both supervised and unsupervised segmentation. Furthermore, the algorithm can be easily parallelized on graphics hardware. Our approach produces competitive results in unsupervised segmentation and surpasses state-of-the-art interactive segmentation techniques on the Graz Benchmark.",1
"Clustering is an important facet of explorative data mining and finds extensive use in several fields. In this paper, we propose an extension of the classical Fuzzy C-Means clustering algorithm. The proposed algorithm, abbreviated as VFC, adopts a multi-dimensional membership vector for each data point instead of the traditional, scalar membership value defined in the original algorithm. The membership vector for each point is obtained by considering each feature of that point separately and obtaining individual membership values for the same. We also propose an algorithm to efficiently allocate the initial cluster centers close to the actual centers, so as to facilitate rapid convergence. Further, we propose a scheme to achieve crisp clustering using the VFC algorithm. The proposed, novel clustering scheme has been tested on two standard data sets in order to analyze its performance. We also examine the efficacy of the proposed scheme by analyzing its performance on image segmentation examples and comparing it with the classical Fuzzy C-means clustering algorithm.",0
"Explorative data mining heavily relies on clustering, which is widely used in various fields. This paper introduces an enhanced version of the conventional Fuzzy C-Means clustering method called VFC. Unlike the traditional scalar membership value, VFC utilizes a multi-dimensional membership vector for each data point by calculating individual membership values for each feature. Additionally, we present an algorithm that efficiently allocates initial cluster centers near the actual centers to expedite convergence. We also propose a method that achieves clear-cut clustering with the VFC algorithm. To evaluate its performance, we test the novel clustering scheme on two standard data sets and compare it with the classical Fuzzy C-means clustering algorithm by analyzing its effectiveness on image segmentation examples.",1
"We develop a generalized active contour formalism for image segmentation based on shape templates. The shape template is subjected to a restricted affine transformation (RAT) in order to segment the object of interest. RAT allows for translation, rotation, and scaling, which give a total of five degrees of freedom. The proposed active contour comprises an inner and outer contour pair, which are closed and concentric. The active contour energy is a contrast function defined based on the intensities of pixels that lie inside the inner contour and those that lie in the annulus between the inner and outer contours. We show that the contrast energy functional is optimal under certain conditions. The optimal RAT parameters are computed by maximizing the contrast function using a gradient descent optimizer. We show that the calculations are made efficient through use of Green's theorem. The proposed formalism is capable of handling a variety of shapes because for a chosen template, optimization is carried with respect to the RAT parameters only. The proposed formalism is validated on multiple images to show robustness to Gaussian and Poisson noise, to initialization, and to partial loss of structure in the object to be segmented.",0
"A shape template-based active contour formalism for image segmentation is introduced, utilizing a restricted affine transformation (RAT) for object segmentation via translation, rotation, and scaling. The active contour consists of an inner and outer contour pair, with a contrast energy function defined based on pixel intensities within the annulus between the contours. The proposed method is shown to be optimal under certain conditions, with the optimal RAT parameters obtained via gradient descent optimization. The use of Green's theorem enables efficient computation, and the approach is capable of handling various shapes through optimization of RAT parameters based on a chosen template. The method is validated for robustness to noise, initialization, and partial loss of object structure across multiple images.",1
"Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. Consequently, there is need for efficient optimization procedures for submodular functions, especially for minimization problems. While general submodular minimization is challenging, we propose a new method that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our method is a formulation of the discrete submodular minimization problem as a continuous best approximation problem that is solved through a sequence of reflections, and its solution can be easily thresholded to obtain an optimal discrete solution. This method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we illustrate the benefits of our method on two image segmentation tasks.",0
"It has been noted that submodularity is a common occurrence in machine learning, signal processing, and computer vision. As a result, there is a need for effective optimization techniques for submodular functions, particularly for minimization problems. Although general submodular minimization is difficult, we propose a new method that takes advantage of the decomposability of submodular functions. Unlike previous approaches, our method is not approximate, impractical, or reliant on cumbersome parameter tuning. Additionally, it is simple to implement and parallelize. Our method formulates the discrete submodular minimization problem as a continuous best approximation problem, which is solved using a series of reflections. The solution can be easily thresholded to obtain an optimal discrete solution. Our method solves both the continuous and discrete versions of the problem and has applications in learning, inference, and reconstruction. We demonstrate the advantages of our method on two image segmentation tasks in our experiments.",1
"Image segmentation is the process of partitioning a image into different regions or groups based on some characteristics like color, texture, motion or shape etc. Active contours is a popular variational method for object segmentation in images, in which the user initializes a contour which evolves in order to optimize an objective function designed such that the desired object boundary is the optimal solution. Recently, imaging modalities that produce Manifold valued images have come up, for example, DT-MRI images, vector fields. The traditional active contour model does not work on such images. In this paper, we generalize the active contour model to work on Manifold valued images. As expected, our algorithm detects regions with similar Manifold values in the image. Our algorithm also produces expected results on usual gray-scale images, since these are nothing but trivial examples of Manifold valued images. As another application of our general active contour model, we perform texture segmentation on gray-scale images by first creating an appropriate Manifold valued image. We demonstrate segmentation results for manifold valued images and texture images.",0
"The act of dividing an image into distinct groups or regions based on attributes such as color, texture, motion, or shape is known as image segmentation. Object segmentation in images is often achieved using active contours, a popular variational method in which the user initializes a contour that evolves to optimize an objective function, resulting in the desired object boundary as the optimal solution. However, traditional active contour models are inadequate for Manifold valued images produced by modern imaging modalities like DT-MRI images and vector fields. This paper presents a generalized active contour model that can be applied to Manifold valued images, detecting regions with similar values. The algorithm also produces expected results for gray-scale images, which are trivial examples of Manifold valued images. Furthermore, the model can be used for texture segmentation on gray-scale images by first creating an appropriate Manifold valued image. The paper includes segmentation results for both Manifold valued and texture images.",1
"We consider the problem of clustering data that reside on discrete, low dimensional lattices. Canonical examples for this setting are found in image segmentation and key point extraction. Our solution is based on a recent approach to information theoretic clustering where clusters result from an iterative procedure that minimizes a divergence measure. We replace costly processing steps in the original algorithm by means of convolutions. These allow for highly efficient implementations and thus significantly reduce runtime. This paper therefore bridges a gap between machine learning and signal processing.",0
"The focus of our investigation is clustering data residing on discrete, low dimensional lattices, which is commonly seen in image segmentation and key point extraction. We propose a solution based on an information theoretic clustering approach that utilizes an iterative process to minimize divergence measures for cluster creation. To improve efficiency, we replace the costly processing steps in the original algorithm with convolutions, enabling faster implementation and reducing runtime. As a result, our work connects machine learning and signal processing.",1
"Curve evolution schemes for image segmentation based on a region based contour model allowing for junctions, vector-valued images and topology changes are introduced. Together with an a posteriori denoising in the segmented homogeneous regions this leads to a fast and efficient method for image segmentation and restoration. An uneven spread of mesh points is avoided by using the tangential degrees of freedom. Several numerical simulations on artificial test problems and on real images illustrate the performance of the method.",0
"This paper presents curve evolution schemes that utilize a region-based contour model to enable junctions, vector-valued images, and topology changes in image segmentation. By incorporating a posteriori denoising in the segmented homogeneous regions, the method becomes both fast and efficient. Tangential degrees of freedom are employed to prevent an uneven distribution of mesh points. The method's efficacy is demonstrated through numerous numerical simulations on both artificial test problems and real images.",1
"Energy minimization algorithms, such as graph cuts, enable the computation of the MAP solution under certain probabilistic models such as Markov random fields. However, for many computer vision problems, the MAP solution under the model is not the ground truth solution. In many problem scenarios, the system has access to certain statistics of the ground truth. For instance, in image segmentation, the area and boundary length of the object may be known. In these cases, we want to estimate the most probable solution that is consistent with such statistics, i.e., satisfies certain equality or inequality constraints.   The above constrained energy minimization problem is NP-hard in general, and is usually solved using Linear Programming formulations, which relax the integrality constraints. This paper proposes a novel method that finds the discrete optimal solution of such problems by maximizing the corresponding Lagrangian dual. This method can be applied to any constrained energy minimization problem whose unconstrained version is polynomial time solvable, and can handle multiple, equality or inequality, and linear or non-linear constraints. We demonstrate the efficacy of our method on the foreground/background image segmentation problem, and show that it produces impressive segmentation results with less error, and runs more than 20 times faster than the state-of-the-art LP relaxation based approaches.",0
"Algorithms for minimizing energy, such as graph cuts, can calculate the maximum a posteriori (MAP) solution for certain probabilistic models like Markov random fields. However, the MAP solution may not always be the actual solution for many computer vision problems. In such cases, the system has access to specific statistics of the ground truth, such as the area and boundary length of an object in image segmentation. The aim is to estimate the most probable solution that adheres to these statistics by satisfying equality or inequality constraints. This constrained energy minimization problem is generally NP-hard and is often handled by Linear Programming formulations that ease the integrality constraints. This paper introduces a new technique that determines the optimal discrete solution of such problems by maximizing the related Lagrangian dual. The method is applicable to any constrained energy minimization problem whose unconstrained version can be solved in polynomial time. It can handle various constraints, including multiple, equality or inequality, linear or non-linear ones. The results demonstrate the effectiveness of this approach on the foreground/background image segmentation problem, showing impressive segmentation outcomes with fewer errors and faster performance than the state-of-the-art LP relaxation-based strategies.",1
"In this paper, we propose novel algorithms for inferring the Maximum a Posteriori (MAP) solution of discrete pairwise random field models under multiple constraints. We show how this constrained discrete optimization problem can be formulated as a multi-dimensional parametric mincut problem via its Lagrangian dual, and prove that our algorithm isolates all constraint instances for which the problem can be solved exactly. These multiple solutions enable us to even deal with `soft constraints' (higher order penalty functions). Moreover, we propose two practical variants of our algorithm to solve problems with hard constraints. We also show how our method can be applied to solve various constrained discrete optimization problems such as submodular minimization and shortest path computation. Experimental evaluation using the foreground-background image segmentation problem with statistic constraints reveals that our method is faster and its results are closer to the ground truth labellings compared with the popular continuous relaxation based methods.",0
"This article presents new techniques for determining the Maximum a Posteriori (MAP) solution of discrete pairwise random field models while considering multiple limitations. Our approach involves formulating the constrained discrete optimization problem as a multi-dimensional parametric mincut problem through the Lagrangian dual. We prove that our method identifies all instances of the constraints that can be accurately solved, allowing us to handle 'soft constraints' (higher order penalty functions). Additionally, we introduce two practical versions of our algorithm to address problems with hard constraints. We also demonstrate how our approach can be employed to solve diverse constrained discrete optimization issues, including submodular minimization and shortest path computation. Our experimental study using the foreground-background image segmentation problem with statistic constraints shows that our technique is quicker and yields outcomes that are closer to the actual labeling compared to popular continuous relaxation-based methods.",1
"Image segmentation is a crucial step in a wide range of method image processing systems. It is useful in visualization of the different objects present in the image. In spite of the several methods available in the literature, image segmentation still a challenging problem in most of image processing applications. The challenge comes from the fuzziness of image objects and the overlapping of the different regions. Detection of edges in an image is a very important step towards understanding image features. There are large numbers of edge detection operators available, each designed to be sensitive to certain types of edges. The Quality of edge detection can be measured from several criteria objectively. Some criteria are proposed in terms of mathematical measurement, some of them are based on application and implementation requirements. Since edges often occur at image locations representing object boundaries, edge detection is extensively used in image segmentation when images are divided into areas corresponding to different objects. This can be used specifically for enhancing the tumor area in mammographic images. Different methods are available for edge detection like Roberts, Sobel, Prewitt, Canny, Log edge operators. In this paper a novel algorithms for edge detection has been proposed for mammographic images. Breast boundary, pectoral region and tumor location can be seen clearly by using this method. For comparison purpose Roberts, Sobel, Prewitt, Canny, Log edge operators are used and their results are displayed. Experimental results demonstrate the effectiveness of the proposed approach.",0
"Image segmentation is a crucial process in various image processing systems as it aids in identifying objects present in an image. Despite the existence of multiple segmentation techniques, it remains a challenging task in most applications due to the unclear boundaries and overlapping regions of the objects. The detection of edges in images plays a crucial role in comprehending image features. Numerous edge detection operators are available, each designed to capture specific types of edges. The quality of edge detection can be measured using different criteria, including mathematical measurements and application or implementation requirements. Edge detection is extensively used in image segmentation to divide images into areas corresponding to different objects, particularly for enhancing the tumor area in mammographic images. Various edge detection techniques, such as Roberts, Sobel, Prewitt, Canny, and Log edge operators, have been compared to a novel algorithm proposed in this paper for mammographic image edge detection. The proposed approach effectively displays the breast boundary, pectoral region, and tumor location, as demonstrated in the experimental results.",1
"In this paper, we study statistical classification accuracy of two different Markov field environments for pixelwise image segmentation, considering the labels of the image as hidden states and solving the estimation of such labels as a solution of the MAP equation. The emission distribution is assumed the same in all models, and the difference lays in the Markovian prior hypothesis made over the labeling random field. The a priori labeling knowledge will be modeled with a) a second order anisotropic Markov Mesh and b) a classical isotropic Potts model. Under such models, we will consider three different segmentation procedures, 2D Path Constrained Viterbi training for the Hidden Markov Mesh, a Graph Cut based segmentation for the first order isotropic Potts model, and ICM (Iterated Conditional Modes) for the second order isotropic Potts model.   We provide a unified view of all three methods, and investigate goodness of fit for classification, studying the influence of parameter estimation, computational gain, and extent of automation in the statistical measures Overall Accuracy, Relative Improvement and Kappa coefficient, allowing robust and accurate statistical analysis on synthetic and real-life experimental data coming from the field of Dental Diagnostic Radiography. All algorithms, using the learned parameters, generate good segmentations with little interaction when the images have a clear multimodal histogram. Suboptimal learning proves to be frail in the case of non-distinctive modes, which limits the complexity of usable models, and hence the achievable error rate as well.   All Matlab code written is provided in a toolbox available for download from our website, following the Reproducible Research Paradigm.",0
"This paper examines the statistical accuracy of two types of Markov field environments for pixelwise image segmentation. The image labels are treated as hidden states, and the MAP equation is solved to estimate these labels. The models assume the same emission distribution, but differ in their Markovian prior hypothesis over the labeling random field. One model uses a second order anisotropic Markov Mesh, while the other uses a classical isotropic Potts model. Three segmentation procedures are considered: 2D Path Constrained Viterbi training for the Hidden Markov Mesh, Graph Cut based segmentation for the first order isotropic Potts model, and ICM for the second order isotropic Potts model. The study investigates the fit of each method, analyzing parameter estimation, computational gain, and automation in statistical measures such as Overall Accuracy, Relative Improvement, and Kappa coefficient. The study uses synthetic and real-life experimental data from the field of Dental Diagnostic Radiography. The algorithms generate good segmentations with little interaction when images have clear multimodal histograms. However, suboptimal learning is limited in cases where modes are not distinctive, which affects the complexity of usable models and the achievable error rate. The paper offers a unified view of all three methods and provides a toolbox with all Matlab code available for download from the authors' website, following the Reproducible Research Paradigm.",1
"Image Segmentation is a technique of partitioning the original image into some distinct classes. Many possible solutions may be available for segmenting an image into a certain number of classes, each one having different quality of segmentation. In our proposed method, multilevel thresholding technique has been used for image segmentation. A new approach of Cuckoo Search (CS) is used for selection of optimal threshold value. In other words, the algorithm is used to achieve the best solution from the initial random threshold values or solutions and to evaluate the quality of a solution correlation function is used. Finally, MSE and PSNR are measured to understand the segmentation quality.",0
"The partitioning of an original image into distinct classes is known as Image Segmentation. There are various options available for segmenting an image into different classes, each offering a varying quality of segmentation. Our proposed method utilizes the multilevel thresholding technique for image segmentation. We have employed a novel approach called Cuckoo Search (CS) to select the optimal threshold value. This algorithm is leveraged to achieve the best solution from the initial random threshold values or solutions, and a correlation function is employed to evaluate the quality of a solution. Lastly, we measure MSE and PSNR to comprehend the segmentation quality.",1
"This report describes the design, implementation, evaluation and original enhancements to the Live-Wire method for 2D and 3D image segmentation. Live-Wire 2D employs a semi-automatic paradigm; the user is asked to select a few boundary points of the object to segment, to steer the process in the right direction, while the result is displayed in real time. In our implementation segmentation is extended to three dimensions by performing this process on a slice-by-slice basis. User's time and involvement is further reduced by allowing him to specify object contours in planes orthogonal to the slices. If these planes are chosen strategically, Live-Wire 3D can perform 2D segmentation in the plane of each slice automatically. This report also proposes two improvements to the original method, path heating and a new graph edge feature function based on variance of path properties along the boundary. We show that these improvements lead up to a 33% reduction in interaction with the user, and improved delineation in presence of strong interfering edges.",0
"In this report, the Live-Wire method for 2D and 3D image segmentation is discussed, including its design, implementation, evaluation, and original enhancements. Live-Wire 2D uses a semi-automatic approach where the user selects a few boundary points to guide the segmentation process, while the output is displayed in real time. Our implementation extends the segmentation to 3D by applying the same process on each slice. To further reduce user involvement, object contours can be specified in planes perpendicular to the slices, allowing Live-Wire 3D to perform 2D segmentation automatically. Additionally, two improvements are proposed: path heating and a new graph edge feature function based on variance of path properties, leading to a 33% reduction in user interaction and improved delineation in the presence of strong interfering edges.",1
"Image segmentation is a critical step in computer vision tasks constituting an essential issue for pattern recognition and visual interpretation. In this paper, we propose a new stopping criterion for the mean shift iterative algorithm by using images defined in Zn ring, with the goal of reaching a better segmentation. We carried out also a study on the weak and strong of equivalence classes between two images. An analysis on the convergence with this new stopping criterion is carried out too.",0
"Computer vision tasks rely heavily on image segmentation, a crucial step that is essential for pattern recognition and visual interpretation. This study presents a novel approach to improving segmentation by introducing a new stopping criterion for the mean shift iterative algorithm. The proposed method utilizes images defined in the Zn ring. Additionally, we conducted a study on the equivalence classes of two images, comparing their strengths and weaknesses. Furthermore, we analyzed the convergence of the mean shift iterative algorithm with the newly proposed stopping criterion.",1
Piecewise constant image approximations of sequential number of segments or clusters of disconnected pixels are treated. The method of majorizing of optimal approximation sequence by hierarchical sequence of image approximations is proposed. A generalization for multidimensional case of color and multispectral images is foreseen.,0
"The focus is on dealing with approximations of images that have a sequence of disconnected pixels divided into segments or clusters. A proposed approach is to utilize a hierarchical sequence of image approximations to maximize the optimal approximation sequence. There is also a plan to extend this method to apply to multidimensional images, such as color and multispectral images.",1
"A novel locally statistical active contour model (ACM) for image segmentation in the presence of intensity inhomogeneity is presented in this paper. The inhomogeneous objects are modeled as Gaussian distributions of different means and variances, and a moving window is used to map the original image into another domain, where the intensity distributions of inhomogeneous objects are still Gaussian but are better separated. The means of the Gaussian distributions in the transformed domain can be adaptively estimated by multiplying a bias field with the original signal within the window. A statistical energy functional is then defined for each local region, which combines the bias field, the level set function, and the constant approximating the true signal of the corresponding object. Experiments on both synthetic and real images demonstrate the superiority of our proposed algorithm to state-of-the-art and representative methods.",0
"This paper introduces a new active contour model (ACM) for image segmentation that addresses the issue of intensity inhomogeneity. In this model, inhomogeneous objects are represented as Gaussian distributions with varying means and variances. To better separate these distributions, a moving window is used to transform the original image to another domain where the Gaussian distributions remain but are more distinct. The means of these distributions are estimated by multiplying a bias field with the original signal within the window. A statistical energy functional is then defined for each local region, combining the bias field, level set function, and an approximate constant for the true signal of the object. Experiments on synthetic and real images demonstrate the superiority of this approach over current methods.",1
"We present a novel segmentation algorithm based on a hierarchical representation of images. The main contribution of this work is to explore the capabilities of the A Contrario reasoning when applied to the segmentation problem, and to overcome the limitations of current algorithms within that framework. This exploratory approach has three main goals.   Our first goal is to extend the search space of greedy merging algorithms to the set of all partitions spanned by a certain hierarchy, and to cast the segmentation as a selection problem within this space. In this way we increase the number of tested partitions and thus we potentially improve the segmentation results. In addition, this space is considerably smaller than the space of all possible partitions, thus we still keep the complexity controlled.   Our second goal aims to improve the locality of region merging algorithms, which usually merge pairs of neighboring regions. In this work, we overcome this limitation by introducing a validation procedure for complete partitions, rather than for pairs of regions.   The third goal is to perform an exhaustive experimental evaluation methodology in order to provide reproducible results.   Finally, we embed the selection process on a statistical A Contrario framework which allows us to have only one free parameter related to the desired scale.",0
"A new segmentation algorithm based on hierarchical image representation is introduced in this work. The primary objective is to utilize A Contrario reasoning to enhance segmentation and overcome current algorithm limitations. This exploratory approach has three key goals. Firstly, it aims to expand the search space of greedy merging algorithms to encompass all partitions within a particular hierarchy and make segmentation a selection problem in this space. This would increase the number of tested partitions and potentially improve segmentation results while keeping complexity under control. Secondly, it intends to improve the locality of region merging algorithms that typically merge neighboring regions by introducing a validation procedure for complete partitions. Thirdly, it aims to conduct a thorough experimental evaluation to ensure reproducible results. Finally, the selection process is embedded in a statistical A Contrario framework that requires only one free parameter related to the desired scale.",1
"In this article a novel algorithm for color image segmentation has been developed. The proposed algorithm based on combining two existing methods in such a novel way to obtain a significant method to partition the color image into significant regions. On the first phase, the traditional Otsu method for gray channel image segmentation were applied for each of the R,G, and B channels separately to determine the suitable automatic threshold for each channel. After that, the new modified channels are integrated again to formulate a new color image. The resulted image suffers from some kind of distortion. To get rid of this distortion, the second phase is arise which is the median filter to smooth the image and increase the segmented regions. This process looks very significant by the ocular eye. Experimental results were presented on a variety of test images to support the proposed algorithm.",0
"In this article, a new approach to color image segmentation has been developed by combining two established methods in a novel way. The proposed algorithm effectively partitions the color image into distinct regions. The initial step involves applying the traditional Otsu method for gray channel image segmentation to each of the R, G, and B channels separately to determine an automatic threshold for each channel. The modified channels are then integrated to create a new color image, which may have some distortion. To address this distortion, the second phase utilizes a median filter to smooth the image and increase the segmented regions. This process is visually significant. Experimental results using a variety of test images have been presented to support the proposed algorithm.",1
"Many computer vision problems can be formulated as binary quadratic programs (BQPs). Two classic relaxation methods are widely used for solving BQPs, namely, spectral methods and semidefinite programming (SDP), each with their own advantages and disadvantages. Spectral relaxation is simple and easy to implement, but its bound is loose. Semidefinite relaxation has a tighter bound, but its computational complexity is high for large scale problems. We present a new SDP formulation for BQPs, with two desirable properties. First, it has a similar relaxation bound to conventional SDP formulations. Second, compared with conventional SDP methods, the new SDP formulation leads to a significantly more efficient and scalable dual optimization approach, which has the same degree of complexity as spectral methods. Extensive experiments on various applications including clustering, image segmentation, co-segmentation and registration demonstrate the usefulness of our SDP formulation for solving large-scale BQPs.",0
"Binary quadratic programs (BQPs) often arise in computer vision problems, and they can be solved using two well-known relaxation methods: spectral methods and semidefinite programming (SDP). However, these methods have their own strengths and weaknesses. Spectral relaxation is easy to implement but provides a loose bound, while SDP yields a tighter bound but is computationally complex for large-scale problems. To address these issues, we introduce a new SDP formulation for BQPs that has a similar relaxation bound to conventional SDP methods. Moreover, our formulation offers a more efficient and scalable dual optimization approach, which is as simple as spectral methods. We demonstrate the effectiveness of our SDP formulation through extensive experiments on various applications such as clustering, image segmentation, co-segmentation, and registration for solving large-scale BQPs.",1
"Segmentation of medical images is a challenging task owing to their complexity. A standard segmentation problem within Magnetic Resonance Imaging (MRI) is the task of labeling voxels according to their tissue type. Image segmentation provides volumetric quantification of liver area and thus helps in the diagnosis of disorders, such as Hepatitis, Cirrhosis, Jaundice, Hemochromatosis etc.This work deals with comparison of segmentation by applying Level Set Method,Fuzzy Level Information C-Means Clustering Algorithm and Gradient Vector Flow Snake Algorithm.The results are compared using the parameters such as Number of pixels correctly classified, and percentage of area segmented.",0
"Due to their complexity, medical image segmentation is a challenging task. An example of this is the standard segmentation problem found in Magnetic Resonance Imaging (MRI), which involves labeling voxels based on their tissue type. The segmentation of images provides a volumetric quantification of the liver area, aiding in the diagnosis of various disorders, including Hepatitis, Cirrhosis, Jaundice, and Hemochromatosis. This study compares the effectiveness of segmentation techniques, specifically the Level Set Method, Fuzzy Level Information C-Means Clustering Algorithm, and Gradient Vector Flow Snake Algorithm, using parameters such as the number of accurately classified pixels and the percentage of segmented area.",1
"This paper describes a novel method for partitioning image into meaningful segments. The proposed method employs watershed transform, a well-known image segmentation technique. Along with that, it uses various auxiliary schemes such as Binary Gradient Masking, dilation which segment the image in proper way. The algorithm proposed in this paper considers all these methods in effective way and takes little time. It is organized in such a manner so that it operates on input image adaptively. Its robustness and efficiency makes it more convenient and suitable for all types of images.",0
"In this paper, a new approach to segmenting images into significant parts is presented. The method utilizes the commonly used watershed transform for image segmentation, and incorporates additional techniques like Binary Gradient Masking and dilation to properly segment the image. This algorithm effectively combines all of these methods and is designed to operate on input images in an adaptive manner, while being both robust and efficient. As a result, this approach is suitable for all types of images and provides a more convenient solution.",1
"In this paper, we consider unsupervised partitioning problems, such as clustering, image segmentation, video segmentation and other change-point detection problems. We focus on partitioning problems based explicitly or implicitly on the minimization of Euclidean distortions, which include mean-based change-point detection, K-means, spectral clustering and normalized cuts. Our main goal is to learn a Mahalanobis metric for these unsupervised problems, leading to feature weighting and/or selection. This is done in a supervised way by assuming the availability of several potentially partially labelled datasets that share the same metric. We cast the metric learning problem as a large-margin structured prediction problem, with proper definition of regularizers and losses, leading to a convex optimization problem which can be solved efficiently with iterative techniques. We provide experiments where we show how learning the metric may significantly improve the partitioning performance in synthetic examples, bioinformatics, video segmentation and image segmentation problems.",0
"This paper examines unsupervised partitioning problems, including clustering, video segmentation, image segmentation, and change-point detection problems. Our focus is on partitioning problems that explicitly or implicitly minimize Euclidean distortions, such as K-means, spectral clustering, mean-based change-point detection, and normalized cuts. Our primary objective is to develop a Mahalanobis metric for these unsupervised problems, which involves feature weighting and/or selection. We approach this in a supervised manner by assuming the presence of several partially labeled datasets that share the same metric. We frame the metric learning problem as a large-margin structured prediction problem, with appropriate regularizers and losses, resulting in a convex optimization problem that is solvable using iterative techniques. We present experiments demonstrating how learning the metric can significantly improve partitioning performance in synthetic examples, bioinformatics, video segmentation, and image segmentation problems.",1
"Nuclear image has emerged as a promising research work in medical field. Images from different modality meet its own challenge. Positron Emission Tomography (PET) image may help to precisely localize disease to assist in planning the right treatment for each case and saving valuable time. In this paper, a novel approach of Spatial Fuzzy C Means (PET SFCM) clustering algorithm is introduced on PET scan image datasets. The proposed algorithm is incorporated the spatial neighborhood information with traditional FCM and updating the objective function of each cluster. This algorithm is implemented and tested on huge data collection of patients with brain neuro degenerative disorder such as Alzheimers disease. It has demonstrated its effectiveness by testing it for real world patient data sets. Experimental results are compared with conventional FCM and K Means clustering algorithm. The performance of the PET SFCM provides satisfactory results compared with other two algorithms",0
"Medical research has shown promising results with the use of nuclear imaging. However, each imaging modality presents its own challenges. Positron Emission Tomography (PET) imaging can assist in localizing diseases more accurately, which can save valuable time in determining the proper course of treatment. This study introduces a new approach to PET scan image datasets through a Spatial Fuzzy C Means (PET SFCM) clustering algorithm. The algorithm incorporates spatial neighborhood information with traditional FCM and updates the objective function of each cluster. The algorithm was tested on a large collection of patient data with brain neurodegenerative disorders, such as Alzheimer's disease, and proved to be effective in comparison to conventional FCM and K Means clustering algorithms. The satisfactory results of the PET SFCM algorithm demonstrate its potential usefulness in medical imaging research.",1
"Medical image segmentation is referred to the segmentation of known anatomic structures from different medical images. Normally, the medical data researches are more complicated and an exclusive structures. This computer aided diagnosis is used for assisting doctors in evaluating medical imagery or in recognizing abnormal findings in a medical image. To integrate the specialized knowledge for medical data processing is helpful to form a real useful healthcare decision making system. This paper studies the different symmetry based distances applied in clustering algorithms and analyzes symmetry approach for Positron Emission Tomography (PET) scan image segmentation. Unlike CT and MRI, the PET scan identifies the structure of blood flow to and from organs. PET scan also helps in early diagnosis of cancer and heart, brain and gastro intestinal ailments and to detect the progress of treatment. In this paper, the scope diagnostic task expands for PET image in various brain functions.",0
"The process of medical image segmentation involves identifying specific anatomical structures from various medical images. Medical data can be complex and unique, making computer aided diagnosis a valuable tool for assisting doctors in evaluating medical imagery and detecting abnormal findings. By integrating specialized knowledge into medical data processing, a healthcare decision making system can be developed. This paper examines the use of symmetry based distances in clustering algorithms and how a symmetry approach can be applied to segment Positron Emission Tomography (PET) scan images. PET scans differ from CT and MRI scans as they identify blood flow to and from organs, aiding in the early diagnosis of cancer and various ailments, as well as monitoring treatment progress. This paper focuses on expanding the diagnostic capabilities of PET images to various brain functions.",1
"Shape-based regularization has proven to be a useful method for delineating objects within noisy images where one has prior knowledge of the shape of the targeted object. When a collection of possible shapes is available, the specification of a shape prior using kernel density estimation is a natural technique. Unfortunately, energy functionals arising from kernel density estimation are of a form that makes them impossible to directly minimize using efficient optimization algorithms such as graph cuts. Our main contribution is to show how one may recast the energy functional into a form that is minimizable iteratively and efficiently using graph cuts.",0
"Using shape-based regularization is an effective technique for defining objects in noisy images, particularly when one has prior knowledge of the desired shape. If there is a range of potential shapes, kernel density estimation can be used to specify a shape prior. However, the energy functionals that result from this method cannot be directly minimized using efficient optimization algorithms like graph cuts. Our primary contribution is to demonstrate a method for reformulating the energy functional so that it can be iteratively and efficiently minimized using graph cuts.",1
"In photography, low depth of field (DOF) is an important technique to emphasize the object of interest (OOI) within an image. Thus, low DOF images are widely used in the application area of macro, portrait or sports photography. When viewing a low DOF image, the viewer implicitly concentrates on the regions that are sharper regions of the image and thus segments the image into regions of interest and non regions of interest which has a major impact on the perception of the image. Thus, a robust algorithm for the fully automatic detection of the OOI in low DOF images provides valuable information for subsequent image processing and image retrieval. In this paper we propose a robust and parameterless algorithm for the fully automatic segmentation of low DOF images. We compare our method with three similar methods and show the superior robustness even though our algorithm does not require any parameters to be set by hand. The experiments are conducted on a real world data set with high and low DOF images.",0
"Low depth of field (DOF) is a widely used photography technique to highlight the object of interest (OOI) in an image, particularly in macro, portrait, and sports photography. An image with low DOF allows the viewer to focus on the sharper regions, which can greatly affect how the image is perceived. Detecting the OOI in low DOF images is crucial for subsequent image processing and retrieval. In this study, we introduce a robust and parameterless algorithm for fully automatic segmentation of low DOF images. We compare our approach with three similar methods and demonstrate its superior performance on a real-world dataset containing high and low DOF images.",1
"We present a fast algorithm for training MaxPooling Convolutional Networks to segment images. This type of network yields record-breaking performance in a variety of tasks, but is normally trained on a computationally expensive patch-by-patch basis. Our new method processes each training image in a single pass, which is vastly more efficient.   We validate the approach in different scenarios and report a 1500-fold speed-up. In an application to automated steel defect detection and segmentation, we obtain excellent performance with short training times.",0
"Our paper introduces a speedy algorithm for training MaxPooling Convolutional Networks designed for image segmentation. Despite their exceptional performance in various tasks, these networks are typically trained on a computationally demanding patch-by-patch basis. Our innovative approach enables the processing of each training image in one go, resulting in significantly improved efficiency. We verified the effectiveness of our technique in diverse situations and achieved a 1500-fold acceleration. Our experiments in automated steel defect detection and segmentation demonstrated outstanding results and required minimal training time.",1
"""Background subtraction"" is an old technique for finding moving objects in a video sequence for example, cars driving on a freeway. The idea is that subtracting the current image from a timeaveraged background image will leave only nonstationary objects. It is, however, a crude approximation to the task of classifying each pixel of the current image; it fails with slow-moving objects and does not distinguish shadows from moving objects. The basic idea of this paper is that we can classify each pixel using a model of how that pixel looks when it is part of different classes. We learn a mixture-of-Gaussians classification model for each pixel using an unsupervised technique- an efficient, incremental version of EM. Unlike the standard image-averaging approach, this automatically updates the mixture component for each class according to likelihood of membership; hence slow-moving objects are handled perfectly. Our approach also identifies and eliminates shadows much more effectively than other techniques such as thresholding. Application of this method as part of the Roadwatch traffic surveillance project is expected to result in significant improvements in vehicle identification and tracking.",0
"The conventional method of ""background subtraction"" is a dated technique for detecting mobile objects in a video sequence, such as cars traveling on a freeway. The approach involves subtracting the current image from a background image averaged over time to isolate non-stationary objects. However, this technique is an oversimplification of the task of categorizing each pixel of the current image. It is ineffective when dealing with slow-moving objects and cannot differentiate between shadows and moving objects. The central concept of this research is to use a classification model for each pixel based on how it appears when part of different classes. We employ an unsupervised technique, an efficient, incremental version of EM, to learn a mixture-of-Gaussians classification model for each pixel. Unlike the typical image-averaging method, our approach automatically updates the mixture component for each class based on the likelihood of membership, which makes it ideal for handling slow-moving objects. Furthermore, our approach is more effective when it comes to identifying and eliminating shadows compared to other techniques such as thresholding. We anticipate that implementing this method as part of the Roadwatch traffic surveillance project will significantly enhance vehicle identification and tracking.",1
"Image thresholding has played an important role in image segmentation. This paper presents a hybrid approach for image segmentation based on the thresholding by fuzzy c-means (THFCM) algorithm for image segmentation. The goal of the proposed approach is to find a discerner cluster able to find an automatic threshold. The algorithm is formulated by applying the standard FCM clustering algorithm to the frequencies (y-values) on the smoothed histogram. Hence, the frequencies of an image can be used instead of the conventional whole data of image. The cluster that has the highest peak which represents the maximum frequency in the image histogram will play as an excellent role in determining a discerner cluster to the grey level image. Then, the pixels belong to the discerner cluster represent an object in the gray level histogram while the other clusters represent a background. Experimental results with standard test images have been obtained through the proposed approach (THFCM).",0
"The use of image thresholding has been significant in the process of image segmentation. This paper introduces a novel technique for image segmentation, which combines the thresholding approach with the fuzzy c-means (THFCM) algorithm. The proposed method aims to identify a distinct cluster that can automatically determine a threshold for image segmentation. To achieve this, the standard FCM clustering algorithm is applied to the smoothed histogram's frequencies. Instead of using the entire image data, the frequency distribution of the image is utilized. The cluster with the highest peak in the histogram is identified as the discerner cluster, which helps in determining the object in the gray level histogram. The remaining clusters represent the background. The effectiveness of the proposed approach (THFCM) has been validated through experiments conducted on standard test images.",1
"A framework of M-estimation based fuzzy C-means clustering (MFCM) algorithm is proposed with iterative reweighted least squares (IRLS) algorithm, and penalty constraint and kernelization extensions of MFCM algorithms are also developed. Introducing penalty information to the object functions of MFCM algorithms, the spatially constrained fuzzy C-means (SFCM) is extended to penalty constraints MFCM algorithms(abbr. pMFCM).Substituting the Euclidean distance with kernel method, the MFCM and pMFCM algorithms are extended to kernelized MFCM (abbr. KMFCM) and kernelized pMFCM (abbr.pKMFCM) algorithms. The performances of MFCM, pMFCM, KMFCM and pKMFCM algorithms are evaluated in three tasks: pattern recognition on 10 standard data sets from UCI Machine Learning databases, noise image segmentation performances on a synthetic image, a magnetic resonance brain image (MRI), and image segmentation of a standard images from Berkeley Segmentation Dataset and Benchmark. The experimental results demonstrate the effectiveness of our proposed algorithms in pattern recognition and image segmentation.",0
"The authors propose a framework for M-estimation based fuzzy C-means clustering (MFCM) algorithm, which incorporates iterative reweighted least squares (IRLS) algorithm. Additionally, extensions to the MFCM algorithm are developed through penalty constraint and kernelization. By integrating penalty information into the object functions of MFCM algorithms, the spatially constrained fuzzy C-means (SFCM) algorithm is extended to penalty constraints MFCM algorithms (pMFCM). Furthermore, the MFCM and pMFCM algorithms are modified to incorporate kernel methods, and are referred to as kernelized MFCM (KMFCM) and kernelized pMFCM (pKMFCM) algorithms. The effectiveness of these algorithms is evaluated through three tasks: pattern recognition on 10 standard data sets from UCI Machine Learning databases, noise image segmentation performances on a synthetic image, a magnetic resonance brain image (MRI), and image segmentation of a standard images from Berkeley Segmentation Dataset and Benchmark. The experimental results demonstrate that the proposed algorithms are effective in pattern recognition and image segmentation.",1
"In this project, we first study the Gaussian-based hidden Markov random field (HMRF) model and its expectation-maximization (EM) algorithm. Then we generalize it to Gaussian mixture model-based hidden Markov random field. The algorithm is implemented in MATLAB. We also apply this algorithm to color image segmentation problems and 3D volume segmentation problems.",0
"The Gaussian-based hidden Markov random field (HMRF) model and its expectation-maximization (EM) algorithm are initially studied in this project. Subsequently, we extend it to the Gaussian mixture model-based hidden Markov random field and implement the algorithm using MATLAB. Furthermore, we utilize this algorithm to solve color image segmentation issues and 3D volume segmentation challenges.",1
"In this project, we study the hidden Markov random field (HMRF) model and its expectation-maximization (EM) algorithm. We implement a MATLAB toolbox named HMRF-EM-image for 2D image segmentation using the HMRF-EM framework. This toolbox also implements edge-prior-preserving image segmentation, and can be easily reconfigured for other problems, such as 3D image segmentation.",0
"Our research focuses on investigating the HMRF model and its EM algorithm. To achieve this, we have created the HMRF-EM-image toolbox, which employs the HMRF-EM framework for 2D image segmentation. This toolbox also supports edge-prior-preserving image segmentation and can be conveniently adapted for other applications, such as 3D image segmentation.",1
Detection and segmentation of Brain tumor is very important because it provides anatomical information of normal and abnormal tissues which helps in treatment planning and patient follow-up. There are number of techniques for image segmentation. Proposed research work uses ANFIS (Artificial Neural Network Fuzzy Inference System) for image classification and then compares the results with FCM (Fuzzy C means) and K-NN (K-nearest neighbor). ANFIS includes benefits of both ANN and the fuzzy logic systems. A comprehensive feature set and fuzzy rules are selected to classify an abnormal image to the corresponding tumor type. Experimental results illustrate promising results in terms of classification accuracy. A comparative analysis is performed with the FCM and K-NN to show the superior nature of ANFIS systems.,0
"The detection and segmentation of brain tumors holds great significance in providing information about normal and abnormal tissues, aiding in treatment planning and patient follow-up. Various image segmentation techniques exist, with this research utilizing ANFIS (Artificial Neural Network Fuzzy Inference System) for image classification, and comparing results with FCM (Fuzzy C means) and K-NN (K-nearest neighbor). ANFIS combines the benefits of both ANN and fuzzy logic systems, selecting a comprehensive feature set and fuzzy rules to classify abnormal images to the corresponding tumor type. Experimental results demonstrate promising classification accuracy, with a comparative analysis conducted to show the superior nature of ANFIS systems over FCM and K-NN.",1
"The incidence of thyroid nodule is very high and generally increases with the age. Thyroid nodule may presage the emergence of thyroid cancer. The thyroid nodule can be completely cured if detected early. Fine needle aspiration cytology is a recognized early diagnosis method of thyroid nodule. There are still some limitations in the fine needle aspiration cytology, and the ultrasound diagnosis of thyroid nodule has become the first choice for auxiliary examination of thyroid nodular disease. If we could combine medical imaging technology and fine needle aspiration cytology, the diagnostic rate of thyroid nodule would be improved significantly. The properties of ultrasound will degrade the image quality, which makes it difficult to recognize the edges for physicians. Image segmentation technique based on graph theory has become a research hotspot at present. Normalized cut (Ncut) is a representative one, which is suitable for segmentation of feature parts of medical image. However, how to solve the normalized cut has become a problem, which needs large memory capacity and heavy calculation of weight matrix. It always generates over segmentation or less segmentation which leads to inaccurate in the segmentation. The speckle noise in B ultrasound image of thyroid tumor makes the quality of the image deteriorate. In the light of this characteristic, we combine the anisotropic diffusion model with the normalized cut in this paper. After the enhancement of anisotropic diffusion model, it removes the noise in the B ultrasound image while preserves the important edges and local details. This reduces the amount of computation in constructing the weight matrix of the improved normalized cut and improves the accuracy of the final segmentation results. The feasibility of the method is proved by the experimental results.",0
"The prevalence of thyroid nodules is high and typically increases with age, and they may indicate the development of thyroid cancer. Early detection of thyroid nodules is key to a complete cure, with fine needle aspiration cytology being a recognized diagnostic method. However, this technique has some limitations, and ultrasound has become the preferred auxiliary examination for thyroid nodular disease. Combining medical imaging technology with fine needle aspiration cytology could significantly improve the diagnostic rate of thyroid nodules. Graph theory-based image segmentation, such as normalized cut, is a current research hotspot, but it requires a large memory capacity and heavy calculation of weight matrices, resulting in inaccurate segmentation. To address this, we propose combining the anisotropic diffusion model with normalized cut to reduce speckle noise in B ultrasound images while preserving important edges and local details. This improves the accuracy of segmentation results and reduces computational requirements. Experimental results demonstrate the feasibility of this method.",1
"We report a method for super-resolution of range images. Our approach leverages the interpretation of LR image as sparse samples on the HR grid. Based on this interpretation, we demonstrate that our recently reported approach, which reconstructs dense range images from sparse range data by exploiting a registered colour image, can be applied for the task of resolution enhancement of range images. Our method only uses a single colour image in addition to the range observation in the super-resolution process. Using the proposed approach, we demonstrate super-resolution results for large factors (e.g. 4) with good localization accuracy.",0
"Our study introduces a technique for improving the resolution of range images. By viewing the low-resolution image as a meager sampling on the high-resolution grid, we show that our previously described method for reconstructing dense range images from sparse range data, which involves using a matched color image, can also be employed for enhancing the resolution of range images. Our method employs only one color image and the range observation during the super-resolution process. We conduct experiments using our approach and achieve high localization accuracy with significant enhancement of resolution, even at large magnification ratios such as 4x.",1
"Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.",0
"Many advanced methods for multi-class image segmentation and labeling utilize conditional random fields which are either defined over pixels or image regions. While region-level models have dense pairwise connectivity, pixel-level models are much larger and only allow for sparse graph structures. This study focuses on fully connected CRF models which are defined on all pixels in an image. However, these graphs have billions of edges, making traditional inference algorithms infeasible. The primary contribution of this study is an efficient approximate inference algorithm for fully connected CRF models using Gaussian kernel-defined pairwise edge potentials. The experiments show that dense connectivity at the pixel level leads to significant improvements in segmentation and labeling accuracy.",1
"Image colorization adds color to grayscale images. It not only increases the visual appeal of grayscale images, but also enriches the information contained in scientific images that lack color information. Most existing methods of colorization require laborious user interaction for scribbles or image segmentation. To eliminate the need for human labor, we develop an automatic image colorization method using epitome. Built upon a generative graphical model, epitome is a condensed image appearance and shape model which also proves to be an effective summary of color information for the colorization task. We train the epitome from the reference images and perform inference in the epitome to colorize grayscale images, rendering better colorization results than previous method in our experiments.",0
"The process of image colorization involves adding color to grayscale images, resulting in enhanced visual appeal and increased information in scientific images that lack color information. However, most current colorization methods require significant user input, such as scribbles or image segmentation. To address this issue and eliminate the need for human labor, we have developed an automated image colorization technique using epitome. Epitome is a condensed image appearance and shape model that serves as an effective summary of color information for the colorization process, built upon a generative graphical model. By training the epitome with reference images and performing inference within it, we have achieved superior colorization results compared to previous methods in our experiments.",1
"Extracting text objects from the PDF images is a challenging problem. The text data present in the PDF images contain certain useful information for automatic annotation, indexing etc. However variations of the text due to differences in text style, font, size, orientation, alignment as well as complex structure make the problem of automatic text extraction extremely difficult and challenging job. This paper presents two techniques under block-based classification. After a brief introduction of the classification methods, two methods were enhanced and results were evaluated. The performance metrics for segmentation and time consumption are tested for both the models.",0
"The extraction of text objects from PDF images poses a difficult problem, as the text data provides valuable information for automatic annotation and indexing. However, the complexity of the text's style, font, size, orientation, alignment, and structure creates obstacles for automatic text extraction. In this paper, two techniques are presented that utilize block-based classification to address these challenges. The classification methods are introduced, and their effectiveness is evaluated through enhancements and performance metric testing for both segmentation and time consumption.",1
"In the paper the optimal image segmentation by means of piecewise constant approximations is considered. The optimality is defined by a minimum value of the total squared error or by equivalent value of standard deviation of the approximation from the image. The optimal approximations are defined independently on the method of their obtaining and might be generated in different algorithms. We investigate the computation of the optimal approximation on the grounds of stability with respect to a given set of modifications. To obtain the optimal approximation the Mumford-Shuh model is generalized and developed, which in the computational part is combined with the Otsu method in multi-thresholding version. The proposed solution is proved analytically and experimentally on the example of the standard image.",0
"The article discusses achieving the best possible image segmentation using piecewise constant approximations. The measure of optimality is determined by minimizing the total squared error or an equivalent value of the standard deviation of the approximation from the original image. The optimal approximations are not dependent on the method used to generate them, and various algorithms can be used. The study focuses on the stability of the optimal approximation when subjected to modifications. To achieve this, the Mumford-Shuh model is generalized and combined with the Otsu method in the multi-thresholding version for computation. The proposed solution is validated through analytical and experimental methods using a standard image.",1
We describe a new optimization scheme for finding high-quality correlation clusterings in planar graphs that uses weighted perfect matching as a subroutine. Our method provides lower-bounds on the energy of the optimal correlation clustering that are typically fast to compute and tight in practice. We demonstrate our algorithm on the problem of image segmentation where this approach outperforms existing global optimization techniques in minimizing the objective and is competitive with the state of the art in producing high-quality segmentations.,0
"A novel optimization technique is presented in this study for obtaining excellent correlation clusterings in planar graphs. This approach employs weighted perfect matching as a subroutine and produces lower energy bounds for the optimal correlation clustering, which are usually quick to compute and precise in practice. The efficacy of our algorithm is demonstrated in the context of image segmentation, where it surpasses existing global optimization methods in minimizing the objective function and is comparable to the latest techniques in generating superior quality segmentations.",1
"In this paper, we carry out a comparative study of the efficacy of wavelets belonging to Daubechies and Coiflet family in achieving image segmentation through a fast statistical algorithm.The fact that wavelets belonging to Daubechies family optimally capture the polynomial trends and those of Coiflet family satisfy mini-max condition, makes this comparison interesting. In the context of the present algorithm, it is found that the performance of Coiflet wavelets is better, as compared to Daubechies wavelet.",0
This paper conducts a comparative analysis of the effectiveness of Daubechies and Coiflet wavelets in fast statistical algorithm-based image segmentation. The comparison is intriguing due to Daubechies wavelets' optimal capture of polynomial trends and Coiflet wavelets' satisfaction of the mini-max condition. Results show that the Coiflet wavelets outperform the Daubechies wavelets in the studied algorithm.,1
"We present an approach for polarimetric Synthetic Aperture Radar (SAR) image region boundary detection based on the use of B-Spline active contours and a new model for polarimetric SAR data: the GHP distribution. In order to detect the boundary of a region, initial B-Spline curves are specified, either automatically or manually, and the proposed algorithm uses a deformable contours technique to find the boundary. In doing this, the parameters of the polarimetric GHP model for the data are estimated, in order to find the transition points between the region being segmented and the surrounding area. This is a local algorithm since it works only on the region to be segmented. Results of its performance are presented.",0
"Our approach for detecting boundaries in polarimetric Synthetic Aperture Radar (SAR) images involves utilizing B-Spline active contours and a novel model for polarimetric SAR data called the GHP distribution. The detection process begins by specifying initial B-Spline curves, which can be done automatically or manually, and then using a deformable contours technique to locate the boundary of the region. During this process, we estimate the parameters of the polarimetric GHP model for the data, which helps identify the transition points between the segmented region and its surroundings. This is a localized algorithm that operates solely on the targeted region. We present the results of our approach to demonstrate its effectiveness.",1
"In spectral clustering and spectral image segmentation, the data is partioned starting from a given matrix of pairwise similarities S. the matrix S is constructed by hand, or learned on a separate training set. In this paper we show how to achieve spectral clustering in unsupervised mode. Our algorithm starts with a set of observed pairwise features, which are possible components of an unknown, parametric similarity function. This function is learned iteratively, at the same time as the clustering of the data. The algorithm shows promosing results on synthetic and real data.",0
"The process of dividing data in spectral clustering and spectral image segmentation depends on a pre-existing matrix of pairwise similarities S, which can either be manually created or acquired from a different set of training data. Our study presents a novel approach to accomplishing unsupervised spectral clustering. We initiate the process with a collection of observed pairwise features that could potentially serve as the constituents of an unrecognizable, parametric similarity function. As the data clustering and function learning take place concurrently, our algorithm delivers encouraging outcomes when tested on both theoretical and practical data.",1
